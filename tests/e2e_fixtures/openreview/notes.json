{
  "notes": [
    {
      "id": "paper_abc123",
      "number": 1001,
      "cdate": 1705334400000,
      "mdate": 1705334400000,
      "tcdate": 1705334400000,
      "tmdate": 1705334400000,
      "forum": "paper_abc123",
      "replyto": null,
      "invitation": "ICLR.cc/2024/Conference/-/Submission",
      "signatures": ["~Author_One1", "~Author_Two1"],
      "readers": ["everyone"],
      "nonreaders": [],
      "writers": ["ICLR.cc/2024/Conference", "~Author_One1", "~Author_Two1"],
      "content": {
        "title": {
          "value": "Scaling Language Models with Mixture of Experts"
        },
        "abstract": {
          "value": "We present a new approach to scaling language models using a mixture of experts architecture. Our method achieves better performance with fewer compute resources compared to dense models of equivalent capacity."
        },
        "keywords": {
          "value": ["mixture of experts", "language models", "scaling", "efficiency"]
        },
        "primary_area": {
          "value": "deep learning"
        },
        "venue": {
          "value": "ICLR 2024"
        },
        "venueid": {
          "value": "ICLR.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{author2024scaling,\n  title={Scaling Language Models with Mixture of Experts},\n  author={One, Author and Two, Author},\n  booktitle={ICLR},\n  year={2024}\n}"
        }
      }
    },
    {
      "id": "paper_def456",
      "number": 1002,
      "cdate": 1705248000000,
      "mdate": 1705248000000,
      "tcdate": 1705248000000,
      "tmdate": 1705248000000,
      "forum": "paper_def456",
      "replyto": null,
      "invitation": "ICLR.cc/2024/Conference/-/Submission",
      "signatures": ["~Researcher_A1"],
      "readers": ["everyone"],
      "nonreaders": [],
      "writers": ["ICLR.cc/2024/Conference", "~Researcher_A1"],
      "content": {
        "title": {
          "value": "Efficient Attention Mechanisms for Long Sequences"
        },
        "abstract": {
          "value": "We propose a novel attention mechanism that scales linearly with sequence length, enabling efficient processing of very long sequences in transformer models."
        },
        "keywords": {
          "value": ["attention", "transformers", "long sequences", "efficiency"]
        },
        "primary_area": {
          "value": "deep learning"
        },
        "venue": {
          "value": "ICLR 2024"
        },
        "venueid": {
          "value": "ICLR.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{researcher2024efficient,\n  title={Efficient Attention Mechanisms for Long Sequences},\n  author={A, Researcher},\n  booktitle={ICLR},\n  year={2024}\n}"
        }
      }
    },
    {
      "id": "paper_ghi789",
      "number": 1003,
      "cdate": 1705161600000,
      "mdate": 1705161600000,
      "tcdate": 1705161600000,
      "tmdate": 1705161600000,
      "forum": "paper_ghi789",
      "replyto": null,
      "invitation": "ICLR.cc/2024/Conference/-/Submission",
      "signatures": ["~Dr_Smith1", "~Prof_Jones1"],
      "readers": ["everyone"],
      "nonreaders": [],
      "writers": ["ICLR.cc/2024/Conference", "~Dr_Smith1", "~Prof_Jones1"],
      "content": {
        "title": {
          "value": "Self-Supervised Learning for Multi-Modal Representations"
        },
        "abstract": {
          "value": "We introduce a self-supervised learning framework for learning joint representations across multiple modalities including text, images, and audio."
        },
        "keywords": {
          "value": ["self-supervised learning", "multi-modal", "representation learning"]
        },
        "primary_area": {
          "value": "representation learning"
        },
        "venue": {
          "value": "ICLR 2024"
        },
        "venueid": {
          "value": "ICLR.cc/2024/Conference"
        },
        "_bibtex": {
          "value": "@inproceedings{smith2024self,\n  title={Self-Supervised Learning for Multi-Modal Representations},\n  author={Smith, Dr and Jones, Prof},\n  booktitle={ICLR},\n  year={2024}\n}"
        }
      }
    }
  ],
  "count": 3
}
