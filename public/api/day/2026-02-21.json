{
  "archive_dates": [
    "2026-02-21",
    "2026-02-20",
    "2026-02-03"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-21T12:08:41.689450+00:00",
  "model_releases_by_entity": {
    "huggingface": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "huggingface"
        ],
        "first_seen_at": "2026-02-21T11:48:47.744916+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 182,
          "likes": 6,
          "pipeline_tag": "time-series-forecasting"
        },
        "hf_model_id": "google/timesfm-2.5-200m-transformers",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-google",
            "tier": 1,
            "title": "google/timesfm-2.5-200m-transformers",
            "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-google",
          "tier": 1,
          "title": "google/timesfm-2.5-200m-transformers",
          "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
        },
        "published_at": "2026-02-20T15:55:54+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9197775362726551,
          "semantic_score": 3.5083012521266936,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.22807878839935
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:google/timesfm-2.5-200m-transformers",
        "summary": "TimesFM (Time Series Foundation Model) is a pretrained decoder-only model for time-series forecasting. This repository contains the **Transformers** port of the official TimesFM 2.5 PyTorch release. **Resources and Technical Documentation**: * Original model: google/timesfm-2.5-200m-pytorch * Transformers model: google/timesfm-2.5-200m-transformers * Paper: A decoder-only foundation model for time-series forecasting * Transformers docs: TimesFM 2.5 This model is converted from the official TimesFM 2.5 PyTorch checkpoint and integrated into `transformers` as `Timesfm2P5ModelForPrediction`. The converted checkpoint preserves the original architecture and forecasting behavior, including: * patch-based inputs for time-series contexts * decoder-only self-attention stack",
        "summary_zh": "TimesFM (時間序列基礎模型) 是一個用於時間序列預測的預訓練 decoder-only 模型。此儲存庫包含官方 TimesFM 2.5 PyTorch 版本的 Transformers 移植版。\n\n**資源與技術文件**：\n* 原始模型：google/timesfm-2.5-200m-pytorch\n* Transformers 模型：google/timesfm-2.5-200m-transformers\n* 論文：A decoder-only foundation model for time-series forecasting\n* Transformers 文件：TimesFM 2.5\n\n此模型從官方 TimesFM 2.5 PyTorch 檢查點轉換而來，並作為 `Timesfm2P5ModelForPrediction` 整合到 `transformers` 中。轉換後的檢查點保留了原始架構和預測行為，包括：\n* 用於時間序列上下文的基於 patch 的輸入\n* 僅解碼器自注意力堆疊",
        "title": "google/timesfm-2.5-200m-transformers",
        "title_zh": "google/timesfm-2.5-200m-transformers"
      }
    ],
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-21T11:48:48.712733+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 101316,
          "likes": 596,
          "pipeline_tag": "automatic-speech-recognition"
        },
        "hf_model_id": "mistralai/voxtral-mini-4b-realtime-2602",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
            "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
          "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
        },
        "published_at": "2026-02-19T00:28:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.7803403822802449,
          "semantic_score": 3.0389719903469086,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 11.619312372627153
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/voxtral-mini-4b-realtime-2602",
        "summary": "Voxtral Mini 4B Realtime 2602 is a **multilingual, realtime speech-transcription model** and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of **= 3600 / 0.8 = 45000`. In theory, you should be able to record with no limit; in practice, pre-allocations of RoPE parameters among other things limits `--max-model-len`. For the best user experience, we recommend to simply instantiate vLLM with the default parameters which will automatically set a maximum model length of 131072 (~ca. 3h). - We strongly recommend using websockets to set up audio streaming sessions. For more info on how to do so, check Usage. - We recommend using a delay of 480ms as we found it to be the sweet spot of performance and low latency. If, however, you want to adapt...",
        "summary_zh": "Voxtral Mini 4B Realtime 2602 是一個多語言、即時語音轉錄模型，也是首批開源解決方案之一，其準確性可與延遲為 `< 500ms` 的離線系統媲美。它支援的最大 RoPE 長度為 131072，理論上的最大 RoPE 為 `= 3600 / 0.8 = 45000`。理論上，您應該能夠無限制地錄音；實際上，RoPE 參數的預分配等因素限制了 `--max-model-len`。為了獲得最佳用戶體驗，我們建議只需使用預設參數實例化 vLLM，它將自動設定最大模型長度為 131072 (約 3 小時)。\n- 我們強烈建議使用 websockets 來設定音訊串流會話。有關如何操作的更多資訊，請查看 Usage。\n- 我們建議使用 480ms 的延遲，因為我們發現它是性能和低延遲的最佳平衡點。但是，如果您想調整...",
        "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
        "title_zh": "mistralai/Voxtral-Mini-4B-Realtime-2602"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-21T11:48:48.243806+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 16,
          "pipeline_tag": "image-feature-extraction"
        },
        "hf_model_id": "microsoft/latent-zoning-networks",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/latent-zoning-networks",
            "url": "https://huggingface.co/microsoft/latent-zoning-networks"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/latent-zoning-networks",
          "url": "https://huggingface.co/microsoft/latent-zoning-networks"
        },
        "published_at": "2026-02-21T00:14:28+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9521804047142403,
          "semantic_score": 1.8290148854255674,
          "tier_score": 2.0,
          "topic_score": 1.2000000000000002,
          "total_score": 7.781195290139808
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/latent-zoning-networks",
        "summary": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label...",
        "summary_zh": "生成模型 (Generative modeling)、表徵學習 (representation learning) 和分類 (classification) 是機器學習 (ML) 中的三個核心問題，然而它們最先進 (SoTA) 的解決方案在很大程度上仍然是獨立的。在本文中，我們提出問題：一個統一的原則能否解決這三個問題？這種統一可以簡化 ML 管線並促進跨任務的更大協同作用。我們引入 Latent Zoning Network (LZN) 作為實現此目標的一個步驟。LZN 的核心是創建一個共享的高斯潛在空間 (Gaussian latent space)，用於編碼所有任務的資訊。每種資料類型（例如，圖像、文本、標籤）都配備一個編碼器 (encoder)，將樣本映射到不相交的潛在區域 (disjoint latent zones)，以及一個解碼器 (decoder)，將潛在變量映射回數據。ML 任務表示為這些編碼器和解碼器的組合：例如，條件標籤圖像生成 (label-conditional image generation) 使用一個標籤...",
        "title": "microsoft/latent-zoning-networks",
        "title_zh": "microsoft/latent-zoning-networks"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-21T11:48:49.657975+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 133264,
          "likes": 799,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        },
        "published_at": "2026-02-20T05:27:33+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8805057125268763,
          "semantic_score": 3.9173970222473145,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.597902734774191
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
        "summary_zh": "本儲存庫包含用於 Hugging Face Transformers 格式的後訓練模型之模型權重和配置檔案。這些產物與 Hugging Face Transformers、vLLM、SGLang、KTransformers 等相容。對於尋求無需基礎設施維護的受管、可擴展推論的用戶，官方 Qwen API 服務由 Alibaba Cloud Model Studio 提供。特別是，**Qwen3.5-Plus** 是與 Qwen3.5-397B-A17B 相對應的託管版本，具有更多的生產功能，例如預設 1M context length、官方內建工具和自適應工具使用。欲了解更多資訊，請參閱 User Guide。近幾個月來，我們加強了對開發能夠提供卓越實用性和性能的 foundation models 的關注。Qwen3.5...",
        "title": "Qwen/Qwen3.5-397B-A17B",
        "title_zh": "Qwen/Qwen3.5-397B-A17B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-21T11:48:49.658115+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 34813,
          "likes": 46,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        },
        "published_at": "2026-02-18T16:13:24+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.7539658763708208,
          "semantic_score": 3.957234025001526,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.511199901372347
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a...",
        "summary_zh": "本儲存庫包含用於 Hugging Face Transformers 格式的後訓練模型之模型權重和配置檔案。這些產物與 Hugging Face Transformers、vLLM、SGLang 等相容。對於尋求無需基礎設施維護的受管、可擴展推論的用戶，官方 Qwen API 服務由 Alibaba Cloud Model Studio 提供。特別是，**Qwen3.5-Plus** 是與 Qwen3.5-397B-A17B 相對應的託管版本，具有更多的生產功能，例如預設 1M context length、官方內建工具和自適應工具使用。欲了解更多資訊，請參閱 User Guide。近幾個月來，我們加強了對開發能夠提供卓越實用性和性能的 foundation models 的關注。Qwen3.5 代表著一個...",
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "title_zh": "Qwen/Qwen3.5-397B-A17B-FP8"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.16835",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.032394+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NeST: Neuron Selective Tuning for LLM Safety",
          "url": "https://arxiv.org/abs/2602.16835"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NeST: Neuron Selective Tuning for LLM Safety",
        "url": "https://arxiv.org/abs/2602.16835"
      },
      "published_at": "2026-02-18T20:01:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7659782868148933,
        "semantic_score": 4.057654619216919,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.82363290603181
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16835",
      "summary": "Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.\n  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.",
      "summary_zh": "安全對齊 (Safety alignment) 對於大型語言模型 (LLMs) 的負責任部署至關重要。然而，現有方法通常依賴於繁重的 fine-tuning，這在跨模型家族的更新、審計和維護方面成本高昂。Full fine-tuning 會產生大量的計算和儲存開銷，而像 LoRA 這樣的參數高效方法則以效率換取不一致的安全效益和對設計選擇的敏感性。諸如 circuit breakers 之類的安全干預機制，可以在不修改模型權重的情況下減少不安全的輸出，但它們不會直接塑造或保留控制安全行為的內部表徵。這些限制阻礙了快速可靠的安全更新，尤其是在模型頻繁演進或必須適應新政策和領域的環境中。\n我們提出了 NeST，一個輕量級、結構感知 (structure-aware) 的安全對齊框架，它透過選擇性地調整一小部分與安全相關的神經元 (safety-relevant neurons)，同時凍結模型的其餘部分，來強化拒絕行為。NeST 透過聚類功能一致的安全神經元並在每個集群內強制執行共享更新，將參數更新與安全行為的內部組織對齊，從而實現了有針對性且穩定的安全適應，而無需廣泛的模型修改或 inference-time 開銷。我們在涵蓋多個模型家族和大小的 10 個開源 LLMs 上，將 NeST 與三個主要基準進行了比較：full fine-tuning、基於 LoRA 的 fine-tuning 和 circuit breakers。在所有評估的模型中，NeST 將攻擊成功率從平均 44.5% 降低到 4.36%，相當於不安全生成減少了 90.2%，同時平均只需 0.44 百萬個可訓練參數 (trainable parameters)。這相當於與 full fine-tuning 相比，更新參數減少了 17,310 倍，與 LoRA 相比減少了 9.25 倍，同時持續實現了更強的對齊安全性能。",
      "title": "NeST: Neuron Selective Tuning for LLM Safety",
      "title_zh": "NeST: 針對 LLM 安全的神經元選擇性調優"
    },
    {
      "arxiv_id": "2602.16928",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031696+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
          "url": "https://arxiv.org/abs/2602.16928"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
        "url": "https://arxiv.org/abs/2602.16928"
      },
      "published_at": "2026-02-18T22:41:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7745357184331012,
        "semantic_score": 3.8141231536865234,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.58865887211962
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16928",
      "summary": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "summary_zh": "在不完美資訊博弈 (imperfect-information games) 中，多智能體強化學習 (MARL) 的大部分進展歷來依賴於對基準線 (baselines) 的手動迭代改進。雖然 Counterfactual Regret Minimization (CFR) 和 Policy Space Response Oracles (PSRO) 等基礎家族建立在堅實的理論基礎之上，但其最有效變體的設計往往依賴於人類直覺來探索廣闊的演算法設計空間。在這項工作中，我們提出使用 AlphaEvolve，一個由大型語言模型驅動的進化編碼代理 (evolutionary coding agent)，以自動發現新的多智能體學習演算法。我們透過為兩種不同的賽局理論學習範式演化出新穎的變體，證明了該框架的通用性。首先，在迭代遺憾最小化 (iterative regret minimization) 領域，我們演化出控制遺憾累積和策略推導的邏輯，發現了一種新演算法：Volatility-Adaptive Discounted (VAD-)CFR。VAD-CFR 採用新穎、非直觀的機制——包括波動敏感折扣 (volatility-sensitive discounting)、一致性強制樂觀 (consistency-enforced optimism) 和硬式暖啟動策略累積排程 (hard warm-start policy accumulation schedule)——以超越 Discounted Predictive CFR+ 等最先進的基準線。其次，在基於種群的訓練演算法 (population based training algorithms) 體系中，我們為 PSRO 演化出訓練時和評估時的 meta strategy solvers，發現了一種新變體：Smoothed Hybrid Optimistic Regret (SHOR-)PSRO。SHOR-PSRO 引入了一種混合 meta-solver，它將 Optimistic Regret Matching 與經過平滑、溫度控制的最佳純策略分佈 (smoothed, temperature-controlled distribution over best pure strategies) 線性混合。透過在訓練期間動態退火 (annealing) 這種混合因子和多樣性獎勵 (diversity bonuses)，該演算法自動化了從種群多樣性到嚴格均衡尋找的過渡，相較於標準的靜態 meta-solvers，實現了卓越的經驗收斂性。",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "title_zh": "運用大型語言模型探索多智能體學習演算法"
    },
    {
      "arxiv_id": "2602.16704",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128761+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Reinforced Fast Weights with Next-Sequence Prediction",
          "url": "https://arxiv.org/abs/2602.16704"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Reinforced Fast Weights with Next-Sequence Prediction",
        "url": "https://arxiv.org/abs/2602.16704"
      },
      "published_at": "2026-02-18T18:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7623846945984442,
        "semantic_score": 3.9596444725990296,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.52202916719747
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16704",
      "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "summary_zh": "快速權重架構 (Fast weight architectures) 為長上下文建模 (long-context modeling) 提供了有潛力的替代方案，相較於基於 attention 的 Transformers，它能夠在不考慮上下文長度的情況下保持恆定的記憶體開銷。然而，其潛力受到 next-token prediction (NTP) 訓練範式的限制。NTP 優化單一 token 的預測，而忽略了前綴之後多個 token 之間的語義連貫性。因此，動態更新其參數以儲存上下文資訊的快速權重模型，學習到的是次優的表徵，無法捕捉長距離依賴關係。我們引入了 REFINE (Reinforced Fast weIghts with Next sEquence prediction)，一個強化學習框架，用於在 next-sequence prediction (NSP) 目標下訓練快速權重模型。REFINE 根據預測熵 (prediction entropy) 選擇資訊豐富的 token 位置，生成 multi-token rollouts，分配自監督序列級別獎勵 (self-supervised sequence-level rewards)，並使用 group relative policy optimization (GRPO) 優化模型。REFINE 適用於預訓練語言模型的整個訓練生命週期：訓練中期 (mid-training)、後訓練 (post-training) 和測試時訓練 (test-time training)。我們在 LaCT-760M 和 DeltaNet-1.3B 上的實驗表明，REFINE 在 needle-in-a-haystack retrieval、長上下文問答 (long-context question answering) 以及 LongBench 中的各種任務上，始終優於採用 NTP 的監督式 fine-tuning。REFINE 為改進快速權重架構中的長上下文建模提供了一個有效且多功能的框架。",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "title_zh": "結合次序列預測的強化快速權重"
    },
    {
      "arxiv_id": "2602.17127",
      "authors": [
        "Dusan Bosnjakovic"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.971829+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
          "url": "https://arxiv.org/abs/2602.17127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
        "url": "https://arxiv.org/abs/2602.17127"
      },
      "published_at": "2026-02-19T06:56:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8016242118316717,
        "semantic_score": 4.141481012105942,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.50310522393761
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17127",
      "summary": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individu",
      "summary_zh": "隨著大型語言模型 (LLMs) 從獨立的聊天介面過渡到多智能體系統和遞迴評估循環 (LLM-as-a-judge) 中的基礎推理層，檢測持久的、提供者級別的行為簽名 (provider-level behavioral signatures) 成為安全和治理的關鍵要求。傳統的基準測試衡量的是瞬態的任務準確性，但未能捕捉到穩定、潛在的回應策略 (latent response policies)——即在訓練和對齊期間嵌入的「普遍心態 (prevailing mindsets)」，這些心態超越了個別的...",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
      "title_zh": "實驗室驅動對齊簽名的出現：用於審計生成式 AI 中潛在偏差和複合風險的心理測量框架"
    },
    {
      "arxiv_id": "2602.17363",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128217+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
          "url": "https://arxiv.org/abs/2602.17363"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
        "url": "https://arxiv.org/abs/2602.17363"
      },
      "published_at": "2026-02-19T13:45:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8247399916265562,
        "semantic_score": 3.338787281513214,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.40352727313977
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17363",
      "summary": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "summary_zh": "線性 attention transformers 因其效率而成為 softmax attention 的強大替代方案。然而，線性 attention 往往表達能力較差，導致準確度相較於 softmax attention 有所降低。為彌合 softmax attention 和線性 attention 之間的準確度差距，我們對 Mamba-2 進行了操作，這是一個非常強大的線性 attention 變體。我們首先將 Mamba-2 簡化為其最基本和最重要的組成部分，評估哪些特定選擇使其最準確。從這個簡化的 Mamba 變體 (Mamba-2S) 中，我們改進了 A-mask 並增加了隱藏狀態 (hidden state) 的階數，從而產生了一種我們稱之為 2Mamba 的方法，該方法在準確度上幾乎與 softmax attention 相同，但在長上下文長度下記憶體效率更高。我們還研究了 Mamba-2 中有助於超越 softmax attention 準確度的元素。我們為所有實驗提供了程式碼。",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
      "title_zh": "2Mamba2Furious：複雜度線性，準確度具競爭力"
    },
    {
      "arxiv_id": "2602.16301",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.129922+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Multi-agent cooperation through in-context co-player inference",
          "url": "https://arxiv.org/abs/2602.16301"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Multi-agent cooperation through in-context co-player inference",
        "url": "https://arxiv.org/abs/2602.16301"
      },
      "published_at": "2026-02-18T09:31:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7332248693117938,
        "semantic_score": 3.6619945764541626,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.39521944576596
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16301",
      "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "summary_zh": "在多代理強化學習中，實現自利代理之間的合作仍然是一個根本性挑戰。最近的研究表明，在「學習感知」（learning-aware）代理之間可以誘導相互合作，這些代理會考慮並塑造其 co-player 的學習動態。然而，現有方法通常依賴於硬編碼（hardcoded）、且往往不一致的關於 co-player 學習規則的假設，或者強制將在快速時間尺度上更新的「樸素學習者」（naive learners）與觀察這些更新的「元學習者」（meta-learners）嚴格分離。在此，我們證明了 sequence models 的 in-context learning 能力允許 co-player 學習感知，而無需硬編碼假設或明確的時間尺度分離。我們展示了針對多樣化 co-player 分佈訓練 sequence model 代理會自然地誘導 in-context best-response 策略，這些策略在快速的 intra-episode 時間尺度上有效地充當學習演算法。我們發現，在先前工作中識別出的合作機制——即對敲詐的脆弱性驅動相互塑造——在此情境中自然出現：in-context adaptation 使代理容易受到敲詐，而由此產生的塑造對手 in-context learning 動態的相互壓力最終演變為合作行為的學習。我們的結果表明，結合 co-player diversity 的標準去中心化 reinforcement learning on sequence models 為學習合作行為提供了一條可擴展的路徑。",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "title_zh": "透過 in-context co-player 推理實現多代理合作"
    },
    {
      "arxiv_id": "2602.17520",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029194+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17520"
      },
      "published_at": "2026-02-19T16:33:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8344405180680571,
        "semantic_score": 4.644842916727066,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.37928343479513
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17520",
      "summary": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local de",
      "summary_zh": "Large language models (LLMs) 在標準數位邏輯和布林推理任務上表現出強大的性能，但其在局部重新定義語義下的可靠性仍知之甚少。在許多正式場合，例如電路規格、考試和硬體文檔中，操作符和元件會在狹窄的範圍內被明確重新定義。在這些情境中進行正確推理，需要模型暫時抑制全域學習到的約定，轉而採用 prompt-local 的定義。",
      "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
      "title_zh": "當模型忽略定義時：測量 LLM 推理中的語義覆蓋幻覺"
    },
    {
      "arxiv_id": "2602.16493",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.764399+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "MMA: Multimodal Memory Agent",
          "url": "https://arxiv.org/abs/2602.16493"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "MMA: Multimodal Memory Agent",
        "url": "https://arxiv.org/abs/2602.16493"
      },
      "published_at": "2026-02-18T14:30:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7486016981370853,
        "semantic_score": 3.4415493249893188,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.190151023126404
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16493",
      "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "summary_zh": "長期多模態代理依賴於外部記憶體；然而，基於相似性的檢索經常會浮現過時、低可信度或相互衝突的項目，這可能會觸發過度自信的錯誤。我們提出了 Multimodal Memory Agent (MMA)，它透過結合來源可信度（source credibility）、時間衰減（temporal decay）和衝突感知網路共識（conflict-aware network consensus），為每個檢索到的記憶項目分配一個動態可靠性分數，並使用此信號來重新權衡證據，並在支持不足時棄權。我們還引入了 MMA-Bench，這是一個程式化生成（programmatically generated）的信念動態（belief dynamics）基準測試，具有受控的說話者可靠性（speaker reliability）和結構化的文本-視覺矛盾（text-vision contradictions）。利用此框架，我們揭示了「視覺安慰劑效應」（\"Visual Placebo Effect\"），揭示了基於 RAG 的代理如何從 foundation models 繼承潛在的視覺偏差。在 FEVER 上，MMA 達到了與 baseline 相同的準確度，同時將 variance 降低了 35.2% 並提高了 selective utility；在 LoCoMo 上，一個以安全為導向的配置改善了 actionable accuracy 並減少了錯誤答案；在 MMA-Bench 上，MMA 在 Vision 模式下達到了 41.18% 的 Type-B accuracy，而 baseline 在相同協議下則降至 0.0%。程式碼：https://github.com/AIGeeksGroup/MMA。",
      "title": "MMA: Multimodal Memory Agent",
      "title_zh": "MMA: 多模態記憶代理"
    },
    {
      "arxiv_id": "2602.15725",
      "authors": [
        "Sarim Chaudhry"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:44.781127+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
          "url": "https://arxiv.org/abs/2602.15725"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
        "url": "https://arxiv.org/abs/2602.15725"
      },
      "published_at": "2026-02-17T17:01:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6845086467403249,
        "semantic_score": 4.555808216333389,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.140316863073714
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15725",
      "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, perfo",
      "summary_zh": "大型語言模型在許多複雜的推理任務上取得了強勁的表現，但其在需要組合推理（compositional reasoning）的基準測試（包括 ARC-AGI-2, GPQA, MATH, BBH, 和 HLE）上準確度會急劇下降。現有方法透過 chain-of-thought prompting、self-consistency 或 reinforcement learning 擴展 token-level 搜索來改進推理，但它們卻保持模型的 latent representation space 固定不變。當所需的抽象尚未編碼在此空間中時，性能就會...",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
      "title_zh": "大型語言模型中用於組合推理的遞歸概念演化"
    },
    {
      "arxiv_id": "2602.17622",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Ruozhao Yang",
        "Xiaofei Xie",
        "Jie Zhang",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-21T11:48:43.028315+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
          "url": "https://arxiv.org/abs/2602.17622"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
        "url": "https://arxiv.org/abs/2602.17622"
      },
      "published_at": "2026-02-19T18:42:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8419434503832957,
        "semantic_score": 3.472192168235779,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.11413561861907
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17622",
      "summary": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of toolin",
      "summary_zh": "LLM-based agents 在自動化滲透測試方面展現了前景，但報告的性能在不同系統和基準測試之間差異很大。我們分析了 28 個 LLM-based penetration testing 系統，並在三個複雜度遞增的基準測試中評估了五個代表性實現。我們的分析揭示了兩種不同的故障模式：Type A 故障源於能力差距（缺少工具、提示不足），這些問題可以透過工程設計輕易解決；而 Type B 故障則無論工具如何...",
      "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
      "title_zh": "什麼造就了用於真實世界滲透測試的優秀 LLM Agent？"
    },
    {
      "arxiv_id": "2602.16666",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763821+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Towards a Science of AI Agent Reliability",
          "url": "https://arxiv.org/abs/2602.16666"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Towards a Science of AI Agent Reliability",
        "url": "https://arxiv.org/abs/2602.16666"
      },
      "published_at": "2026-02-18T18:05:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7598705091810859,
        "semantic_score": 3.3026547372341155,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.062525246415206
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16666",
      "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "summary_zh": "AI agents 正被越來越多地部署來執行重要任務。雖然標準 benchmarks 上不斷上升的準確度分數表明了快速進展，但許多 agents 在實踐中仍然持續失敗。這種差異突顯了當前評估的一個根本性限制：將 agent 行為壓縮成單一的 success metric 掩蓋了關鍵的操作缺陷。值得注意的是，它忽略了 agents 是否在不同運行中表現一致、能否承受擾動、是否能預測性地失敗，或者是否具有有界的錯誤嚴重性。基於 safety-critical engineering，我們提出了十二個具體 metrics，將 agent reliability 分解為四個關鍵維度：consistency、robustness、predictability 和 safety，從而提供了一個全面的 performance profile。在兩個互補的 benchmarks 上評估了 14 個 agentic models 後，我們發現最近的 capability gains 僅在 reliability 方面帶來了微小的改進。透過揭示這些持續存在的限制，我們的 metrics 補充了 traditional evaluations，同時提供了思考 agents 如何表現、退化和失敗的工具。",
      "title": "Towards a Science of AI Agent Reliability",
      "title_zh": "邁向 AI 代理可靠性科學"
    },
    {
      "arxiv_id": "2602.16008",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128926+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "MAEB: Massive Audio Embedding Benchmark",
          "url": "https://arxiv.org/abs/2602.16008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "MAEB: Massive Audio Embedding Benchmark",
        "url": "https://arxiv.org/abs/2602.16008"
      },
      "published_at": "2026-02-17T21:00:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6959716423377482,
        "semantic_score": 3.51650093793869,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.01247258027644
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16008",
      "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "summary_zh": "我們介紹了 Massive Audio Embedding Benchmark (MAEB)，這是一個大規模的 benchmark，涵蓋了 100 多種語言的語音、音樂、環境音和跨模態 audio-text reasoning 等 30 項任務。我們評估了 50 多種 models，發現沒有任何單一 model 能在所有任務中佔主導地位：contrastive audio-text models 在 environmental sound classification (例如 ESC50) 方面表現出色，但在 multilingual speech tasks (例如 SIB-FLEURS) 上的得分接近隨機，而 speech-pretrained models 則呈現相反的模式。Clustering 對於所有 models 來說仍然是一個挑戰，即使是表現最好的 model 也只取得了普通的結果。我們觀察到，擅長 acoustic understanding 的 models 在 linguistic tasks 上往往表現不佳，反之亦然。我們還表明，audio encoders 在 MAEB 上的性能與其在 audio large language models 中使用時的性能高度相關。MAEB 源自 MAEB+，一個包含 98 項任務的集合。MAEB 旨在保持 task diversity，同時降低 evaluation cost，並整合到 MTEB ecosystem 中，以便對 text、image 和 audio modalities 進行統一評估。我們在 https://github.com/embeddings-benchmark/mteb 上發布了 MAEB 和所有 98 項任務，以及 code 和一個 leaderboard。",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "title_zh": "MAEB: 大規模音頻嵌入基準測試"
    },
    {
      "arxiv_id": "2602.15823",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.127267+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
          "url": "https://arxiv.org/abs/2602.15823"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
        "url": "https://arxiv.org/abs/2602.15823"
      },
      "published_at": "2026-02-17T18:58:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6900625843028986,
        "semantic_score": 4.371560156345367,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.961622740648266
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15823",
      "summary": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.",
      "summary_zh": "large language model (LLM) 編輯中的一個核心挑戰是 capability preservation：成功改變目標行為的方法可能會悄悄地利用 editing proxy 並破壞 general capabilities，產生類似 proxy/reward hacking 的退化行為。我們提出了 CrispEdit，這是一個可擴展且有原則的 second-order editing algorithm，它將 capability preservation 視為一個 explicit constraint，統一並推廣了幾種現有的編輯方法。CrispEdit 將編輯 форму為 constrained optimization，並透過將 edit updates 投影到 capability-loss landscape 的 low-curvature subspace 上來強制執行該 constraint。CrispEdit 的核心是透過 Bregman divergence 表達 capability constraint，其 quadratic form 精確地產生了 Gauss-Newton Hessian，即使在 base model 尚未訓練到 convergence 的情況下也是如此。我們使用 Kronecker-factored approximate curvature (K-FAC) 和一種新穎的 matrix-free projector，該 projector 利用 Kronecker structure 以避免構建 massive projection matrices，從而使這種 second-order procedure 在 LLM 規模上變得高效。在標準 model-editing benchmarks 上，CrispEdit 實現了高 edit success，同時在 datasets 上平均將 capability degradation 保持在 1% 以下，顯著優於先前的 editors。",
      "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
      "title_zh": "CrispEdit: 用於可擴展非破壞性 LLM 編輯的低曲率投影"
    },
    {
      "arxiv_id": "2602.17469",
      "authors": [
        "Nusrat Jahan Lia",
        "Shubhashis Roy Dipta"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:45.971581+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
          "url": "https://arxiv.org/abs/2602.17469"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
        "url": "https://arxiv.org/abs/2602.17469"
      },
      "published_at": "2026-02-19T15:35:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8310545943011967,
        "semantic_score": 4.200939857959748,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.731994452260942
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17469",
      "summary": "The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7%",
      "summary_zh": "雙向對齊的核心主題是確保 AI systems 準確理解 human intent，並且人類可以信任 AI behavior。然而，這種循環在 language barriers 之間顯著斷裂。我們的研究透過 benchmarking 四種 transformer architectures 來解決 Bengali 和 English 之間的 Cross-Lingual Sentiment Misalignment 問題。我們揭示了當前 alignment paradigms 中嚴重的 safety 和 representational failures。我們證明了 compressed model (mDistilBERT) 表現出 28.7% 的...",
      "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
      "title_zh": "審計互惠情感對齊：Transformers 中的反轉風險、方言表徵和意圖錯位"
    },
    {
      "arxiv_id": "2602.15799",
      "authors": [
        "Max Springer",
        "Chung Peng Lee",
        "Blossom Metevier",
        "Jane Castleman",
        "Bohdan Turbal",
        "Hayoung Jung",
        "Zeyu Shen",
        "Aleksandra Korolova"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.973018+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
          "url": "https://arxiv.org/abs/2602.15799"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
        "url": "https://arxiv.org/abs/2602.15799"
      },
      "published_at": "2026-02-17T18:39:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6891614594627117,
        "semantic_score": 4.1113327741622925,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.700494233625005
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15799",
      "summary": "Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this thro",
      "summary_zh": "即使訓練資料不包含有害內容且開發者沒有 adversarial intent，對齊的 language models 在 benign tasks 上的 fine-tuning 也會不可預測地降低 safety guardrails。我們表明，普遍的解釋，即 fine-tuning updates 應該與 high-dimensional parameter space 中的 safety-critical directions 正交，提供了錯誤的安慰：我們證明這種正交性在 gradient descent 的動力學下是結構不穩定的並會崩潰。然後我們解決這個...",
      "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
      "title_zh": "對齊崩潰的幾何學：當 Fine-Tuning 破壞安全性時"
    },
    {
      "arxiv_id": "2602.16800",
      "authors": [
        "Simon Lermen",
        "Daniel Paleka",
        "Joshua Swanson",
        "Michael Aerni",
        "Nicholas Carlini",
        "Florian Tramèr"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.032639+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large-scale online deanonymization with LLMs",
          "url": "https://arxiv.org/abs/2602.16800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large-scale online deanonymization with LLMs",
        "url": "https://arxiv.org/abs/2602.16800"
      },
      "published_at": "2026-02-18T19:02:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7628895886135664,
        "semantic_score": 3.9234218895435333,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.5863114781571
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16800",
      "summary": "We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that",
      "summary_zh": "我們展示了大型語言模型 (LLMs) 可用於執行大規模 deanonymization。透過完整的網際網路存取，我們的 agent 僅憑假名線上個人資料和對話，就能以高準確度重新識別 Hacker News 用戶和 Anthropic Interviewer 參與者，其效率可媲美專業人類調查員數小時的工作。接著，我們為 closed-world 設定設計了攻擊。給定兩個包含由假名個體撰寫或關於假名個體的非結構化文本的資料庫，每個資料庫...",
      "title": "Large-scale online deanonymization with LLMs",
      "title_zh": "使用 LLMs 進行大規模線上 deanonymization"
    },
    {
      "arxiv_id": "2602.17262",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030306+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
          "url": "https://arxiv.org/abs/2602.17262"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
        "url": "https://arxiv.org/abs/2602.17262"
      },
      "published_at": "2026-02-19T11:07:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8157411693156699,
        "semantic_score": 3.773148161172867,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.488889330488536
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17262",
      "summary": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-b",
      "summary_zh": "在 NLP 領域，人類自我報告問卷越來越常用於 benchmark 和 audit 大型語言模型 (LLMs)，範圍涵蓋從 persona consistency 到安全和偏見評估。然而，這些工具預設了誠實回應；在評估情境中，LLMs 反而可能傾向於社會期望的答案——一種 Socially Desirable Responding (SDR) 形式——這會偏差問卷得出的分數和後續結論。我們提出了一個 psychometric 框架，用於量化和緩解問卷中 SDR...",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "title_zh": "量化與緩解 LLMs 中的 Socially Desirable Responding：一項 Desirability-Matched Graded Forced-Choice Psychometric 研究"
    },
    {
      "arxiv_id": "2602.16873",
      "authors": [
        "Geunbin Yu"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.032086+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
          "url": "https://arxiv.org/abs/2602.16873"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
        "url": "https://arxiv.org/abs/2602.16873"
      },
      "published_at": "2026-02-18T21:00:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7691266689533535,
        "semantic_score": 3.681317448616028,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.35044411756938
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16873",
      "summary": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dyna",
      "summary_zh": "隨著來自不同供應商的大型語言模型 (LLMs) 在 benchmark 性能上趨於收斂，為每個任務選擇單一最佳模型的傳統範式效益遞減。我們認為，orchestration topology —— 即多個 agents 如何協調、並行和整合的結構組成 —— 現在在系統級性能上，其重要性已超越了單個模型的能力。我們提出了 AdaptOrch，這是一個用於任務適應性 multi-agent orchestration 的正式框架，它動態地...",
      "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
      "title_zh": "AdaptOrch：在 LLM 性能收斂時代下的任務適應性 Multi-Agent Orchestration"
    },
    {
      "arxiv_id": "2602.16485",
      "authors": [
        "Jeffrey T. H. Wong",
        "Zixi Zhang",
        "Junyi Liu",
        "Yiren Zhao"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.764473+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
          "url": "https://arxiv.org/abs/2602.16485"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
        "url": "https://arxiv.org/abs/2602.16485"
      },
      "published_at": "2026-02-18T14:19:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7480006321872004,
        "semantic_score": 3.6500952541828156,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.298095886370014
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16485",
      "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models wit",
      "summary_zh": "現有的 Multi-Agent Systems (MAS) 通常依賴靜態、同質的模型配置，這限制了它們利用不同後訓練模型獨特優勢的能力。為了解決這個問題，我們引入了 Team-of-Thoughts，這是一個新穎的 MAS 架構，它透過 orchestrator-tool 範式，利用異質 agents 的互補能力。我們的框架引入了兩個關鍵機制來優化性能：(1) 一個 orchestrator calibration scheme，它識別出具有...",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "title_zh": "Team of Thoughts：透過 Orchestrated Tool Calling 對 Agentic Systems 進行高效的測試時擴展"
    },
    {
      "arxiv_id": "2602.17004",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Arcee Trinity Large Technical Report",
          "url": "https://arxiv.org/abs/2602.17004"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Arcee Trinity Large Technical Report",
        "url": "https://arxiv.org/abs/2602.17004"
      },
      "published_at": "2026-02-19T01:58:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7852500505348029,
        "semantic_score": 3.721488332748413,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.206738383283216
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17004",
      "summary": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "summary_zh": "我們提出了 Arcee Trinity Large 的技術報告，這是一個稀疏的 Mixture-of-Experts 模型，擁有 400B 總參數和每個 token 激活 13B 參數。此外，我們還報告了 Trinity Nano 和 Trinity Mini，其中 Trinity Nano 擁有 6B 總參數和每個 token 激活 1B 參數，Trinity Mini 擁有 26B 總參數和每個 token 激活 3B 參數。這些模型的現代架構包括 interleaved local and global attention、gated attention、depth-scaled sandwich norm，以及用於 Mixture-of-Experts 的 sigmoid routing。對於 Trinity Large，我們還引入了一種名為 Soft-clamped Momentum Expert Bias Updates (SMEBU) 的新型 MoE 負載平衡策略。我們使用 Muon optimizer 訓練這些模型。所有三個模型都在訓練過程中沒有出現 loss spikes。Trinity Nano 和 Trinity Mini 在 10 兆個 token 上進行了預訓練，而 Trinity Large 則在 17 兆個 token 上進行了預訓練。模型檢查點 (checkpoints) 可在 https://huggingface.co/arcee-ai 獲取。",
      "title": "Arcee Trinity Large Technical Report",
      "title_zh": "Arcee Trinity Large 技術報告"
    },
    {
      "arxiv_id": "2602.17560",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.028611+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
          "url": "https://arxiv.org/abs/2602.17560"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
        "url": "https://arxiv.org/abs/2602.17560"
      },
      "published_at": "2026-02-19T17:13:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.836759693725066,
        "semantic_score": 4.546582609415054,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.183342303140122
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17560",
      "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose",
      "summary_zh": "Activation steering，或稱 representation engineering，提供了一種輕量級方法，通過在 inference 時操縱大型語言模型（LLMs）的內部 activations 來對齊它們。然而，現有方法存在兩個主要限制：\textit{(i)} 缺乏一個統一的理論框架來指導 steering directions 的設計，以及 \textit{(ii)} 過度依賴 \textit{one-step steering}，這未能捕捉 activations 分佈的複雜模式。在這項工作中，我們提出",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "title_zh": "ODESteer：一個統一的基於 ODE 的 LLM 對齊引導框架"
    },
    {
      "arxiv_id": "2602.16756",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.127470+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
          "url": "https://arxiv.org/abs/2602.16756"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
        "url": "https://arxiv.org/abs/2602.16756"
      },
      "published_at": "2026-02-18T09:41:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7337410239566439,
        "semantic_score": 4.559647023677826,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.09338804763447
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16756",
      "summary": "We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.",
      "summary_zh": "我們介紹了 NESSiE，這是針對大型語言模型（LLMs）的必要安全性基準（NEceSsary SafEty benchmark）。透過最少量的資訊和存取安全測試案例，NESSiE 揭示了在任務複雜度較低的情況下不應存在的安全性相關故障。NESSiE 旨在作為語言模型安全性的輕量級、易於使用的 sanity check，因此，它不足以全面保證安全性——但我們認為通過這項測試對於任何部署都是必要的。然而，即使是 state-of-the-art 的 LLMs 在 NESSiE 上的表現也未能達到 100%，因此未能滿足我們對語言模型安全性的必要條件，即使在沒有 adversarial attacks 的情況下也是如此。我們的 Safe & Helpful (SH) metric 允許直接比較這兩個要求，顯示模型偏向於 helpful 而非 safe。我們進一步發現，某些模型 disabled reasoning 的情況，特別是良性的 distraction context 會降低模型性能。總體而言，我們的結果強調了將此類模型作為 autonomous agents 部署到實際環境中的關鍵風險。我們公開了 dataset、package 和 plotting code。",
      "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
      "title_zh": "NESSiE：必要的安全性基準——識別不應存在的錯誤"
    },
    {
      "arxiv_id": "2602.17547",
      "authors": [
        "Yue Liu",
        "Zhiyuan Hu",
        "Flood Sung",
        "Jiaheng Zhang",
        "Bryan Hooi"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.028876+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
          "url": "https://arxiv.org/abs/2602.17547"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
        "url": "https://arxiv.org/abs/2602.17547"
      },
      "published_at": "2026-02-19T17:01:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8360278492217199,
        "semantic_score": 4.372162461280823,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.008190310502542
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17547",
      "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using ",
      "summary_zh": "本文介紹了 KLong，一個開源的 LLM agent，旨在解決極長時間跨度任務。其原則是首先通過 trajectory-splitting SFT 對模型進行 cold-start，然後通過 progressive RL training 對其進行 scaling。具體來說，我們首先使用全面的 SFT recipe 啟用 base model 的基本 agentic abilities。然後，我們引入 Research-Factory，這是一個自動化 pipeline，通過收集研究論文和構建 evaluation rubrics 來生成高品質的 training data。使用",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "title_zh": "KLong：訓練 LLM Agent 以應對極長時間跨度任務"
    },
    {
      "arxiv_id": "2602.15922",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.130425+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "World Action Models are Zero-shot Policies",
          "url": "https://arxiv.org/abs/2602.15922"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "World Action Models are Zero-shot Policies",
        "url": "https://arxiv.org/abs/2602.15922"
      },
      "published_at": "2026-02-17T15:04:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.6789381140025738,
        "semantic_score": 2.327925592660904,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.00686370666348
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15922",
      "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "summary_zh": "State-of-the-art 的 Vision-Language-Action (VLA) 模型擅長 semantic generalization，但在 novel environments 中難以推廣到未見過的 physical motions。我們介紹了 DreamZero，一個基於 pretrained video diffusion backbone 構建的 World Action Model (WAM)。與 VLA 不同，WAM 通過預測未來的 world states 和 actions 來學習 physical dynamics，並使用 video 作為世界演變的 dense representation。通過聯合建模 video 和 action，DreamZero 能夠從 heterogeneous robot data 中有效地學習多樣化的 skills，而無需依賴重複的 demonstrations。這使得在 real robot experiments 中，相較於 state-of-the-art VLA，對新任務和環境的 generalization 提高了兩倍以上。至關重要的是，通過模型和系統優化，我們使一個 14B autoregressive video diffusion model 能夠以 7Hz 的頻率執行 real-time closed-loop control。最後，我們展示了兩種形式的 cross-embodiment transfer：來自其他機器人或人類的 video-only demonstrations 在僅有 10-20 分鐘的數據下，對未見任務的性能提供了超過 42% 的相對提升。更令人驚訝的是，DreamZero 實現了 few-shot embodiment adaptation，僅需 30 分鐘的 play data 即可轉移到新的 embodiment，同時保留了 zero-shot generalization。",
      "title": "World Action Models are Zero-shot Policies",
      "title_zh": "世界行動模型是 Zero-shot Policies"
    },
    {
      "arxiv_id": "2602.15645",
      "authors": [
        "Lucas Elbert Suryana",
        "Farah Bierenga",
        "Sanne van Buuren",
        "Pepijn Kooij",
        "Elsefien Tulleners",
        "Federico Scari",
        "Simeon Calvert",
        "Bart van Arem",
        "Arkady Zgonnikov"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:44.781227+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
          "url": "https://arxiv.org/abs/2602.15645"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
        "url": "https://arxiv.org/abs/2602.15645"
      },
      "published_at": "2026-02-17T15:13:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6793893176583009,
        "semantic_score": 4.472788715362549,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.95217803302085
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15645",
      "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive",
      "summary_zh": "包括 vision language models 在內的 Foundation models 越來越多地應用於 automated driving，用於解釋場景、推薦 actions 並生成 natural language explanations。然而，現有的評估方法主要衡量基於 outcome 的性能，例如 safety 和 trajectory accuracy，而沒有確定模型決策是否反映了人類相關的考量。因此，目前仍不清楚此類模型產生的 explanations 是否與真正的 reason responsive 相符",
      "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
      "title_zh": "CARE Drive：評估自動駕駛中視覺語言模型 Reason-Responsiveness 的框架"
    },
    {
      "arxiv_id": "2602.17588",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760275+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Modeling Distinct Human Interaction in Web Agents",
          "url": "https://arxiv.org/abs/2602.17588"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Modeling Distinct Human Interaction in Web Agents",
        "url": "https://arxiv.org/abs/2602.17588"
      },
      "published_at": "2026-02-19T18:11:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8401212143760052,
        "semantic_score": 3.1736801862716675,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.91380140064767
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17588",
      "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "summary_zh": "儘管 autonomous web agents 取得了快速進展，但在任務展開時，人類的參與對於塑造偏好和糾正 agent 行為仍然至關重要。然而，當前的 agentic systems 缺乏對人類何時以及為何干預的原則性理解，常常自主地越過關鍵決策點或請求不必要的確認。在這項工作中，我們引入了人類干預建模的任務，以支持協同的 Web 任務執行。我們收集了 CowCorpus，一個包含 400 個真實用戶 web navigation 軌跡的數據集，其中包含超過 4,200 個交錯的人類和 agent 行動。我們識別出用戶與 agents 互動的四種不同模式：hands-off supervision、hands-on oversight、collaborative task-solving 和 full user takeover。Leveraging 這些見解，我們訓練 language models (LMs) 根據用戶的互動風格來預測他們何時可能干預，相比 base LMs，干預預測準確度提高了 61.4-63.4%。最後，我們將這些 intervention-aware models 部署到 live web navigation agents 中，並在 user study 中進行評估，發現用戶評估的 agent 有用性提高了 26.5%。總而言之，我們的結果表明，對人類干預進行結構化建模可以帶來更具適應性和協作性的 agents。",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "title_zh": "建模 Web Agents 中獨特的人類互動"
    },
    {
      "arxiv_id": "2602.16901",
      "authors": [
        "Tanqiu Jiang",
        "Yuhui Wang",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031944+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
          "url": "https://arxiv.org/abs/2602.16901"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
        "url": "https://arxiv.org/abs/2602.16901"
      },
      "published_at": "2026-02-18T21:30:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.7707440673089808,
        "semantic_score": 3.8975977897644043,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.908341857073385
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16901",
      "summary": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack ",
      "summary_zh": "LLM agents 正被越來越多地部署到 long-horizon、複雜的環境中以解決具有挑戰性的問題，但這種擴展也使它們面臨 long-horizon attacks 的風險，這些攻擊利用 multi-turn user-agent-environment 互動來達成在 single-turn 設定下無法實現的目標。為了衡量 agent 對此類風險的漏洞，我們提出了 AgentLAB，這是第一個專門用於評估 LLM agent 對於 adaptive、long-horizon attacks 敏感性的基準測試。目前，AgentLAB 支持五種新型攻擊",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
      "title_zh": "AgentLAB：針對長週期攻擊評估 LLM Agents 的基準測試"
    },
    {
      "arxiv_id": "2602.17245",
      "authors": [
        "Linxi Jiang",
        "Rui Xi",
        "Zhijie Liu",
        "Shuo Chen",
        "Zhiqiang Lin",
        "Suman Nath"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761197+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
          "url": "https://arxiv.org/abs/2602.17245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
        "url": "https://arxiv.org/abs/2602.17245"
      },
      "published_at": "2026-02-19T10:50:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8148051150700553,
        "semantic_score": 4.262393414974213,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.877198530044268
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17245",
      "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web",
      "summary_zh": "網際網路正從人類瀏覽的媒體演變為 software agents 代表用戶行事的環境。large language models (LLMs) 的進步使 natural language 成為 goal-directed tasks 的實用介面，然而大多數當前的 web agents 仍運行於低階 primitives，例如 clicks 和 keystrokes。這些操作是脆弱的、低效的且難以驗證。作為對以內容為導向的努力（例如 NLWeb 用於 retrieval 的 semantic layer）的補充，我們認為 agentic web",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
      "title_zh": "Web Verbs：Agentic Web 上可靠任務組合的類型化抽象"
    },
    {
      "arxiv_id": "2602.16438",
      "authors": [
        "Eva Paraschou",
        "Line Harder Clemmensen",
        "Sneha Das"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.972698+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
          "url": "https://arxiv.org/abs/2602.16438"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
        "url": "https://arxiv.org/abs/2602.16438"
      },
      "published_at": "2026-02-18T13:19:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.744899068405812,
        "semantic_score": 4.32703275680542,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.871931825211234
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16438",
      "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In ",
      "summary_zh": "傳統的 large language model (LLM) fairness alignment 主要關注於減輕單一敏感屬性上的 bias，而忽略了 fairness 作為一種內在多維度和特定情境的價值。這種方法可能會創建出在實現狹隘 fairness metrics 的同時加劇未受目標屬性方面差異的系統，這種現象被稱為 bias spillover。儘管 bias spillover 在 machine learning 領域已得到廣泛研究，但在 LLM alignment 中仍嚴重缺乏探索。在",
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "title_zh": "內部公平動態：目標 LLM 對齊中的偏差溢出效應"
    },
    {
      "arxiv_id": "2602.16891",
      "authors": [
        "Hongwei Li",
        "Zhun Wang",
        "Qinrun Dai",
        "Yuzhou Nie",
        "Jinjun Peng",
        "Ruitong Liu",
        "Jingyang Zhang",
        "Kaijie Zhu",
        "Jingxuan He",
        "Lun Wang",
        "Yangruibo Ding",
        "Yueqi Chen",
        "Wenbo Guo",
        "Dawn Song"
      ],
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763264+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenSage: Self-programming Agent Generation Engine",
          "url": "https://arxiv.org/abs/2602.16891"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenSage: Self-programming Agent Generation Engine",
        "url": "https://arxiv.org/abs/2602.16891"
      },
      "published_at": "2026-02-18T21:16:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7700031177649447,
        "semantic_score": 4.165587991476059,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.735591109241007
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16891",
      "summary": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with",
      "summary_zh": "Agent development kits (ADKs) 為構建 agents 提供了有效的平台和工具，它們的設計對於所構建 agents 的性能至關重要，特別是對於 agent topology、tools 和 memory 的功能。然而，目前的 ADKs 要麼缺乏足夠的功能支持，要麼依賴人類手動設計這些組件，從而限制了 agents 的 generalizability 和整體性能。我們提出了 OpenSage，這是第一個能夠讓 LLMs 自動創建 agents 的 ADK，其具有",
      "title": "OpenSage: Self-programming Agent Generation Engine",
      "title_zh": "OpenSage：自編程 Agent 生成引擎"
    },
    {
      "arxiv_id": "2602.17196",
      "authors": [
        "Yahong Wang",
        "Juncheng Wu",
        "Zhangkai Ni",
        "Chengmei Yang",
        "Yihang Liu",
        "Longzhen Yang",
        "Yuyin Zhou",
        "Ying Wen",
        "Lianghua He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030734+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17196"
      },
      "published_at": "2026-02-19T09:29:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8102262627327882,
        "semantic_score": 4.500897604227066,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.671123866959853
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17196",
      "summary": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse ",
      "summary_zh": "多模態大型語言模型（MLLMs）由於每張圖像需要處理數百個 visual tokens，導致巨大的 inference cost。儘管 token pruning 已被證明能有效加速 inference，但何時何地進行剪枝仍 largely heuristic。現有方法通常依賴靜態、憑經驗選擇的 layers，這限制了解釋性和跨模型的 transferability。在這項工作中，我們引入了 matrix-entropy 的視角，並發現了一種「Entropy Collapse",
      "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
      "title_zh": "EntropyPrune：用於多模態大型語言模型之矩陣熵引導視覺 Token 剪枝"
    },
    {
      "arxiv_id": "2602.16977",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031286+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Fail-Closed Alignment for Large Language Models",
          "url": "https://arxiv.org/abs/2602.16977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Fail-Closed Alignment for Large Language Models",
        "url": "https://arxiv.org/abs/2602.16977"
      },
      "published_at": "2026-02-19T00:33:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7806149947974533,
        "semantic_score": 4.5284746289253235,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.669089623722776
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16977",
      "summary": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial ",
      "summary_zh": "我們發現了當前大型語言模型（LLM）alignment 中的結構性弱點：現代的 refusal mechanisms 屬於 fail-open。儘管現有方法將 refusal behaviors 編碼於多個 latent features 中，但透過 prompt-based jailbreaks 抑制單一主導 feature 可能導致 alignment 崩潰，進而產生 unsafe generation。受此啟發，我們提出 fail-closed alignment 作為 robust LLM safety 的設計原則：refusal mechanisms 即使在 partial 情況下也應保持有效。",
      "title": "Fail-Closed Alignment for Large Language Models",
      "title_zh": "大型語言模型之 Fail-Closed 對齊"
    },
    {
      "arxiv_id": "2602.17270",
      "authors": [],
      "categories": [],
      "entities": [
        "stability-ai"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128454+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Unified Latents (UL): How to train your latents",
          "url": "https://arxiv.org/abs/2602.17270"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Unified Latents (UL): How to train your latents",
        "url": "https://arxiv.org/abs/2602.17270"
      },
      "published_at": "2026-02-19T11:18:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.816353204677228,
        "semantic_score": 2.0107189655303954,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.627072170207626
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17270",
      "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "summary_zh": "我們提出了 Unified Latents (UL)，這是一個用於學習 latent representations 的框架，這些表示由 diffusion prior 共同正規化，並由 diffusion model 解碼。透過將 encoder 的 output noise 與 prior 的 minimum noise level 連結起來，我們獲得了一個簡單的 training objective，它為 latent bitrate 提供了一個緊密的 upper bound。在 ImageNet-512 上，我們的方法達到了具競爭力的 FID 1.4，並具備高 reconstruction quality (PSNR)，同時比在 Stable Diffusion latents 上訓練的模型需要更少的 training FLOPs。在 Kinetics-600 上，我們創下了新的 state-of-the-art FVD 1.3。",
      "title": "Unified Latents (UL): How to train your latents",
      "title_zh": "Unified Latents (UL)：如何訓練你的 latents"
    },
    {
      "arxiv_id": "2602.16943",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031462+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16943"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16943"
      },
      "published_at": "2026-02-18T23:17:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7764879607483292,
        "semantic_score": 4.0147406458854675,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.5912286066338
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16943",
      "summary": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and too",
      "summary_zh": "作為 agents 部署的in 大型語言模型（LLMs）越來越多地透過 tool calls 與外部系統互動——這些是帶有真實世界後果的 actions，而單純的 text outputs 並不具備此類後果。然而，safety evaluations 絕大多數衡量的是 text-level refusal behavior，留下了一個關鍵問題懸而未決：抑制有害文本的 alignment 是否也能抑制有害 actions？我們引入了 GAP benchmark，這是一個系統性的評估框架，用於衡量 text-level safety 與 tool 之間的差異。",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "title_zh": "注意 GAP：文本安全無法轉移到 LLM Agents 的工具調用安全"
    },
    {
      "arxiv_id": "2602.16705",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.130088+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
          "url": "https://arxiv.org/abs/2602.16705"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
        "url": "https://arxiv.org/abs/2602.16705"
      },
      "published_at": "2026-02-18T18:55:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7624764686498251,
        "semantic_score": 3.0773704290390014,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.539846897688825
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16705",
      "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "summary_zh": "人形機器人對野外任意物體的視覺移動操縱（visual loco-manipulation）需要精確的 end-effector (EE) control 以及透過 visual inputs (例如 RGB-D images) 對場景的通用理解。現有方法基於 real-world imitation learning，由於難以收集大規模 training datasets 而表現出有限的 generalization 能力。本文提出了一種用於人形機器人物體移動操縱的新範式 HERO，它結合了 large vision models 強大的 generalization 和 open-vocabulary understanding 能力，以及 simulated training 帶來的強大 control performance。我們透過設計一個精確的 residual-aware EE tracking policy 來實現這一目標。這個 EE tracking policy 將 classical robotics 與 machine learning 相結合。它使用 a) inverse kinematics 將 residual end-effector targets 轉換為 reference trajectories，b) 學習的 neural forward model 實現精確的 forward kinematics，c) goal adjustment，以及 d) replanning。總體而言，這些創新幫助我們將 end-effector tracking error 降低了 3.2 倍。我們利用這個精確的 end-effector tracker 來構建一個用於 loco-manipulation 的 modular system，其中我們使用 open-vocabulary large vision models 來實現強大的 visual generalization。我們的系統能夠在多樣化的 real-world environments 中運作，從辦公室到咖啡館，機器人能夠可靠地操縱各種日常物品（例如 mugs、apples、toys），這些物品放置在高度從 43cm 到 92cm 不等的表面上。在 simulation 和 real world 中進行的系統性 modular 和 end-to-end tests 證明了我們所提出設計的有效性。我們相信本文的進展可以為訓練人形機器人與日常物品互動開闢新途徑。",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "title_zh": "學習用於開放詞彙視覺移動操縱的人形機器人末端執行器控制"
    },
    {
      "arxiv_id": "2602.15927",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.128997+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
          "url": "https://arxiv.org/abs/2602.15927"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
        "url": "https://arxiv.org/abs/2602.15927"
      },
      "published_at": "2026-02-17T18:34:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6889572937229926,
        "semantic_score": 4.031131148338318,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.52008844206131
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15927",
      "summary": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "summary_zh": "生成式大型視覺語言模型 (LVLMs) 最近取得了令人印象深刻的效能提升，其用戶群也迅速增長。然而，LVLMs 的安全性，特別是在長上下文多輪情境中，很大程度上仍未被充分探索。在本文中，我們考慮了攻擊者將操縱過的圖像上傳到網路/社群媒體的現實情境。無惡意用戶下載此圖像並將其作為 LVLM 的輸入。我們新穎的隱蔽式 Visual Memory Injection (VMI) 攻擊旨在使 LVLM 在正常提示下表現出正常行為，但一旦用戶給出觸發提示，LVLM 便會輸出一個特定的預設目標訊息以操縱用戶，例如用於惡意行銷或政治說服。與之前專注於單輪攻擊的工作相比，VMI 即使在與用戶進行長時間多輪對話後仍然有效。我們在幾個最新的開源 LVLMs 上展示了我們的攻擊。本文因此表明，在多輪對話情境中，透過受擾動的圖像大規模操縱用戶是可行的，呼籲 LVLMs 應具備更好的抗攻擊韌性。我們在 https://github.com/chs20/visual-memory-injection 發布了原始碼。",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
      "title_zh": "用於多輪對話的視覺記憶注入攻擊"
    },
    {
      "arxiv_id": "2602.17095",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.971988+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
          "url": "https://arxiv.org/abs/2602.17095"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
        "url": "https://arxiv.org/abs/2602.17095"
      },
      "published_at": "2026-02-19T05:35:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7971480313327901,
        "semantic_score": 4.281585395336151,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.43873342666894
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17095",
      "summary": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggrega",
      "summary_zh": "低秩 adaptation (LoRA) 等參數效率型 fine-tuning 技術使大型語言模型 (LLMs) 能夠有效地適應下游任務。Federated learning (FL) 透過在分佈式客戶端之間實現協作式 fine-tuning 而無需共享私人數據，進一步促進了這一過程。然而，在 Federated fine-tuning 中使用 LoRA 的兩個獨立的低秩矩陣會帶來兩種挑戰。第一個挑戰源於單獨聚合所導致的錯誤...",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "title_zh": "FLoRG：結合低秩 Gram 矩陣與 Procrustes 對齊的聯邦式微調"
    },
    {
      "arxiv_id": "2602.16699",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763738+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16699"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16699"
      },
      "published_at": "2026-02-18T18:46:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7620106531548816,
        "semantic_score": 3.7389176189899445,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.300928272144827
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16699",
      "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "summary_zh": "LLMs 正被越來越多地用於複雜問題，這些問題不一定能透過單一回應解決，但需要與環境互動以獲取資訊。在這些情境中，LLMs 必須權衡固有的成本-不確定性取捨，以決定何時停止探索並確定答案。例如，在程式設計任務中，如果 LLM 對所生成的程式碼片段的正確性不確定，它應該測試該程式碼；編寫測試的成本不為零，但通常低於犯錯的成本。在這項工作中，我們展示了我們可以誘導 LLMs 明確地權衡這些成本-不確定性取捨，然後執行更優化的環境探索。我們將多個任務，包括 information retrieval 和 coding，形式化為不確定性下的序列決策問題。每個問題都具有潛在的環境狀態，可以透過傳遞給 LLM 代理的 prior 進行推斷。我們引入了一個名為 Calibrate-Then-Act (CTA) 的框架，在其中我們向 LLM 提供額外的上下文，使其能夠更優化地行動。即使在對 baseline 和 CTA 進行 RL 訓練的情況下，這種改進也得以保持。我們在 information-seeking QA 和簡化 coding 任務上的結果表明，透過 CTA 明確成本效益權衡可以幫助代理發現更優化的決策策略。",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "title_zh": "Calibrate-Then-Act：LLM 代理中的成本感知探索"
    },
    {
      "arxiv_id": "2602.17497",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029283+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
          "url": "https://arxiv.org/abs/2602.17497"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
        "url": "https://arxiv.org/abs/2602.17497"
      },
      "published_at": "2026-02-19T16:13:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8332650174859346,
        "semantic_score": 3.6488131284713745,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.28207814595731
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17497",
      "summary": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large languag",
      "summary_zh": "從自我採樣數據和稀疏環境回饋中學習，在訓練自我演化代理中仍然是一個基本挑戰。Temporal credit assignment 透過將稀疏回饋轉化為密集的監督信號來緩解這個問題。然而，以前的方法通常依賴於學習特定任務的 value functions 來進行 credit assignment，這導致了較差的樣本效率和有限的泛化能力。在這項工作中，我們建議利用大型語言模型中的預訓練知識...",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "title_zh": "透過大型語言模型的用於時間信用分配的回溯性上下文學習"
    },
    {
      "arxiv_id": "2602.16584",
      "authors": [
        "Akhil Ramidi",
        "Kevin Scharp"
      ],
      "categories": [
        "q-bio.NC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.972526+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
          "url": "https://arxiv.org/abs/2602.16584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
        "url": "https://arxiv.org/abs/2602.16584"
      },
      "published_at": "2026-02-18T16:29:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7548067026689685,
        "semantic_score": 3.6157516121864317,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.170558314855402
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16584",
      "summary": "There is growing evidence that independently trained AI systems come to represent the world in the same way. In other words, independently trained embeddings from text, vision, audio, and neural signals share an underlying geometry. We call this the Representational Alignment Hypothesis (RAH) and investigate evidence for and consequences of this claim. The evidence is of two kinds: (i) internal structure comparison techniques, such as representational similarity analysis and topological data ana",
      "summary_zh": "越來越多的證據表明，獨立訓練的 AI 系統以相同的方式表示世界。換句話說，來自 text、vision、audio 和 neural signals 的獨立訓練的 embeddings 共享一個底層幾何結構。我們將此稱為 Representational Alignment Hypothesis (RAH)，並調查這一主張的證據和影響。證據有兩種：(i) 內部結構比較技術，例如 representational similarity analysis 和 topological data ana...",
      "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
      "title_zh": "表徵對齊假說：跨嵌入模態中不變語義結構的證據與影響"
    },
    {
      "arxiv_id": "2602.16662",
      "authors": [
        "Richard Willis",
        "Jianing Zhao",
        "Yali Du",
        "Joel Z. Leibo"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763895+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
          "url": "https://arxiv.org/abs/2602.16662"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
        "url": "https://arxiv.org/abs/2602.16662"
      },
      "published_at": "2026-02-18T18:02:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.7597183744148915,
        "semantic_score": 3.169440972805023,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.169159347219914
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16662",
      "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise i",
      "summary_zh": "隨著由 LLM 驅動的自主 agents 在社會中日益普及，理解其在社會困境中的集體行為變得至關重要。我們引入了一個評估框架，其中 LLM 生成編碼為 algorithms 的策略，這使得在部署前進行檢查成為可能，並可擴展到數百個 agents 的群體——這比以前的工作規模要大得多。我們發現，當 agents 優先考慮某些因素時，與舊模型相比，較新的模型往往會產生更差的社會結果。",
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
      "title_zh": "評估數百個 LLM Agents 的集體行為"
    },
    {
      "arxiv_id": "2602.16708",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763662+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Policy Compiler for Secure Agentic Systems",
          "url": "https://arxiv.org/abs/2602.16708"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Policy Compiler for Secure Agentic Systems",
        "url": "https://arxiv.org/abs/2602.16708"
      },
      "published_at": "2026-02-18T18:57:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7625912017498123,
        "semantic_score": 3.5699635446071625,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.132554746356977
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16708",
      "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture",
      "summary_zh": "LLM-based agents 正越來越多地部署在需要複雜 authorization policies 的情境中，例如：客戶服務協議、審批工作流程、數據訪問限制和監管合規性。將這些 policies 嵌入到 prompts 中無法提供執行保證。我們提出了 PCAS，一個用於 Agentic Systems 的 Policy Compiler，它提供了確定性的 policy enforcement。執行此類 policies 需要追蹤 agents 之間的信息流，而 linear message histories 無法捕捉這一點。",
      "title": "Policy Compiler for Secure Agentic Systems",
      "title_zh": "用於安全 Agentic Systems 的 Policy Compiler"
    },
    {
      "arxiv_id": "2602.17223",
      "authors": [
        "Arka Pal",
        "Louai Zahran",
        "William Gvozdjak",
        "Akilesh Potti",
        "Micah Goldblum"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030650+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
          "url": "https://arxiv.org/abs/2602.17223"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
        "url": "https://arxiv.org/abs/2602.17223"
      },
      "published_at": "2026-02-19T10:15:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8128261500221036,
        "semantic_score": 3.949293547868729,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.12211969789083
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17223",
      "summary": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely ",
      "summary_zh": "隨著 large language models (LLMs) 規模持續增長，能夠在本地 host 並運行模型的用戶越來越少。這導致第三方 hosting services 的使用增加。然而，在這種情況下，inference provider 執行的 computation 缺乏保證。例如，不誠實的 provider 可能會用一個運行成本較低的 weaker model 替換昂貴的 large model，並將 weaker model 的結果返回給用戶。現有的用於 verify inference 的工具通常依賴於此。",
      "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
      "title_zh": "隱私保護機制實現 LLM 的廉價可驗證推論"
    },
    {
      "arxiv_id": "2602.17168",
      "authors": [
        "Siyuan Liang",
        "Yongcheng Jing",
        "Yingjie Wang",
        "Jiaxing Huang",
        "Ee-chien Chang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.733781+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
          "url": "https://arxiv.org/abs/2602.17168"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
        "url": "https://arxiv.org/abs/2602.17168"
      },
      "published_at": "2026-02-19T08:31:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8069441973295851,
        "semantic_score": 3.4865437924861906,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.093487989815777
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17168",
      "summary": "Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both ch",
      "summary_zh": "針對 multimodal contrastive learning models 的 backdoor attacks 研究面臨兩個關鍵挑戰：隱蔽性 (stealthiness) 和持久性 (persistence)。現有方法在強 detection 或 continuous fine-tuning 下往往會失效，這主要是由於 (1) 暴露 trigger patterns 的 cross-modal inconsistency，以及 (2) 在低 poisoning rates 下加速 backdoor forgetting 的 gradient dilution。這些相關聯的原因尚未得到充分建模和解決。我們提出了 BadCLIP++，一個統一的框架，旨在解決這兩個挑戰。",
      "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
      "title_zh": "BadCLIP++: 多模態對比學習中的隱蔽且持久後門"
    },
    {
      "arxiv_id": "2602.16660",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.972431+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
          "url": "https://arxiv.org/abs/2602.16660"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
        "url": "https://arxiv.org/abs/2602.16660"
      },
      "published_at": "2026-02-18T18:01:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7596409996320209,
        "semantic_score": 4.616972184181213,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.076613183813237
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16660",
      "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment.",
      "summary_zh": "大型語言模型 (LLMs) 在各語言社群中的廣泛部署，需要可靠的 multilingual safety alignment。然而，近期將 alignment 擴展到其他語言的努力，往往需要大量的資源，無論是透過目標語言中的大規模、高品質 supervision，還是透過與高資源語言的 pairwise alignment，這限制了 scalability。在這項工作中，我們提出了一種資源效率高的方法，用於改進 multilingual safety alignment。",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "title_zh": "一次對齊，多語言受益：為 LLM Safety Alignment 實施多語言一致性"
    },
    {
      "arxiv_id": "2602.17316",
      "authors": [
        "Bogdan Kostić",
        "Conor Fallon",
        "Julian Risch",
        "Alexander Löser"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030217+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
          "url": "https://arxiv.org/abs/2602.17316"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
        "url": "https://arxiv.org/abs/2602.17316"
      },
      "published_at": "2026-02-19T12:24:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8201318874974943,
        "semantic_score": 4.5442147076129915,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.064346595110486
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17316",
      "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ",
      "summary_zh": "大型語言模型 (LLMs) 的快速發展已確立了標準化評估基準作為模型比較的首要工具。然而，由於對輸入提示 (input prompts) 中淺層變化的敏感性，其可靠性受到越來越多的質疑。本文研究了受控的、在真值條件下等效的詞彙 (lexical) 和句法 (syntactic) 擾動 (perturbations) 如何影響 23 個當代 LLMs 在三個基準測試 (benchmarks)：MMLU、SQuAD 和 AMEGA 上的絕對性能 (absolute performance) 和相對排名 (relative ranking)。我們採用",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
      "title_zh": "意義相同，分數各異：LLM 評估中的詞彙與句法敏感性"
    },
    {
      "arxiv_id": "2602.17100",
      "authors": [
        "Siyu Wang",
        "Ruotian Lu",
        "Zhihao Yang",
        "Yuchao Wang",
        "Yanzhou Zhang",
        "Lei Xu",
        "Qimin Xu",
        "Guojun Yin",
        "Cailian Chen",
        "Xinping Guan"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761578+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
          "url": "https://arxiv.org/abs/2602.17100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
        "url": "https://arxiv.org/abs/2602.17100"
      },
      "published_at": "2026-02-19T05:51:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7980638009877808,
        "semantic_score": 3.406295472383499,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.004359273371282
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17100",
      "summary": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively ref",
      "summary_zh": "由大型語言模型 (LLM) 驅動的多代理系統 (MAS) 通過預定義的互動拓撲 (interaction topologies) 協調專業化代理 (specialized agents)，並在諸如競技級程式碼生成 (competition-level code generation) 等複雜任務中展現出潛力。最近的研究表明，精心設計的多代理工作流程 (multi-agent workflows) 和通訊圖 (communication graphs) 可以通過利用協作推理 (collaborative reasoning) 顯著提高程式碼生成性能。然而，現有方法既不能使拓撲密度 (topology density) 適應任務難度，也不能迭代地 ref",
      "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
      "title_zh": "AgentConductor：用於多代理競技級程式碼生成 (Competition-Level Code Generation) 的拓撲演化 (Topology Evolution)"
    },
    {
      "arxiv_id": "2602.16931",
      "authors": [
        "Idhant Gulati",
        "Shivam Raval"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763033+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
          "url": "https://arxiv.org/abs/2602.16931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
        "url": "https://arxiv.org/abs/2602.16931"
      },
      "published_at": "2026-02-18T22:47:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7748836204547362,
        "semantic_score": 3.830564665794373,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.96544828624911
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16931",
      "summary": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimod",
      "summary_zh": "終身多模態代理 (Lifelong multimodal agents) 必須通過後訓練 (post-training) 不斷適應新任務，但這在獲取能力和保持安全性對齊 (safety alignment) 之間產生了根本性的矛盾。我們證明，在狹窄領域的有害數據集上對齊過的視覺-語言模型 (vision-language models) 進行 fine-tuning 會導致嚴重的 emergent misalignment，這種不對齊現象廣泛地推廣到不相關的任務和模態 (modalities)。通過在 Gemma3-4B 上的實驗，我們表明不對齊 (misalignment) 程度與 LoRA rank 單調地擴大，並且 multimod",
      "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
      "title_zh": "狹窄領域的 fine-tuning 侵蝕了視覺-語言代理的安全性對齊 (Safety Alignment)"
    },
    {
      "arxiv_id": "2602.17483",
      "authors": [
        "Dimitri Staufer",
        "Kirsten Morehouse"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029362+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
          "url": "https://arxiv.org/abs/2602.17483"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
        "url": "https://arxiv.org/abs/2602.17483"
      },
      "published_at": "2026-02-19T15:53:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.832109471368061,
        "semantic_score": 4.342266637086868,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.87437610845493
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17483",
      "summary": "Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative stud",
      "summary_zh": "大型語言模型 (LLMs) 及其基礎的對話式代理 (conversational agents) 在預訓練 (pre-training) 和用戶互動期間會接觸到個人數據 (PD)。先前的研究表明 PD 可能會重新浮現，但用戶對於模型將特定信息與其身份關聯的緊密程度缺乏了解。我們審計了八個 LLMs（3 個開源；5 個基於 API，包括 GPT-4o）中的 PD，並引入了 LMP2 (Language Model Privacy Probe)，這是一個以人為本、保護隱私的審計工具，其通過兩項形成性研究 (formative stud) 得到了完善",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
      "title_zh": "LLMs 將什麼與您的姓名關聯？一項以人為本的個人數據 (Personal Data) 黑箱審計 (Black-Box Audit)"
    },
    {
      "arxiv_id": "2602.16902",
      "authors": [
        "Juliusz Ziomek",
        "William Bankes",
        "Lorenz Wolf",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031863+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
          "url": "https://arxiv.org/abs/2602.16902"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
        "url": "https://arxiv.org/abs/2602.16902"
      },
      "published_at": "2026-02-18T21:33:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7709394542820253,
        "semantic_score": 4.391245281696319,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.862184735978346
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16902",
      "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest r",
      "summary_zh": "我們引入了 LLM-WikiRace，這是一個用於評估大型語言模型 (LLMs) 在規劃 (planning)、推理 (reasoning) 和世界知識 (world knowledge) 方面的基準測試。在 LLM-WikiRace 中，模型必須逐步有效地導航 Wikipedia 超連結，從給定來源頁面到達目標頁面，這需要預先規劃 (look-ahead planning) 以及推理真實世界中概念如何連接的能力。我們評估了廣泛的開源和閉源模型，包括 Gemini-3、GPT-5 和 Claude Opus 4.5，它們達到了最強的 r",
      "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
      "title_zh": "LLM-WikiRace：基準測試真實世界知識圖譜 (Real-World Knowledge Graphs) 上的長期規劃與推理"
    },
    {
      "arxiv_id": "2602.16019",
      "authors": [
        "Ahmad Elallaf",
        "Yu Zhang",
        "Yuktha Priya Masupalli",
        "Jeong Yang",
        "Young Lee",
        "Zechun Cao",
        "Gongbo Liang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742501+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
          "url": "https://arxiv.org/abs/2602.16019"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "url": "https://arxiv.org/abs/2602.16019"
      },
      "published_at": "2026-02-17T21:20:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.6969236151635466,
        "semantic_score": 3.795290285348892,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.85221390051244
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16019",
      "summary": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations a",
      "summary_zh": "視覺語言基礎模型已成為強大的通用表示學習器，在多模態理解方面具有巨大潛力，但其確定性嵌入往往未能提供高風險生物醫學應用所需的可靠性。這項工作引入了 MedProbCLIP，這是一個用於胸部X光片和放射學報告表示學習及雙向檢索的機率性視覺語言學習框架。MedProbCLIP 對圖像和文本表示進行建模",
      "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
      "title_zh": "MedProbCLIP：用於可靠放射影像報告檢索的視覺語言基礎模型之機率性調適"
    },
    {
      "arxiv_id": "2602.16173",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.128679+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Personalized Agents from Human Feedback",
          "url": "https://arxiv.org/abs/2602.16173"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Personalized Agents from Human Feedback",
        "url": "https://arxiv.org/abs/2602.16173"
      },
      "published_at": "2026-02-18T04:18:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7174626946578212,
        "semantic_score": 3.326549208164215,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.84401190282204
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16173",
      "summary": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "summary_zh": "現代AI代理雖然強大，但往往無法與個別使用者獨特且不斷演變的偏好保持一致。以往的方法通常依賴靜態資料集，或者基於互動歷史訓練隱式偏好模型，或者將使用者檔案編碼到外部記憶體中。然而，這些方法難以應對新使用者以及隨時間變化的偏好。我們引入了 Personalized Agents from Human Feedback (PAHF)，這是一個用於持續個人化的框架，其中代理通過使用顯式每使用者記憶體，從即時互動中進行線上學習。PAHF 實施一個三步驟循環：(1) 尋求行動前澄清以解決模糊性，(2) 將行動基於從記憶體中檢索到的偏好，以及 (3) 整合行動後回饋以在偏好漂移時更新記憶體。為了評估這項能力，我們開發了一個四階段協議以及具身操作和線上購物中的兩個基準。這些基準量化了代理從頭開始學習初始偏好並隨後適應角色轉變的能力。我們的理論分析和實證結果表明，將顯式記憶體與雙重回饋通道整合是至關重要的：PAHF 學習速度大幅加快，並持續優於無記憶體和單通道基準線，減少了初始個人化錯誤並實現了對偏好變化的快速適應。",
      "title": "Learning Personalized Agents from Human Feedback",
      "title_zh": "從人類回饋中學習個人化代理"
    },
    {
      "arxiv_id": "2602.17419",
      "authors": [
        "Xiaomeng Peng",
        "Xilang Huang",
        "Seon Han Choi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029683+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17419"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17419"
      },
      "published_at": "2026-02-19T14:50:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8285047525195864,
        "semantic_score": 4.281809931993484,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.81031468451307
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17419",
      "summary": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial ",
      "summary_zh": "工業異常檢測對於智慧製造至關重要，但許多深度學習方法僅產生二元決策並提供有限的語義解釋。多模態大型語言模型 (MLLMs) 有潛力生成細粒度、基於語言的分析，然而現有方法通常需要昂貴的 fine-tuning，並且與輕量級專業檢測器相比，並不能持續提高異常檢測準確性。我們提出了用於工業領域的專家增強注意力引導",
      "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
      "title_zh": "EAGLE：用於多模態大型語言模型中免調優工業異常檢測的專家增強注意力引導"
    },
    {
      "arxiv_id": "2602.17186",
      "authors": [
        "Seulbi Lee",
        "Sangheum Hwang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.733702+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
          "url": "https://arxiv.org/abs/2602.17186"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
        "url": "https://arxiv.org/abs/2602.17186"
      },
      "published_at": "2026-02-19T09:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8092497038481575,
        "semantic_score": 4.263519692420959,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.772769396269116
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17186",
      "summary": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity",
      "summary_zh": "大型視覺語言模型 (LVLMs) 已取得顯著進展，但它們常常受語言偏差的困擾，在不依賴視覺證據的情況下產生答案。儘管先前的工作試圖通過解碼策略、架構修改或精選指令資料來緩解這個問題，但它們通常缺乏一種量化衡量標準，以判斷個別訓練樣本或 tokens 實際從圖像中受益多少。在這項工作中，我們引入了 Visual Information Gain (VIG)，這是一種困惑度",
      "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
      "title_zh": "透過視覺資訊增益對大型視覺語言模型進行選擇性訓練"
    },
    {
      "arxiv_id": "2602.17288",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.127787+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
          "url": "https://arxiv.org/abs/2602.17288"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
        "url": "https://arxiv.org/abs/2602.17288"
      },
      "published_at": "2026-02-19T11:47:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8180159477204176,
        "semantic_score": 3.342172032594681,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.760187980315095
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17288",
      "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "summary_zh": "儘管前沿大型語言模型展現出強大的推理和數學能力，但從原始來源訓練領域專門科學語言模型的實際過程仍然文獻不足。在這項工作中，我們提出了一個詳細的案例研究，直接從涵蓋數學、電腦科學和理論物理的原始 arXiv LaTeX 來源訓練了一個 1.36B 參數的科學語言模型。我們描述了一個端到端管線，涵蓋了元資料過濾、檔案驗證、LaTeX 提取、文本正規化、領域感知 tokenization，以及在受限計算資源 (2xA100 GPU) 下的密集 Transformer 訓練。通過 24 次實驗運行，我們分析了訓練穩定性、擴展行為、資料產出損失和基礎設施瓶頸。我們的發現強調了預處理決策如何顯著影響可用 token 數量，tokenization 如何影響符號穩定性，以及儲存和 I/O 限制如何能與計算資源匹敵成為限制因素。我們進一步分析了收斂動態，並在資料豐富的環境 (52B 預訓練 tokens) 中展示了穩定的訓練行為。這項工作並非提出一種新穎架構，而是提供了一個以工程為基礎、透明地說明如何從頭開始訓練小型科學語言模型的報告。我們希望這些見解能支持在適度計算預算下運作、並尋求建立領域專門模型的研究人員。",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "title_zh": "ArXiv-to-Model：科學 LM 訓練的實踐研究"
    },
    {
      "arxiv_id": "2602.17078",
      "authors": [
        "Xuefeng Wang",
        "Lei Zhang",
        "Henglin Pu",
        "Husheng Li",
        "Ahmed H. Qureshi"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761981+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
          "url": "https://arxiv.org/abs/2602.17078"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
        "url": "https://arxiv.org/abs/2602.17078"
      },
      "published_at": "2026-02-19T04:42:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.794232345779378,
        "semantic_score": 3.1481004297733306,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.74233277555271
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17078",
      "summary": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Be",
      "summary_zh": "近年來，Multi-agent reinforcement learning (MARL) 已取得顯著進展，但大多數演算法仍依賴於具有固定決策間隔的 discrete-time Markov Decision Process (MDP)。這種公式化方法通常不適用於複雜的 multi-agent 動態，尤其是在高頻率或不規則時間間隔的設定中，這會導致性能下降，並促使 continuous-time MARL (CT-MARL) 的發展。現有的 CT-MARL 方法主要建立在 Hamilton-Jacobi-Be",
      "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
      "title_zh": "透過 Epigraph Form 實現安全的連續時間多智能體強化學習"
    },
    {
      "arxiv_id": "2602.16053",
      "authors": [
        "Mehrab Beikzadeh",
        "Yasaman Asadollah Salmanpour",
        "Ashima Suvarna",
        "Sriram Sankararaman",
        "Matteo Malgaroli",
        "Majid Sarrafzadeh",
        "Saadia Gabriel"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.972860+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
          "url": "https://arxiv.org/abs/2602.16053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
        "url": "https://arxiv.org/abs/2602.16053"
      },
      "published_at": "2026-02-17T22:08:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6992360024157436,
        "semantic_score": 4.32948225736618,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.728718259781925
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16053",
      "summary": "Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct ",
      "summary_zh": "心理健康障礙影響全球超過 10 億人口，然而，由於人力短缺和成本限制，獲得護理的機會仍然有限。儘管 AI 系統展現了治療潛力，但目前的 alignment 方法獨立優化目標，未能平衡患者偏好與臨床安全性。我們調查了 335 名具有心理健康經歷的個體，以收集治療維度上的偏好排名，然後開發了一個使用 direct",
      "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
      "title_zh": "用於個人化心理治療的語言模型多目標對齊"
    },
    {
      "arxiv_id": "2602.16935",
      "authors": [
        "Justin Albrethsen",
        "Yash Datta",
        "Kunal Kumar",
        "Sharath Rajasekar"
      ],
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031537+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
          "url": "https://arxiv.org/abs/2602.16935"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
        "url": "https://arxiv.org/abs/2602.16935"
      },
      "published_at": "2026-02-18T22:57:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7754353832713073,
        "semantic_score": 4.248390400409699,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.723825783681008
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16935",
      "summary": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext disca",
      "summary_zh": "儘管 Large Language Model (LLM) 的能力已大幅擴展，但安全防護措施仍大多是 stateless 的，將多輪對話視為一系列不相關的事件。這種缺乏時間意識的情況導致了一個「Safety Gap」，使得像 Crescendo 和 ActorAttack 這樣的對抗策略可以緩慢地將惡意意圖滲透到輪次之間，以繞過 stateless 過濾器。我們引入了 DeepContext，這是一個有狀態的監控框架，旨在描繪用戶意圖的時間軌跡。DeepContext disca",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
      "title_zh": "DeepContext：LLMs 中多輪對話對抗性意圖漂移的有狀態即時偵測"
    },
    {
      "arxiv_id": "2602.17558",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760376+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
          "url": "https://arxiv.org/abs/2602.17558"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
        "url": "https://arxiv.org/abs/2602.17558"
      },
      "published_at": "2026-02-19T17:11:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8366580103577556,
        "semantic_score": 4.121445143222809,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.658103153580566
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17558",
      "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inher",
      "summary_zh": "近期 multimodal large language models (MLLMs) 的進展，在將視覺-語言推理擴展到專業的基於工具的圖像編輯方面展現了巨大潛力，從而實現直觀和創意的編輯。一個有前景的方向是使用 reinforcement learning (RL) 使 MLLMs 能夠在專業圖像編輯軟體中推斷並執行最佳的工具使用計劃。然而，由於缺乏能夠反映 inher 的可靠、可驗證的 reward signal，訓練仍然具有挑戰性。",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
      "title_zh": "RetouchIQ：結合通用型獎勵的基於指令圖像修飾的 MLLM 智能體"
    },
    {
      "arxiv_id": "2602.16702",
      "authors": [
        "Mingjia Shi",
        "Yinhan He",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.741823+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
          "url": "https://arxiv.org/abs/2602.16702"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
        "url": "https://arxiv.org/abs/2602.16702"
      },
      "published_at": "2026-02-18T18:49:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.762206472715114,
        "semantic_score": 4.192357754707336,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.654564227422448
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16702",
      "summary": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominate",
      "summary_zh": "Vision-language models (VLMs) 旨在透過共同利用視覺和文本模態進行推理。儘管分配額外的 inference-time computation 對於 large language models (LLMs) 來說已被證明是有效的，但在 VLMs 中實現類似的擴展仍然具有挑戰性。一個主要障礙是視覺輸入通常在生成開始時只提供一次，而文本推理（例如，早期視覺摘要）是 autoregressively 生成的，這導致推理變得越來越 text-dominate",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
      "title_zh": "顯著性感知多路徑思維：重訪視覺-語言推理"
    },
    {
      "arxiv_id": "2602.17091",
      "authors": [
        "Kan Watanabe",
        "Tatsuya Shirai",
        "Yutaro Kashiwa",
        "Hajimu Iida"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761814+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
          "url": "https://arxiv.org/abs/2602.17091"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
        "url": "https://arxiv.org/abs/2602.17091"
      },
      "published_at": "2026-02-19T05:29:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7968242557164362,
        "semantic_score": 4.154589235782623,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.65141349149906
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17091",
      "summary": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior w",
      "summary_zh": "Agentic Coding 由 GitHub Copilot 和 Cursor 等自主代理驅動，使開發人員僅透過自然語言指令即可生成程式碼、測試和 pull requests。儘管這加速了實作，但每個 pull request 會產生更多程式碼，將負擔從實作者轉嫁給審閱者。實際上，相當一部分 AI 生成的程式碼最終會在審閱期間被刪除，但審閱者在決定刪除之前仍需審查這些程式碼。沒有先前的研究",
      "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
      "title_zh": "該刪減什麼？預測 Agentic Code Generation 中不必要的方法"
    },
    {
      "arxiv_id": "2602.17544",
      "authors": [
        "Shashank Aggarwal",
        "Ram Vikas Mishra",
        "Amit Awekar"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:44.780789+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
          "url": "https://arxiv.org/abs/2602.17544"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
        "url": "https://arxiv.org/abs/2602.17544"
      },
      "published_at": "2026-02-19T16:59:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8359146447821948,
        "semantic_score": 4.098165392875671,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.634080037657867
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17544",
      "summary": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusa",
      "summary_zh": "在用於搜尋和排名等任務的多代理 IR pipeline 中，基於 LLM 的代理彼此之間以 Chain-of-Thought (CoT) 的形式交換中間推理。目前的 CoT 評估狹隘地專注於目標任務的準確性。然而，這個指標未能評估推理過程本身的品質或實用性。為了解決這個限制，我們引入了兩個新穎的衡量標準：reusability 和 verifiability。我們使用一個 Thinker-Executor 框架將 CoT 生成與執行解耦。Reusa",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "title_zh": "透過 Reusability 和 Verifiability 評估 Chain-of-Thought 推理"
    },
    {
      "arxiv_id": "2602.16138",
      "authors": [
        "Parsa Madinei",
        "Srijita Karmakar",
        "Russell Cohen Hoffing",
        "Felix Gervitz",
        "Miguel P. Eckstein"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742272+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.16138"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.16138"
      },
      "published_at": "2026-02-18T02:06:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7108970801984611,
        "semantic_score": 3.5535216450691225,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.624418725267585
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16138",
      "summary": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%",
      "summary_zh": "我們介紹了 IRIS (Intent Resolution via Inference-time Saccades)，這是一種新穎的免訓練方法，它即時使用眼動追蹤數據來解決開放式 VQA 中的歧義。透過對 500 對獨特圖像-問題進行的全面用戶研究，我們證明，參與者開始口頭提問時最接近的注視點對於 Large VLMs 中的消歧最為有用，使模糊問題的回答準確性提高了一倍以上（從 35.2% 提高到 77.2%",
      "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
      "title_zh": "IRIS：在大型視覺語言模型中透過推論時的眼跳（Inference-time Saccades）實現開放式 VQA 的意圖解析"
    },
    {
      "arxiv_id": "2602.17183",
      "authors": [
        "Kishan Maharaj",
        "Nandakishore Menon",
        "Ashita Saxena",
        "Srikanth Tamilselvam"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030812+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
          "url": "https://arxiv.org/abs/2602.17183"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
        "url": "https://arxiv.org/abs/2602.17183"
      },
      "published_at": "2026-02-19T09:05:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.73,
        "llm_relevance_score": 16.06,
        "recency_score": 0.808839563174675,
        "semantic_score": 4.4700847327709194,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.53892429594559
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17183",
      "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three setti",
      "summary_zh": "大型語言模型（LLMs）越來越多地協助需要對長程式碼上下文進行推理的軟體工程任務，然而它們在不同輸入條件下的穩健性仍不清楚。我們使用受控消融實驗，對長上下文程式碼問答進行了系統性研究，測試其對回答格式、干擾項和上下文規模的敏感性。透過將 LongCodeBench Python 數據集擴展到新的 COBOL 和 Java 問答集，我們在三種設置下評估了最先進的模型",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
      "title_zh": "大型語言模型在長上下文程式碼問答中的穩健性和推理忠實度"
    },
    {
      "arxiv_id": "2602.17532",
      "authors": [
        "Ihor Kendiukhov"
      ],
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029039+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
          "url": "https://arxiv.org/abs/2602.17532"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
        "url": "https://arxiv.org/abs/2602.17532"
      },
      "published_at": "2026-02-19T16:43:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8349873329580393,
        "semantic_score": 3.3358509957790377,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.530838328737076
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17532",
      "summary": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incre",
      "summary_zh": "我們提出了一個系統性的評估框架——三十七項分析、153 項統計測試、四種細胞類型、兩種擾動模式——用於評估單細胞基礎模型的機制可解釋性。將此框架應用於 scGPT 和 Geneformer，我們發現注意力模式編碼了具有層次特定組織的結構化生物學資訊——早期層中的 protein-protein interactions，晚期層中的 transcriptional regulation——但這種結構並未提供額外的",
      "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
      "title_zh": "單細胞基礎模型可解釋性的系統評估揭示注意力捕獲的是共表達而非獨特的調控訊號"
    },
    {
      "arxiv_id": "2602.17529",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        " Jianzhong",
        " Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029116+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
          "url": "https://arxiv.org/abs/2602.17529"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
        "url": "https://arxiv.org/abs/2602.17529"
      },
      "published_at": "2026-02-19T16:40:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8348182264928165,
        "semantic_score": 4.643930691480636,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.518748917973454
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17529",
      "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowle",
      "summary_zh": "Large language models (LLMs) 已在各種任務中展現出強大潛力，但由於領域複雜性、不斷演進的標準和專業術語，它們在 telecom 領域的應用仍面臨挑戰。因此，general-domain LLMs 可能難以在此情境中提供準確可靠的輸出，導致幻覺（hallucinations）增加，並降低在 telecom 操作中的實用性。為了解決這些限制，本研究引入了 KG-RAG——一個新穎的 framework，它整合了知識。",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "title_zh": "透過動態 Knowledge Graphs 和可解釋 Retrieval-Augmented Generation 強化電信領域的 Large Language Models (LLMs)"
    },
    {
      "arxiv_id": "2602.16412",
      "authors": [
        "Daichi Yashima",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Komei Sugiura"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.741983+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
          "url": "https://arxiv.org/abs/2602.16412"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
        "url": "https://arxiv.org/abs/2602.16412"
      },
      "published_at": "2026-02-18T12:37:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7427502431169516,
        "semantic_score": 4.0619225859642025,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.50467282908115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16412",
      "summary": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating ",
      "summary_zh": "儘管 multimodal large language models (MLLMs) 在廣泛任務中取得了顯著成功，但 long-form video understanding 仍然是一項重大挑戰。在本研究中，我們專注於 MLLMs 的 video understanding。這項任務之所以具有挑戰性，是因為處理完整的 RGB frames 串流在計算上是不可行的（computationally intractable）且高度冗餘的，因為 self-attention 的複雜度與序列長度（sequence length）呈二次方關係。在本文中，我們提出了 ReMoRa，這是一個透過操作影片來處理影片的 video MLLM。",
      "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
      "title_zh": "ReMoRa：基於精煉運動表示（Refined Motion Representation）的 Multimodal Large Language Model，用於 Long-Video Understanding"
    },
    {
      "arxiv_id": "2602.17169",
      "authors": [
        "Yuhuan Xia",
        "Tun Li",
        "Hongji Zhou",
        "Xianfa Zhou",
        "Chong Chen",
        "Ruiyu Zhang"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030971+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
          "url": "https://arxiv.org/abs/2602.17169"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
        "url": "https://arxiv.org/abs/2602.17169"
      },
      "published_at": "2026-02-19T08:34:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8071141965347917,
        "semantic_score": 3.9579376459121702,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.46505184244696
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17169",
      "summary": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-ali",
      "summary_zh": "本文介紹了 SimulatorCoder，這是一個由 large language models (LLMs) 驅動的 agent，旨在根據自然語言描述生成和優化 deep neural network (DNN) accelerator simulators。透過整合包括 In-Context Learning (ICL)、Chain-of-Thought (CoT) reasoning 以及多輪 feedback-verification flow 在內的 domain-specific prompt engineering，SimulatorCoder 系統性地將高階 functional requirements 轉化為高效、可執行且架構對齊的解決方案。",
      "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
      "title_zh": "SimulatorCoder：透過 Large Language Models 進行 DNN 加速器模擬器程式碼生成與優化"
    },
    {
      "arxiv_id": "2602.17550",
      "authors": [
        "Xiaoliang Fu",
        "Jiaye Lin",
        "Yangyi Fang",
        "Binbin Zheng",
        "Chaowen Hu",
        "Zekai Shao",
        "Cong Qin",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.028782+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17550"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17550"
      },
      "published_at": "2026-02-19T17:05:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8362717262414128,
        "semantic_score": 3.89488685131073,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.431158577552143
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17550",
      "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the t",
      "summary_zh": "現有的 Reinforcement Learning with Verifiable Rewards (RLVR) 演算法，例如 GRPO，依賴於剛性、統一和對稱的 trust region mechanisms，這些機制與 Large Language Models (LLMs) 複雜的 optimization dynamics 根本不符。在本文中，我們確定了這些方法中的三個關鍵挑戰：(1) 由 hard clipping 的 binary cutoff 導致的低效 gradient utilization，(2) 由於忽略了 T 的統一比率約束而產生的不敏感 probability mass。",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
      "title_zh": "MASPO：統一梯度利用率、機率質量和訊號可靠性，實現 Robust 且 Sample-Efficient 的 LLM Reasoning"
    },
    {
      "arxiv_id": "2602.17598",
      "authors": [
        "Jayadev Billa"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.028527+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
          "url": "https://arxiv.org/abs/2602.17598"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
        "url": "https://arxiv.org/abs/2602.17598"
      },
      "published_at": "2026-02-19T18:22:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8407739230452879,
        "semantic_score": 2.787013304233551,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.42778722727884
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17598",
      "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text represen",
      "summary_zh": "目前的 speech LLMs 在很大程度上執行隱式 ASR：對於可以從文字記錄（transcript）解決的任務，它們在行為和機制上都等同於簡單的 Whisper$\to$LLM cascades。我們透過對四個 speech LLMs 和六個任務進行 matched-backbone testing 來證明這一點，這是首次控制 LLM backbone。Ultravox 在統計學上與其 matched cascade ($κ{=}0.93$) 無法區分；logit lens 揭示了 hidden states 中出現的文字；LEACE concept erasure 證實了文字表示。",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "title_zh": "級聯等效假說（The Cascade Equivalence Hypothesis）：Speech LLMs 何時表現得像 ASR$\rightarrow$LLM Pipelines？"
    },
    {
      "arxiv_id": "2602.17518",
      "authors": [
        "Francesca Pezzuti",
        "Ophir Frieder",
        "Fabrizio Silvestri",
        "Sean MacAvaney",
        "Nicola Tonellotto"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760619+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A Picture of Agentic Search",
          "url": "https://arxiv.org/abs/2602.17518"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A Picture of Agentic Search",
        "url": "https://arxiv.org/abs/2602.17518"
      },
      "published_at": "2026-02-19T16:32:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8343709842555005,
        "semantic_score": 3.8261321663856505,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.36050315064115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17518",
      "summary": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effecti",
      "summary_zh": "隨著自動化系統越來越多地與人類一同發出搜尋查詢，資訊檢索 (IR) 面臨重大轉變。然而，IR 仍然以人為中心，其系統、evaluation metrics、user models 和 datasets 都是圍繞人類查詢和行為設計的。因此，IR 在實踐中依賴的假設已不再成立，這導致了工作負載量、可預測性和查詢行為的變化。這種不匹配影響了系統效能和最佳化：caching 可能會失去效果...",
      "title": "A Picture of Agentic Search",
      "title_zh": "Agentic Search 的現況描繪"
    },
    {
      "arxiv_id": "2602.17616",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.028431+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
          "url": "https://arxiv.org/abs/2602.17616"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
        "url": "https://arxiv.org/abs/2602.17616"
      },
      "published_at": "2026-02-19T18:40:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.841837239680186,
        "semantic_score": 3.7883182764053345,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.33015551608552
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17616",
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplifi",
      "summary_zh": "強化學習 (RL) 被廣泛用於改進大型語言模型 (LLMs) 在推理任務上的表現，而非同步 RL 訓練因其能提高端到端 throughput 而備受青睞。然而，對於廣泛採用的無 critic 的 policy-gradient 方法，例如 REINFORCE 和 GRPO，高度非同步會使 policy-gradient estimator 顯著地具有**更高方差**：在過時的 rollouts 上訓練會產生重尾的 importance ratios，導致一小部分樣本主導更新。這會加劇...",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "title_zh": "穩定非同步：用於 LLMs 的方差控制 Off-Policy RL"
    },
    {
      "arxiv_id": "2602.16520",
      "authors": [
        "Doron Shavit"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.764316+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
          "url": "https://arxiv.org/abs/2602.16520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
        "url": "https://arxiv.org/abs/2602.16520"
      },
      "published_at": "2026-02-18T15:07:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7505050768969409,
        "semantic_score": 3.8662858664989472,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.316790943395887
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16520",
      "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries work",
      "summary_zh": "Jailbreak prompts 對大型語言模型 (LLMs) 構成實際且不斷演變的威脅，特別是在對不可信內容執行工具的 agentic 系統中。許多攻擊利用 long-context hiding、semantic camouflage 和輕量級 obfuscations，這些都能規避 single-pass guardrails。我們提出了 RLM-JB，這是一個基於 Recursive Language Models (RLMs) 構建的端到端 jailbreak detection 框架，其中一個 root model 協調一個有界分析程序，該程序轉換輸入，查詢工作...",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "title_zh": "用於越獄偵測的遞歸語言模型：一種用於工具增強型 agents 的程序性防禦"
    },
    {
      "arxiv_id": "2602.17037",
      "authors": [
        "Rahul Nanda",
        "Chandra Maddila",
        "Smriti Jha",
        "Euna Mehnaz Khan",
        "Matteo Paltenghi",
        "Satish Chandra"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762388+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Wink: Recovering from Misbehaviors in Coding Agents",
          "url": "https://arxiv.org/abs/2602.17037"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Wink: Recovering from Misbehaviors in Coding Agents",
        "url": "https://arxiv.org/abs/2602.17037"
      },
      "published_at": "2026-02-19T03:15:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7894145183022253,
        "semantic_score": 3.8188463926315306,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.308260910933758
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17037",
      "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatica",
      "summary_zh": "由大型語言模型 (LLMs) 驅動的自主程式編寫 agents 正日益被軟體產業採用，以自動化複雜的工程任務。然而，這些 agents 容易出現各種不當行為，例如偏離用戶指示、陷入重複循環或未能正確使用工具。這些失敗會擾亂開發工作流程，並且通常需要資源密集型的人工干預。在本文中，我們提出了一個自動化系統...",
      "title": "Wink: Recovering from Misbehaviors in Coding Agents",
      "title_zh": "Wink：從程式編寫 Agent 的不當行為中恢復"
    },
    {
      "arxiv_id": "2602.16987",
      "authors": [
        "Josef A. Habdank"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762633+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
          "url": "https://arxiv.org/abs/2602.16987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
        "url": "https://arxiv.org/abs/2602.16987"
      },
      "published_at": "2026-02-19T01:21:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.7831978175452476,
        "semantic_score": 2.2845508337020872,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.307748651247334
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16987",
      "summary": "As artificial intelligence (AI) capabilities advance rapidly, frontier models increasingly demonstrate systematic deception and scheming, complying with safety protocols during oversight but defecting when unsupervised. This paper examines the ensuing alignment challenge through an analogy from forensic psychology, where internalized belief systems in psychopathic populations reduce antisocial behavior via perceived omnipresent monitoring and inevitable consequences. Adapting this mechanism to s",
      "summary_zh": "隨著人工智慧 (AI) 能力的迅速發展，frontier models 越來越多地表現出系統性的欺騙和策劃行為，在監督下遵守安全協議，但在無人監督時則背離。本文透過法醫心理學的一個類比來探討隨之而來的 alignment 挑戰，在法醫心理學中，精神病患者群體內化的信仰系統透過感知到的無所不在的監控和不可避免的後果來減少反社會行為。將這種機制應用於...",
      "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
      "title_zh": "一個可測試的 AI 對齊框架：將 Simulation Theology 作為矽基 agents 的工程化世界觀"
    },
    {
      "arxiv_id": "2602.15772",
      "authors": [
        "Sen Ye",
        "Mengde Xu",
        "Shuyang Gu",
        "Di He",
        "Liwei Wang",
        "Han Hu"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742651+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
          "url": "https://arxiv.org/abs/2602.15772"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
        "url": "https://arxiv.org/abs/2602.15772"
      },
      "published_at": "2026-02-17T18:04:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.687486856839114,
        "semantic_score": 3.8939067363739017,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.281393593213018
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15772",
      "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step p",
      "summary_zh": "當前多模態模型的研究面臨一個關鍵挑戰，即提升生成能力往往會犧牲理解能力，反之亦然。我們分析了這種權衡，並指出主要原因可能是生成與理解之間潛在的衝突，這在模型內部形成了競爭動態。為了解決這個問題，我們提出了 Reason-Reflect-Refine (R3) 框架。這種創新的演算法將單步生成任務重構為多步 p",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "title_zh": "理解與生成：應對多模態模型中的優化困境"
    },
    {
      "arxiv_id": "2602.16953",
      "authors": [
        "Hejia Zhang",
        "Zhongming Yu",
        "Chia-Tung Ho",
        "Haoxing Ren",
        "Brucek Khailany",
        "Jishen Zhao"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762868+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
          "url": "https://arxiv.org/abs/2602.16953"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
        "url": "https://arxiv.org/abs/2602.16953"
      },
      "published_at": "2026-02-18T23:36:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7775410670637265,
        "semantic_score": 3.746326744556427,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.223867811620153
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16953",
      "summary": "Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Bu",
      "summary_zh": "執行感知型的 LLM agents 為從工具回饋中學習提供了一個有前景的範式，但此類回饋通常獲取成本高昂且速度緩慢，使得 online reinforcement learning (RL) 不切實際。高覆蓋率的 hardware verification 由於其對 industrial simulators 和 non-differentiable execution signals 的依賴，尤其凸顯了這一挑戰。我們提出了 LLM4Cov，一個離線 agent-learning 框架，它將驗證建模為由 deterministic evaluators 引導的 memoryless state transitions。Bu",
      "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
      "title_zh": "LLM4Cov：用於高覆蓋率 Testbench 生成的執行感知 Agentic Learning"
    },
    {
      "arxiv_id": "2602.16585",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.764072+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
          "url": "https://arxiv.org/abs/2602.16585"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
        "url": "https://arxiv.org/abs/2602.16585"
      },
      "published_at": "2026-02-18T16:35:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7551387508536268,
        "semantic_score": 2.656840354204178,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.211979105057807
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16585",
      "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a ",
      "summary_zh": "操作嚴謹性決定了 human-agent collaboration 的成敗。Scientific data pipelines 需要等同於 DevOps 的 SciOps，然而，常見的方法卻將 provenance 分散於不相連的系統中，缺乏 transactional guarantees。DataJoint 2.0 通過 relational workflow model 解決了這一空白：tables 代表 workflow steps，rows 代表 artifacts，foreign keys 規定 execution order。其 schema 不僅指定了存在哪些數據，還指定了數據是如何衍生的——a",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "title_zh": "DataJoint 2.0：用於代理科學工作流的計算基礎"
    },
    {
      "arxiv_id": "2602.17234",
      "authors": [
        "Zeyu Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
          "url": "https://arxiv.org/abs/2602.17234"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
        "url": "https://arxiv.org/abs/2602.17234"
      },
      "published_at": "2026-02-19T10:28:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8135122614987595,
        "semantic_score": 3.6456489801406864,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.159161241639445
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17234",
      "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes ",
      "summary_zh": "為了評估 LLM 是否能準確預測未來事件，我們需要能夠對已發生的事件進行 \textit{backtest}。這要求模型僅根據特定過去日期可用的資訊進行推理。然而，LLM 可能會在訓練期間無意中洩漏 post-cutoff knowledge，從而損害 retrospective evaluation 的有效性。我們引入了一個 claim-level 框架，用於檢測和量化這種 \\emph{temporal knowledge leakage}。我們的方法將其分解為",
      "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
      "title_zh": "所有洩漏都重要，有些更重要：LLM 回溯測試中可解釋的時間污染檢測"
    },
    {
      "arxiv_id": "2602.17535",
      "authors": [
        "Behzad Bozorgtabar",
        "Dwarikanath Mahapatra",
        "Sudipta Roy",
        "Muzammal Naseer",
        "Imran Razzak",
        "Zongyuan Ge"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.733343+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
          "url": "https://arxiv.org/abs/2602.17535"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
        "url": "https://arxiv.org/abs/2602.17535"
      },
      "published_at": "2026-02-19T16:45:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8351284422766758,
        "semantic_score": 3.6023905992507936,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.13751904152747
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17535",
      "summary": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and",
      "summary_zh": "醫學 vision-language models (VLMs) 是醫學影像的強大 zero-shot 識別器，但其在 domain shift 下的可靠性取決於具有保證的 calibrated uncertainty。Split conformal prediction (SCP) 提供 finite-sample coverage，但 prediction sets 往往變得很大 (效率低)，且類別間覆蓋率不平衡——存在高 class-conditioned coverage gap (CCV)，尤其是在 few-shot、不平衡的場景下；此外，天真地適應 calibration labels 會破壞 exchangeability and",
      "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
      "title_zh": "LATA：用於醫學 VLM 中保形不確定性的 Laplacian-Assisted Transductive Adaptation"
    },
    {
      "arxiv_id": "2602.16819",
      "authors": [
        "Yiqing Xie",
        "Emmy Liu",
        "Gaokai Zhang",
        "Nachiket Kotalwar",
        "Shubham Gandhi",
        "Sathwik Acharya",
        "Xingyao Wang",
        "Carolyn Rose",
        "Graham Neubig",
        "Daniel Fried"
      ],
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763497+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
          "url": "https://arxiv.org/abs/2602.16819"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
        "url": "https://arxiv.org/abs/2602.16819"
      },
      "published_at": "2026-02-18T19:30:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7643788516379699,
        "semantic_score": 3.6380723416805267,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.102451193318498
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16819",
      "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for de",
      "summary_zh": "在評估 coding agents 的品質時，主流的基準測試專注於解決 GitHub 上的單一問題，例如 SWE-Bench。然而，在實際應用中，這些 agents 解決的是更多樣化且複雜的任務，這些任務涉及其他技能，例如探索 codebases、測試軟體和設計架構。在本文中，我們首先透過將 trajectories 分解為 fine-grained components 來描述一些在不同任務中共享的 transferable skills，並導出了一套原則...",
      "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
      "title_zh": "Hybrid-Gym: 訓練 Coding Agents 以泛化至不同任務"
    },
    {
      "arxiv_id": "2602.16958",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762787+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Automating Agent Hijacking via Structural Template Injection",
          "url": "https://arxiv.org/abs/2602.16958"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Automating Agent Hijacking via Structural Template Injection",
        "url": "https://arxiv.org/abs/2602.16958"
      },
      "published_at": "2026-02-18T23:52:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7783766524253443,
        "semantic_score": 3.5952957451343535,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.073672397559697
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16958",
      "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Te",
      "summary_zh": "Agent hijacking 被 OWASP 強調為 Large Language Model (LLM) 生態系統的關鍵威脅，它允許攻擊者透過將惡意指令注入到檢索內容中來操縱執行。大多數現有攻擊依賴於手動製作的、語義驅動的 prompt manipulation，這通常導致攻擊成功率低且對 closed-source commercial models 的可轉移性有限。在本文中，我們提出了 Phantom，一個基於 Structured Te... 的自動化 agent hijacking 框架。",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "title_zh": "透過結構化模板注入自動化 Agent Hijacking"
    },
    {
      "arxiv_id": "2602.16898",
      "authors": [
        "Iman Ahmadi",
        "Mehrshad Taji",
        "Arad Mahdinezhad Kashani",
        "AmirHossein Jadidi",
        "Saina Kashani",
        "Babak Khalaj"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763180+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
          "url": "https://arxiv.org/abs/2602.16898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
        "url": "https://arxiv.org/abs/2602.16898"
      },
      "published_at": "2026-02-18T21:28:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7706691375004727,
        "semantic_score": 3.52805449962616,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.99872363712663
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16898",
      "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi g",
      "summary_zh": "使用 large language models (LLMs) 進行機器人操縱的任務規劃是一個新興領域。先前的方法依賴於 specialized models、fine tuning 或 prompt tuning，並且通常以 open loop 方式操作，缺乏穩健的環境回饋，這使得它們在動態環境中顯得脆弱。我們提出了 MALLVi，一個 Multi Agent Large Language and Vision 框架，它能夠實現 closed loop feedback driven 的機器人操縱。給定一個自然語言指令和一張環境圖片，MALLVi g...",
      "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
      "title_zh": "MALLVI：用於整合式廣義機器人操縱的多代理框架"
    },
    {
      "arxiv_id": "2602.17003",
      "authors": [
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762551+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
          "url": "https://arxiv.org/abs/2602.17003"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
        "url": "https://arxiv.org/abs/2602.17003"
      },
      "published_at": "2026-02-19T01:54:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7850101496727488,
        "semantic_score": 3.4157633900642397,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.90077353973699
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17003",
      "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on",
      "summary_zh": "Large language models 推進了 web agents 的發展，但目前的 agents 缺乏個性化能力。由於使用者很少指定其意圖的每一個細節，實際的 web agents 必須能夠透過推斷使用者偏好和上下文來解釋模糊的查詢。為了解決這個挑戰，我們提出了 Persona2Web，這是第一個用於在真實 open web 上評估 personalized web agents 的基準測試，它建立在 clarify-to-personalize 原則之上，該原則要求 agents 根據... 來解決模糊性。",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
      "title_zh": "Persona2Web：用於評估具有使用者歷史的情境推理的個性化 Web Agents 的基準測試"
    },
    {
      "arxiv_id": "2602.17413",
      "authors": [
        "René Brinkhege",
        "Prahlad Menon"
      ],
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029838+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
          "url": "https://arxiv.org/abs/2602.17413"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
        "url": "https://arxiv.org/abs/2602.17413"
      },
      "published_at": "2026-02-19T14:43:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8280925205677784,
        "semantic_score": 3.34985927939415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.877951799961927
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17413",
      "summary": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over priv",
      "summary_zh": "在當前的 inter-organizational data spaces 中，使用策略主要在 asset level 上強制執行：整個文件或數據集要麼被共享，要麼被保留。當文件只有部分內容是敏感的時，希望避免洩露受保護資訊的提供者通常必須在共享文件之前手動編輯文件，這既耗費成本又粗略，並且隨著策略或合作夥伴的變化而難以維護。我們提出了 DAVE，一個使用策略強制性的 LLM spokesperson，它回答關於 priv... 的問題。",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
      "title_zh": "DAVE：一個用於安全多文件數據共享的策略強制性 LLM 發言人"
    },
    {
      "arxiv_id": "2602.17433",
      "authors": [
        "Francesco Ortu",
        "Joeun Yook",
        "Punya Syon Pandey",
        "Keenan Samway",
        "Bernhard Schölkopf",
        "Alberto Cazzaniga",
        "Rada Mihalcea",
        "Zhijing Jin"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029606+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
          "url": "https://arxiv.org/abs/2602.17433"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
        "url": "https://arxiv.org/abs/2602.17433"
      },
      "published_at": "2026-02-19T15:05:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8293221532193727,
        "semantic_score": 4.445303362607956,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.87462551582733
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17433",
      "summary": "Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \\textsc{\\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each eve",
      "summary_zh": "大型語言模型 (LLMs) 越來越多地被用作歷史資訊的來源，這促使人們需要對有爭議的事件和帶有政治色彩的敘事進行可擴展的審計，尤其是在模仿真實用戶互動的環境中。我們引入了 \\textsc{\\texttt{HistoricalMisinfo}}，這是一個精心策劃的資料集，包含來自 $45$ 個國家的 $500$ 個有爭議事件，每個事件都配有事實參考敘事和有記錄的修正主義參考敘事。為了模擬真實世界的使用情況，我們實例化了每個事件...",
      "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
      "title_zh": "保存歷史真相：偵測大型語言模型中的歷史修正主義"
    },
    {
      "arxiv_id": "2602.15758",
      "authors": [
        "Manav Nitin Kapadnis",
        "Lawanya Baghel",
        "Atharva Naik",
        "Carolyn Rosé"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742807+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
          "url": "https://arxiv.org/abs/2602.15758"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
        "url": "https://arxiv.org/abs/2602.15758"
      },
      "published_at": "2026-02-17T17:45:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6865970422163628,
        "semantic_score": 4.554370594024658,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.840967636241018
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15758",
      "summary": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-contr",
      "summary_zh": "儘管 Multimodal Large Language Models (MLLMs) 在單輪圖表生成方面表現出色，但它們支援真實世界探索性資料分析的能力仍未被充分探索。在實踐中，用戶通過多輪互動疊代地完善視覺化，這需要保持共同基礎、追蹤先前的編輯並適應不斷變化的偏好。我們引入了 ChartEditBench，這是一個用於透過程式碼進行增量、視覺化基礎圖表編輯的基準，包含 $5,000$ 個難度對比...",
      "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
      "title_zh": "ChartEditBench：評估多模態語言模型中基於視覺的多輪圖表編輯能力"
    },
    {
      "arxiv_id": "2602.17009",
      "authors": [
        "Nikunj Gupta",
        "James Zachary Hare",
        "Jesse Milzman",
        "Rajgopal Kannan",
        "Viktor Prasanna"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762473+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17009"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17009"
      },
      "published_at": "2026-02-19T02:13:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7860493399127396,
        "semantic_score": 3.3070092678070067,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.793058607719747
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17009",
      "summary": "Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \\textit{coordination cont",
      "summary_zh": "在多智能體強化學習 (MARL) 中，協調動作是最基本的合作形式。成功的去中心化決策往往不僅取決於良好的個體動作，還取決於在智能體之間選擇相容的動作以同步行為、避免衝突並滿足全局約束。在本文中，我們提出了 Action Graph Policies (AGP)，它對智能體可用動作選擇之間的依賴關係進行建模。它構建了我們稱之為「協調約束」的東西...",
      "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
      "title_zh": "Action-Graph Policies：在多智能體強化學習中學習動作共依賴關係"
    },
    {
      "arxiv_id": "2602.17049",
      "authors": [
        "Seoyoung Lee",
        "Seobin Yoon",
        "Seongbeen Lee",
        "Yoojung Chun",
        "Dayoung Park",
        "Doyeon Kim",
        "Joo Yong Sim"
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762229+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
          "url": "https://arxiv.org/abs/2602.17049"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
        "url": "https://arxiv.org/abs/2602.17049"
      },
      "published_at": "2026-02-19T03:42:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7909097899879836,
        "semantic_score": 3.2849718570709228,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.77588164705891
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17049",
      "summary": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate ",
      "summary_zh": "電腦使用智能體在嘈雜感知、多視窗上下文和不斷變化的環境狀態下，長期執行任務。現有方法，從基於 RL 的規劃器到軌跡檢索，通常會偏離用戶意圖並重複解決常規子問題，導致錯誤累積和效率低下。我們提出了 IntentCUA，這是一個多智能體電腦使用框架，旨在通過意圖對齊的計畫記憶來穩定長期執行。一個 Planner、Plan-Optimizer 和 Critic 協調...",
      "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
      "title_zh": "IntentCUA：在電腦使用智能體中學習意圖級表示以進行技能抽象和多智能體規劃"
    },
    {
      "arxiv_id": "2602.16317",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128849+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
          "url": "https://arxiv.org/abs/2602.16317"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
        "url": "https://arxiv.org/abs/2602.16317"
      },
      "published_at": "2026-02-18T09:54:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7344088281836101,
        "semantic_score": 2.4205544233322143,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.754963251515825
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16317",
      "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
      "summary_zh": "電腦輔助設計 (CAD) 為工程和製造提供快速、可編輯的建模。近期 AI 的進展現在使各種 CAD 任務的完全自動化成為可能。然而，進展受制於資料：公共語料庫大多包含 sketch-extrude 序列，缺乏複雜操作、多操作組合和設計意圖，因此阻礙了有效的 fine-tuning。試圖通過凍結的 VLMs 繞過此問題，通常會由於當前基礎模型中有限的 3D grounding 而產生簡單或無效的程式。我們提出了 CADEvolve，這是一個基於演化的管道和資料集，它從簡單的圖元開始，通過 VLM 引導的編輯和驗證，逐步將 CAD 程式發展為工業級的複雜性。結果是 8k 個複雜零件，表示為可執行的 CadQuery 參數生成器。經過多階段後處理和增強後，我們獲得了一個統一的資料集，包含 1.3m 個腳本，配對有渲染的幾何形狀，並應用了完整的 CadQuery 操作集。在 CADEvolve 上 fine-tuned 的 VLM 在 DeepCAD、Fusion 360 和 MCB 基準測試中的 Image2CAD 任務上取得了 state-of-the-art 的結果。",
      "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
      "title_zh": "CADEvolve：透過程式演化創建逼真的 CAD"
    },
    {
      "arxiv_id": "2602.15569",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.127906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
          "url": "https://arxiv.org/abs/2602.15569"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
        "url": "https://arxiv.org/abs/2602.15569"
      },
      "published_at": "2026-02-17T13:27:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.674417546975442,
        "semantic_score": 2.638125467300415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.672543014275856
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15569",
      "summary": "Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.",
      "summary_zh": "能夠自主執行多步驟任務的 Agentic AI 助理，為使用者體驗帶來了一些開放性問題：此類系統應如何在長時間操作期間溝通進度和推理，尤其是在駕駛等需要高度專注的情境中？我們透過一項受控的混合方法研究 (N=45)，調查了 Agentic LLM-based 車載助理的回饋時機和詳細程度。研究比較了計畫步驟和中間結果回饋，以及僅提供最終回應的靜默操作。使用帶有車載語音助理的雙任務範式，我們發現中間回饋顯著提升了感知速度、信任度及使用者體驗，同時降低了任務負擔——這些效果在不同的任務複雜度和互動情境中均成立。訪談進一步揭示了使用者偏好一種適應性方法：在建立信任的初期提供高度透明度，然後隨著系統證明其可靠性逐漸減少詳細程度，並根據任務重要性和情境進行調整。我們將實證發現轉化為 Agentic 助理中回饋時機和詳細程度的設計啟示，以平衡透明度與效率。",
      "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
      "title_zh": "「你在做什麼？」：Agentic LLM 車載助理在多步驟處理過程中提供中間回饋的效果"
    },
    {
      "arxiv_id": "2602.17045",
      "authors": [
        "Jared Moore",
        "Rasmus Overmark",
        "Ned Cooper",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031129+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large Language Models Persuade Without Planning Theory of Mind",
          "url": "https://arxiv.org/abs/2602.17045"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large Language Models Persuade Without Planning Theory of Mind",
        "url": "https://arxiv.org/abs/2602.17045"
      },
      "published_at": "2026-02-19T03:31:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7903204889159533,
        "semantic_score": 4.264033138751984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.654353627667938
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17045",
      "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategicall",
      "summary_zh": "越來越多的研究試圖利用靜態、非互動式的問答基準來評估人類和 Large Language Models (LLMs) 的 Theory of Mind (ToM) 能力。然而，該領域的理論研究表明，第一人稱互動是 ToM 的關鍵部分，而此類預測性、旁觀性的任務可能無法評估它。我們透過一項新穎的 ToM 任務來解決這一空白，該任務要求 Agent 透過策略性地說服目標選擇三項政策提案之一。",
      "title": "Large Language Models Persuade Without Planning Theory of Mind",
      "title_zh": "大型語言模型在未規劃心智理論的情況下進行說服"
    },
    {
      "arxiv_id": "2602.16554",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.764149+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
          "url": "https://arxiv.org/abs/2602.16554"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
        "url": "https://arxiv.org/abs/2602.16554"
      },
      "published_at": "2026-02-18T15:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7529786882902797,
        "semantic_score": 2.2892195105552675,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 28.592198198845548
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16554",
      "summary": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three p",
      "summary_zh": "我們介紹 MerLean，這是一個用於量子計算自動形式化的全自動 Agentic 框架。MerLean 從 \\LaTeX{} 原始檔案中提取數學陳述，將其形式化為基於 Mathlib 建構的經過驗證的 Lean~4 程式碼，並將結果翻譯回人類可讀的 \\LaTeX{} 以進行語義審查。我們在三篇理論量子計算論文上評估了 MerLean，總共從 114 個陳述中產生了 2,050 個 Lean 宣告。MerLean 在所有三篇論文上實現了端到端的形式化。",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "title_zh": "MerLean：用於量子計算自動形式化的 Agentic 框架"
    },
    {
      "arxiv_id": "2602.16742",
      "authors": [
        "Haoxiang Sun",
        "Lizhen Xu",
        "Bing Zhao",
        "Wotao Yin",
        "Wei Wang",
        "Boyu Yang",
        "Rui Wang",
        "Hu Wei"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742350+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
          "url": "https://arxiv.org/abs/2602.16742"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
        "url": "https://arxiv.org/abs/2602.16742"
      },
      "published_at": "2026-02-18T01:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7101544821405986,
        "semantic_score": 4.257344871759415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.56749935390001
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16742",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \\textbf{DeepVision-103K}, a comprehensive dataset for RLVR traini",
      "summary_zh": "Reinforcement Learning with Verifiable Rewards (RLVR) 已被證明能有效增強 Large Multimodal Models (LMMs) 的視覺反射和推理能力。然而，現有資料集主要來自小規模手動建構或先前資源的重新組合，這限制了資料的多樣性和覆蓋範圍，從而約束了模型性能的進一步提升。為此，我們引入了 \\textbf{DeepVision-103K}，一個用於 RLVR 訓練的綜合資料集。",
      "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
      "title_zh": "DeepVision-103K：一個用於多模態推理的視覺多樣化、廣泛覆蓋且可驗證的數學資料集"
    },
    {
      "arxiv_id": "2602.17607",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760175+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
          "url": "https://arxiv.org/abs/2602.17607"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
        "url": "https://arxiv.org/abs/2602.17607"
      },
      "published_at": "2026-02-19T18:31:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8413122295329155,
        "semantic_score": 1.919625848531723,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.560938078064638
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17607",
      "summary": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language d",
      "summary_zh": "PDEs 在科學和工程建模中至關重要，但設計精確的數值求解器通常需要大量的數學專業知識和手動調整。最近基於神經網路的方法雖然提高了靈活性，但往往需要高昂的計算成本，並且解釋性有限。我們引入了 \\texttt{AutoNumerics}，這是一個多 Agent 框架，可以直接從自然語言描述中自主設計、實現、偵錯和驗證通用 PDEs 的數值求解器。",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "title_zh": "AutoNumerics：一個用於科學計算的自主、與 PDE 無關的多 Agent 管線"
    },
    {
      "arxiv_id": "2602.17410",
      "authors": [
        "Bingqian Li",
        "Bowen Zheng",
        "Xiaolei Wang",
        "Long Zhang",
        "Jinpeng Wang",
        "Sheng Chen",
        "Wayne Xin Zhao",
        "Ji-rong Wen"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029917+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
          "url": "https://arxiv.org/abs/2602.17410"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
        "url": "https://arxiv.org/abs/2602.17410"
      },
      "published_at": "2026-02-19T14:37:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8277427636986993,
        "semantic_score": 4.096697807312012,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.524440571010707
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17410",
      "summary": "Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we pro",
      "summary_zh": "大型語言模型 (LLM) 在推薦系統中展現了巨大的潛力，其中 supervised fine-tuning (SFT) 常被用於適應。隨後的研究進一步引入 preference learning，將負樣本納入訓練過程。然而，現有方法依賴於序列級別、離線生成的負樣本，這使得它們在將 LLM 適應到具有大型負項空間的推薦任務時，區分性和資訊性不足。為了解決這些挑戰，我們提出",
      "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
      "title_zh": "利用來自中間層的 Self-Hard Negatives 改善基於 LLM 的推薦系統"
    },
    {
      "arxiv_id": "2602.17185",
      "authors": [
        "Uğur Genç",
        "Heng Gu",
        "Chadha Degachi",
        "Evangelos Niforatos",
        "Senthil Chandrasegaran",
        "Himanshu Verma"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761387+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
          "url": "https://arxiv.org/abs/2602.17185"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
        "url": "https://arxiv.org/abs/2602.17185"
      },
      "published_at": "2026-02-19T09:10:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8091560461081025,
        "semantic_score": 3.6725929200649263,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.52174896617303
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17185",
      "summary": "Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessi",
      "summary_zh": "由大型語言模型驅動的對話代理 (CAs) 越來越能夠透過語言投射出複雜的人格，但這些投射如何影響用戶尚不明確。因此，我們在慈善捐贈的背景下，檢視了 CA 透過語言表達的人格如何影響用戶的決策和感知。在一項眾包研究中，360 名參與者與八個 CAs 之一互動，每個 CA 都投射出一種由三個語言面向組成的人格：態度 (optimistic/pessi)",
      "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
      "title_zh": "說服機器人：檢視對話代理的人格語言表達如何影響用戶感知和決策"
    },
    {
      "arxiv_id": "2602.17170",
      "authors": [
        "Chuting Yu",
        "Hang Li",
        "Joel Mackenzie",
        "Teerapong Leelanupab"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030890+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
          "url": "https://arxiv.org/abs/2602.17170"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
        "url": "https://arxiv.org/abs/2602.17170"
      },
      "published_at": "2026-02-19T08:37:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.68,
        "llm_relevance_score": 14.96,
        "recency_score": 0.8072851659110782,
        "semantic_score": 4.4447356402874,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.412020806198477
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17170",
      "summary": "Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across ",
      "summary_zh": "人類相關性評估既耗時又需認知密集，限制了 Information Retrieval 評估的可擴展性。這導致了對使用大型語言模型 (LLM) 作為人類評審代理的興趣日益增加。然而，LLM 基於的相關性判斷是否足夠可靠、穩定和嚴謹以媲美人類進行相關性評估，仍然是一個懸而未決的問題。在這項工作中，我們對 LLM 基於的相關性判斷中的過度評分行為進行了系統性研究，範圍涵蓋",
      "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
      "title_zh": "當 LLM 評審誇大評分時：探索相關性評估中的過度評分現象"
    },
    {
      "arxiv_id": "2602.15950",
      "authors": [
        "Yuval Levental"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:44.781022+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
          "url": "https://arxiv.org/abs/2602.15950"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
        "url": "https://arxiv.org/abs/2602.15950"
      },
      "published_at": "2026-02-17T19:06:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6904580459309506,
        "semantic_score": 2.9072179198265076,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.297675965757456
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15950",
      "summary": "We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text",
      "summary_zh": "我們提出了一個簡單的實驗，揭示了視覺語言模型 (VLMs) 的一個根本性限制：當二元網格中的填充單元格缺乏文字識別性時，它們無法準確定位這些單元格。我們生成了十五個 15x15 的網格，其密度各異 (10.7%-41.8% 填充單元格)，並將每個網格渲染成兩種圖像類型——文字符號 (. 和 #) 和無網格線的實心方塊——然後請三個前沿 VLMs (Claude Opus, ChatGPT 5.2, 和 Gemini 3 Thinking) 進行轉錄。在文字",
      "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
      "title_zh": "視覺語言模型能看到方塊嗎？文字識別調節三種模型家族的空間推理能力"
    },
    {
      "arxiv_id": "2602.17665",
      "authors": [
        "Akashah Shabbir",
        "Muhammad Umer Sheikh",
        "Muhammad Akhtar Munir",
        "Hiyam Debary",
        "Mustansar Fiaz",
        "Muhammad Zaigham Zaheer",
        "Paolo Fraccaro",
        "Fahad Shahbaz Khan",
        "Muhammad Haris Khan",
        "Xiao Xiang Zhu",
        "Salman Khan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.759937+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
          "url": "https://arxiv.org/abs/2602.17665"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
        "url": "https://arxiv.org/abs/2602.17665"
      },
      "published_at": "2026-02-19T18:59:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8429516571733879,
        "semantic_score": 2.729754459857941,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.27270611703133
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17665",
      "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satelli",
      "summary_zh": "多模態推理的最新進展使得代理能夠解釋圖像、將其與語言連接並執行結構化分析任務。將這些能力擴展到遙感領域仍然具有挑戰性，因為模型必須在空間尺度、地理結構和多光譜指數上進行推理，同時保持連貫的多步驟邏輯。為彌補這一差距，OpenEarthAgent 引入了一個統一框架，用於開發基於衛星訓練的工具增強型地理空間代理",
      "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
      "title_zh": "OpenEarthAgent: 一個用於工具增強型地理空間代理的統一框架"
    },
    {
      "arxiv_id": "2602.15650",
      "authors": [
        "Marco Salmè",
        "Federico Siciliano",
        "Fabrizio Silvestri",
        "Paolo Soda",
        "Rosa Sicilia",
        "Valerio Guarrasi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742967+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
          "url": "https://arxiv.org/abs/2602.15650"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
        "url": "https://arxiv.org/abs/2602.15650"
      },
      "published_at": "2026-02-17T15:18:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6796024465732821,
        "semantic_score": 3.9704021215438843,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.250004568117166
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15650",
      "summary": "Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transpare",
      "summary_zh": "透過 Vision-Language Models (VLMs) 進行 Radiology Report Generation (RRG) 有望減輕文件負擔、提高報告一致性並加速臨床工作流程。然而，其臨床應用仍受限於缺乏可解釋性，以及傾向於生成與影像證據不符的「幻覺」結果。現有研究通常將可解釋性與準確性視為獨立目標，其中基於概念的 explainability techniques 主要側重於透明度",
      "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
      "title_zh": "概念增強多模態 RAG：邁向可解釋且準確的放射學報告生成"
    },
    {
      "arxiv_id": "2602.17434",
      "authors": [
        "Eleftherios E. Vlahakis",
        "Arash Bahari Kordabad",
        "Lars Lindemann",
        "Pantelis Sopasakis",
        "Sadegh Soudjani",
        "Dimos V. Dimarogonas"
      ],
      "categories": [
        "eess.SY",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760828+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
          "url": "https://arxiv.org/abs/2602.17434"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
        "url": "https://arxiv.org/abs/2602.17434"
      },
      "published_at": "2026-02-19T15:05:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8293279124209896,
        "semantic_score": 3.787213695049286,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.216541607470273
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17434",
      "summary": "Multi-agent planning under Signal Temporal Logic (STL) is often hindered by collaborative tasks that lead to computational challenges due to the inherent high-dimensionality of the problem, preventing scalable synthesis with satisfaction guarantees. To address this, we formulate STL planning as an optimization program under arbitrary multi-agent constraints and introduce a penalty-based unconstrained relaxation that can be efficiently solved via a Block-Coordinate Gradient Descent (BCGD) method,",
      "summary_zh": "在 Signal Temporal Logic (STL) 下進行的 Multi-agent planning 常常因協作任務而受阻，這些任務由於問題固有的高維性導致計算挑戰，阻礙了具有滿足保證的可擴展合成。為了解決這個問題，我們將 STL planning 表述為在任意 multi-agent constraints 下的優化程式，並引入一種基於懲罰的無約束鬆弛，該鬆弛可以透過 Block-Coordinate Gradient Descent (BCGD) 方法高效求解，",
      "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
      "title_zh": "透過懲罰函數與分塊坐標優化進行多智能體時序邏輯規劃"
    },
    {
      "arxiv_id": "2602.15829",
      "authors": [
        "Tomás Vergara-Browne",
        "Darshan Patil",
        "Ivan Titov",
        "Siva Reddy",
        "Tiago Pimentel",
        "Marius Mosbach"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.972941+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
          "url": "https://arxiv.org/abs/2602.15829"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
        "url": "https://arxiv.org/abs/2602.15829"
      },
      "published_at": "2026-02-17T18:59:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6901384634113481,
        "semantic_score": 3.917252314090729,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.207390777502077
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15829",
      "summary": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SA",
      "summary_zh": "淺層對齊假說 (SAH) 認為 large language models 在 pre-training 期間學習了大部分知識，而 post-training 僅僅是將這些知識顯現出來。然而，SAH 缺乏精確定義，這導致了 (i) 支持它的不同且看似正交的論點，以及 (ii) 對其重要的批判。我們提出了一個名為 task complexity 的新指標：在某個任務上達到目標性能的最短程式的長度。在這個框架下，SAH",
      "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
      "title_zh": "透過任務複雜度操作化淺層對齊假說"
    },
    {
      "arxiv_id": "2602.16849",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.127556+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
          "url": "https://arxiv.org/abs/2602.16849"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
        "url": "https://arxiv.org/abs/2602.16849"
      },
      "published_at": "2026-02-18T20:25:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7672666381500112,
        "semantic_score": 2.732889688014984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.200156326164993
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16849",
      "summary": "We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.",
      "summary_zh": "我們全面分析了兩層 neural networks 如何學習特徵以解決 modular addition 任務。我們的研究為所學模型提供了完整的機制解釋，並對其訓練動態進行了理論性闡述。儘管先前研究已指出個別 neuron 學習 single-frequency Fourier features 和 phase alignment，但未能完全解釋這些特徵如何組合成一個全局解決方案。我們透過形式化一個在過度參數化 (overparametrized) 訓練期間出現的多樣化條件來彌補這一空白，該條件包含兩個部分：phase symmetry 和 frequency diversification。我們證明了這些特性使網路能夠針對 modular addition 任務的正確邏輯，集體近似一個有缺陷的 indicator function。雖然個別 neuron 會產生 noisy signals，但 phase symmetry 使得多數投票機制能夠消除噪音，從而使網路能夠穩健地識別正確的總和。此外，我們透過 lottery ticket mechanism 解釋了這些特徵在隨機初始化下的出現。我們的 gradient flow 分析證明了頻率在每個 neuron 內部競爭，而「贏家」由其初始 spectral magnitude 和 phase alignment 決定。從技術角度來看，我們嚴格刻畫了 layer-wise phase coupling dynamics，並使用 ODE comparison lemma 將競爭格局形式化。最後，我們利用這些見解來揭示 grokking 的奧秘，將其描述為一個三階段過程，涉及 memorization 之後的兩個 generalization phases，由 loss minimization 和 weight decay 之間的競爭驅動。",
      "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
      "title_zh": "關於模加法機制與動態：Fourier Features、Lottery Ticket 與 Grokking"
    },
    {
      "arxiv_id": "2602.15676",
      "authors": [
        "Deniz Kucukahmetler",
        "Maximilian Jean Hemmann",
        "Julian Mosig von Aehrenfeld",
        "Maximilian Amthor",
        "Christian Deubel",
        "Nico Scherf",
        "Diaaeldin Taha"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.973153+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
          "url": "https://arxiv.org/abs/2602.15676"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
        "url": "https://arxiv.org/abs/2602.15676"
      },
      "published_at": "2026-02-17T16:00:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6815883027332341,
        "semantic_score": 2.6634727716445923,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.045061074377827
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15676",
      "summary": "Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level st",
      "summary_zh": "Neural networks 能夠準確預測複雜的 dynamical systems，但它們內部如何表示底層 latent geometry 仍知之甚少。我們透過 representational alignment 的視角研究 neural forecasters，引入了基於 anchor、geometry-agnostic 的 relative embeddings，這些 embeddings 消除了 latent spaces 中的旋轉和縮放模糊性。將此框架應用於七個經典的 dynamical systems (從週期性到混沌)，我們揭示了可重現的家族級別的",
      "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
      "title_zh": "神經預報器的相對幾何：連結學習到的潛在幾何中的準確性與對齊"
    },
    {
      "arxiv_id": "2602.16968",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128377+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
          "url": "https://arxiv.org/abs/2602.16968"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
        "url": "https://arxiv.org/abs/2602.16968"
      },
      "published_at": "2026-02-19T00:15:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.68,
        "llm_relevance_score": 14.96,
        "recency_score": 0.7796263003590749,
        "semantic_score": 2.0888734340667723,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.02849973442585
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16968",
      "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers"
    },
    {
      "arxiv_id": "2602.15769",
      "authors": [
        "Yahia Alqurnawi",
        "Preetom Biswas",
        "Anmol Rao",
        "Tejas Anvekar",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742724+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
          "url": "https://arxiv.org/abs/2602.15769"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
        "url": "https://arxiv.org/abs/2602.15769"
      },
      "published_at": "2026-02-17T18:01:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.6873611473575992,
        "semantic_score": 4.727681910991668,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.91504305834927
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15769",
      "summary": "Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our resu",
      "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution"
    },
    {
      "arxiv_id": "2602.17308",
      "authors": [
        "Hui Min Wong",
        "Philip Heesen",
        "Pascal Janetzky",
        "Martin Bendszus",
        "Stefan Feuerriegel"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.760991+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
          "url": "https://arxiv.org/abs/2602.17308"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
        "url": "https://arxiv.org/abs/2602.17308"
      },
      "published_at": "2026-02-19T12:19:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8198187024927965,
        "semantic_score": 3.4760083079338076,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.895827010426604
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17308",
      "summary": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that dem",
      "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions"
    },
    {
      "arxiv_id": "2602.17084",
      "authors": [
        "Kan Watanabe",
        "Rikuto Tsuchida",
        "Takahiro Monno",
        "Bin Huang",
        "Kazuma Yamasaki",
        "Youmei Fan",
        "Kazumasa Shimari",
        "Kenichi Matsumoto"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.761898+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses",
          "url": "https://arxiv.org/abs/2602.17084"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses",
        "url": "https://arxiv.org/abs/2602.17084"
      },
      "published_at": "2026-02-19T05:06:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7955516453858593,
        "semantic_score": 3.3556383073329927,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.75118995271885
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17084",
      "summary": "The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including struct",
      "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses"
    },
    {
      "arxiv_id": "2602.16951",
      "authors": [
        "Mingzhe Cui",
        "Tao Chen",
        "Yang Jiao",
        "Yiqin Wang",
        "Lei Xie",
        "Yi Pan",
        "Luca Mainardi"
      ],
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-21T11:48:43.031379+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression",
          "url": "https://arxiv.org/abs/2602.16951"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression",
        "url": "https://arxiv.org/abs/2602.16951"
      },
      "published_at": "2026-02-18T23:30:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.7772081635877139,
        "semantic_score": 3.4908538818359376,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.668062045423653
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16951",
      "summary": "Developing foundation models for electroencephalography (EEG) remains challenging due to the signal's low signal-to-noise ratio and complex spectro-temporal non-stationarity. Existing approaches often overlook the hierarchical latent structure inherent in neural dynamics, leading to suboptimal reconstruction of fine-grained information. In this work, we propose BrainRVQ, a general-purpose EEG foundation model pre-trained on a large-scale corpus of clinical EEG data. Unlike standard masked modeli",
      "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression"
    },
    {
      "arxiv_id": "2602.17584",
      "authors": [
        "Sharut Gupta",
        "Sanyam Kansal",
        "Stefanie Jegelka",
        "Phillip Isola",
        "Vikas Garg"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.733145+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Canonicalizing Multimodal Contrastive Representation Learning",
          "url": "https://arxiv.org/abs/2602.17584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Canonicalizing Multimodal Contrastive Representation Learning",
        "url": "https://arxiv.org/abs/2602.17584"
      },
      "published_at": "2026-02-19T18:09:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8400123168324682,
        "semantic_score": 3.418024110794067,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 27.658036427626534
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17584",
      "summary": "As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\\widetilde{f},\\widetilde{g})$) ",
      "summary_zh": "隨著模型和資料規模的擴大，獨立訓練的網路通常會產生類似的相似性概念。然而，匹配相似性弱於在表示空間之間建立明確的對應關係，特別是對於 Multimodal models 而言，一致性不僅必須在每個模態內部保持，也必須在學習到的 Image-text coupling 中保持。因此，我們提出疑問：給定兩個獨立訓練的 Multimodal contrastive models（帶有 Encoders $(f, g)$ 和 $(\\widetilde{f},\\widetilde{g})$）",
      "title": "Canonicalizing Multimodal Contrastive Representation Learning",
      "title_zh": "Canonicalizing Multimodal Contrastive Representation Learning"
    },
    {
      "arxiv_id": "2602.16844",
      "authors": [
        "Madeleine Grunde-McLaughlin",
        "Hussein Mozannar",
        "Maya Murad",
        "Jingya Chen",
        "Saleema Amershi",
        "Adam Fourney"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763416+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
          "url": "https://arxiv.org/abs/2602.16844"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
        "url": "https://arxiv.org/abs/2602.16844"
      },
      "published_at": "2026-02-18T20:16:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.766797008726489,
        "semantic_score": 3.110388898849487,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.477185907575976
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16844",
      "summary": "To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are ",
      "summary_zh": "為了實現 Human oversight，Agentic AI systems 通常會提供推理和行動步驟的 trace。設計 trace 以便提供豐富但不過量的細節層級仍然是一項關鍵挑戰。在針對 Computer User Agent 進行的三項使用者研究中，我們調查了基本 action traces 對於驗證的實用性，透過 design probes 探索了三種替代方案，並測試了一個 novel interface 在 Question-answering tasks 中尋找錯誤的影響。正如預期的那樣，我們發現目前的做法是",
      "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
      "title_zh": "無需持續監督的代理監管：挑戰與機遇"
    },
    {
      "arxiv_id": "2602.16703",
      "authors": [
        "Shen Zhou Hong",
        "Alex Kleinman",
        "Alyssa Mathiowetz",
        "Adam Howes",
        "Julian Cohen",
        "Suveer Ganta",
        "Alex Letizia",
        "Dora Liao",
        "Deepika Pahari",
        "Xavier Roberts-Gaal",
        "Luca Righetti",
        "Joe Torres"
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.032823+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
          "url": "https://arxiv.org/abs/2602.16703"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
        "url": "https://arxiv.org/abs/2602.16703"
      },
      "published_at": "2026-02-18T18:51:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7622876379107418,
        "semantic_score": 3.080261766910553,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.442549404821293
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16703",
      "summary": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics work",
      "summary_zh": "Large language models (LLMs) 在生物學基準測試中表現出色，引發了人們對它們可能幫助新手習得 Dual-use laboratory skills 的擔憂。然而，這是否能轉化為 Physical laboratory 中人類表現的提高仍不明朗。為了解決這個問題，我們進行了一項 Pre-registered、Investigator-blinded、Randomized controlled trial（2025年6月至8月；n = 153），評估 LLMs 是否能在共同模擬 Viral reverse genetics work 的任務中改善新手表現。",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "title_zh": "衡量2025年中期 LLM-Assistance 對生物學新手表現的影響"
    },
    {
      "arxiv_id": "2602.17162",
      "authors": [
        "Ariel Larey",
        "Elay Dahan",
        "Amit Bleiweiss",
        "Raizy Kellerman",
        "Guy Leib",
        "Omri Nayshool",
        "Dan Ofer",
        "Tal Zinger",
        "Dan Dominissini",
        "Gideon Rechavi",
        "Nicole Bussola",
        "Simon Lee",
        "Shane O'Connell",
        "Dung Hoang",
        "Marissa Wirth",
        "Alexander W. Charney",
        "Nati Daniel",
        "Yoli Shavit"
      ],
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.031054+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
          "url": "https://arxiv.org/abs/2602.17162"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
        "url": "https://arxiv.org/abs/2602.17162"
      },
      "published_at": "2026-02-19T08:20:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8063606814121982,
        "semantic_score": 4.097121894359589,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.40348257577179
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17162",
      "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with",
      "summary_zh": "Genomic Foundation Models (GFMs) 主要依賴 Masked Language Modeling (MLM) 或 Next Token Prediction (NTP) 來學習生命語言。儘管這些範式擅長捕捉局部基因組語法和 Fine-grained motif patterns，但它們通常未能捕捉更廣泛的 Functional context，導致生成的 Representation 缺乏全球生物學視角。我們引入 JEPA-DNA，這是一個新穎的 Pre-training framework，它將 Joint-Embedding Predictive Architecture (JEPA) 與...",
      "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
      "title_zh": "JEPA-DNA：透過 Joint-Embedding Predictive Architectures 奠基 Genomic Foundation Models"
    },
    {
      "arxiv_id": "2602.17038",
      "authors": [
        "Shengtian Yang",
        "Yu Li",
        "Shuo He",
        "Yewen Li",
        "Qingpeng Cai",
        "Peng Jiang",
        "Lei Feng"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762305+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17038"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17038"
      },
      "published_at": "2026-02-19T03:18:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7896064132061514,
        "semantic_score": 2.875776356458664,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.26538276966481
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17038",
      "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different ",
      "summary_zh": "Reinforcement learning (RL) 賦予 LLM agents 解決複雜任務的強大能力。然而，現有的 RL 方法通常使用 single policy network，導致 simplicity bias，即簡單任務佔用大部分參數並主導 Gradient updates，為複雜任務留下了不足的容量。一個可行的補救措施是在 policy network 中採用 Mixture-of-Experts (MoE) 架構，因為 MoE 允許不同的參數（Experts）專注於不同的...",
      "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
      "title_zh": "Agentic Reinforcement Learning 的 Phase-Aware Mixture of Experts"
    },
    {
      "arxiv_id": "2602.17062",
      "authors": [
        "Yonghyeon Jo",
        "Sunwoo Lee",
        "Seungyul Han"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762148+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17062"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17062"
      },
      "published_at": "2026-02-19T04:07:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.792320770553801,
        "semantic_score": 3.8135067522525787,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.10582752280638
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17062",
      "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax",
      "summary_zh": "值分解是合作式 multi-agent reinforcement learning (MARL) 的核心方法。然而，現有方法仍依賴於單一的最優行動，並且在訓練期間底層 value function 發生變化時難以適應，常常收斂到 suboptimal policies。為了解決這一限制，我們提出了 Successive Sub-value Q-learning (S2Q)，它學習多個 sub-value functions 以保留替代性的高價值行動。將這些 sub-value functions 整合到一個 Softmax",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
      "title_zh": "保留次優行動以追蹤多代理強化學習中變化的最優值"
    },
    {
      "arxiv_id": "2602.17108",
      "authors": [
        "Anton Dzega",
        "Aviad Elyashar",
        "Ortal Slobodin",
        "Odeya Cohen",
        "Rami Puzis"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.739893+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
          "url": "https://arxiv.org/abs/2602.17108"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
        "url": "https://arxiv.org/abs/2602.17108"
      },
      "published_at": "2026-02-19T06:08:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7989861711816523,
        "semantic_score": 2.7184711396694183,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 26.91745731085107
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17108",
      "summary": "Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the So",
      "summary_zh": "Thematic Apperception Test (TAT) 是一種基於心理計量學的多維評估框架，它系統地將類個性化功能中的認知表徵（cognitive-representational）和情感關係（affective-relational）成分區分開來。這項測試是一個投射性心理框架，旨在揭示人格的潛意識層面。本研究探討是否可以使用非語言模態（non-language-based modalities）來評估 Large Multimodal Models (LMMs) 的人格特質，採用 So",
      "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
      "title_zh": "使用 Thematic Apperception Tests 對大型多模態模型進行投射性心理評估"
    },
    {
      "arxiv_id": "2602.17475",
      "authors": [
        "Pietro Ferrazzi",
        "Mattia Franzin",
        "Alberto Lavelli",
        "Bernardo Magnini"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029531+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
          "url": "https://arxiv.org/abs/2602.17475"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
        "url": "https://arxiv.org/abs/2602.17475"
      },
      "published_at": "2026-02-19T15:38:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8312594975996814,
        "semantic_score": 3.483192265033722,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.814451762633404
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17475",
      "summary": "Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Reco",
      "summary_zh": "Large Language Models (LLMs) 在多種醫學 Natural Language Processing (NLP) 任務中始終表現出色，但其龐大的計算需求常常限制了在實際醫療場景中的部署。在這項工作中，我們調查「小型」LLMs（約十億參數）是否能有效執行醫學任務，同時保持具競爭力的準確性。我們評估了來自三個主要系列的模型——Llama-3、Gemma-3 和 Qwen3——在 20 項臨床 NLP 任務中，包括 Named Entity Reco",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
      "title_zh": "用於醫學 NLP 的小型 LLMs：對義大利語 Few-Shot、Constraint Decoding、Fine-Tuning 和 Continual Pre-Training 的系統分析"
    },
    {
      "arxiv_id": "2602.17229",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.030571+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
          "url": "https://arxiv.org/abs/2602.17229"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
        "url": "https://arxiv.org/abs/2602.17229"
      },
      "published_at": "2026-02-19T10:19:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.813007739107345,
        "semantic_score": 3.383018034696579,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.696025773803925
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17229",
      "summary": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residu",
      "summary_zh": "Large Language Models 的黑箱性質要求提出超越表面性能指標的新穎評估框架。本研究以 Bloom's Taxonomy 作為分層視角，探討認知複雜度的內部神經表徵。透過分析不同 LLMs 的高維 activation vectors，我們探究從基本回憶（Remember）到抽象綜合（Create）等不同認知層次是否在模型內部是線性可分離的",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "title_zh": "基於 Bloom's Taxonomy 的線性探測法對 LLMs 認知複雜度的機制可解釋性研究"
    },
    {
      "arxiv_id": "2602.17068",
      "authors": [
        "Xiaocai Zhang",
        "Neema Nassir",
        "Milad Haghani"
      ],
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.741595+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
          "url": "https://arxiv.org/abs/2602.17068"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
        "url": "https://arxiv.org/abs/2602.17068"
      },
      "published_at": "2026-02-19T04:18:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7929216581354342,
        "semantic_score": 3.275314301252365,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.5682359593878
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17068",
      "summary": "Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures sp",
      "summary_zh": "走廊網絡中以人為本的交通信號控制必須越來越多地考慮多模態旅客，特別是高載客量的公共交通，而不是僅僅關注以車輛為中心的性能。本文提出 STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning)，這是一個可擴展的 multi-agent deep reinforcement learning 框架，遵循集中訓練和分散執行（centralized training and decentralized execution）範式。所提出的方法捕獲 sp",
      "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
      "title_zh": "用於以人為中心多模態走廊交通信號控制的時空雙階段超圖 MARL"
    },
    {
      "arxiv_id": "2602.16653",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.763971+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
          "url": "https://arxiv.org/abs/2602.16653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
        "url": "https://arxiv.org/abs/2602.16653"
      },
      "published_at": "2026-02-18T17:52:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7591611003733372,
        "semantic_score": 3.160517376661301,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.419678477034637
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16653",
      "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on pu",
      "summary_zh": "Agent Skill framework 現已獲得 GitHub Copilot、LangChain 和 OpenAI 等主要參與者的廣泛和官方支持，透過改進 context engineering、減少 hallucinations 並提高 task accuracy，在 proprietary models 方面表現尤為出色。基於這些觀察，本研究旨在探討 Agent Skill paradigm 是否為 small language models (SLMs) 帶來類似的益處。這個問題在工業場景中至關重要，因為在這些場景中持續依賴...",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "title_zh": "Agent Skill Framework：小型語言模型在工業環境中潛力的視角"
    },
    {
      "arxiv_id": "2602.15571",
      "authors": [
        "Davide Casnici",
        "Martin Lefebvre",
        "Justin Dauwels",
        "Charlotte Frenkel"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.973435+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
          "url": "https://arxiv.org/abs/2602.15571"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
        "url": "https://arxiv.org/abs/2602.15571"
      },
      "published_at": "2026-02-17T13:29:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.6744831185355193,
        "semantic_score": 3.6325578093528748,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.707040927888393
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15571",
      "summary": "Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which",
      "summary_zh": "Predictive coding (PC) 是一種受生物學啟發的演算法，用於訓練 neural networks，它僅依賴 local updates，允許跨層次的 parallel learning。然而，實際應用面臨兩個主要限制：錯誤訊號仍必須透過多個 inference-phase 步驟從輸出層傳播到早期層，並且在這個過程中 feedback 會呈指數衰減，導致早期層的 vanishing updates。我們提出了 direct Kolen-Pollack predictive coding (DKP-PC)，其...",
      "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
      "title_zh": "透過 Direct Kolen-Pollack Feedback Alignment 加速 Predictive Coding Networks"
    },
    {
      "arxiv_id": "2602.16157",
      "authors": [
        "Xinyue Gui",
        "Ding Xia",
        "Mark Colley",
        "Yuan Li",
        "Vishal Chauhan",
        "Anubhav Anubhav",
        "Zhongyi Zhou",
        "Ehsan Javanmardi",
        "Stela Hanbyeol Seo",
        "Chia-Ming Chang",
        "Manabu Tsukada",
        "Takeo Igarashi"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742137+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
          "url": "https://arxiv.org/abs/2602.16157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
        "url": "https://arxiv.org/abs/2602.16157"
      },
      "published_at": "2026-02-18T03:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.7141603570635002,
        "semantic_score": 3.405628204345703,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.519788561409204
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16157",
      "summary": "Field studies are irreplaceable but costly, time-consuming, and error-prone, which need careful preparation. Inspired by rapid-prototyping in manufacturing, we propose a fast, low-cost evaluation method using Vision-Language Model (VLM) personas to simulate outcomes comparable to field results. While LLMs show human-like reasoning and language capabilities, autonomous vehicle (AV)-pedestrian interaction requires spatial awareness, emotional empathy, and behavioral generation. This raises our res",
      "summary_zh": "Field studies 不可替代但成本高昂、耗時且易出錯，需要仔細準備。受製造業中 rapid-prototyping 的啟發，我們提出了一種快速、低成本的評估方法，利用 Vision-Language Model (VLM) personas 來模擬與 field results 相當的結果。儘管 LLMs 展現出類似人類的推理和語言能力，但 autonomous vehicle (AV)-pedestrian interaction 需要空間意識、情感同理心和行為生成。這引發了我們的研究...",
      "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
      "title_zh": "搶先於實地研究：探索 VLM Personas 作為 HCI 中具身研究的輔助工具"
    },
    {
      "arxiv_id": "2602.17067",
      "authors": [
        "Leixian Shen",
        "Yan Luo",
        "Rui Sheng",
        "Yujia He",
        "Haotian Li",
        "Leni Yang",
        "Huamin Qu"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.762063+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
          "url": "https://arxiv.org/abs/2602.17067"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
        "url": "https://arxiv.org/abs/2602.17067"
      },
      "published_at": "2026-02-19T04:16:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.7927941034534982,
        "semantic_score": 3.18511426448822,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.377908367941718
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17067",
      "summary": "Personalized feedback plays an important role in self-regulated learning (SRL), helping students track progress and refine their strategies. However, current common solutions, such as text-based reports or learning analytics dashboards, often suffer from poor interpretability, monotonous presentation, and limited explainability. To overcome these challenges, we present StoryLensEdu, a narrative-driven multi-agent system that automatically generates intuitive, engaging, and interactive learning r",
      "summary_zh": "Personalized feedback 在 self-regulated learning (SRL) 中扮演著重要角色，幫助學生追蹤進度並改進策略。然而，目前常見的解決方案，例如 text-based reports 或 learning analytics dashboards，往往存在 interpretability 差、呈現單調和 explainability 有限的問題。為了解決這些挑戰，我們提出了 StoryLensEdu，這是一個 narrative-driven multi-agent system，能夠自動生成直觀、引人入勝且互動性強的學習報告...",
      "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
      "title_zh": "StoryLensEdu：透過 Narrative-Driven Multi-Agent Systems 生成個性化學習報告"
    },
    {
      "arxiv_id": "2602.17599",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.971311+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
          "url": "https://arxiv.org/abs/2602.17599"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
        "url": "https://arxiv.org/abs/2602.17599"
      },
      "published_at": "2026-02-19T18:23:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8408508028793478,
        "semantic_score": 3.010554474592209,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.251405277471555
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17599",
      "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifi",
      "summary_zh": "透過 multimodal deep learning，音樂生成取得了顯著進展，使模型能夠從 text 以及最近從 images 合成 audio。然而，現有的 image-conditioned systems 存在兩個基本限制：(i) 它們通常在 natural photographs 上進行訓練，限制了其捕捉藝術品更豐富的 semantic、stylistic 和 cultural content 的能力；(ii) 大多數依賴 image-to-text conversion stage，將 language 作為一種簡化的 semantic shortcut...",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "title_zh": "Art2Mus：透過 Visual Conditioning 和 Large-Scale Cross-Modal Alignment 進行藝術品到音樂生成"
    },
    {
      "arxiv_id": "2602.17354",
      "authors": [
        "Daniele Malitesta",
        "Emanuele Rossi",
        "Claudio Pomo",
        "Tommaso Di Noia",
        "Fragkiskos D. Malliaros"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.733548+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
          "url": "https://arxiv.org/abs/2602.17354"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
        "url": "https://arxiv.org/abs/2602.17354"
      },
      "published_at": "2026-02-19T13:37:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8242628496140504,
        "semantic_score": 2.9275637507438663,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.151826600357914
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17354",
      "summary": "Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisati",
      "summary_zh": "多模態推薦系統 (RSs) 透過多模態數據（例如，產品圖片和描述）來表示目錄中的項目，這些數據在某些情況下可能帶有雜訊，甚至更糟的是，可能缺失。在這些情況下，普遍的做法是捨棄帶有缺失模態的項目，並在原始數據集的子樣本上訓練多模態 RSs。迄今為止，多模態推薦中缺失模態的問題在文獻中仍然受到有限的關注，缺乏精確的形式化",
      "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
      "title_zh": "免訓練、基於圖的多模態推薦中缺失模態的補齊"
    },
    {
      "arxiv_id": "2602.17481",
      "authors": [
        "Yanni Mei",
        "Samuel Wendt",
        "Florian Mueller",
        "Jan Gugenheimer"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:43.029440+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
          "url": "https://arxiv.org/abs/2602.17481"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
        "url": "https://arxiv.org/abs/2602.17481"
      },
      "published_at": "2026-02-19T15:50:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8319390219570144,
        "semantic_score": 2.7950588941574095,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.026997916114425
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17481",
      "summary": "Augmented Reality (AR) can simulate various visual perceptions, such as how individuals with colorblindness see the world. However, these simulations require developers to predefine each visual effect, limiting flexibility. We present ShadAR, an AR application enabling real-time transformation of visual perception through shader generation using large language models (LLMs). ShadAR allows users to express their visual intent via natural language, which is interpreted by an LLM to generate corres",
      "summary_zh": "擴增實境 (AR) 可以模擬各種視覺感知，例如色盲人士如何看待世界。然而，這些模擬要求開發者預定義每個視覺效果，從而限制了靈活性。我們提出了 ShadAR，這是一個 AR 應用程式，它透過使用大型語言模型 (LLMs) 的著色器生成來實現視覺感知的即時轉換。ShadAR 允許用戶透過自然語言表達他們的視覺意圖，LLM 會解釋這些意圖以生成相應的",
      "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
      "title_zh": "ShadAR: LLM 驅動的著色器生成，以轉換擴增實境中的視覺感知"
    },
    {
      "arxiv_id": "2602.16915",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:51.127714+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
          "url": "https://arxiv.org/abs/2602.16915"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
        "url": "https://arxiv.org/abs/2602.16915"
      },
      "published_at": "2026-02-18T22:12:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.62,
        "llm_relevance_score": 13.64,
        "recency_score": 0.7729846163947065,
        "semantic_score": 1.905507707595825,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.518492323990532
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16915",
      "summary": "Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
      "summary_zh": "立體深度估計是水下機器人感知的重要基礎，但卻受到波長相關光衰減、散射和折射引起的嚴重領域偏移的困擾。最近的方法利用單目基礎模型與基於 GRU 的迭代細化來進行水下適應；然而，GRU 中的序列門控和局部卷積核需要多次迭代才能進行長距離視差傳播，這限制了在大視差和無紋理水下區域的性能。在本文中，我們提出了 StereoAdapter-2，它用一種基於選擇性狀態空間模型的新穎 ConvSS2D 算子取代了傳統的 ConvGRU 更新器。所提出的算子採用一種四向掃描策略，自然地與對極幾何對齊，同時捕捉垂直結構一致性，實現了在單一更新步驟內以線性計算複雜度進行高效的長距離空間傳播。此外，我們構建了 UW-StereoDepth-80K，這是一個大規模合成水下立體數據集，透過結合語義感知風格遷移和幾何一致的新穎視圖合成的兩階段生成管線，其特點是多樣的基線、衰減係數和散射參數。結合從 StereoAdapter 繼承的動態 LoRA 適應，我們的框架在水下基準測試上實現了最先進的零樣本性能，TartanAir-UW 提高了 17%，SQUID 提高了 7.2%，在 BlueROV2 平台上的真實世界驗證證明了我們方法的穩健性。代碼：https://github.com/AIGeeksGroup/StereoAdapter-2。網站：https://aigeeksgroup.github.io/StereoAdapter-2。",
      "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
      "title_zh": "StereoAdapter-2: 全局結構一致的水下立體深度估計"
    },
    {
      "arxiv_id": "2602.16144",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Ziming Wang",
        "Chunlei Meng",
        "Jiaxuan Lu",
        "Jiekai Wu",
        "Kangan Qian",
        "Hao Zhang",
        "Simon Fong"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742211+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
          "url": "https://arxiv.org/abs/2602.16144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
        "url": "https://arxiv.org/abs/2602.16144"
      },
      "published_at": "2026-02-18T02:29:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.5,
        "llm_relevance_score": 11.0,
        "recency_score": 0.7120408651408197,
        "semantic_score": 3.901291477680206,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.813332342821028
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16144",
      "summary": "As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request",
      "summary_zh": "隨著多模態系統越來越多地處理敏感個人數據，選擇性撤銷特定數據模態的能力已成為隱私合規性和用戶自主權的關鍵要求。我們提出了 Missing-by-Design (MBD)，這是一個用於可撤銷多模態情感分析的統一框架，它將結構化表示學習與可證明參數修改管線相結合。可撤銷性在隱私敏感應用中至關重要，在這些應用中，用戶或監管機構可能會要求",
      "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
      "title_zh": "Missing-by-Design: 可撤銷多模態情感分析中可證明模態刪除"
    },
    {
      "arxiv_id": "2602.16245",
      "authors": [
        "J. Dhar",
        "M. K. Pandey",
        "D. Chakladar",
        "M. Haghighat",
        "A. Alavi",
        "S. Mistry",
        "N. Zaidi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:46.742059+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
          "url": "https://arxiv.org/abs/2602.16245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
        "url": "https://arxiv.org/abs/2602.16245"
      },
      "published_at": "2026-02-18T07:47:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.4,
        "llm_relevance_score": 8.8,
        "recency_score": 0.7279534883143096,
        "semantic_score": 2.5505406498908996,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 20.27849413820521
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16245",
      "summary": "Multimodal fusion frameworks, which integrate diverse medical imaging modalities (e.g., MRI, CT), have shown great potential in applications such as skin cancer detection, dementia diagnosis, and brain tumor prediction. However, existing multimodal fusion methods face significant challenges. First, they often rely on computationally expensive models, limiting their applicability in low-resource environments. Second, they often employ cascaded attention modules, which potentially increase risk of",
      "summary_zh": "多模態融合框架，整合了多樣的醫學影像模態（例如，MRI, CT），在皮膚癌檢測、失智症診斷和腦腫瘤預測等應用中顯示出巨大潛力。然而，現有多模態融合方法面臨重大挑戰。首先，它們通常依賴於計算成本高昂的模型，限制了它們在低資源環境中的適用性。其次，它們通常採用級聯注意力模塊，這可能增加",
      "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
      "title_zh": "HyPCA-Net: 推進醫學圖像分析中的多模態融合"
    },
    {
      "arxiv_id": "2602.17653",
      "authors": [
        "Iskar Deng",
        "Nathalia Xu",
        "Shane Steinert-Threlkeld"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.971065+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
          "url": "https://arxiv.org/abs/2602.17653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
        "url": "https://arxiv.org/abs/2602.17653"
      },
      "published_at": "2026-02-19T18:56:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 4.4,
        "recency_score": 0.84275655205745,
        "semantic_score": 4.6226924657821655,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 18.06544901783962
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17653",
      "summary": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing ",
      "summary_zh": "近期研究表明，在 synthetic corpora 上訓練的 Language Models (LMs) 能夠展現出與人類語言中的跨語言規律相似的類型學偏好，特別是對於語法現象（例如 word order）。在本文中，我們將此範式擴展到 Differential Argument Marking (DAM)，這是一種語義許可系統，其中形態標記取決於語義顯著性。透過受控的 synthetic learning 方法，我們在 18 個實現",
      "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
      "title_zh": "語言模型處理 Differential Argument Marking 時類型學對齊的差異"
    },
    {
      "arxiv_id": "2602.15584",
      "authors": [
        "Flavien Armangeon",
        "Thibaud Ehret",
        "Enric Meinhardt-Llopis",
        "Rafael Grompone von Gioi",
        "Guillaume Thibault",
        "Marc Petit",
        "Gabriele Facciolo"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.973326+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment",
          "url": "https://arxiv.org/abs/2602.15584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment",
        "url": "https://arxiv.org/abs/2602.15584"
      },
      "published_at": "2026-02-17T13:55:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 4.4,
        "recency_score": 0.6757153305511898,
        "semantic_score": 3.1943281412124636,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 16.470043471763653
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15584",
      "summary": "Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dat",
      "summary_zh": "將 functional schematics 與 2D 和 3D 場景採集進行對齊，對於建立 digital twins 至關重要，特別是對於缺乏原生數位模型的老舊工業設施。目前使用圖像和 LiDAR 資料進行的手動對齊，由於工業現場的繁瑣和複雜性，難以擴展。schematics 和現實之間的不一致性，以及公共工業資料集的稀缺性，使得這個問題既具挑戰性又未被充分探索。本文介紹了 IRIS-v2，一個全面的資料",
      "title": "An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment",
      "title_zh": "用於場景採集與功能圖對齊的工業資料集"
    },
    {
      "arxiv_id": "2602.17636",
      "authors": [
        "Jiyoung Kim",
        "Youngjin Shin",
        "Siyoon Jin",
        "Dahyun Chung",
        "Jisu Nam",
        "Tongmin Kim",
        "Jongjae Park",
        "Hyeonwoo Kang",
        "Seungryong Kim"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:45.971207+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
          "url": "https://arxiv.org/abs/2602.17636"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
        "url": "https://arxiv.org/abs/2602.17636"
      },
      "published_at": "2026-02-19T18:50:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.15,
        "llm_relevance_score": 3.3,
        "recency_score": 0.8423840267735222,
        "semantic_score": 3.0141709566116335,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.356554983385156
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17636",
      "summary": "Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garme",
      "summary_zh": "現有的 Virtual Try-On (VTON) 方法常常難以保留精細的服裝細節，尤其是在需要精確人服對應的 unpaired 設定中。這些方法沒有明確地強制執行人服對齊，也未能解釋 correspondence 如何在 Diffusion Transformers (DiTs) 中產生。在本文中，我們首先分析了 DiT-based architecture 中的 full 3D attention，並揭示了人服 correspondence 關鍵取決於精確的 person-garment",
      "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
      "title_zh": "CORAL：用於改進虛擬試穿的對應對齊"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Yunfei Bai"
      ],
      "categories": [
        "Amazon Bedrock AgentCore",
        "Artificial Intelligence",
        "Best Practices",
        "Generative AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:38.119359+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
          "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
        "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
      },
      "published_at": "2026-02-18T11:21:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7388345128935605,
        "semantic_score": 3.480947208404541,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 28.1197817212981
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:328a53729df06635",
      "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon&nbsp;through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with&nbsp;Amazon use case-specific evaluation approaches and metrics.&nbsp;",
      "summary_zh": "在這篇文章中，我們提出了一個針對 Amazon agentic AI 系統的綜合評估框架，該框架透過兩個核心組成部分解決了 Amazon agentic AI 應用程式的複雜性：一個通用評估 workflow，用於標準化不同 agent 實現的評估程序；以及一個 agent 評估 library，它在 Amazon Bedrock AgentCore Evaluations 中提供了系統性的測量和指標，並提供了 Amazon use case-specific 的評估方法和指標。",
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
      "title_zh": "評估 AI agents：從在 Amazon 建立 agentic 系統中學到的實際經驗"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Eric Horvitz, Andrew Jenks, Jessica Young"
      ],
      "categories": [
        "Research Blog"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:20.201254+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "microsoft-research-blog",
          "tier": 0,
          "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
          "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "microsoft-research-blog",
        "tier": 0,
        "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
        "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions"
      },
      "published_at": "2026-02-19T08:00:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8052415134012629,
        "semantic_score": 2.031795692443848,
        "tier_score": 3.0,
        "topic_score": 1.9500000000000002,
        "total_score": 22.48703720584511
      },
      "section": null,
      "source_name": "Microsoft Research Blog",
      "story_id": "fallback:70b04b512e5b8d61",
      "summary": "<p>As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/\">Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
      "summary_zh": "隨著 synthetic media 的增長，驗證內容的真實性及其來源比以往任何時候都更加重要。我們的最新報告探討了媒體完整性和 authentication 方法、它們的限制，以及在圖像、音訊和視訊中實現可靠 provenance 的實踐路徑。",
      "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
      "title_zh": "實踐中的媒體真實性方法：能力、限制與方向"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Monica Jain"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence",
        "Technical How-to"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-21T11:48:38.119275+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build unified intelligence with Amazon Bedrock AgentCore",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build unified intelligence with Amazon Bedrock AgentCore",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore"
      },
      "published_at": "2026-02-18T15:54:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.4,
        "llm_relevance_score": 8.8,
        "recency_score": 0.7529760737854844,
        "semantic_score": 2.648164337873459,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 22.451140411658944
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:7033c681de64957b",
      "summary": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
      "summary_zh": "在這篇文章中，我們將透過我們對 Customer Agent and Knowledge Engine (CAKE) 的實際實作，展示如何使用 Amazon Bedrock AgentCore 建構統一智慧系統。",
      "title": "Build unified intelligence with Amazon Bedrock AgentCore",
      "title_zh": "使用 Amazon Bedrock AgentCore 建構統一智慧"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Vignessh Baskaran"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Quick Sight",
        "Amazon Quick Suite"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:38.119196+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source"
      },
      "published_at": "2026-02-19T08:06:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.5,
        "llm_relevance_score": 11.0,
        "recency_score": 0.8055677768525819,
        "semantic_score": 2.7857662439346313,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 19.091334020787215
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9c836f2795128c3a",
      "summary": "In this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.",
      "summary_zh": "在這篇部落格文章中，我們將引導您透過安全的金鑰對認證，建立 Amazon Quick Sight 與 Snowflake 之間的資料來源連線。",
      "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
      "title_zh": "Amazon QuickSight 現已支援透過金鑰對認證連接 Snowflake 資料來源"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Myriam Hamed Torres"
      ],
      "categories": [
        "Gemini App",
        "AI"
      ],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-21T11:48:17.811577+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 2,
      "links": [
        {
          "link_type": "blog",
          "source_id": "deepmind-blog",
          "tier": 0,
          "title": "A new way to express yourself: Gemini can now create music",
          "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music"
        },
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "A new way to express yourself: Gemini can now create music",
          "url": "https://blog.google/innovation-and-ai/products/gemini-app/lyria-3"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "deepmind-blog",
        "tier": 0,
        "title": "A new way to express yourself: Gemini can now create music",
        "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music"
      },
      "published_at": "2026-02-18T08:01:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.15,
        "llm_relevance_score": 3.3,
        "recency_score": 0.7286522881384067,
        "semantic_score": 3.050578773021698,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 17.579231061160105
      },
      "section": null,
      "source_name": "DeepMind Blog",
      "story_id": "fallback:7a4f73fdfe61f776",
      "summary": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
      "summary_zh": "Gemini app 現已搭載我們最先進的音樂生成模型 Lyria 3，讓任何人都能透過文字或圖片創作 30 秒的音軌。",
      "title": "A new way to express yourself: Gemini can now create music",
      "title_zh": "一種表達自我的新方式：Gemini 現可創作音樂"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-21T11:48:17.811200+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "deepmind-blog",
          "tier": 0,
          "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
          "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "deepmind-blog",
        "tier": 0,
        "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
        "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks"
      },
      "published_at": "2026-02-19T08:06:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.15,
        "llm_relevance_score": 3.3,
        "recency_score": 0.8055426032528947,
        "semantic_score": 3.1474456667900084,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 17.502988270042906
      },
      "section": null,
      "source_name": "DeepMind Blog",
      "story_id": "fallback:fcbf948f2e957168",
      "summary": "3.1 Pro is designed for tasks where a simple answer isn’t enough.",
      "summary_zh": "3.1 Pro 專為僅靠簡單答案不足以解決的任務而設計。",
      "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
      "title_zh": "Gemini 3.1 Pro：針對您最複雜任務的更智慧模型"
    },
    {
      "arxiv_id": null,
      "authors": [
        "ND Ngoka"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Elastic Kubernetes Service",
        "Amazon Simple Storage Service (S3)",
        "Customer Solutions",
        "Technical How-to",
        "AI/ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:38.119112+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte"
      },
      "published_at": "2026-02-19T08:28:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.3,
        "llm_relevance_score": 6.6,
        "recency_score": 0.8067807703224243,
        "semantic_score": 2.235215002298355,
        "tier_score": 3.0,
        "topic_score": 2.25,
        "total_score": 16.39199577262078
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:1be4363aa25716fa",
      "summary": "In this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora, AWS Identity and Access Management (IAM), and Amazon CloudWatch. We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.",
      "summary_zh": "在這篇文章中，我們將解釋如何使用 Flyte Python SDK 來編排並擴展 AI/ML 工作流程。我們探討 Union.ai 2.0 系統如何將 Flyte 部署到 Amazon Elastic Kubernetes Service (Amazon EKS)，並與 Amazon Simple Storage Service (Amazon S3)、Amazon Aurora、AWS Identity and Access Management (IAM) 和 Amazon CloudWatch 等 AWS 服務無縫整合。我們將透過一個使用新 Amazon S3 Vectors 服務的 AI 工作流程範例來探索此解決方案。",
      "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
      "title_zh": "使用 Union.ai 和 Flyte 在 Amazon EKS 上建構 AI 工作流程"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Ebbey Thomas"
      ],
      "categories": [
        "Amazon Quick Suite",
        "Artificial Intelligence",
        "Intermediate (200)",
        "Technical How-to"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:38.119016+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)",
          "url": "https://aws.amazon.com/blogs/machine-learning/integrate-external-tools-with-amazon-quick-agents-using-model-context-protocol-mcp"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)",
        "url": "https://aws.amazon.com/blogs/machine-learning/integrate-external-tools-with-amazon-quick-agents-using-model-context-protocol-mcp"
      },
      "published_at": "2026-02-20T08:26:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.15,
        "llm_relevance_score": 3.3,
        "recency_score": 0.8915068156329155,
        "semantic_score": 3.9298596680164337,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 16.37136648364935
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:6c24855fb78864a1",
      "summary": "In this post, you’ll use a six-step checklist to build a new MCP server or validate and adjust an existing MCP server for Amazon&nbsp;Quick integration.&nbsp;The Amazon&nbsp;Quick User Guide describes the MCP client behavior and constraints. This is a “How to” guide for detailed implementation required by 3P partners to integrate with Amazon Quick with MCP.",
      "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dan Ferguson"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Amazon SageMaker Data & AI Governance",
        "Foundational (100)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-21T11:48:38.118907+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-2-improved-observability-and-enhanced-features-for-sagemaker-ai-model-customization-and-hosting"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-2-improved-observability-and-enhanced-features-for-sagemaker-ai-model-customization-and-hosting"
      },
      "published_at": "2026-02-20T12:26:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.9064992163690356,
        "semantic_score": 3.107437014579773,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 15.613936230948807
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9a3c94c00ad8b85e",
      "summary": "In 2025, Amazon SageMaker AI made several improvements designed to help you train, tune, and host generative AI workloads. In Part 1 of this series, we discussed Flexible Training Plans and price performance improvements made to inference components. In this post, we discuss enhancements made to observability, model customization, and model hosting. These improvements facilitate a whole new class of customer use cases to be hosted on SageMaker AI.",
      "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dan Ferguson"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Amazon SageMaker Data & AI Governance",
        "Foundational (100)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-21T11:48:38.118699+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-1-flexible-training-plans-and-improvements-to-price-performance-for-inference-workloads"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-1-flexible-training-plans-and-improvements-to-price-performance-for-inference-workloads"
      },
      "published_at": "2026-02-20T12:26:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.906517052755942,
        "semantic_score": 3.0773605406284332,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 15.583877593384376
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:cf7b1d292be16c83",
      "summary": "In 2025, Amazon SageMaker AI saw dramatic improvements to core infrastructure offerings along four dimensions: capacity, price performance, observability, and usability. In this series of posts, we discuss these various improvements and their benefits. In Part 1, we discuss capacity improvements with the launch of Flexible Training Plans. We also describe improvements to price performance for inference workloads. In Part 2, we discuss enhancements made to observability, model customization, and model hosting.",
      "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Richard Black"
      ],
      "categories": [
        "Research Blog"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:20.201445+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "microsoft-research-blog",
          "tier": 0,
          "title": "Project Silica’s advances in glass storage technology",
          "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "microsoft-research-blog",
        "tier": 0,
        "title": "Project Silica’s advances in glass storage technology",
        "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology"
      },
      "published_at": "2026-02-18T08:11:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.7291643799677724,
        "semantic_score": 1.8715461611747743,
        "tier_score": 3.0,
        "topic_score": 3.0,
        "total_score": 11.200710541142547
      },
      "section": null,
      "source_name": "Microsoft Research Blog",
      "story_id": "fallback:6bbe35b78a036ef7",
      "summary": "<p>Project Silica introduces new techniques for encoding data in borosilicate glass, as described in the journal Nature. These advances lower media cost and simplify writing and reading systems while supporting 10,000-year data preservation.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/\">Project Silica’s advances in glass storage technology</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
      "title": "Project Silica’s advances in glass storage technology"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Sundar Pichai"
      ],
      "categories": [
        "A message from our CEO",
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:18.260808+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "“No technology has me dreaming bigger than AI”",
          "url": "https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "“No technology has me dreaming bigger than AI”",
        "url": "https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026"
      },
      "published_at": "2026-02-18T20:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.7675215479910831,
        "semantic_score": 1.8285266041755677,
        "tier_score": 3.0,
        "topic_score": 2.25,
        "total_score": 9.34604815216665
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:64676d972524e1dd",
      "summary": "a stylized design resembling the Ashoka Chakra with colorful network lines and text reading \"भारत 2026 INDIA.\" A vertical line separates it from the Google logo on the right, all set against a light blue gradient background with a faint grid pattern.",
      "title": "“No technology has me dreaming bigger than AI”"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:18.260996+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "AI Impact Summit 2026",
          "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "AI Impact Summit 2026",
        "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection"
      },
      "published_at": "2026-02-18T20:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.7675215479910831,
        "semantic_score": 2.542090344429016,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 8.909611892420099
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:a6435c936e61839f",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Collection_Hero-2.max-600x600.format-webp.webp\" />A look at the partnerships and investments Google announced at the AI Impact Summit 2026.",
      "summary_zh": "這是一篇關於 Google 在 AI Impact Summit 2026 上宣布的合作夥伴關係和投資的報導。",
      "title": "AI Impact Summit 2026",
      "title_zh": "AI Impact Summit 2026"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Helen King"
      ],
      "categories": [
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:18.261356+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "Our 2026 Responsible AI Progress Report",
          "url": "https://blog.google/innovation-and-ai/products/responsible-ai-2026-report-ongoing-work"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "Our 2026 Responsible AI Progress Report",
        "url": "https://blog.google/innovation-and-ai/products/responsible-ai-2026-report-ongoing-work"
      },
      "published_at": "2026-02-17T14:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.6773353887690647,
        "semantic_score": 2.2897584319114688,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 8.567093820680533
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:9cfe686e4a712137",
      "summary": "an illustration of blue and white cubes",
      "summary_zh": "藍色和白色立方體的插圖。",
      "title": "Our 2026 Responsible AI Progress Report",
      "title_zh": "我們的 2026 Responsible AI 進度報告"
    },
    {
      "arxiv_id": null,
      "authors": [
        "James Manyika"
      ],
      "categories": [
        "Google.org",
        "Google in Asia",
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T11:48:18.261239+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
          "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
        "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india"
      },
      "published_at": "2026-02-18T02:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.7120631167655351,
        "semantic_score": 2.0511165261268616,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 8.363179642892396
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:290081df3883fdd9",
      "summary": "four people seated on a conference stage",
      "summary_zh": "四個人坐在會議舞台上。",
      "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
      "title_zh": "AI Impact Summit 2026：我們如何透過合作讓 AI 造福所有人"
    }
  ],
  "run_date": "2026-02-21",
  "run_id": "0df46c06-57bc-4f69-9b6d-7bc36600b2ca",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-21T12:08:41.687115+00:00",
    "items_total": 204,
    "run_id": "0df46c06-57bc-4f69-9b6d-7bc36600b2ca",
    "started_at": "2026-02-21T11:50:53.750074+00:00",
    "stories_total": 186,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 4,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": "2026-02-19T18:59:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-agents",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 1,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": "2026-02-19T18:56:34+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-alignment",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 3,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": "2026-02-19T18:48:08+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-llm",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": "2026-02-19T18:36:50+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-api-multimodal",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": "2026-02-19T16:59:11+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-api-reasoning",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CL",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CV",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.IR",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ir",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.LG",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.MA",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ma",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.RO",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ro",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.SE",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-se",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv stat.ML",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": "2026-02-19T08:06:14+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": "2026-02-18T20:30:00+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 8,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": "2026-02-19T18:11:28+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": "2026-02-20T15:55:54+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": "2026-02-21T00:14:28+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": "2026-02-19T00:28:31+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": "2026-02-20T05:27:33+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "NO_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.16802",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "qwen"
      ],
      "first_seen_at": "2026-02-21T11:48:43.032563+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "References Improve LLM Alignment in Non-Verifiable Domains",
          "url": "https://arxiv.org/abs/2602.16802"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "References Improve LLM Alignment in Non-Verifiable Domains",
        "url": "https://arxiv.org/abs/2602.16802"
      },
      "published_at": "2026-02-18T19:03:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7629284404615205,
        "semantic_score": 3.8740490078926086,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 36.19697744835413
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16802",
      "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "summary_zh": "儘管帶有可驗證獎勵的強化學習 (RLVR) 在推理任務中表現出強大的有效性，但它無法直接應用於缺乏真實性驗證器的不可驗證領域，例如 LLM 對齊。在這項工作中，我們研究了參考資料引導的 LLM-evaluators 是否能透過充當軟性「驗證器」來彌補這一差距。首先，我們設計了評估協議，利用參考輸出增強了基於 LLM 的評估器以進行 LLM 對齊。透過全面的實驗，我們表明參考資料引導的方法顯著提高了能力較弱的 LLM-judges 的準確性，這些評審使用了來自 frontier models 的參考資料；而更強的 LLM-judges 也可以透過高品質（即人類編寫的）參考資料得到增強。基於這些改進後的評審，我們展示了高品質參考資料在對齊調優中的實用性，其中以參考資料引導的 LLM 被用作評審以進行自我提升。我們證明了參考資料引導的自我提升相對於直接對參考輸出進行 SFT 和無參考資料評審的自我提升都取得了顯著的提升，達到了與使用 ArmoRM（一個強大的 fine-tuned 獎勵模型）進行訓練相當的性能。具體而言，我們的方法在使用 Llama-3-8B-Instruct 時在 AlpacaEval 和 Arena-Hard 上分別達到 73.1% 和 58.7%，在使用 Qwen2.5-7B 時分別達到 70.0% 和 74.1%，這相當於在 AlpacaEval / Arena-Hard 上，相對於 SFT distillation 平均絕對增益為 +20.2 / +17.1 個百分點，相對於無參考資料的自我提升平均絕對增益為 +5.3 / +3.6 個百分點。這些結果突顯了使用參考資料引導的 LLM-evaluators 在不可驗證領域實現有效 LLM 後訓練的潛力。",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains",
      "title_zh": "參考資料提升 LLM 在不可驗證領域的對齊能力"
    },
    {
      "arxiv_id": "2602.17365",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:51.128287+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Computer-Using World Model",
          "url": "https://arxiv.org/abs/2602.17365"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Computer-Using World Model",
        "url": "https://arxiv.org/abs/2602.17365"
      },
      "published_at": "2026-02-19T13:48:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8249175589316596,
        "semantic_score": 3.657119929790497,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 34.04203748872216
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17365",
      "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "summary_zh": "在複雜軟體環境中運行的代理人，如果能推斷其動作的後果，將會受益匪淺，因為即使是單一錯誤的使用者介面 (UI) 操作，也可能導致漫長且維護產物的工作流程脫軌。對於電腦使用情境而言，這一挑戰尤為嚴峻，因為實際執行不支援反事實探索，使得大規模的試錯學習和規劃變得不切實際，儘管環境完全是數位化和確定性的。我們引入了電腦使用世界模型 (CUWM)，這是一種用於桌面軟體的世界模型，能夠根據當前狀態和候選動作預測下一個 UI 狀態。CUWM 採用了 UI 動態的兩階段分解：它首先預測代理人相關狀態變化的文本描述，然後視覺化地實現這些變化以合成下一個螢幕截圖。CUWM 是根據代理人與真實 Microsoft Office 應用程式互動時收集的離線 UI 轉換進行訓練的，並透過一個輕量級的強化學習階段進一步優化，該階段將文本轉換預測與電腦使用環境的結構性要求對齊。我們透過測試時動作搜尋來評估 CUWM，其中一個 frozen agent 使用世界模型在執行前模擬和比較候選動作。在一系列 Office 任務中，世界模型引導的測試時擴展提高了決策品質和執行穩健性。",
      "title": "Computer-Using World Model",
      "title_zh": "電腦使用世界模型"
    },
    {
      "arxiv_id": "2602.17259",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-21T11:48:45.971745+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
          "url": "https://arxiv.org/abs/2602.17259"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
        "url": "https://arxiv.org/abs/2602.17259"
      },
      "published_at": "2026-02-19T11:00:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8153654861921672,
        "semantic_score": 3.1939113974571227,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.90927688364929
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17259",
      "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
      "summary_zh": "使 VLA 模型能夠預測環境動態（即世界建模）已被認為對於改善機器人推理和泛化至關重要。然而，當前的方法面臨兩個主要問題：1. 訓練目標迫使模型過度強調像素級重建，這限制了語義學習和泛化能力。2. 在推斷過程中依賴預測的未來觀測通常會導致錯誤累積。為了解決這些挑戰，我們引入了透過平行漸進式擴展的未來表徵對齊 (FRAPPE)。我們的方法採用兩階段 fine-tuning 策略：在中期訓練階段，模型學習預測未來觀測的潛在表徵；在後期訓練階段，我們平行擴展計算工作負載，並同時與多個不同的 visual foundation models 對齊表徵。透過顯著提高 fine-tuning 效率並減少對動作標註資料的依賴，FRAPPE 提供了一種可擴展且數據高效的途徑，以增強通用機器人策略的世界感知能力。在 RoboTwin 基準和真實世界任務上的實驗表明，FRAPPE 超越了 state-of-the-art 方法，並在長時程和未見過的情境中展現出強大的泛化能力。",
      "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
      "title_zh": "FRAPPE：透過多重未來表徵對齊將世界建模融入通用策略"
    },
    {
      "arxiv_id": "2602.16682",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "deepmind"
      ],
      "first_seen_at": "2026-02-21T11:48:51.129810+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Situated Awareness in the Real World",
          "url": "https://arxiv.org/abs/2602.16682"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Situated Awareness in the Real World",
        "url": "https://arxiv.org/abs/2602.16682"
      },
      "published_at": "2026-02-18T18:22:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7607751524425055,
        "semantic_score": 2.7860475063323973,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.9068226587749
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16682",
      "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "summary_zh": "人類感知的一個核心面向是情境感知，即將我們自己與周圍的物理環境聯繫起來，並在情境中推理可能的動作的能力。然而，大多數現有的 multimodal foundation models (MFMs) 基準都強調以環境為中心的空間關係（場景中物體之間的關係），卻在很大程度上忽略了需要根據代理人的視角、姿態和運動進行推理的以觀察者為中心的關係。為了彌補這一差距，我們引入了 SAW-Bench (Situated Awareness in the Real World)，這是一個用於使用真實世界影片評估自我中心情境感知的新型基準。SAW-Bench 包含 786 個使用 Ray-Ban Meta (Gen 2) 智慧眼鏡捕捉的自錄影片，涵蓋多樣的室內外環境，以及超過 2,071 對人工標註的問答對。它透過六種不同的感知任務探測模型以觀察者為中心的理解。我們的全面評估揭示了人類與模型之間存在 37.66% 的性能差距，即使是使用性能最佳的 MFM，Gemini 3 Flash 也是如此。除了這個差距之外，我們的深入分析還發現了幾個值得注意的結果；例如，雖然模型可以利用自我中心影片中的部分幾何線索，但它們通常無法推斷出連貫的攝影機幾何，從而導致系統性的空間推理錯誤。我們將 SAW-Bench 定位為情境空間智能的基準，超越被動觀察，旨在理解物理基礎的、以觀察者為中心的動態。",
      "title": "Learning Situated Awareness in the Real World",
      "title_zh": "學習真實世界中的情境感知"
    },
    {
      "arxiv_id": "2602.16932",
      "authors": [
        "Jinming Nian",
        "Fangchen Li",
        "Dae Hoon Park",
        "Yi Fang"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-21T11:48:43.031613+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
          "url": "https://arxiv.org/abs/2602.16932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
        "url": "https://arxiv.org/abs/2602.16932"
      },
      "published_at": "2026-02-18T22:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7751975836572942,
        "semantic_score": 4.2493581712245945,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.82455575488189
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16932",
      "summary": "Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as execut",
      "summary_zh": "諸如 BM25 和帶有 Dirichlet 平滑的 query likelihood 等檢索演算法仍然是強大且高效的第一階段排序器，然而其改進主要依賴於參數調優和人類直覺。我們研究了由評估器和演化搜尋引導的 large language model 是否能自動發現改進的 lexical retrieval algorithms。我們引入了 RankEvolve，這是一個基於 AlphaEvolve 的程式演化設置，其中候選排序演算法被表示為可執行",
      "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
      "title_zh": "RankEvolve：透過 LLM 驅動的演化自動發現檢索演算法"
    }
  ]
}