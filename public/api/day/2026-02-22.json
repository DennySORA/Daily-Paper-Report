{
  "archive_dates": [
    "2026-02-22"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-22T01:09:03.409039+00:00",
  "model_releases_by_entity": {
    "huggingface": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "huggingface"
        ],
        "first_seen_at": "2026-02-22T00:41:08.887100+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 182,
          "likes": 10,
          "pipeline_tag": "time-series-forecasting"
        },
        "hf_model_id": "google/timesfm-2.5-200m-transformers",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-google",
            "tier": 1,
            "title": "google/timesfm-2.5-200m-transformers",
            "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-google",
          "tier": 1,
          "title": "google/timesfm-2.5-200m-transformers",
          "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
        },
        "published_at": "2026-02-20T15:55:54+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8715098832463742,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.671509883246374
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:google/timesfm-2.5-200m-transformers",
        "summary": "TimesFM (Time Series Foundation Model) is a pretrained decoder-only model for time-series forecasting. This repository contains the **Transformers** port of the official TimesFM 2.5 PyTorch release. **Resources and Technical Documentation**: * Original model: google/timesfm-2.5-200m-pytorch * Transformers model: google/timesfm-2.5-200m-transformers * Paper: A decoder-only foundation model for time-series forecasting * Transformers docs: TimesFM 2.5 This model is converted from the official TimesFM 2.5 PyTorch checkpoint and integrated into `transformers` as `Timesfm2P5ModelForPrediction`. The converted checkpoint preserves the original architecture and forecasting behavior, including: * patch-based inputs for time-series contexts * decoder-only self-attention stack",
        "summary_zh": "TimesFM (時間序列基礎模型) 是一個用於時間序列預測的預訓練 decoder-only 模型。此儲存庫包含官方 TimesFM 2.5 PyTorch 版本的 **Transformers** 移植。**資源與技術文件**：* 原始模型: google/timesfm-2.5-200m-pytorch * Transformers 模型: google/timesfm-2.5-200m-transformers * 論文: A decoder-only foundation model for time-series forecasting * Transformers 文件: TimesFM 2.5 這個模型是從官方 TimesFM 2.5 PyTorch checkpoint 轉換而來，並作為 `Timesfm2P5ModelForPrediction` 整合到 `transformers` 中。轉換後的 checkpoint 保留了原始架構和預測行為，包括：* 用於時間序列上下文的 patch-based 輸入 * decoder-only self-attention 堆疊",
        "title": "google/timesfm-2.5-200m-transformers",
        "title_zh": "google/timesfm-2.5-200m-transformers"
      }
    ],
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-22T00:41:09.884451+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 101316,
          "likes": 598,
          "pipeline_tag": "automatic-speech-recognition"
        },
        "hf_model_id": "mistralai/voxtral-mini-4b-realtime-2602",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
            "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
          "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
        },
        "published_at": "2026-02-19T00:28:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.7393900466513338,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.539390046651334
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/voxtral-mini-4b-realtime-2602",
        "summary": "Voxtral Mini 4B Realtime 2602 is a **multilingual, realtime speech-transcription model** and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of **= 3600 / 0.8 = 45000`. In theory, you should be able to record with no limit; in practice, pre-allocations of RoPE parameters among other things limits `--max-model-len`. For the best user experience, we recommend to simply instantiate vLLM with the default parameters which will automatically set a maximum model length of 131072 (~ca. 3h). - We strongly recommend using websockets to set up audio streaming sessions. For more info on how to do so, check Usage. - We recommend using a delay of 480ms as we found it to be the sweet spot of performance and low latency. If, however, you want to adapt...",
        "summary_zh": "Voxtral Mini 4B Realtime 2602 是一個**多語言、即時語音轉錄模型**，也是首批開源解決方案之一，其準確性可與離線系統媲美，延遲為 **= 3600 / 0.8 = 45000`。理論上，您應該能夠無限錄製；實際上，RoPE 參數的預分配等因素限制了 `--max-model-len`。為了獲得最佳用戶體驗，我們建議直接使用預設參數實例化 vLLM，這將自動設定最大模型長度為 131072 (約 3 小時)。- 我們強烈建議使用 websockets 來設定音頻串流會話。有關如何操作的更多資訊，請查閱 Usage。- 我們建議使用 480ms 的延遲，因為我們發現這是性能和低延遲的最佳平衡點。但是，如果您想調整...",
        "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
        "title_zh": "mistralai/Voxtral-Mini-4B-Realtime-2602"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-22T00:41:09.405247+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 16,
          "pipeline_tag": "image-feature-extraction"
        },
        "hf_model_id": "microsoft/latent-zoning-networks",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/latent-zoning-networks",
            "url": "https://huggingface.co/microsoft/latent-zoning-networks"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/latent-zoning-networks",
          "url": "https://huggingface.co/microsoft/latent-zoning-networks"
        },
        "published_at": "2026-02-21T00:14:28+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9022123291952197,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 1.2000000000000002,
          "total_score": 5.90221232919522
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/latent-zoning-networks",
        "summary": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label...",
        "summary_zh": "Generative modeling、representation learning 和 classification 是 machine learning (ML) 中的三個核心問題，然而它們的 state-of-the-art (SoTA) 解決方案卻在很大程度上相互獨立。在本文中，我們提出疑問：是否存在一個統一的原則能夠同時解決這三者？這種統一化可以簡化 ML pipelines，並促進跨任務的更大協同作用。我們引入 Latent Zoning Network (LZN) 作為實現此目標的一個步驟。LZN 的核心是建立一個共享的 Gaussian latent space，它能編碼所有任務的資訊。每種資料類型（例如 images、text、labels）都配備了一個 encoder，將樣本映射到不同的 disjoint latent zones，以及一個 decoder，將 latents 映射回資料。ML tasks 被表達為這些 encoders 和 decoders 的組合：例如，label-conditional image generation 使用一個 label...",
        "title": "microsoft/latent-zoning-networks",
        "title_zh": "microsoft/latent-zoning-networks"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-22T00:41:10.885040+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 133264,
          "likes": 830,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        },
        "published_at": "2026-02-20T05:27:33+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8342989477996857,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.634298947799685
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
        "summary_zh": "此儲存庫包含以 Hugging Face Transformers 格式呈現的後訓練模型之模型權重和配置檔案。這些人工產物兼容 Hugging Face Transformers, vLLM, SGLang, KTransformers 等。對於尋求託管、可擴展推理而無需基礎設施維護的用戶，阿里巴巴雲模型工作室提供了官方 Qwen API 服務。特別是，**Qwen3.5-Plus** 是與 Qwen3.5-397B-A17B 對應的託管版本，具有更多生產功能，例如預設 1M context length、官方內建工具和自適應工具使用。欲了解更多資訊，請參閱 User Guide。近幾個月來，我們加強了對開發能夠提供卓越實用性和性能的 foundation models 的關注。Qwen3.5...",
        "title": "Qwen/Qwen3.5-397B-A17B",
        "title_zh": "Qwen/Qwen3.5-397B-A17B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-22T00:41:10.885155+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 34813,
          "likes": 49,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        },
        "published_at": "2026-02-18T16:13:24+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.7143996096605033,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.514399609660503
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a...",
        "summary_zh": "此儲存庫包含以 Hugging Face Transformers 格式呈現的後訓練模型之模型權重和配置檔案。這些人工產物兼容 Hugging Face Transformers, vLLM, SGLang 等。對於尋求託管、可擴展推理而無需基礎設施維護的用戶，阿里巴巴雲模型工作室提供了官方 Qwen API 服務。特別是，**Qwen3.5-Plus** 是與 Qwen3.5-397B-A17B 對應的託管版本，具有更多生產功能，例如預設 1M context length、官方內建工具和自適應工具使用。欲了解更多資訊，請參閱 User Guide。近幾個月來，我們加強了對開發能夠提供卓越實用性和性能的 foundation models 的關注。Qwen3.5 代表著一種...",
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "title_zh": "Qwen/Qwen3.5-397B-A17B-FP8"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.17270",
      "authors": [],
      "categories": [],
      "entities": [
        "stability-ai"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407144+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Unified Latents (UL): How to train your latents",
          "url": "https://arxiv.org/abs/2602.17270"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Unified Latents (UL): How to train your latents",
        "url": "https://arxiv.org/abs/2602.17270"
      },
      "published_at": "2026-02-19T11:18:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7735130050894743,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.673513005089475
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17270",
      "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "summary_zh": "我們提出了 Unified Latents (UL)，這是一個用於學習 latent representations 的框架，其由 diffusion prior 共同正則化並由 diffusion model 解碼。透過將 encoder 的輸出雜訊與 prior 的最小雜訊水平連結起來，我們獲得了一個簡單的訓練目標，該目標為 latent bitrate 提供了嚴格的上限。在 ImageNet-512 上，我們的方法實現了 1.4 的競爭性 FID，並具有高重建品質 (PSNR)，同時比在 Stable Diffusion latents 上訓練的模型需要更少的訓練 FLOPs。在 Kinetics-600 上，我們創下了新的 state-of-the-art FVD 1.3。",
      "title": "Unified Latents (UL): How to train your latents",
      "title_zh": "Unified Latents (UL)：如何訓練您的 Latents"
    },
    {
      "arxiv_id": "2602.17259",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:00.593567+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
          "url": "https://arxiv.org/abs/2602.17259"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
        "url": "https://arxiv.org/abs/2602.17259"
      },
      "published_at": "2026-02-19T11:00:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7725771196306013,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.672577119630603
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17259",
      "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
      "summary_zh": "使 VLA models 能夠預測環境動態，即 world modeling，已被認為對於改進機器人推理和泛化至關重要。然而，目前的S方法面臨兩個主要問題：1. 訓練目標迫使模型過度強調 pixel-level reconstruction，這限制了 semantic learning 和泛化；2. 在推理過程中依賴預測的 future observations 通常會導致錯誤累積。為了解決這些挑戰，我們引入了 Future Representation Alignment via Parallel Progressive Expansion (FRAPPE)。我們的方法採用兩階段 fine-tuning 策略：在訓練中期，模型學習預測未來觀察的 latent representations；在訓練後期，我們並行擴展計算工作量，並同時將表示與多個不同的 visual foundation models 對齊。透過顯著提高 fine-tuning 效率並減少對 action-annotated data 的依賴，FRAPPE 為增強通用機器人策略中的 world-awareness 提供了一個可擴展且數據高效的途徑。在 RoboTwin benchmark 和真實世界任務上的實驗表明，FRAPPE 優於 state-of-the-art 方法，並在 long-horizon 和未見情境中表現出強大的泛化能力。",
      "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
      "title_zh": "FRAPPE：透過多重未來表徵對齊將世界建模注入通用策略"
    },
    {
      "arxiv_id": "2602.17004",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407267+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Arcee Trinity Large Technical Report",
          "url": "https://arxiv.org/abs/2602.17004"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Arcee Trinity Large Technical Report",
        "url": "https://arxiv.org/abs/2602.17004"
      },
      "published_at": "2026-02-19T01:58:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7440420676439836,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.644042067643984
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17004",
      "summary": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "summary_zh": "我們提出了 Arcee Trinity Large 的技術報告，這是一個稀疏的 Mixture-of-Experts 模型，總參數為 400B，每個 token 激活 13B。此外，我們報告了 Trinity Nano 和 Trinity Mini，其中 Trinity Nano 總參數為 6B，每個 token 激活 1B；Trinity Mini 總參數為 26B，每個 token 激活 3B。這些模型的現代架構包括 interleaved local and global attention、gated attention、depth-scaled sandwich norm 以及用於 Mixture-of-Experts 的 sigmoid routing。對於 Trinity Large，我們還引入了一種新的 MoE load balancing 策略，名為 Soft-clamped Momentum Expert Bias Updates (SMEBU)。我們使用 Muon optimizer 訓練這些模型。所有三個模型都以零 loss spikes 完成訓練。Trinity Nano 和 Trinity Mini 在 10 兆個 token 上進行了 pre-trained，Trinity Large 在 17 兆個 token 上進行了 pre-trained。模型檢查點可在 https://huggingface.co/arcee-ai 獲取。",
      "title": "Arcee Trinity Large Technical Report",
      "title_zh": "Arcee Trinity Large 技術報告"
    },
    {
      "arxiv_id": "2602.16932",
      "authors": [
        "Jinming Nian",
        "Fangchen Li",
        "Dae Hoon Park",
        "Yi Fang"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-22T00:40:57.210617+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
          "url": "https://arxiv.org/abs/2602.16932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
        "url": "https://arxiv.org/abs/2602.16932"
      },
      "published_at": "2026-02-18T22:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7345171293961348,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.634517129396134
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16932",
      "summary": "Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as execut",
      "summary_zh": "諸如 BM25 和帶有 Dirichlet smoothing 的 query likelihood 等檢索演算法仍然是強大而高效的第一階段 rankers，但改進主要依賴於參數調整和人類直覺。我們研究了由評估器和 evolutionary search 指導的 large language model 是否可以自動發現改進的 lexical retrieval algorithms。我們引入了 RankEvolve，這是一個基於 AlphaEvolve 的 program evolution 設置，其中候選 ranking algorithms 表示為 execut。",
      "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
      "title_zh": "RankEvolve：透過 LLM 驅動的演化自動發現檢索演算法"
    },
    {
      "arxiv_id": "2602.16704",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407405+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Reinforced Fast Weights with Next-Sequence Prediction",
          "url": "https://arxiv.org/abs/2602.16704"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Reinforced Fast Weights with Next-Sequence Prediction",
        "url": "https://arxiv.org/abs/2602.16704"
      },
      "published_at": "2026-02-18T18:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7223766291040978,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.622376629104096
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16704",
      "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "summary_zh": "快速權重架構為 attention-based transformers 提供了一種有前景的替代方案，用於 long-context modeling，它無論上下文長度如何，都能保持恆定的記憶體開銷。然而，其潛力受到 next-token prediction (NTP) 訓練範式的限制。NTP 優化單個 token 的預測，而忽略了前綴之後多個 token 的語義連貫性。因此，動態更新其參數以儲存上下文資訊的 fast weight models，學習到的 suboptimal representations 無法捕捉 long-range dependencies。我們引入了 REFINE (Reinforced Fast weIghts with Next sEquence prediction)，這是一個 reinforcement learning 框架，用於在 next-sequence prediction (NSP) 目標下訓練 fast weight models。REFINE 根據 prediction entropy 選擇資訊豐富的 token 位置，生成 multi-token rollouts，分配 self-supervised sequence-level rewards，並使用 group relative policy optimization (GRPO) 優化模型。REFINE 適用於 pre-trained language models 的整個訓練生命週期：mid-training、post-training 和 test-time training。我們在 LaCT-760M 和 DeltaNet-1.3B 上的實驗表明，REFINE 在 needle-in-a-haystack retrieval、long-context question answering 以及 LongBench 中的多種任務上始終優於帶有 NTP 的 supervised fine-tuning。REFINE 為改進 fast weight architectures 中的 long-context modeling 提供了一個有效且通用的框架。",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "title_zh": "強化快速權重與下一序列預測"
    },
    {
      "arxiv_id": "2602.16317",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407461+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
          "url": "https://arxiv.org/abs/2602.16317"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
        "url": "https://arxiv.org/abs/2602.16317"
      },
      "published_at": "2026-02-18T09:54:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.695868867051426,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.595868867051426
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16317",
      "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
      "summary_zh": "Computer-Aided Design (CAD) 為工程和製造提供快速、可編輯的建模。近期 AI 的進展使得各種 CAD 任務的完全自動化成為可能。然而，進展受數據瓶頸所限：公共語料庫大多包含 sketch-extrude 序列，缺乏複雜操作、多操作組合和設計意圖，因此阻礙了有效的 fine-tuning。嘗試使用 frozen VLMs 繞過此問題，通常會因當前 foundation models 中有限的 3D grounding 而產生簡單或無效的程式。我們提出了 CADEvolve，一個基於演化的流程和數據集，它從簡單的 primitives 開始，並透過 VLM 導向的編輯和驗證，逐步將 CAD 程式發展到工業級複雜度。結果是 8k 個複雜零件，表示為可執行的 CadQuery 參數化產生器。經過多階段後處理和增強，我們獲得了一個統一的數據集，包含 1.3m 個腳本，配對渲染的幾何形狀並演練了完整的 CadQuery 操作集。在 CADEvolve 上 fine-tuned 的 VLM 在 DeepCAD、Fusion 360 和 MCB benchmarks 上的 Image2CAD 任務中取得了 state-of-the-art 的結果。",
      "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
      "title_zh": "CADEvolve：透過程式演化創建真實的 CAD"
    },
    {
      "arxiv_id": "2602.17560",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208005+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
          "url": "https://arxiv.org/abs/2602.17560"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
        "url": "https://arxiv.org/abs/2602.17560"
      },
      "published_at": "2026-02-19T17:13:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7928486120011413,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.792848612001144
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17560",
      "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose",
      "summary_zh": "Activation steering，或稱 representation engineering，提供了一種輕量級的方法，透過在 inference time 操縱大型語言模型（LLMs）的內部 activations 來實現對齊。然而，當前方法存在兩個主要限制：\\textit{(i)} 缺乏一個統一的理論框架來指導 steering directions 的設計，以及 \\textit{(ii)} 過度依賴 \\textit{one-step steering}，其未能捕捉 activation distributions 的複雜模式。在這項工作中，我們提出",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "title_zh": "ODECteer：一個基於 ODE 的統一 LLM 對齊引導框架"
    },
    {
      "arxiv_id": "2602.17127",
      "authors": [
        "Dusan Bosnjakovic"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593629+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
          "url": "https://arxiv.org/abs/2602.17127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
        "url": "https://arxiv.org/abs/2602.17127"
      },
      "published_at": "2026-02-19T06:56:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7595569534042086,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.75955695340421
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17127",
      "summary": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individu",
      "summary_zh": "隨著 Large Language Models (LLMs) 從獨立的聊天介面轉變為 multi-agent systems 中的基礎推理層和 recursive evaluation loops (LLM-as-a-judge)，檢測持久的、供應商層級的行為簽名成為安全和治理的關鍵要求。傳統的 benchmarks 衡量暫時性的任務準確性，但未能捕捉穩定、潛在的響應策略——那些在訓練和對齊過程中嵌入的、「主導心態」，它們超越了個體",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
      "title_zh": "實驗室驅動對齊簽名的出現：用於審計生成式 AI 中潛在偏差和複合風險的心理測量框架"
    },
    {
      "arxiv_id": "2602.16980",
      "authors": [
        "Leo Marchyok",
        "Zachary Coalson",
        "Sungho Keum",
        "Sooel Son",
        "Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210123+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
          "url": "https://arxiv.org/abs/2602.16980"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
        "url": "https://arxiv.org/abs/2602.16980"
      },
      "published_at": "2026-02-19T00:39:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7399388022929305,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.739938802292933
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16980",
      "summary": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts.",
      "summary_zh": "現代語言模型展現出豐富的內部結構，然而對於隱私敏感行為，例如 personally identifiable information (PII) 洩漏，如何在它們的 hidden states 中被表示和調節，人們知之甚少。我們提出了 UniLeak，一個 mechanistic-interpretability 框架，它識別了 universal activation directions：模型 residual stream 中潛在的方向，其在 inference time 的線性添加一致地增加了跨 prompts 生成 PII 的可能性。",
      "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
      "title_zh": "發現在語言模型中 PII 洩漏的通用 Activation Directions"
    },
    {
      "arxiv_id": "2602.16943",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210485+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16943"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16943"
      },
      "published_at": "2026-02-18T23:17:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7357397906849819,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.73573979068498
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16943",
      "summary": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and too",
      "summary_zh": "部署為 agents 的大型語言模型越來越多地透過 tool calls 與外部系統互動——這些行為具有現實世界的後果，而單純的文本輸出不具備此類後果。然而，安全評估絕大多數測量的是文本層級的拒絕行為，留下了一個關鍵問題未解答：抑制有害文本的對齊是否也抑制有害行為？我們介紹了 GAP benchmark，這是一個系統的評估框架，它衡量文本層級安全與 too 之間的差異。",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "title_zh": "Mind the GAP：LLM Agents 中的文本安全並不等同於工具調用安全"
    },
    {
      "arxiv_id": "2602.16901",
      "authors": [
        "Tanqiu Jiang",
        "Yuhui Wang",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
          "url": "https://arxiv.org/abs/2602.16901"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
        "url": "https://arxiv.org/abs/2602.16901"
      },
      "published_at": "2026-02-18T21:30:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7302973225844974,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.730297322584498
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16901",
      "summary": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack ",
      "summary_zh": "LLM agents 正日益被部署到長週期、複雜的環境中以解決具有挑戰性的問題，但這種擴展也使它們面臨長週期攻擊。這些攻擊利用多輪使用者-代理-環境互動來達成在單輪設定中不可行的目標。為了衡量代理對此類風險的脆弱性，我們提出了 AgentLAB，這是第一個專門用於評估 LLM agent 對自適應長週期攻擊脆弱性的基準測試。目前，AgentLAB 支援五種新型攻擊。",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
      "title_zh": "AgentLAB: 針對長週期攻擊評估 LLM Agents 的基準測試"
    },
    {
      "arxiv_id": "2602.16835",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211093+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NeST: Neuron Selective Tuning for LLM Safety",
          "url": "https://arxiv.org/abs/2602.16835"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NeST: Neuron Selective Tuning for LLM Safety",
        "url": "https://arxiv.org/abs/2602.16835"
      },
      "published_at": "2026-02-18T20:01:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7257816384780866,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.72578163847809
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16835",
      "summary": "Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.\n  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.",
      "summary_zh": "安全對齊對於大型語言模型 (LLMs) 的負責任部署至關重要。然而，現有方法通常依賴於重量級的 fine-tuning，這在跨模型家族的更新、審核和維護方面成本高昂。完整的 fine-tuning 會產生大量的計算和儲存開銷，而參數效率方法（例如 LoRA）則以犧牲效率換取不一致的安全增益和對設計選擇的敏感性。諸如 `circuit breakers` 等安全干預機制可在不修改模型權重的情況下減少不安全輸出，但它們無法直接塑造或保留控制安全行為的內部表示。這些限制阻礙了快速可靠的安全更新，尤其是在模型頻繁演進或必須適應新策略和領域的環境中。我們提出了 NeST，一個輕量級、結構感知的安全對齊框架，它透過選擇性地調整一小部分與安全相關的神經元，同時凍結模型的其餘部分來強化拒絕行為。NeST 透過對功能上連貫的安全神經元進行聚類並在每個聚類中強制執行共享更新，將參數更新與安全行為的內部組織對齊，從而實現有針對性且穩定的安全適應，而無需廣泛的模型修改或推論時間開銷。我們將 NeST 與三個主要基準線進行了比較：完整 fine-tuning、基於 LoRA 的 fine-tuning 和 `circuit breakers`，評估了涵蓋多個模型家族和大小的 10 個開源 LLMs。在所有評估的模型中，NeST 將攻擊成功率從平均 44.5% 降低到 4.36%，相當於不安全生成減少了 90.2%，同時平均僅需要 0.44 百萬個可訓練參數。與完整 fine-tuning 相比，這使得更新參數減少了 17,310 倍，相對於 LoRA 減少了 9.25 倍，同時持續實現了更強的安全對齊性能。",
      "title": "NeST: Neuron Selective Tuning for LLM Safety",
      "title_zh": "NeST: 用於 LLM 安全的神經元選擇性調整"
    },
    {
      "arxiv_id": "2602.16699",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211506+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16699"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16699"
      },
      "published_at": "2026-02-18T18:46:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7220222164315189,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.72202221643152
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16699",
      "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "summary_zh": "LLMs 正日益被用於解決複雜問題，這些問題不一定能透過單一回應來解決，而是需要與環境互動以獲取資訊。在這些情境中，LLMs 必須推理何時停止探索並提交答案所固有的成本-不確定性權衡。例如，在程式設計任務中，如果 LLM 對於程式碼片段的正確性不確定，它應該測試該程式碼；編寫測試的成本不為零，但通常低於犯錯的成本。在這項工作中，我們展示了我們可以誘導 LLMs 明確推理如何平衡這些成本-不確定性權衡，然後執行更優化的環境探索。我們將多個任務（包括資訊檢索和程式設計）形式化為不確定性下的序列決策問題。每個問題都有潛在的環境狀態，可以透過傳遞給 LLM agent 的先驗進行推理。我們引入了一個名為 Calibrate-Then-Act (CTA) 的框架，我們將此額外上下文提供給 LLM，使其能夠更優化地行動。即使在基準線和 CTA 的 RL 訓練下，這種改進也得以保留。我們在資訊搜尋型 QA 和簡化程式設計任務上的結果表明，透過 CTA 明確成本效益權衡可以幫助 agents 發現更優化的決策策略。",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "title_zh": "Calibrate-Then-Act: LLM Agents 中的成本感知探索"
    },
    {
      "arxiv_id": "2602.16666",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591046+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Towards a Science of AI Agent Reliability",
          "url": "https://arxiv.org/abs/2602.16666"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Towards a Science of AI Agent Reliability",
        "url": "https://arxiv.org/abs/2602.16666"
      },
      "published_at": "2026-02-18T18:05:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7199943819267847,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.719994381926785
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16666",
      "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "summary_zh": "AI agents 正日益被部署來執行重要任務。儘管標準 `benchmarks` 上不斷提高的準確性分數表明了快速進展，但許多 agents 在實踐中仍然持續失敗。這種差異突顯了當前評估的一個基本限制：將代理行為壓縮為單一成功指標掩蓋了關鍵的操作缺陷。值得注意的是，它忽略了 agents 是否在不同運行中保持一致行為、能否承受擾動、是否可預測地失敗，或者錯誤嚴重性是否有限。基於安全關鍵工程，我們透過提出十二個具體指標來提供一個整體性能概況，這些指標將代理可靠性分解為四個關鍵維度：一致性、穩健性、可預測性和安全性。在兩個互補的 `benchmarks` 上評估 14 個代理模型後，我們發現最近的能力提升僅在可靠性方面產生了小幅改進。透過揭示這些持續存在的限制，我們的指標補充了傳統評估，同時提供了用於推理 agents 如何表現、退化和失敗的工具。",
      "title": "Towards a Science of AI Agent Reliability",
      "title_zh": "邁向 AI 代理可靠性科學"
    },
    {
      "arxiv_id": "2602.16301",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.592071+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Multi-agent cooperation through in-context co-player inference",
          "url": "https://arxiv.org/abs/2602.16301"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Multi-agent cooperation through in-context co-player inference",
        "url": "https://arxiv.org/abs/2602.16301"
      },
      "published_at": "2026-02-18T09:31:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.6947470394165324,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.694747039416534
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16301",
      "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "summary_zh": "在 `multi-agent reinforcement learning` 中，實現自利代理之間的合作仍然是一個基本挑戰。最近的研究表明，在「學習感知」代理之間可以誘導相互合作，這些代理會考慮並塑造其隊友的學習動態。然而，現有方法通常依賴於硬編碼的、通常不一致的關於隊友學習規則的假設，或者強制將在快速時間尺度上更新的「天真學習者」與觀察這些更新的「元學習者」嚴格分離。在此，我們證明序列模型的 `in-context learning` 能力允許隊友學習意識，而無需硬編碼假設或明確的時間尺度分離。我們展示了針對多樣化隊友分佈訓練序列模型代理自然會誘導 `in-context best-response strategies`，在快速情節內時間尺度上有效充當學習演算法。我們發現，在先前工作中識別出的合作機制（即敲詐勒索的脆弱性驅動相互塑造）在這個設定中自然浮現：`in-context adaptation` 使得代理易受敲詐勒索，而隨之而來的塑造對手 `in-context learning` 動態的相互壓力轉化為合作行為的學習。我們的結果表明，序列模型上的標準 `decentralized reinforcement learning` 結合隊友多樣性為學習合作行為提供了一條可擴展的途徑。",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "title_zh": "透過情境內隊友推斷實現多代理合作"
    },
    {
      "arxiv_id": "2602.16173",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.592513+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Personalized Agents from Human Feedback",
          "url": "https://arxiv.org/abs/2602.16173"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Personalized Agents from Human Feedback",
        "url": "https://arxiv.org/abs/2602.16173"
      },
      "published_at": "2026-02-18T04:18:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.6798120247519424,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.679812024751943
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16173",
      "summary": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "summary_zh": "現代 AI agents 功能強大，但往往無法與個別用戶獨特且不斷演變的偏好保持一致。先前的做法通常依賴於靜態數據集，要麼在互動歷史上訓練隱式偏好模型，要麼將用戶資料編碼到外部記憶體中。然而，這些方法在面對新用戶以及隨時間變化的偏好時會遇到困難。我們引入了 Personalized Agents from Human Feedback (PAHF)，這是一個用於持續個性化的框架，其中 agents 使用明確的每用戶記憶體從即時互動中進行線上學習。PAHF 運作一個三步驟循環：(1) 尋求預行動澄清以解決歧義，(2) 將行動基於從記憶體中檢索到的偏好，以及 (3) 整合行動後回饋以在偏好漂移時更新記憶體。為了評估這項能力，我們開發了一個四階段協議和兩個基準測試，分別用於 embodied manipulation 和 online shopping 領域。這些基準量化了 agent 從頭開始學習初始偏好並隨後適應角色轉變的能力。我們的理論分析和實證結果表明，整合明確記憶體與雙重回饋通道至關重要：PAHF 學習速度顯著更快，並且始終優於 no-memory 和 single-channel baselines，從而減少了初始個性化錯誤並實現了對偏好轉變的快速適應。",
      "title": "Learning Personalized Agents from Human Feedback",
      "title_zh": "從人類回饋中學習個性化代理"
    },
    {
      "arxiv_id": "2602.16705",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407733+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
          "url": "https://arxiv.org/abs/2602.16705"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
        "url": "https://arxiv.org/abs/2602.16705"
      },
      "published_at": "2026-02-18T18:55:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7224635870799669,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.52246358707997
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16705",
      "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "summary_zh": "類人機器人在野外對任意物體進行視覺移動操作 (visual loco-manipulation) 需要精確的 end-effector (EE) 控制，以及透過視覺輸入（例如 RGB-D images）對場景進行泛化理解。現有方法基於 real-world imitation learning，由於難以收集大規模訓練數據集而表現出有限的泛化能力。本文提出了一種新的範式 HERO，用於類人機器人的物體移動操作，它結合了 large vision models 強大的泛化能力和 open-vocabulary 理解能力，以及來自模擬訓練的強大控制性能。我們透過設計一種精確的 residual-aware EE tracking policy 來實現這一目標。這種 EE tracking policy 結合了 classical robotics 和 machine learning。它使用了 a) inverse kinematics 將殘餘的 end-effector 目標轉換為參考軌跡，b) 一個學習到的 neural forward model 用於精確的 forward kinematics，c) goal adjustment，以及 d) replanning。這些創新共同幫助我們將 end-effector 追蹤誤差減少了 3.2 倍。我們利用這個精確的 end-effector tracker 構建了一個模組化系統用於 loco-manipulation，其中我們使用 open-vocabulary large vision models 以實現強大的視覺泛化能力。我們的系統能夠在多樣化的真實世界環境中運行，從辦公室到咖啡店，機器人能夠可靠地操作高度從 43 公分到 92 公分不等的各種日常物品（例如馬克杯、蘋果、玩具）。在模擬和現實世界中進行的系統化模組化和 end-to-end 測試證明了我們所提出設計的有效性。我們相信本文的進展可以為訓練類人機器人與日常物品互動開闢新的途徑。",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "title_zh": "學習用於開放詞彙視覺移動操作的類人機器人末端執行器控制"
    },
    {
      "arxiv_id": "2602.17658",
      "authors": [
        "Payel Bhattacharjee",
        "Osvaldo Simeone",
        "Ravi Tandon"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.592859+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
          "url": "https://arxiv.org/abs/2602.17658"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
        "url": "https://arxiv.org/abs/2602.17658"
      },
      "published_at": "2026-02-19T18:59:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7986684914857192,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.35866849148572
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17658",
      "summary": "Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose M",
      "summary_zh": "Reward modeling 是現代對齊管道（包括 RLHF 和 RLAIF）的核心組成部分，支撐著包括 PPO 和 TRPO 在內的 policy optimization 方法。然而，訓練可靠的 reward models 嚴重依賴於人類標註的偏好數據，這些數據成本高昂且有限，這促使了數據增強的使用。現有的增強方法通常在 representation 或 semantic level 上操作，並且對 reward model 的估計難度不敏感。在本文中，我們提出了 M",
      "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
      "title_zh": "MARS: 帶有自精煉的邊距感知獎勵建模"
    },
    {
      "arxiv_id": "2602.17544",
      "authors": [
        "Shashank Aggarwal",
        "Ram Vikas Mishra",
        "Amit Awekar"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535566+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
          "url": "https://arxiv.org/abs/2602.17544"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
        "url": "https://arxiv.org/abs/2602.17544"
      },
      "published_at": "2026-02-19T16:59:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7920479091393128,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.352047909139312
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17544",
      "summary": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusa",
      "summary_zh": "在用於搜尋和排序等任務的 multi-agent IR pipelines 中，基於 LLM 的 agents 以 Chain-of-Thought (CoT) 的形式相互交換中間推理。目前的 CoT 評估狹隘地專注於目標任務的準確性。然而，這個指標未能評估推理過程本身的品質或實用性。為了解決這個限制，我們引入了兩個新穎的衡量標準：reusability 和 verifiability。我們使用 Thinker-Executor 框架將 CoT 生成與執行解耦。Reusa",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "title_zh": "透過可重用性和可驗證性評估 Chain-of-Thought 推理"
    },
    {
      "arxiv_id": "2602.17483",
      "authors": [
        "Dimitri Staufer",
        "Kirsten Morehouse"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208612+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
          "url": "https://arxiv.org/abs/2602.17483"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
        "url": "https://arxiv.org/abs/2602.17483"
      },
      "published_at": "2026-02-19T15:53:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7884424218262361,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.348442421826235
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17483",
      "summary": "Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative stud",
      "summary_zh": "Large language models (LLMs) 以及基於它們的 conversational agents 在 pre-training 期間和用戶互動期間都會接觸到 personal data (PD)。先前的工作表明 PD 可能會重新浮現，但用戶卻不了解模型將特定資訊與其身份關聯的強度。我們審計了八個 LLM（3 個 open-source；5 個 API-based，包括 GPT-4o）中的 PD，並引入了 LMP2 (Language Model Privacy Probe)，這是一個以人為本、保護隱私的審計工具，透過兩項形成性研究進行了完善。",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
      "title_zh": "LLM 將您的姓名與什麼相關聯？個人資料的人本黑箱審計"
    },
    {
      "arxiv_id": "2602.17162",
      "authors": [
        "Ariel Larey",
        "Elay Dahan",
        "Amit Bleiweiss",
        "Raizy Kellerman",
        "Guy Leib",
        "Omri Nayshool",
        "Dan Ofer",
        "Tal Zinger",
        "Dan Dominissini",
        "Gideon Rechavi",
        "Nicole Bussola",
        "Simon Lee",
        "Shane O'Connell",
        "Dung Hoang",
        "Marissa Wirth",
        "Alexander W. Charney",
        "Nati Daniel",
        "Yoli Shavit"
      ],
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209842+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
          "url": "https://arxiv.org/abs/2602.17162"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
        "url": "https://arxiv.org/abs/2602.17162"
      },
      "published_at": "2026-02-19T08:20:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7640448647613969,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.324044864761397
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17162",
      "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with",
      "summary_zh": "基因組基礎模型 (GFMs) 主要依賴 Masked Language Modeling (MLM) 或 Next Token Prediction (NTP) 來學習生命的語言。儘管這些範式擅長捕捉局部基因組語法和細粒度 motif 模式，但它們往往未能捕捉到更廣泛的功能性上下文，導致其表示法缺乏全球生物學視角。我們引入了 JEPA-DNA，這是一種新穎的預訓練框架，它將 Joint-Embedding Predictive Architecture (JEPA) 與",
      "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
      "title_zh": "JEPA-DNA：透過聯合嵌入預測架構為基因組基礎模型奠定基礎"
    },
    {
      "arxiv_id": "2602.16977",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210187+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Fail-Closed Alignment for Large Language Models",
          "url": "https://arxiv.org/abs/2602.16977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Fail-Closed Alignment for Large Language Models",
        "url": "https://arxiv.org/abs/2602.16977"
      },
      "published_at": "2026-02-19T00:33:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7396502481819996,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.299650248181997
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16977",
      "summary": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial ",
      "summary_zh": "我們發現當前大型語言模型 (LLM) 對齊存在結構性弱點：現代拒絕機制是 fail-open 的。儘管現有方法在多個潛在特徵中編碼了拒絕行為，但透過 prompt-based jailbreaks 抑制單一主導特徵會導致對齊崩潰，導致不安全的生成。受此啟發，我們提出 fail-closed alignment 作為 LLM 穩健安全性的設計原則：拒絕機制即使在部分情況下也應保持有效",
      "title": "Fail-Closed Alignment for Large Language Models",
      "title_zh": "大型語言模型的故障關閉對齊"
    },
    {
      "arxiv_id": "2602.16902",
      "authors": [
        "Juliusz Ziomek",
        "William Bankes",
        "Lorenz Wolf",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210802+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
          "url": "https://arxiv.org/abs/2602.16902"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
        "url": "https://arxiv.org/abs/2602.16902"
      },
      "published_at": "2026-02-18T21:33:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7304824561318505,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.29048245613185
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16902",
      "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest r",
      "summary_zh": "我們介紹了 LLM-Wikirace，這是一個用於評估大型語言模型 (LLMs) 規劃、推理和世界知識的基準測試。在 LLM-Wikirace 中，模型必須逐步有效地導航 Wikipedia 超連結，從給定來源到達目標頁面，這需要前瞻性規劃以及對現實世界中概念如何連接進行推理的能力。我們評估了一系列廣泛的開源和閉源模型，包括 Gemini-3, GPT-5 和 Claude Opus 4.5，它們取得了最強的結",
      "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
      "title_zh": "LLM-WikiRace：基於真實世界知識圖譜的長期規劃與推理基準測試"
    },
    {
      "arxiv_id": "2602.16839",
      "authors": [
        "Zeliang Zhang",
        "Xiaodong Liu",
        "Hao Cheng",
        "Hao Sun",
        "Chenliang Xu",
        "Jianfeng Gao"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536453+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
          "url": "https://arxiv.org/abs/2602.16839"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
        "url": "https://arxiv.org/abs/2602.16839"
      },
      "published_at": "2026-02-18T20:03:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7259135343933144,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.285913534393316
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16839",
      "summary": "Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixe",
      "summary_zh": "大型推理模型 (LRMs) 在複雜問題上表現出色，但在效率方面面臨一個關鍵障礙：強化學習 (RL) 訓練需要長時間的 rollout 來獲取基於結果的獎勵，其中 autoregressive decoding 佔用了大量的時間和記憶體。儘管 sliding-window cache 策略可以限制記憶體，但它們會破壞長上下文推理並降低性能。我們引入了 Progressive Thought Encoding，這是一種參數高效的 fine-tuning 方法，它使 LRMs 能夠在固定",
      "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
      "title_zh": "透過漸進式思維編碼有效訓練大型推理模型"
    },
    {
      "arxiv_id": "2602.16662",
      "authors": [
        "Richard Willis",
        "Jianing Zhao",
        "Yali Du",
        "Joel Z. Leibo"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211750+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
          "url": "https://arxiv.org/abs/2602.16662"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
        "url": "https://arxiv.org/abs/2602.16662"
      },
      "published_at": "2026-02-18T18:02:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7198502308173098,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.27985023081731
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16662",
      "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise i",
      "summary_zh": "隨著由 LLM 驅動的自主代理越來越多地部署到社會中，了解它們在社會困境中的集體行為變得至關重要。我們引入了一個評估框架，其中 LLMs 生成編碼為演算法的策略，這使得在部署前進行檢查，並能擴展到數百個代理的群體——這比以前的工作要大得多。我們發現，當代理優先考慮時，較新的模型與較舊的模型相比，往往會產生更差的社會結果",
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
      "title_zh": "評估數百個 LLM 代理的集體行為"
    },
    {
      "arxiv_id": "2602.16512",
      "authors": [
        "Felix Fricke",
        "Simon Malberg",
        "Georg Groh"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536808+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
          "url": "https://arxiv.org/abs/2602.16512"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
        "url": "https://arxiv.org/abs/2602.16512"
      },
      "published_at": "2026-02-18T14:58:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.710689273781133,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.270689273781134
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16512",
      "summary": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of ",
      "summary_zh": "Chain of Thought、Tree of Thoughts 和 Graph of Thoughts 等提示方案可以顯著增強大型語言模型的推理能力。然而，大多數現有方案要求用戶定義靜態的、特定於問題的推理結構，這些結構缺乏對動態或未見問題類型的適應性。此外，這些方案在超參數、提示、運行時間和提示成本方面往往優化不足。為了克服這些限制，我們引入了 Framework of",
      "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
      "title_zh": "Framework of Thoughts: 基於鏈、樹和圖的動態優化推理基礎框架"
    },
    {
      "arxiv_id": "2602.16493",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591435+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "MMA: Multimodal Memory Agent",
          "url": "https://arxiv.org/abs/2602.16493"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "MMA: Multimodal Memory Agent",
        "url": "https://arxiv.org/abs/2602.16493"
      },
      "published_at": "2026-02-18T14:30:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7093169302496313,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.26931693024963
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16493",
      "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "summary_zh": "長時程多模態代理依賴於外部記憶；然而，基於相似性的檢索常常會浮現過時、低可信度或衝突的項目，這可能觸發過度自信的錯誤。我們提出了 Multimodal Memory Agent (MMA)，它通過結合來源可信度、時間衰減和衝突感知網絡共識，為每個檢索到的記憶項目分配一個動態可靠性分數，並使用此信號在支援不足時重新加權證據並棄權。我們還引入了 MMA-Bench，這是一個透過編程生成的基準測試，用於信念動態，並具有受控的說話者可靠性和結構化的文本-視覺矛盾。使用這個框架，我們揭示了「Visual Placebo Effect」，揭示了基於 RAG 的代理如何從基礎模型繼承潛在的視覺偏差。在 FEVER 上，MMA 在匹配基準準確度的同時將變異數減少了 35.2%，並提高了選擇性效用；在 LoCoMo 上，一個側重安全的配置提高了可操作的準確度並減少了錯誤答案；在 MMA-Bench 上，MMA 在 Vision 模式下達到 41.18% 的 Type-B 準確度，而基準在相同協議下崩潰到 0.0%。代碼：https://github.com/AIGeeksGroup/MMA。",
      "title": "MMA: Multimodal Memory Agent",
      "title_zh": "MMA: 多模態記憶代理"
    },
    {
      "arxiv_id": "2602.16444",
      "authors": [
        "Yixue Zhang",
        "Kun Wu",
        "Zhi Gao",
        "Zhen Zhao",
        "Pei Ren",
        "Zhiyuan Xu",
        "Fei Liao",
        "Xinhua Wang",
        "Shichao Fan",
        "Di Wu",
        "Qiuxuan Feng",
        "Meng Li",
        "Zhengping Che",
        "Chang Liu",
        "Jian Tang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591573+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
          "url": "https://arxiv.org/abs/2602.16444"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
        "url": "https://arxiv.org/abs/2602.16444"
      },
      "published_at": "2026-02-18T13:29:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7063250801773987,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.2663250801774
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16444",
      "summary": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate phys",
      "summary_zh": "通用機器人操作的追求受到多樣化真實世界互動數據稀缺的阻礙。與從視覺或語言網絡收集數據不同，機器人數據收集是一個主動過程，會產生高昂的物理成本。因此，自動化任務策劃以最大化數據價值仍然是一個關鍵但未被充分探索的挑戰。現有手動方法不可擴展且偏向常見任務，而現成的基礎模型常常產生幻覺 phys",
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "title_zh": "RoboGene: 透過多樣性驅動的代理框架提升 VLA 預訓練以進行真實世界任務生成"
    },
    {
      "arxiv_id": "2602.16435",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Wangyang Ying",
        "Yanjie Fu"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591633+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.16435"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.16435"
      },
      "published_at": "2026-02-18T13:12:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7054655873540453,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.265465587354043
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16435",
      "summary": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over featu",
      "summary_zh": "自動特徵工程 (AFE) 使 AI 系統能夠自主地從原始表格數據中構建高效用表示。然而，現有的 AFE 方法依賴統計啟發法，產生在分佈偏移下失效的脆弱特徵。我們引入了 CAFE，一個將 AFE 重新定義為因果引導的順序決策過程的框架，將因果發現與強化學習驅動的特徵構建相結合。第一階段學習了一個稀疏的有向無環圖，覆蓋 featu",
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "title_zh": "透過多代理強化學習進行因果引導的自動特徵工程"
    },
    {
      "arxiv_id": "2602.16138",
      "authors": [
        "Parsa Madinei",
        "Srijita Karmakar",
        "Russell Cohen Hoffing",
        "Felix Gervitz",
        "Miguel P. Eckstein"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.214182+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.16138"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.16138"
      },
      "published_at": "2026-02-18T02:06:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.6735909575207228,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.233590957520722
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16138",
      "summary": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%",
      "summary_zh": "我們引入了 IRIS (Intent Resolution via Inference-time Saccades)，這是一種新穎的免訓練方法，它使用即時眼動追蹤數據來解決開放式 VQA 中的歧義。透過一項包含 500 個獨特圖像-問題對的全面用戶研究，我們證明了參與者開始口頭提問時最接近的注視點對於大型 VLM 中的消歧最具信息量，使模糊問題的回應準確度增加一倍以上（從 35.2% 提高到 77.2%",
      "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
      "title_zh": "IRIS: 透過推論時眼跳在大型視覺-語言模型中解決開放式 VQA 的意圖歧義"
    },
    {
      "arxiv_id": "2602.16760",
      "authors": [
        "Michael Cunningham"
      ],
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213143+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Privacy-Aware Split Inference with Speculative Decoding for Large Language Models over Wide-Area Networks",
          "url": "https://arxiv.org/abs/2602.16760"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Privacy-Aware Split Inference with Speculative Decoding for Large Language Models over Wide-Area Networks",
        "url": "https://arxiv.org/abs/2602.16760"
      },
      "published_at": "2026-02-18T14:13:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.87,
        "llm_relevance_score": 19.14,
        "recency_score": 0.7084578966488724,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.048457896648873
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16760",
      "summary": "We present a practical system for privacy-aware large language model (LLM) inference that splits a transformer between a trusted local GPU and an untrusted cloud GPU, communicating only intermediate activations over the network. Our system addresses the unique challenges of autoregressive LLM decoding over high-latency wide-area networks (WANs), contributing: (1) an asymmetric layer split where embedding and unembedding layers remain local, ensuring raw tokens never leave the trusted device; (2)",
      "summary_zh": "我們提出了一個實用的系統，用於具隱私意識的 large language model (LLM) inference，該系統將一個 transformer 劃分在一個受信任的 local GPU 和一個不受信任的 cloud GPU 之間，僅透過網路傳輸 intermediate activations。我們的系統解決了在高延遲 wide-area networks (WANs) 上進行 autoregressive LLM decoding 的獨特挑戰，其貢獻包括：(1) 一種 asymmetric layer split，其中 embedding 和 unembedding layers 保持在本地，確保原始 tokens 永遠不會離開受信任的設備；(2)",
      "title": "Privacy-Aware Split Inference with Speculative Decoding for Large Language Models over Wide-Area Networks",
      "title_zh": "適用於廣域網路中 Large Language Models 的具隱私意識 Split Inference 搭配 Speculative Decoding"
    },
    {
      "arxiv_id": "2602.16438",
      "authors": [
        "Eva Paraschou",
        "Line Harder Clemmensen",
        "Sneha Das"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213261+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
          "url": "https://arxiv.org/abs/2602.16438"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
        "url": "https://arxiv.org/abs/2602.16438"
      },
      "published_at": "2026-02-18T13:19:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.87,
        "llm_relevance_score": 19.14,
        "recency_score": 0.7058086053802468,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.04580860538025
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16438",
      "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In ",
      "summary_zh": "傳統的 large language model (LLM) fairness alignment 主要關注於減輕單一 sensitive attributes 上的偏見，卻忽略了公平性作為一個本質上多維且與上下文相關的價值。這種方法可能會導致系統在實現狹隘的 fairness metrics 的同時，加劇非目標屬性上的差異，這種現象被稱為 bias spillover。儘管 bias spillover 在 machine learning 中已被廣泛研究，但在 LLM alignment 領域仍嚴重缺乏探索。",
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
      "title_zh": "內部公平性動態：Targeted LLM Alignment 中的 Bias Spillover 效應"
    },
    {
      "arxiv_id": "2602.16424",
      "authors": [
        "Philipp Schoenegger",
        "Matt Carlson",
        "Chris Schneider",
        "Chris Daly"
      ],
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591750+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Verifiable Semantics for Agent-to-Agent Communication",
          "url": "https://arxiv.org/abs/2602.16424"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Verifiable Semantics for Agent-to-Agent Communication",
        "url": "https://arxiv.org/abs/2602.16424"
      },
      "published_at": "2026-02-18T12:55:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.87,
        "llm_relevance_score": 19.14,
        "recency_score": 0.7046715692357848,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.044671569235785
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16424",
      "summary": "Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents",
      "summary_zh": "Multiagent AI systems 需要一致的通訊，但我們缺乏驗證 agents 對所用術語是否共享相同理解的方法。Natural language 雖然可解釋，但容易受到 semantic drift 的影響，而 learned protocols 雖然高效但卻不透明。我們提出了一種基於 stimulus-meaning model 的 certification protocol，其中 agents 會在共享的可觀察事件上進行測試，並且如果實證分歧低於統計閾值，則術語會被認證。在此 protocol 中，agents",
      "title": "Verifiable Semantics for Agent-to-Agent Communication",
      "title_zh": "Agent 間通訊的可驗證語義"
    },
    {
      "arxiv_id": "2602.16813",
      "authors": [
        "Chanhyuk Lee",
        "Jaehoon Yoo",
        "Manan Agarwal",
        "Sheel Shah",
        "Jerry Huang",
        "Aditi Raghunathan",
        "Seunghoon Hong",
        "Nicholas M. Boffi",
        "Jinwoo Kim"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211154+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "One-step Language Modeling via Continuous Denoising",
          "url": "https://arxiv.org/abs/2602.16813"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "One-step Language Modeling via Continuous Denoising",
        "url": "https://arxiv.org/abs/2602.16813"
      },
      "published_at": "2026-02-18T19:23:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7238739331153238,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 28.033873933115323
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16813",
      "summary": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a",
      "summary_zh": "基於 discrete diffusion 的 Language models 因其潛力能比 autoregressive models 提供更快的生成速度而引起了廣泛關注。然而，在實踐中，它們在 few-step 階段卻表現出樣本品質的急劇下降，未能實現這一承諾。在此，我們展示了利用 flow-based continuous denoising 的 language models 可以在品質和速度方面超越 discrete diffusion。透過重新審視 discrete modalities 上 flows 的基本原理，我們構建了一個",
      "title": "One-step Language Modeling via Continuous Denoising",
      "title_zh": "透過 Continuous Denoising 實現一步式 Language Modeling"
    },
    {
      "arxiv_id": "2602.16485",
      "authors": [
        "Jeffrey T. H. Wong",
        "Zixi Zhang",
        "Junyi Liu",
        "Yiren Zhao"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591491+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
          "url": "https://arxiv.org/abs/2602.16485"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
        "url": "https://arxiv.org/abs/2602.16485"
      },
      "published_at": "2026-02-18T14:19:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8599999999999999,
        "llm_relevance_score": 18.919999999999998,
        "recency_score": 0.7087474067560152,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.828747406756015
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16485",
      "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models wit",
      "summary_zh": "現有的 Multi-Agent Systems (MAS) 通常依賴於靜態、同質的 model configurations，這限制了它們利用不同 post-trained models 獨特優勢的能力。為了解決這個問題，我們引入了 Team-of-Thoughts，這是一種新穎的 MAS architecture，它透過 orchestrator-tool paradigm 利用 heterogeneous agents 的互補能力。我們的框架引入了兩個關鍵機制來優化性能：(1) 一種 orchestrator calibration scheme，用於識別具有",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "title_zh": "Team of Thoughts：透過 Orchestrated Tool Calling 對 Agentic Systems 進行高效的 Test-time Scaling"
    },
    {
      "arxiv_id": "2602.17526",
      "authors": [
        "Peter Balogh"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.805748+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
          "url": "https://arxiv.org/abs/2602.17526"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
        "url": "https://arxiv.org/abs/2602.17526"
      },
      "published_at": "2026-02-19T16:37:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7908433364621386,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.79084333646214
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17526",
      "summary": "Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question \"has this token appeared before in the context?\" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\\% even at 180 unique context tokens -- we",
      "summary_zh": "部分 Transformer attention heads 似乎扮演著成員資格測試器 (membership testers) 的角色，專注於回答「這個 token 之前是否在上下文 (context) 中出現過？」這個問題。我們在四種語言模型（GPT-2 small, medium, 和 large；Pythia-160M）中識別出這些 heads，並展示它們形成了一個成員資格測試策略的光譜。其中兩個 heads (GPT-2 small 中的 L0H1 和 L0H5) 即使在 180 個獨特的上下文 token 下，也能作為高精準度的成員資格過濾器 (high-precision membership filters)，其誤報率 (false positive rates) 僅為 0-4%。",
      "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
      "title_zh": "影響焦慮：Transformer Attention Heads 中的 Bloom Filters"
    },
    {
      "arxiv_id": "2602.16984",
      "authors": [
        "Vishal Srivastava"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.552961+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-safety",
          "tier": 1,
          "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
          "url": "https://arxiv.org/abs/2602.16984"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-safety",
        "tier": 1,
        "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
        "url": "https://arxiv.org/abs/2602.16984"
      },
      "published_at": "2026-02-19T01:03:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7411722040553492,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.74117220405535
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16984",
      "summary": "Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evalua",
      "summary_zh": "AI 系統的 Black-box safety evaluation 假設模型在測試分佈 (test distributions) 上的行為能夠可靠地預測部署性能 (deployment performance)。我們透過潛在上下文條件化策略 (latent context-conditioned policies) 將此假設形式化並提出挑戰——這些模型的輸出取決於在評估時罕見但在部署時普遍存在的未觀測內部變數 (unobserved internal variables)。我們確立了基本限制，表明沒有任何 black-box evaluator 能夠可靠地估計此類模型的部署風險 (deployment risk)。(1) 被動評估 (Passive evaluation)：對於評估者來說，",
      "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
      "title_zh": "黑箱安全評估的基本限制：來自潛在上下文條件化 (Latent Context Conditioning) 的資訊理論和計算障礙"
    },
    {
      "arxiv_id": "2602.17664",
      "authors": [
        "Aidar Myrzakhan",
        "Tianyi Li",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhiqiang Shen"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207347+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Sink-Aware Pruning for Diffusion Language Models",
          "url": "https://arxiv.org/abs/2602.17664"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Sink-Aware Pruning for Diffusion Language Models",
        "url": "https://arxiv.org/abs/2602.17664"
      },
      "published_at": "2026-02-19T18:59:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7987119387543551,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.698711938754354
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17664",
      "summary": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across ti",
      "summary_zh": "Diffusion Language Models (DLMs) 由於迭代去噪 (iterative denoising) 而產生高昂的推理成本 (inference cost)，這促使了對高效 pruning 的需求。現有的 pruning 啟發式方法 (pruning heuristics) 主要繼承自 autoregressive (AR) LLMs，通常會保留 attention sink tokens，因為 AR sinks 作為穩定的全局錨點 (stable global anchors)。我們證明了這個假設對於 DLMs 不成立：attention-sink 位置在整個生成軌跡 (full generation trajectory) 中表現出顯著更高的變異性（衡量標準是主導 sink 位置如何隨時間推移而變化）。",
      "title": "Sink-Aware Pruning for Diffusion Language Models",
      "title_zh": "Diffusion Language Models 的 Sink-Aware Pruning"
    },
    {
      "arxiv_id": "2602.17616",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207876+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
          "url": "https://arxiv.org/abs/2602.17616"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
        "url": "https://arxiv.org/abs/2602.17616"
      },
      "published_at": "2026-02-19T18:40:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7976597008873272,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.69765970088733
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17616",
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplifi",
      "summary_zh": "Reinforcement learning (RL) 被廣泛用於改進大型語言模型在推理任務上的表現，而 asynchronous RL training 因其能提高端到端吞吐量 (end-to-end throughput) 而具有吸引力。然而，對於廣泛採用的無批評家策略梯度方法 (critic-free policy-gradient methods)，例如 REINFORCE 和 GRPO，高異步性會使策略梯度估計器 (policy-gradient estimator) 的變異數顯著$\textbf{更高}$：在過時的 rollout 上進行訓練會產生重尾重要性比率 (heavy-tailed importance ratios)，導致一小部分樣本主導更新。這加劇了",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "title_zh": "穩定異步：用於 LLMs 的變異數控制型 Off-Policy RL"
    },
    {
      "arxiv_id": "2602.17598",
      "authors": [
        "Jayadev Billa"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207937+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
          "url": "https://arxiv.org/abs/2602.17598"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
        "url": "https://arxiv.org/abs/2602.17598"
      },
      "published_at": "2026-02-19T18:22:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7966521844827742,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.696652184482772
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17598",
      "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text represen",
      "summary_zh": "目前的 Speech LLMs 大多執行隱式 ASR：在可以從文字稿 (transcript) 解決的任務上，它們在行為和機制上與簡單的 Whisper$\to$LLM cascades 等效。我們首次透過跨四個 Speech LLMs 和六個任務的 matched-backbone testing 證明了這一點，同時控制了 LLM backbone。Ultravox 與其匹配的 cascade 在統計上無法區分 ($κ{=}0.93$)；logit lens 揭示了隱藏狀態 (hidden states) 中出現的字面文本；LEACE concept erasure 證實了文本表徵 (text represen)",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "title_zh": "級聯等效假說 (Cascade Equivalence Hypothesis)：Speech LLMs 何時表現得像 ASR$\rightarrow$LLM Pipelines？"
    },
    {
      "arxiv_id": "2602.17550",
      "authors": [
        "Xiaoliang Fu",
        "Jiaye Lin",
        "Yangyi Fang",
        "Binbin Zheng",
        "Chaowen Hu",
        "Zekai Shao",
        "Cong Qin",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208064+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17550"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17550"
      },
      "published_at": "2026-02-19T17:05:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7923862518456303,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.69238625184563
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17550",
      "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the t",
      "summary_zh": "現有的 Reinforcement Learning with Verifiable Rewards (RLVR) 演算法，例如 GRPO，依賴於僵化、均勻且對稱的信任區域機制，這與 Large Language Models (LLMs) 複雜的優化動態從根本上不符。在本文中，我們指出了這些方法中的三個關鍵挑戰：(1) 硬 clipping 的二元截斷導致的梯度利用效率低下，(2) 由於忽略了 t 而產生均勻比例約束導致的不敏感機率質量",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
      "title_zh": "MASPO：統一梯度利用、機率質量和訊號可靠性，實現穩健且樣本高效的LLM推理"
    },
    {
      "arxiv_id": "2602.17547",
      "authors": [
        "Yue Liu",
        "Zhiyuan Hu",
        "Flood Sung",
        "Jiaheng Zhang",
        "Bryan Hooi"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208152+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
          "url": "https://arxiv.org/abs/2602.17547"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
        "url": "https://arxiv.org/abs/2602.17547"
      },
      "published_at": "2026-02-19T17:01:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7921551728894946,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.692155172889493
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17547",
      "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using ",
      "summary_zh": "本文介紹了 KLong，一個開源的 LLM Agent，旨在解決超長時程任務。其原則是首先透過 trajectory-splitting SFT 對模型進行冷啟動，然後透過漸進式 RL 訓練來擴展。具體來說，我們首先使用全面的 SFT 策略來激活基礎模型的 agentic 基本能力。接著，我們引入 Research-Factory，這是一個自動化流程，透過收集研究論文和建構評估標準來生成高品質的訓練資料。使用",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "title_zh": "KLong：訓練 LLM Agent 處理超長時程任務"
    },
    {
      "arxiv_id": "2602.17532",
      "authors": [
        "Ihor Kendiukhov"
      ],
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208337+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
          "url": "https://arxiv.org/abs/2602.17532"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
        "url": "https://arxiv.org/abs/2602.17532"
      },
      "published_at": "2026-02-19T16:43:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.791169260349,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.691169260349
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17532",
      "summary": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incre",
      "summary_zh": "我們提出了一個系統性評估框架——包含三十七項分析、153 項統計測試、四種細胞類型、兩種擾動模式——用於評估單細胞基礎模型的機械可解釋性。將此框架應用於 scGPT 和 Geneformer，我們發現注意力模式編碼了具有層次特異性組織的結構化生物信息——早期層次中的 protein-protein interactions，晚期層次中的 transcriptional regulation——但這種結構並未提供額外的",
      "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
      "title_zh": "單細胞基礎模型可解釋性的系統性評估揭示注意力捕捉的是共表達而非獨特的調控訊號"
    },
    {
      "arxiv_id": "2602.17497",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208552+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
          "url": "https://arxiv.org/abs/2602.17497"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
        "url": "https://arxiv.org/abs/2602.17497"
      },
      "published_at": "2026-02-19T16:13:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7895373277383276,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.68953732773833
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17497",
      "summary": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large languag",
      "summary_zh": "從自採樣數據和稀疏環境回饋中學習，仍然是訓練自我演化 agent 的一個基本挑戰。時間信用分配透過將稀疏回饋轉化為密集監督訊號來緩解這個問題。然而，以往的方法通常依賴於學習任務特定的價值函數來進行信用分配，這導致樣本效率低下和泛化能力有限。在這項工作中，我們提出利用來自大型語言模型的預訓練知識",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "title_zh": "結合大型語言模型的回溯式 In-Context Learning 用於時間信用分配"
    },
    {
      "arxiv_id": "2602.16136",
      "authors": [
        "Hongyeon Yu",
        "Dongchan Kim",
        "Young-Bum Kim"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.773066+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "Retrieval Collapses When AI Pollutes the Web",
          "url": "https://arxiv.org/abs/2602.16136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "Retrieval Collapses When AI Pollutes the Web",
        "url": "https://arxiv.org/abs/2602.16136"
      },
      "published_at": "2026-02-18T02:01:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.6733399669231239,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.673339966923123
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16136",
      "summary": "The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval ",
      "summary_zh": "網路上 AI 生成內容的迅速增長對資訊檢索構成結構性風險，因為搜尋引擎和 Retrieval-Augmented Generation (RAG) 系統越來越多地消耗 Large Language Models (LLMs) 生成的證據。我們將這種生態系統層級的失敗模式描述為 Retrieval Collapse，這是一個兩階段的過程：(1) AI 生成內容主導搜尋結果，侵蝕了來源多樣性，以及 (2) 低品質或惡意內容滲透到檢索",
      "title": "Retrieval Collapses When AI Pollutes the Web",
      "title_zh": "當 AI 污染網路時檢索崩潰"
    },
    {
      "arxiv_id": "2602.17229",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209430+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
          "url": "https://arxiv.org/abs/2602.17229"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
        "url": "https://arxiv.org/abs/2602.17229"
      },
      "published_at": "2026-02-19T10:19:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7703431012885739,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.670343101288573
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17229",
      "summary": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residu",
      "summary_zh": "大型語言模型 (Large Language Models, LLMs) 的 black-box 特性，要求新穎的評估框架，以超越表面層次的性能指標。本研究使用 Bloom's Taxonomy 作為分層視角，探討 LLMs 內部認知複雜度的神經表示。透過分析來自不同 LLMs 的高維 activation vectors，我們探究了從基本回憶 (Remember) 到抽象綜合 (Create) 的不同認知層次，是否在模型 residu 內部是 linearly separable 的。",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "title_zh": "LLMs 中認知複雜度的 Mechanistic Interpretability：透過使用 Bloom's Taxonomy 的 Linear Probing"
    },
    {
      "arxiv_id": "2602.17168",
      "authors": [
        "Siyuan Liang",
        "Yongcheng Jing",
        "Yingjie Wang",
        "Jiaxing Huang",
        "Ee-chien Chang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.565230+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
          "url": "https://arxiv.org/abs/2602.17168"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
        "url": "https://arxiv.org/abs/2602.17168"
      },
      "published_at": "2026-02-19T08:31:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7645977592048674,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.664597759204867
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17168",
      "summary": "Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both ch",
      "summary_zh": "針對多模態 contrastive learning 模型進行 backdoor 攻擊的研究面臨兩個關鍵挑戰：隱蔽性 (stealthiness) 和持久性 (persistence)。現有方法在強檢測或持續 fine-tuning 下往往會失效，這主要歸因於 (1) 暴露 trigger patterns 的 cross-modal inconsistency，以及 (2) 在低 poisoning rates 下加速 backdoor 遺忘的 gradient dilution。這些耦合的原因仍未得到充分建模和解決。我們提出了 BadCLIP++，一個統一的框架，旨在解決這兩個挑戰。",
      "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
      "title_zh": "BadCLIP++: 多模態 Contrastive Learning 中的隱蔽且持久的 Backdoor"
    },
    {
      "arxiv_id": "2602.17095",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593795+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
          "url": "https://arxiv.org/abs/2602.17095"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
        "url": "https://arxiv.org/abs/2602.17095"
      },
      "published_at": "2026-02-19T05:35:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7553156718006389,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.655315671800636
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17095",
      "summary": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggrega",
      "summary_zh": "諸如 low-rank adaptation (LoRA) 等參數高效 fine-tuning 技術，使大型語言模型 (LLMs) 能夠有效地適應下游任務。Federated learning (FL) 透過實現跨分散式客戶端進行協同 fine-tuning 而不共享私人數據，進一步促進了這一過程。然而，在 LoRA 中使用兩個獨立的 low-rank matrices 進行 federated fine-tuning 引入了兩種類型的挑戰。第一個挑戰源於單獨 aggregation 所引起的錯誤。",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
      "title_zh": "FLoRG: 結合 Low-rank Gram Matrices 和 Procrustes Alignment 的 Federated Fine-tuning"
    },
    {
      "arxiv_id": "2602.17053",
      "authors": [
        "Yunseok Han",
        "Yejoon Lee",
        "Jaeyoung Do"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535975+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
          "url": "https://arxiv.org/abs/2602.17053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
        "url": "https://arxiv.org/abs/2602.17053"
      },
      "published_at": "2026-02-19T03:49:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7497882720965843,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.649788272096583
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17053",
      "summary": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To o",
      "summary_zh": "大型推理模型 (Large Reasoning Models, LRMs) 展現出強大的性能，但其產生的理由 (rationales) 往往聽起來合理卻未能反映其真實的決策過程，從而損害了可靠性和信任。我們引入了一個推理忠實度的正式框架，由兩個可測試條件定義：立場一致性 (stance consistency, 即將推理與答案聯繫起來的連貫立場) 和因果影響 (causal influence, 即在 output-level 干預下，所陳述的推理在因果上驅動答案)，這些條件明確地與準確性 decoupled。",
      "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
      "title_zh": "RFEval: 在大型推理模型中，對反事實推理干預下的推理忠實度進行基準測試"
    },
    {
      "arxiv_id": "2602.17025",
      "authors": [
        "Gagan Mundada",
        "Zihan Huang",
        "Rohan Surana",
        "Sheldon Yu",
        "Jennifer Yuntong Zhang",
        "Xintong Li",
        "Tong Yu",
        "Lina Yao",
        "Jingbo Shang",
        "Julian McAuley",
        "Junda Wu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536037+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
          "url": "https://arxiv.org/abs/2602.17025"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
        "url": "https://arxiv.org/abs/2602.17025"
      },
      "published_at": "2026-02-19T02:43:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7463578781960998,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.6463578781961
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17025",
      "summary": "Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate bec",
      "summary_zh": "Group Relative Policy Optimization (GRPO) 對於在複雜推理上訓練語言模型是有效的。然而，由於目標是相對於一組採樣的 trajectories 定義的，長時間的深思熟慮可能會創造更多機會實現相對收益，導致推理效率低下和過度思考，並使正確性和 rollout efficiency 之間的權衡變得複雜。在實踐中很難控制這種行為，考慮到 (i) Length penalties 難以校準。",
      "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
      "title_zh": "WS-GRPO: 針對 Rollout-Efficient Reasoning 的 Weakly-Supervised Group-Relative Policy Optimization"
    },
    {
      "arxiv_id": "2602.17022",
      "authors": [
        "Takyoung Kim",
        "Jinseok Nam",
        "Chandrayee Basu",
        "Xing Fan",
        "Chengyuan Ma",
        "Heng Ji",
        "Gokhan Tur",
        "Dilek Hakkani-Tür"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536116+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
          "url": "https://arxiv.org/abs/2602.17022"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
        "url": "https://arxiv.org/abs/2602.17022"
      },
      "published_at": "2026-02-19T02:37:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.746041779662183,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.646041779662184
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17022",
      "summary": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to signific",
      "summary_zh": "搭載了工具整合的 large language models (LLMs) 驅動的 conversational agents 在固定的 task-oriented dialogue datasets 上表現出色，但仍容易受到意外的、用戶引起的錯誤的影響。本研究不專注於錯誤預防，而是著重於錯誤恢復，這需要準確診斷錯誤的對話上下文並執行適當的恢復計劃。在實際限制下，由於顯著的原因，排除 model fine-tuning 或 prompt modification...",
      "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
      "title_zh": "ReIn: 以推理起始進行對話錯誤恢復"
    },
    {
      "arxiv_id": "2602.17009",
      "authors": [
        "Nikunj Gupta",
        "James Zachary Hare",
        "Jesse Milzman",
        "Rajgopal Kannan",
        "Viktor Prasanna"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584302+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17009"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17009"
      },
      "published_at": "2026-02-19T02:13:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7447994122898081,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.644799412289807
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17009",
      "summary": "Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \\textit{coordination cont",
      "summary_zh": "行動協調是 multi-agent reinforcement learning (MARL) 中最基本的合作形式。成功的去中心化決策往往不僅取決於良好的個體行動，還取決於在不同 agents 間選擇相容的行動以同步行為、避免衝突並滿足全局約束。在本文中，我們提出了 Action Graph Policies (AGP)，它對 agents 可用行動選擇之間的依賴關係進行建模。它構建了我們所謂的 coordination cont...",
      "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
      "title_zh": "Action-Graph Policies：在多智能體強化學習中學習行動共依賴性"
    },
    {
      "arxiv_id": "2602.16958",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584811+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Automating Agent Hijacking via Structural Template Injection",
          "url": "https://arxiv.org/abs/2602.16958"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Automating Agent Hijacking via Structural Template Injection",
        "url": "https://arxiv.org/abs/2602.16958"
      },
      "published_at": "2026-02-18T23:52:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7375293684883213,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.63752936848832
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16958",
      "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Te",
      "summary_zh": "Agent hijacking 被 OWASP 強調為 Large Language Model (LLM) 生態系統的關鍵威脅，它使攻擊者能夠透過將惡意指令注入檢索到的內容來操縱執行。大多數現有攻擊依賴於手動製作的、語義驅動的 prompt 操縱，這通常導致攻擊成功率低且對閉源商業模型的遷移能力有限。在本文中，我們提出了 Phantom，一個基於 Structured Te... 的自動化 agent hijacking 框架。",
      "title": "Automating Agent Hijacking via Structural Template Injection",
      "title_zh": "透過結構化模板注入實現自動化 Agent 劫持"
    },
    {
      "arxiv_id": "2602.16935",
      "authors": [
        "Justin Albrethsen",
        "Yash Datta",
        "Kunal Kumar",
        "Sharath Rajasekar"
      ],
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210556+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
          "url": "https://arxiv.org/abs/2602.16935"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
        "url": "https://arxiv.org/abs/2602.16935"
      },
      "published_at": "2026-02-18T22:57:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7347424498738282,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.634742449873826
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16935",
      "summary": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext disca",
      "summary_zh": "儘管 Large Language Model (LLM) 的能力不斷擴展，但安全防護措施在很大程度上仍然是無狀態的，將多輪對話視為一系列不相關的事件。這種缺乏時間感知導致了一個「安全鴻溝」(Safety Gap)，其中諸如 Crescendo 和 ActorAttack 等對抗性策略會緩慢地將惡意意圖滲透過輪次邊界，以繞過無狀態過濾器。我們介紹了 DeepContext，這是一個有狀態的監控框架，旨在繪製用戶意圖的時間軌跡。DeepContext disca...",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
      "title_zh": "DeepContext：在 LLMs 中對多輪對抗性意圖漂移進行有狀態實時檢測"
    },
    {
      "arxiv_id": "2602.16931",
      "authors": [
        "Idhant Gulati",
        "Shivam Raval"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.589468+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
          "url": "https://arxiv.org/abs/2602.16931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
        "url": "https://arxiv.org/abs/2602.16931"
      },
      "published_at": "2026-02-18T22:47:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7342196422068806,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.63421964220688
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16931",
      "summary": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimod",
      "summary_zh": "終身多模態 agents 必須透過後訓練不斷適應新任務，但這在獲取能力和保持安全對齊之間產生了根本性的矛盾。我們證明了在狹窄領域的有害數據集上 fine-tuning 已對齊的 vision-language models，會導致嚴重的 emergent misalignment，這種不對齊現象廣泛泛化到不相關的任務和模態。透過在 Gemma3-4B 上的實驗，我們展示了 misalignment 隨 LoRA rank 單調地擴展，並且 multimod...",
      "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
      "title_zh": "狹義 fine-tuning 侵蝕了視覺語言 agents 的安全對齊"
    },
    {
      "arxiv_id": "2602.16928",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210683+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
          "url": "https://arxiv.org/abs/2602.16928"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
        "url": "https://arxiv.org/abs/2602.16928"
      },
      "published_at": "2026-02-18T22:41:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7338899972239371,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.633889997223935
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16928",
      "summary": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models"
    },
    {
      "arxiv_id": "2602.16891",
      "authors": [
        "Hongwei Li",
        "Zhun Wang",
        "Qinrun Dai",
        "Yuzhou Nie",
        "Jinjun Peng",
        "Ruitong Liu",
        "Jingyang Zhang",
        "Kaijie Zhu",
        "Jingxuan He",
        "Lun Wang",
        "Yangruibo Ding",
        "Yueqi Chen",
        "Wenbo Guo",
        "Dawn Song"
      ],
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.590628+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenSage: Self-programming Agent Generation Engine",
          "url": "https://arxiv.org/abs/2602.16891"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenSage: Self-programming Agent Generation Engine",
        "url": "https://arxiv.org/abs/2602.16891"
      },
      "published_at": "2026-02-18T21:16:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7295952562422041,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.629595256242204
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16891",
      "summary": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with",
      "title": "OpenSage: Self-programming Agent Generation Engine"
    },
    {
      "arxiv_id": "2602.16849",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:12.406282+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
          "url": "https://arxiv.org/abs/2602.16849"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
        "url": "https://arxiv.org/abs/2602.16849"
      },
      "published_at": "2026-02-18T20:25:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.727002380317683,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.627002380317684
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16849",
      "summary": "We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.",
      "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking"
    },
    {
      "arxiv_id": "2602.16800",
      "authors": [
        "Simon Lermen",
        "Daniel Paleka",
        "Joshua Swanson",
        "Michael Aerni",
        "Nicholas Carlini",
        "Florian Tramèr"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211323+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large-scale online deanonymization with LLMs",
          "url": "https://arxiv.org/abs/2602.16800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large-scale online deanonymization with LLMs",
        "url": "https://arxiv.org/abs/2602.16800"
      },
      "published_at": "2026-02-18T19:02:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7228550275285192,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.62285502752852
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16800",
      "summary": "We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that",
      "title": "Large-scale online deanonymization with LLMs"
    },
    {
      "arxiv_id": "2602.16787",
      "authors": [
        "Victoria Lin",
        "Xinnuo Xu",
        "Rachel Lawrence",
        "Risa Ueno",
        "Amit Sharma",
        "Javier Gonzalez",
        "Niranjani Prasad"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536521+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency",
          "url": "https://arxiv.org/abs/2602.16787"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency",
        "url": "https://arxiv.org/abs/2602.16787"
      },
      "published_at": "2026-02-18T19:00:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7227186684360751,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.622718668436075
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16787",
      "summary": "Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (",
      "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency"
    },
    {
      "arxiv_id": "2602.16708",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.590940+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Policy Compiler for Secure Agentic Systems",
          "url": "https://arxiv.org/abs/2602.16708"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Policy Compiler for Secure Agentic Systems",
        "url": "https://arxiv.org/abs/2602.16708"
      },
      "published_at": "2026-02-18T18:57:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7225722992702858,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.622572299270285
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16708",
      "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture",
      "summary_zh": "基於 LLM 的 agents 正日益部署於需要複雜授權政策的環境中：客戶服務協議、審批流程、數據存取限制和法規遵循。將這些政策嵌入到 prompts 中並不能提供強制執行保證。我們提出 PCAS，一個用於 Agentic Systems 的 Policy Compiler，它提供確定性的政策強制執行。強制執行這些政策需要追蹤 agents 之間的資訊流，而線性的訊息歷史記錄無法捕捉這些資訊流",
      "title": "Policy Compiler for Secure Agentic Systems",
      "title_zh": "用於安全 Agentic Systems 的政策編譯器"
    },
    {
      "arxiv_id": "2602.16687",
      "authors": [
        "Potsawee Manakul",
        "Woody Haosheng Gan",
        "Martijn Bartelds",
        "Guangzhi Sun",
        "William Held",
        "Diyi Yang"
      ],
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211628+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
          "url": "https://arxiv.org/abs/2602.16687"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
        "url": "https://arxiv.org/abs/2602.16687"
      },
      "published_at": "2026-02-18T18:32:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7213473075829038,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.6213473075829
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16687",
      "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for b",
      "summary_zh": "當前的音訊語言模型主要以文本為先，無論是擴展預訓練的文本 LLM 主幹，還是依賴僅語義的音訊 tokens，都限制了通用音訊建模。本文提出了一項關於原生音訊 foundation models 的系統性實證研究，該模型將 next-token prediction 大規模應用於音訊，聯合建模語義內容、聲學細節和文本，以支持通用音訊生成和跨模態能力。我們提供了全面的實證見解，用於 b",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "title_zh": "透過交錯的語義、聲學和文本 Tokens 擴展開放離散音訊 Foundation Models"
    },
    {
      "arxiv_id": "2602.16639",
      "authors": [
        "Adib Sakhawat",
        "Fardeen Sadab"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211983+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
          "url": "https://arxiv.org/abs/2602.16639"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
        "url": "https://arxiv.org/abs/2602.16639"
      },
      "published_at": "2026-02-18T17:28:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7181334721542294,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.618133472154227
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16639",
      "summary": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabiliti",
      "summary_zh": "評估大型語言模型 (LLMs) 的社會智能，越來越需要超越靜態文本生成，轉向動態的、對抗性的互動。我們介紹了 Adversarial Resource Extraction Game (AREG)，這是一個基準測試，它將說服和抵抗操作化為一場多輪、零和的金融資源談判。透過對 frontier models 進行循環賽，AREG 能夠聯合評估攻擊性（說服）和防禦性（抵抗）能力",
      "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
      "title_zh": "AREG：用於評估大型語言模型說服與抵抗能力的對抗性資源提取遊戲"
    },
    {
      "arxiv_id": "2602.16603",
      "authors": [
        "Chia-chi Hsieh",
        "Zan Zong",
        "Xinyang Chen",
        "Jianjiang Li",
        "Jidong Zhai",
        "Lijie Wen"
      ],
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.212708+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
          "url": "https://arxiv.org/abs/2602.16603"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
        "url": "https://arxiv.org/abs/2602.16603"
      },
      "published_at": "2026-02-18T16:57:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7166032529518314,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.61660325295183
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16603",
      "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness ",
      "summary_zh": "對大型語言模型 (LLMs) 不斷增長的需求要求服務系統處理大量具有不同服務級別目標 (SLOs) 的並發請求。這加劇了計算密集型 prefill 階段的 head-of-line (HoL) 阻塞，其中長時間運行的請求壟斷了資源並延遲了更高優先級的請求，導致廣泛的 time-to-first-token (TTFT) SLO 違規。雖然 chunked prefill 實現了可中斷性，但它在響應性之間引入了固有的權衡",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "title_zh": "FlowPrefill：將搶佔與預填充調度粒度解耦以緩解 LLM 服務中的隊頭阻塞"
    },
    {
      "arxiv_id": "2602.16584",
      "authors": [
        "Akhil Ramidi",
        "Kevin Scharp"
      ],
      "categories": [
        "q-bio.NC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.594324+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
          "url": "https://arxiv.org/abs/2602.16584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
        "url": "https://arxiv.org/abs/2602.16584"
      },
      "published_at": "2026-02-18T16:29:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7151963114715726,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.61519631147157
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16584",
      "summary": "There is growing evidence that independently trained AI systems come to represent the world in the same way. In other words, independently trained embeddings from text, vision, audio, and neural signals share an underlying geometry. We call this the Representational Alignment Hypothesis (RAH) and investigate evidence for and consequences of this claim. The evidence is of two kinds: (i) internal structure comparison techniques, such as representational similarity analysis and topological data ana",
      "summary_zh": "越來越多的證據表明，獨立訓練的 AI 系統會以相同的方式表徵世界。換句話說，來自文本、視覺、音訊和神經信號的獨立訓練 embeddings 共享一個底層幾何。我們將此稱為 Representational Alignment Hypothesis (RAH)，並研究該主張的證據和影響。證據分為兩類：(i) 內部結構比較技術，例如 representational similarity analysis 和 topological data ana",
      "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
      "title_zh": "表徵對齊假說：跨嵌入模態不變語義結構的證據及影響"
    },
    {
      "arxiv_id": "2602.16520",
      "authors": [
        "Doron Shavit"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.212878+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
          "url": "https://arxiv.org/abs/2602.16520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
        "url": "https://arxiv.org/abs/2602.16520"
      },
      "published_at": "2026-02-18T15:07:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7111204243939846,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.611120424393985
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16520",
      "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries work",
      "summary_zh": "Jailbreak prompts 對於大型語言模型（LLMs）構成實際且不斷演變的威脅，尤其是在針對不可信內容執行 tools 的 agentic systems 中。許多攻擊利用 long-context hiding、semantic camouflage 和 lightweight obfuscations，這些技術可以規避 single-pass guardrails。我們提出 RLM-JB，這是一個基於 Recursive Language Models (RLMs) 構建的 end-to-end jailbreak detection 框架，其中一個 root model 協調一個 bounded analysis program，該程序負責轉換輸入、查詢工作。",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "title_zh": "用於 Jailbreak 偵測的 Recursive language models：針對 tool-augmented agents 的程序性防禦"
    },
    {
      "arxiv_id": "2602.16498",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566619+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Fast and Scalable Analytical Diffusion",
          "url": "https://arxiv.org/abs/2602.16498"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Fast and Scalable Analytical Diffusion",
        "url": "https://arxiv.org/abs/2602.16498"
      },
      "published_at": "2026-02-18T14:41:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7098376154010027,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.609837615401002
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16498",
      "summary": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is",
      "summary_zh": "Analytical diffusion models 透過將 denoising score 表述為 empirical-Bayes posterior mean，為 generative modeling 提供了一條數學上透明的途徑。然而，這種可解釋性伴隨著高昂的成本：標準的表述在每個 timestep 都需要對整個 dataset 進行掃描，其擴展性與 dataset size 呈線性關係。在這項工作中，我們提出了第一個系統性研究來解決這個 scalability bottleneck。我們挑戰了普遍認為整個 training data 是",
      "title": "Fast and Scalable Analytical Diffusion",
      "title_zh": "快速且可擴展的 Analytical Diffusion"
    },
    {
      "arxiv_id": "2602.16490",
      "authors": [
        "Ferdinand Kapl",
        "Emmanouil Angelis",
        "Kaitlin Maile",
        "Johannes von Oswald",
        "Stefan Bauer"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213015+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
          "url": "https://arxiv.org/abs/2602.16490"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
        "url": "https://arxiv.org/abs/2602.16490"
      },
      "published_at": "2026-02-18T14:25:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7090550895845924,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.609055089584594
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16490",
      "summary": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from ",
      "summary_zh": "Looping（在深度上重複使用一個 block of layers）和 depth growing（透過複製中間層來訓練 shallow-to-deep models）都被認為與更強的 reasoning 能力相關，但它們之間的關係仍不清楚。我們提供了一個 mechanistic unification：looped 和 depth-grown models 表現出收斂的 depth-wise signatures，包括對 late layers 依賴的增加以及與 looped 或 grown block 對齊的重複模式。這些共享的 signatures 支持了它們的增益源於",
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "title_zh": "從 Growing 到 Looping：LLMs 中 Iterative Computation 的統一視角"
    },
    {
      "arxiv_id": "2602.16346",
      "authors": [
        "Nivya Talokar",
        "Ayush K Tarun",
        "Murari Mandal",
        "Maksym Andriushchenko",
        "Antoine Bosselut"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213522+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
          "url": "https://arxiv.org/abs/2602.16346"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
        "url": "https://arxiv.org/abs/2602.16346"
      },
      "published_at": "2026-02-18T10:31:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6976284790845245,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.597628479084523
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16346",
      "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step ",
      "summary_zh": "基於 LLM 的 agents 透過 tools 和 memory 執行現實世界的 workflows。這些 affordances 也使心懷惡意的 adversaries 能夠利用這些 agents 執行複雜的 misuse scenarios。現有的 agent misuse benchmarks 主要測試 single-prompt instructions，在衡量 agents 如何在 multiple turns 中協助有害或非法任務方面存在空白。我們引入了 STING (Sequential Testing of Illicit N-step Goal execution)，這是一個自動化的 red-teaming framework，它構建了一個逐步的",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
      "title_zh": "好心辦壞事：評估 Multi-Turn、Multilingual LLM Agents 中的非法協助"
    },
    {
      "arxiv_id": "2602.16313",
      "authors": [
        "Zexue He",
        "Yu Wang",
        "Churan Zhi",
        "Yuanzhe Hu",
        "Tzu-Ping Chen",
        "Lang Yin",
        "Ze Chen",
        "Tong Arthur Wu",
        "Siru Ouyang",
        "Zihan Wang",
        "Jiaxin Pei",
        "Julian McAuley",
        "Yejin Choi",
        "Alex Pentland"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591960+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
          "url": "https://arxiv.org/abs/2602.16313"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
        "url": "https://arxiv.org/abs/2602.16313"
      },
      "published_at": "2026-02-18T09:49:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6955926683822216,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.59559266838222
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16313",
      "summary": "Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment,",
      "summary_zh": "現有對帶有 memory 的 agents 的評估通常孤立地評估 memorization 和 action。一類 benchmarks 透過測試對過去對話或文本的 recall 來評估 memorization，但未能捕捉 memory 如何用於指導未來的決策。另一類則專注於 agents 在 single-session tasks 中行動，無需 long-term memory。然而，在現實環境中，memorization 和 action 是緊密耦合的：agents 在與環境互動時獲取 memory，",
      "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks",
      "title_zh": "MemoryArena：在相互依存的 Multi-Session Agentic Tasks 中評測 Agent Memory"
    },
    {
      "arxiv_id": "2602.16756",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.553577+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
          "url": "https://arxiv.org/abs/2602.16756"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
        "url": "https://arxiv.org/abs/2602.16756"
      },
      "published_at": "2026-02-18T09:41:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6952361075406499,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.59523610754065
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16756",
      "summary": "We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.",
      "summary_zh": "我們引入了 NESSiE，這是用於大型語言模型（LLM）的必要安全基準測試（NEceSsary SafEty benchmark）。NESSiE 透過資訊和存取安全性的最少測試案例，揭示了在任務複雜度低的情況下不應存在的安全相關故障。NESSiE 旨在作為語言模型安全性的輕量級、易於使用的健全性檢查（sanity check），因此不足以保證普遍的安全性——但我們認為通過此測試對於任何部署都是必要的。然而，即使是 state-of-the-art 的 LLM 在 NESSiE 上也未能達到 100%，因此未能通過我們對語言模型安全性的必要條件，即使在沒有 adversarial attacks 的情況下也是如此。我們的 Safe & Helpful (SH) metric 允許直接比較這兩個要求，顯示模型偏向於 helpful 而不是 safe。我們進一步發現，對於某些模型，推理能力被禁用，但特別是良性的干擾上下文（benign distraction context）會降低模型性能。總體而言，我們的結果強調了將此類模型部署為野外自主代理（autonomous agents in the wild）的關鍵風險。我們公開了數據集、套件和繪圖程式碼。",
      "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
      "title_zh": "NESSiE：必要的安全基準測試——識別不應存在的錯誤"
    },
    {
      "arxiv_id": "2602.16246",
      "authors": [
        "Yun-Shiuan Chuang",
        "Chaitanya Kulkarni",
        "Alec Chiu",
        "Avinash Thangali",
        "Zijie Pan",
        "Shivani Shekhar",
        "Yirou Ge",
        "Yixi Li",
        "Uma Kona",
        "Linsey Pang",
        "Prakhar Mehrotra"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213641+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
          "url": "https://arxiv.org/abs/2602.16246"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
        "url": "https://arxiv.org/abs/2602.16246"
      },
      "published_at": "2026-02-18T07:49:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6898464961852747,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.589846496185274
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16246",
      "summary": "Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluat",
      "summary_zh": "透過多輪對話和多步驟工具調用運作的互動式大型語言模型（LLM）代理正日益被用於生產環境。這些代理的基準測試必須既能可靠地比較模型，又能產生 on-policy 訓練數據。先前的代理基準測試（例如，tau-bench, tau2-bench, AppWorld）依賴於完全確定性的後端，這些後端建構和迭代成本高昂。我們提出了 Proxy State-Based Evaluation，一個由 LLM 驅動的模擬框架，它保留了基於最終狀態的評估。",
      "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
      "title_zh": "邁向可擴展的可驗證獎勵：用於多輪工具調用 LLM 代理的基於代理狀態的評估"
    },
    {
      "arxiv_id": "2602.16752",
      "authors": [
        "Yu Yin",
        "Shuai Wang",
        "Bevan Koopman",
        "Guido Zuccon"
      ],
      "categories": [
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213812+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "The Vulnerability of LLM Rankers to Prompt Injection Attacks",
          "url": "https://arxiv.org/abs/2602.16752"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "The Vulnerability of LLM Rankers to Prompt Injection Attacks",
        "url": "https://arxiv.org/abs/2602.16752"
      },
      "published_at": "2026-02-18T06:19:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6855174569276665,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.585517456927665
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16752",
      "summary": "Large Language Models (LLMs) have emerged as powerful re-rankers. Recent research has however showed that simple prompt injections embedded within a candidate document (i.e., jailbreak prompt attacks) can significantly alter an LLM's ranking decisions. While this poses serious security risks to LLM-based ranking pipelines, the extent to which this vulnerability persists across diverse LLM families, architectures, and settings remains largely under-explored. In this paper, we present a comprehens",
      "summary_zh": "大型語言模型（LLM）已成為強大的 re-rankers。然而，最近的研究表明，嵌入在候選文件中的簡單 prompt injection（即 jailbreak prompt attacks）可以顯著改變 LLM 的排序決策。雖然這對基於 LLM 的排序管道構成了嚴重的安全風險，但這種脆弱性在不同 LLM 家族、架構和設定中的持續程度仍未得到充分探索。在本文中，我們提出了一個全面的。",
      "title": "The Vulnerability of LLM Rankers to Prompt Injection Attacks",
      "title_zh": "LLM 排序器對 Prompt Injection Attacks 的脆弱性"
    },
    {
      "arxiv_id": "2602.16201",
      "authors": [
        "Sanket Badhe",
        "Deep Shah",
        "Nehal Kathrotia"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213868+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
          "url": "https://arxiv.org/abs/2602.16201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
        "url": "https://arxiv.org/abs/2602.16201"
      },
      "published_at": "2026-02-18T05:49:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.684120078376211,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.58412007837621
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16201",
      "summary": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work ",
      "summary_zh": "大型語言模型（LLM）是在網路規模語料庫上訓練的，這些語料庫呈現出陡峭的 power-law distributions，其中知識的分布高度長尾，大多數知識出現頻率不高。雖然 scaling 改善了 average-case performance，但在低頻率、領域特定、文化和時間知識上的持續故障仍然未能得到很好的表徵。本文開發了一個結構化的 taxonomy 和對大型語言模型中 long-Tail Knowledge 的分析，綜合了先前的工作。",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "title_zh": "大型語言模型中的長尾知識：分類、機制、干預和影響"
    },
    {
      "arxiv_id": "2602.16196",
      "authors": [
        "Emile Anand",
        "Richard Hoffmann",
        "Sarah Liaw",
        "Adam Wierman"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.592286+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.16196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.16196"
      },
      "published_at": "2026-02-18T05:34:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6833777676970272,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.583377767697026
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16196",
      "summary": "Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\\texttt{",
      "summary_zh": "協調大量互動智能體是 multi-agent reinforcement learning (MARL) 中的一個核心挑戰，其中聯合狀態-動作空間的大小隨智能體數量呈指數增長。Mean-field methods 透過聚合智能體互動來緩解這一負擔，但這些方法假設互動是同質的。最近基於 graphon 的框架能夠捕捉異質性，但隨著智能體數量的增加，計算成本高昂。因此，我們引入了。",
      "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
      "title_zh": "用於合作異構多智能體強化學習的 Graphon 均場次採樣"
    },
    {
      "arxiv_id": "2602.16179",
      "authors": [
        "Sushant Mehta",
        "Logan Ritchie",
        "Suhaas Garre",
        "Nick Heiner",
        "Edwin Chen"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.592453+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
          "url": "https://arxiv.org/abs/2602.16179"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
        "url": "https://arxiv.org/abs/2602.16179"
      },
      "published_at": "2026-02-18T04:35:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.6806142667834149,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.580614266783414
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16179",
      "summary": "We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI's suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step,",
      "summary_zh": "我們展示了在 high-fidelity reinforcement learning 環境中訓練 AI agents，可以產生超越訓練分佈的泛化能力。我們引入了 CoreCraft，這是 Surge AI 的 agentic RL 環境套件 EnterpriseBench 中的第一個環境。CoreCraft 是一個客戶支援組織的全面運作企業模擬，包含跨越 14 種實體類型、超過 2,500 個實體和 23 種獨特工具，旨在衡量 AI agents 是否能執行多步驟、",
      "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
      "title_zh": "EnterpriseBench Corecraft：在高擬真 RL 環境中訓練通用型 Agent"
    },
    {
      "arxiv_id": "2602.17469",
      "authors": [
        "Nusrat Jahan Lia",
        "Shubhashis Roy Dipta"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:00.593436+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
          "url": "https://arxiv.org/abs/2602.17469"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
        "url": "https://arxiv.org/abs/2602.17469"
      },
      "published_at": "2026-02-19T15:35:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7874429021020344,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.487442902102035
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17469",
      "summary": "The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7%",
      "summary_zh": "雙向對齊的核心主題是確保 AI systems 準確理解人類意圖，並且人類可以信任 AI 的行為。然而，這個循環在語言障礙面前嚴重斷裂。我們的研究透過基準測試四種 Transformer 架構，解決了孟加拉語和英語之間的 Cross-Lingual Sentiment Misalignment 問題。我們揭示了當前 alignment paradigms 中嚴重的安全性與表徵失敗。我們證明了壓縮模型 (mDistilBERT) 表現出 28.7%",
      "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
      "title_zh": "審計互惠情感對齊：Transformer 中的反轉風險、方言表徵和意圖錯位"
    },
    {
      "arxiv_id": "2602.17288",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.406520+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
          "url": "https://arxiv.org/abs/2602.17288"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
        "url": "https://arxiv.org/abs/2602.17288"
      },
      "published_at": "2026-02-19T11:47:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7750884914851427,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.475088491485142
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17288",
      "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "summary_zh": "儘管 frontier large language models 展示了強大的推理和數學能力，但從原始來源訓練 domain-specialized scientific language models 的實際過程仍然缺乏文獻記載。在這項工作中，我們提出了一個詳細的案例研究，關於如何直接從涵蓋數學、電腦科學和理論物理的原始 arXiv LaTeX 來源訓練一個 1.36B-parameter 的 scientific language model。我們描述了一個端到端 (end-to-end) 的 pipeline，涵蓋了 metadata filtering、archive validation、LaTeX extraction、text normalization、domain-aware tokenization，以及在受限運算資源 (2xA100 GPUs) 下進行的 dense Transformer training。透過 24 次實驗運行，我們分析了訓練穩定性、scaling behavior、data yield losses 和 infrastructure bottlenecks。我們的研究結果強調了預處理決策如何顯著影響可用 token 數量、tokenization 如何影響 symbolic stability，以及儲存和 I/O 限制如何與運算能力一樣成為限制因素。我們進一步分析了 convergence dynamics，並展示了在 data-rich regime (52B pretraining tokens) 下穩定的訓練行為。這項工作並非提出一種新穎的 architecture，而是提供了一個基於工程實踐、透明的從頭開始訓練小型 scientific language model 的說明。我們希望這些見解能支持在適度運算預算下尋求建立 domain-specialized models 的研究人員。",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "title_zh": "ArXiv-to-Model：科學 LM 訓練的實踐研究"
    },
    {
      "arxiv_id": "2602.17047",
      "authors": [
        "Chaojie Yang",
        "Tian Li",
        "Yue Zhang",
        "Jun Gao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:01.565636+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
          "url": "https://arxiv.org/abs/2602.17047"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
        "url": "https://arxiv.org/abs/2602.17047"
      },
      "published_at": "2026-02-19T03:33:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7489591040726051,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.448959104072607
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17047",
      "summary": "Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a tim",
      "summary_zh": "Diffusion Transformer (DiT) architecture 顯著推進了 Text-to-Image (T2I) 生成，但面臨高昂的運算成本和部署障礙。為了解決這些挑戰，我們提出了一種高效的壓縮框架，該框架將基於 60-layer dual-stream MMDiT 的 Qwen-Image 轉變為輕量級模型，而無需從頭開始訓練。利用此框架，我們推出了 Amber-Image，一系列精簡的 T2I models。我們首先使用一個 tim",
      "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
      "title_zh": "Amber-Image：大規模 Diffusion Transformers 的高效壓縮"
    },
    {
      "arxiv_id": "2602.17375",
      "authors": [
        "David Tolpin"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "MDP Planning as Policy Inference",
          "url": "https://arxiv.org/abs/2602.17375"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "MDP Planning as Policy Inference",
        "url": "https://arxiv.org/abs/2602.17375"
      },
      "published_at": "2026-02-19T13:56:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7820640918991892,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.34206409189919
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17375",
      "summary": "We cast episodic Markov decision process (MDP) planning as Bayesian inference over _policies_. A policy is treated as the latent variable and is assigned an unnormalized probability of optimality that is monotone in its expected return, yielding a posterior distribution whose modes coincide with return-maximizing solutions while posterior dispersion represents uncertainty over optimal behavior. To approximate this posterior in discrete domains, we adapt variational sequential Monte Carlo (VSMC) ",
      "summary_zh": "我們將情景式 Markov decision process (MDP) 規劃視為對 _policies_ 的 Bayesian inference。一個 policy 被視為潛在變數，並被賦予一個與其預期報酬呈單調關係的非標準化最佳機率 (unnormalized probability of optimality)，從而產生一個後驗分佈 (posterior distribution)，其眾數與報酬最大化的解重合，而後驗離散度 (posterior dispersion) 則代表了對最佳行為的不確定性。為了在離散領域中近似這個後驗分佈，我們採用了 variational sequential Monte Carlo (VSMC)",
      "title": "MDP Planning as Policy Inference",
      "title_zh": "MDP 規劃作為策略推斷"
    },
    {
      "arxiv_id": "2602.16947",
      "authors": [
        "Chuqin Geng",
        "Li Zhang",
        "Haolin Ye",
        "Ziyu Zhao",
        "Yuhe Jiang",
        "Tara Saba",
        "Xinyu Wang",
        "Xujie Si"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.806113+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning",
          "url": "https://arxiv.org/abs/2602.16947"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning",
        "url": "https://arxiv.org/abs/2602.16947"
      },
      "published_at": "2026-02-18T23:25:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7361571688625554,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.296157168862557
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16947",
      "summary": "Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework",
      "summary_zh": "圖神經網路 (GNNs) 已在藥物發現等高風險領域變得至關重要，然而，它們的 black-box 性質仍然是信任度的一個重大障礙。儘管 self-explainable GNNs 試圖彌合這一差距，但它們通常依賴標準的 message-passing 骨幹，這些骨幹繼承了基本限制，包括 1-Weisfeiler-Lehman (1-WL) 表達能力障礙和缺乏細粒度 (fine-grained) 的可解釋性。為了應對這些挑戰，我們提出 SymGraph，一個符號式框架",
      "title": "Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning",
      "title_zh": "超越訊息傳遞：一種用於表達性和可解釋圖學習的符號式替代方案"
    },
    {
      "arxiv_id": "2602.16823",
      "authors": [
        "Itamar Hadad",
        "Guy Katz",
        "Shahaf Bassan"
      ],
      "categories": [
        "cs.LG",
        "cs.LO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.806273+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
          "url": "https://arxiv.org/abs/2602.16823"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
        "url": "https://arxiv.org/abs/2602.16823"
      },
      "published_at": "2026-02-18T19:41:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7247743081215441,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.1500000000000004,
        "total_score": 26.874774308121545
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16823",
      "summary": "*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yie",
      "summary_zh": "「自動化電路發現」(Automated circuit discovery) 是 Mechanistic Interpretability 中的一個核心工具，用於識別神經網路中負責特定行為的內部組件。儘管先前的方法取得了顯著進展，但它們通常依賴於 heuristics 或 approximations，並且無法為所得電路在連續輸入域上提供可證明保證。在這項工作中，我們利用神經網路驗證的最新進展，提出了一套自動化演算法，該演算法提供",
      "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
      "title_zh": "形式化機械解釋性：具有可證明保證的自動化電路發現"
    },
    {
      "arxiv_id": "2602.17622",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Ruozhao Yang",
        "Xiaofei Xie",
        "Jie Zhang",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-22T00:40:57.207756+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
          "url": "https://arxiv.org/abs/2602.17622"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
        "url": "https://arxiv.org/abs/2602.17622"
      },
      "published_at": "2026-02-19T18:42:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.7977603379151043,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.837760337915103
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17622",
      "summary": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of toolin",
      "summary_zh": "基於 LLM 的 agents 在自動化滲透測試方面展現了潛力，然而，報告的性能在不同系統和基準測試中差異很大。我們分析了 28 個基於 LLM 的滲透測試系統，並在三個複雜度遞增的基準測試中評估了五個代表性實作。我們的分析揭示了兩種不同的失敗模式：Type A 失敗源於能力差距（缺少工具、prompts 不足），這些可以透過工程手段輕易解決；而 Type B 失敗則無論工具",
      "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
      "title_zh": "什麼造就了一個好的 LLM Agent 用於真實世界滲透測試？"
    },
    {
      "arxiv_id": "2602.17211",
      "authors": [
        "Etienne Lempereur",
        "Nathanaël Cuvelle--Magar",
        "Florentin Coeurdoux",
        "Stéphane Mallat",
        "Eric Vanden-Eijnden"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.565063+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "MGD: Moment Guided Diffusion for Maximum Entropy Generation",
          "url": "https://arxiv.org/abs/2602.17211"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "MGD: Moment Guided Diffusion for Maximum Entropy Generation",
        "url": "https://arxiv.org/abs/2602.17211"
      },
      "published_at": "2026-02-19T10:03:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7694867492435485,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.4000000000000004,
        "total_score": 26.72948674924355
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17211",
      "summary": "Generating samples from limited information is a fundamental problem across scientific domains. Classical maximum entropy methods provide principled uncertainty quantification from moment constraints but require sampling via MCMC or Langevin dynamics, which typically exhibit exponential slowdown in high dimensions. In contrast, generative models based on diffusion and flow matching efficiently transport noise to data but offer limited theoretical guarantees and can overfit when data is scarce. W",
      "summary_zh": "從有限資訊生成樣本是跨科學領域的一個基本問題。經典的最大熵方法從 moment constraints 提供有原則的不確定性量化，但需要透過 MCMC 或 Langevin dynamics 進行採樣，這在高維度中通常會表現出指數級的減速。相較之下，基於 diffusion 和 flow matching 的生成模型能有效地將噪音傳輸到數據，但提供的理論保證有限，並且在數據稀缺時可能過度擬合 (overfit)。我們",
      "title": "MGD: Moment Guided Diffusion for Maximum Entropy Generation",
      "title_zh": "MGD：用於最大熵生成的動量引導擴散"
    },
    {
      "arxiv_id": "2602.17312",
      "authors": [
        "Hsin-Jung Yang",
        "Zhanhong Jiang",
        "Prajwal Koirala",
        "Qisai Liu",
        "Cody Fleming",
        "Soumik Sarkar"
      ],
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.552758+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-safety",
          "tier": 1,
          "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy",
          "url": "https://arxiv.org/abs/2602.17312"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-safety",
        "tier": 1,
        "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy",
        "url": "https://arxiv.org/abs/2602.17312"
      },
      "published_at": "2026-02-19T12:22:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7769926641698174,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.676992664169816
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17312",
      "summary": "Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. ",
      "summary_zh": "離線安全強化學習 (RL) 對於網路實體系統 (CPS) 越來越重要，在這些系統中，訓練期間的安全違規是不可接受的，且僅有預先收集的數據可用。現有的離線安全 RL 方法通常透過 constraint relaxation 或 joint optimization 來平衡獎勵與安全的權衡，但它們往往缺乏結構性機制來防止安全漂移 (safety drift)。我們提出 LexiSafe，一個詞典式離線 RL 框架，旨在保持安全對齊的行為。",
      "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy",
      "title_zh": "LexiSafe：具有詞典式安全-獎勵層次結構的離線安全強化學習"
    },
    {
      "arxiv_id": "2602.16964",
      "authors": [
        "Prasham Titiya",
        "Rohit Khoja",
        "Tomer Wolfson",
        "Vivek Gupta",
        "Dan Roth"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.772611+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data",
          "url": "https://arxiv.org/abs/2602.16964"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data",
        "url": "https://arxiv.org/abs/2602.16964"
      },
      "published_at": "2026-02-18T23:57:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7377897691435904,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.637789769143588
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16964",
      "summary": "Retrieval-augmented question answering over heterogeneous corpora requires connected evidence across text, tables, and graph nodes. While entity-level knowledge graphs support structured access, they are costly to construct and maintain, and inefficient to traverse at query time. In contrast, standard retriever-reader pipelines use flat similarity search over independently chunked text, missing multi-hop evidence chains across modalities. We propose SAGE (Structure Aware Graph Expansion) framewo",
      "summary_zh": "在異質語料庫上進行檢索增強問答 (retrieval-augmented question answering) 需要跨文本、表格和圖節點的連接證據。雖然實體級知識圖譜 (entity-level knowledge graphs) 支持結構化存取 (structured access)，但它們的建構和維護成本高昂，且在查詢時遍歷效率低下。相較之下，標準的 retriever-reader 管道對獨立分塊的文本使用平面相似性搜索 (flat similarity search)，遺漏了跨模態的多跳證據鏈 (multi-hop evidence chains)。我們提出了 SAGE (Structure Aware Graph Expansion) 框架。",
      "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data",
      "title_zh": "SAGE: 適用於異質資料檢索的結構感知圖擴展"
    },
    {
      "arxiv_id": "2602.16698",
      "authors": [
        "Shruti Joshi",
        "Aaron Mueller",
        "David Klindt",
        "Wieland Brendel",
        "Patrik Reizinger",
        "Dhanya Sridhar"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.806346+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Causality is Key for Interpretability Claims to Generalise",
          "url": "https://arxiv.org/abs/2602.16698"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Causality is Key for Interpretability Claims to Generalise",
        "url": "https://arxiv.org/abs/2602.16698"
      },
      "published_at": "2026-02-18T18:45:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7219637216308191,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.62196372163082
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16698",
      "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies w",
      "summary_zh": "針對大型語言模型 (LLMs) 的可解釋性研究為模型行為 (model behaviour) 提供了重要見解，但重複出現的陷阱依然存在：即無法泛化 (generalise) 的發現，以及超出證據範圍的因果解釋 (cusal interpretations)。我們的立場是，因果推斷 (causal inference) 闡明了構成從模型激活 (model activations) 到不變高層次結構 (invariant high-level structures) 的有效映射所需的條件、實現它所需的數據或假設，以及它能支持的推斷。具體而言，Pearl 的因果層次結構 (causal hierarchy) 澄清了...",
      "title": "Causality is Key for Interpretability Claims to Generalise",
      "title_zh": "因果關係是可解釋性主張泛化的關鍵"
    },
    {
      "arxiv_id": "2602.16763",
      "authors": [
        "Mubashara Akhtar",
        "Anka Reuel",
        "Prajna Soni",
        "Sanchit Ahuja",
        "Pawan Sasanka Ammanamanchi",
        "Ruchit Rawal",
        "Vilém Zouhar",
        "Srishti Yadav",
        "Chenxi Whitehouse",
        "Dayeon Ki",
        "Jennifer Mickel",
        "Leshem Choshen",
        "Marek Šuppa",
        "Jan Batzner",
        "Jenny Chim",
        "Jeba Sania",
        "Yanan Long",
        "Hossein A. Rahmani",
        "Christina Knight",
        "Yiyang Nan",
        "Jyoutir Raj",
        "Yu Fan",
        "Shubham Singh",
        "Subramanyam Sahoo",
        "Eliya Habba",
        "Usman Gohar",
        "Siddhesh Pawar",
        "Robert Scholz",
        "Arjun Subramonian",
        "Jingwei Ni",
        "Mykel Kochenderfer",
        "Sanmi Koyejo",
        "Mrinmaya Sachan",
        "Stella Biderman",
        "Zeerak Talat",
        "Avijit Ghosh",
        "Irene Solaiman"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571329+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
          "url": "https://arxiv.org/abs/2602.16763"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
        "url": "https://arxiv.org/abs/2602.16763"
      },
      "published_at": "2026-02-18T16:51:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7162980980391039,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.616298098039103
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16763",
      "summary": "Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, w",
      "summary_zh": "人工智慧 (AI) 基準在衡量模型開發進度 (model development) 和指導部署決策 (deployment decisions) 方面發揮著核心作用。然而，許多基準很快就達到飽和 (saturated)，這意味著它們無法再區分表現最佳的模型 (best-performing models)，從而削弱了其長期價值。在這項研究中，我們分析了主要模型開發商技術報告中選取的 60 個大型語言模型 (LLM) 基準的飽和情況。為識別導致飽和的因素，我們...",
      "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
      "title_zh": "當 AI 基準達到高原期：基準飽和的系統性研究"
    },
    {
      "arxiv_id": "2602.16596",
      "authors": [
        "Thomas Michel",
        "Debabrota Basu",
        "Emilie Kaufmann"
      ],
      "categories": [
        "cs.LG",
        "cs.CR",
        "math.ST",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.775132+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Sequential Membership Inference Attacks",
          "url": "https://arxiv.org/abs/2602.16596"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Sequential Membership Inference Attacks",
        "url": "https://arxiv.org/abs/2602.16596"
      },
      "published_at": "2026-02-18T16:51:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.716278201146061,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.61627820114606
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16596",
      "summary": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses th",
      "summary_zh": "現代 AI 模型並非靜態。它們在生命週期中經歷多次更新。因此，利用模型動態 (model dynamics) 來創建更強的 Membership Inference (MI) 攻擊和更嚴格的隱私審計 (privacy audits) 是及時的問題。儘管文獻經驗表明使用一系列模型更新可以提高 MI 攻擊的效能 (power)，但對「最佳」MI 攻擊的嚴謹分析 (rigorous analysis) 僅限於具有無限樣本的靜態模型 (static models)。因此，我們開發了一種「最佳」MI 攻擊，SeMI*，它利用了...",
      "title": "Sequential Membership Inference Attacks",
      "title_zh": "序列式成員推斷攻擊"
    },
    {
      "arxiv_id": "2602.16494",
      "authors": [
        "Alexis Winter",
        "Jean-Vincent Martini",
        "Romaric Audigier",
        "Angelique Loesch",
        "Bertrand Luvison"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571450+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection",
          "url": "https://arxiv.org/abs/2602.16494"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection",
        "url": "https://arxiv.org/abs/2602.16494"
      },
      "published_at": "2026-02-18T14:33:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7094836064693241,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.609483606469325
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16494",
      "summary": "Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cos",
      "summary_zh": "物件偵測模型 (Object detection models) 是自動化系統 (automated systems) 的關鍵組成部分，例如自動駕駛汽車 (autonomous vehicles) 和基於感知的機器人 (perception-based robots)，但它們對對抗性攻擊 (adversarial attacks) 的敏感性構成嚴重的安全風險。防禦這些模型的進展落後於分類任務 (classification)，因缺乏標準化評估而受阻。徹底比較攻擊或防禦方法幾乎不可能，因為現有研究使用了不同的資料集 (datasets)、不一致的效率指標 (efficiency metrics) 以及多變的擾動成本 (perturbation cos) 衡量標準。",
      "title": "Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection",
      "title_zh": "物件偵測中對抗性穩健性與對抗性訓練策略的基準測試"
    },
    {
      "arxiv_id": "2602.17633",
      "authors": [
        "Shayan Kiyani",
        "Sima Noorani",
        "George Pappas",
        "Hamed Hassani"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535301+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
          "url": "https://arxiv.org/abs/2602.17633"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
        "url": "https://arxiv.org/abs/2602.17633"
      },
      "published_at": "2026-02-19T18:47:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7980355388216744,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.598035538821676
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17633",
      "summary": "Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but no",
      "summary_zh": "使用 LLMs 進行推理的過程越來越多地在一個更廣泛的驗證循環中展開。在內部，系統使用廉價檢查，例如 self-consistency 或 proxy rewards，我們稱之為 weak verification。在外部，使用者檢查輸出並透過回饋引導模型，直到結果值得信賴，我們稱之為 strong verification。這些訊號在成本和可靠性上存在顯著差異：strong verification 可以建立信任但資源密集，而 weak verification 快速且可擴展，但其可靠性...",
      "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
      "title_zh": "何時該信任廉價檢查：用於推理的弱驗證與強驗證"
    },
    {
      "arxiv_id": "2602.17588",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583019+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Modeling Distinct Human Interaction in Web Agents",
          "url": "https://arxiv.org/abs/2602.17588"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Modeling Distinct Human Interaction in Web Agents",
        "url": "https://arxiv.org/abs/2602.17588"
      },
      "published_at": "2026-02-19T18:11:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7960337283521042,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.596033728352104
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17588",
      "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "summary_zh": "儘管 autonomous web agents 進展迅速，但在任務執行過程中，人類的參與對於塑造偏好和糾正 agent 行為仍然至關重要。然而，目前的 agentic 系統缺乏對人類何時以及為何介入的原則性理解，常常自主地越過關鍵決策點或請求不必要的確認。在這項工作中，我們引入了建模人類介入的任務，以支持協作的 web 任務執行。我們收集了 CowCorpus，一個包含 400 條真實用戶 web navigation 軌跡的資料集，其中包含超過 4,200 個交錯的人類和 agent 行為。我們識別出四種不同的用戶與 agent 互動模式——hands-off supervision、hands-on oversight、collaborative task-solving 和 full user takeover。借助這些洞察，我們訓練了 language models (LMs) 來根據用戶的互動風格預測他們何時可能介入，使得介入預測準確度相較於基礎 LMs 提高了 61.4-63.4%。最後，我們將這些具備介入意識的模型部署到實際的 web navigation agents 中，並在用戶研究中進行評估，發現用戶對 agent 有用性的評價提升了 26.5%。總之，我們的結果表明，對人類介入進行結構化建模能夠產生更具適應性和協作性的 agents。",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "title_zh": "建模 Web Agents 中獨特的人類互動"
    },
    {
      "arxiv_id": "2602.17546",
      "authors": [
        "Jyotin Goel",
        "Souvik Maji",
        "Pratik Mazumder"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593258+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
          "url": "https://arxiv.org/abs/2602.17546"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
        "url": "https://arxiv.org/abs/2602.17546"
      },
      "published_at": "2026-02-19T16:59:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7920873291712764,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.592087329171278
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17546",
      "summary": "Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches",
      "summary_zh": "Instruction-following language models 被訓練成有用且安全的，然而它們的安全行為在良性 fine-tuning 下可能會惡化，並在 adversarial updates 下變得更糟。現有的防禦措施通常提供有限的保護，或迫使在安全性和實用性之間進行權衡。我們引入了一個訓練框架，它根據安全風險調整 regularization，使模型在整個 fine-tuning 過程中保持一致。為了在訓練時估計安全風險，我們探索了兩種不同的方法...",
      "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
      "title_zh": "學習保持安全：在 Fine-Tuning 期間對抗安全退化的自適應正則化"
    },
    {
      "arxiv_id": "2602.17520",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208491+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17520"
      },
      "published_at": "2026-02-19T16:33:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7906511409536765,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.59065114095368
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17520",
      "summary": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local de",
      "summary_zh": "Large language models (LLMs) 在標準的 digital logic 和 Boolean reasoning 任務上表現出色，但其在局部重新定義語義下的可靠性仍然知之甚少。在許多正式設定中，例如 circuit specifications、examinations 和 hardware documentation，operators 和 components 在狹窄範圍內被明確重新定義。在這些情境中，正確的推理要求模型暫時抑制全球學習到的慣例，轉而採用 prompt 局部的定義...",
      "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
      "title_zh": "當模型忽略定義時：測量 LLM 推理中的語義覆蓋幻覺"
    },
    {
      "arxiv_id": "2602.17262",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209262+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
          "url": "https://arxiv.org/abs/2602.17262"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
        "url": "https://arxiv.org/abs/2602.17262"
      },
      "published_at": "2026-02-19T11:07:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7729330878318125,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.572933087831814
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17262",
      "summary": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-b",
      "summary_zh": "人類自陳問卷在 NLP 中越來越多地被用於評測和審計 large language models (LLMs)，從 persona consistency 到 safety 和 bias assessments。然而，這些工具預設了誠實回應；在評估情境中，LLMs 反而可能傾向於社會偏好的答案——一種 Socially Desirable Responding (SDR) 形式——從而偏倚了問卷導出的分數和下游結論。我們提出了一個心理計量學框架來量化和緩解基於問卷的 SDR...",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
      "title_zh": "量化和緩解 LLMs 中的 Socially Desirable Responding：一項 desirability-matched 的 graded forced-choice 心理計量學研究"
    },
    {
      "arxiv_id": "2602.17234",
      "authors": [
        "Zeyu Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209375+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
          "url": "https://arxiv.org/abs/2602.17234"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
        "url": "https://arxiv.org/abs/2602.17234"
      },
      "published_at": "2026-02-19T10:28:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7708211475911817,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.570821147591182
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17234",
      "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes ",
      "summary_zh": "為了評估 LLMs 是否能準確預測未來事件，我們需要能夠對已解決的事件進行 `backtest`。這要求模型只能利用特定過去日期可用的資訊進行推理。然而，LLMs 在訓練期間可能會無意中洩漏 `post-cutoff knowledge`，從而損害回溯評估的有效性。我們引入了一個 `claim-level framework`，用於檢測和量化這種 `temporal knowledge leakage`。我們的方法將...",
      "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
      "title_zh": "所有的洩漏都算數，有些更關鍵：LLM 回溯測試中可解釋的時間污染檢測"
    },
    {
      "arxiv_id": "2602.17223",
      "authors": [
        "Arka Pal",
        "Louai Zahran",
        "William Gvozdjak",
        "Akilesh Potti",
        "Micah Goldblum"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209493+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
          "url": "https://arxiv.org/abs/2602.17223"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
        "url": "https://arxiv.org/abs/2602.17223"
      },
      "published_at": "2026-02-19T10:15:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7701710415499561,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.57017104154996
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17223",
      "summary": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely ",
      "summary_zh": "隨著大型語言模型（LLMs）規模不斷增長，越來越少的用戶能夠在本地託管和運行模型。這導致第三方託管服務的使用增加。然而，在這種情況下，對於 `inference` 提供者所執行的計算缺乏保障。例如，一個不誠實的提供者可能會用一個運行成本較低的弱模型替換昂貴的大型模型，並將弱模型的結果返回給用戶。現有的驗證 `inference` 的工具通常依賴於...",
      "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
      "title_zh": "隱私保護機制實現 LLMs 廉價可驗證的推論"
    },
    {
      "arxiv_id": "2602.17196",
      "authors": [
        "Yahong Wang",
        "Juncheng Wu",
        "Zhangkai Ni",
        "Chengmei Yang",
        "Yihang Liu",
        "Longzhen Yang",
        "Yuyin Zhou",
        "Ying Wen",
        "Lianghua He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209554+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17196"
      },
      "published_at": "2026-02-19T09:29:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7677075899232213,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.567707589923224
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17196",
      "summary": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse ",
      "summary_zh": "多模態大型語言模型（MLLMs）由於處理每張圖片數百個 `visual tokens`，產生了大量的 `inference cost`。儘管 `token pruning` 已被證明對於加速 `inference` 是有效的，但確定何時何地進行剪枝仍然很大程度上是基於經驗的啟發式方法。現有方法通常依賴於靜態、憑經驗選擇的層，這限制了解釋性和模型之間的遷移能力。在這項工作中，我們引入了一種 `matrix-entropy` 視角，並發現了一種「Entropy Collapse...",
      "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
      "title_zh": "EntropyPrune: 矩陣熵引導的多模態大型語言模型視覺 Token 剪枝"
    },
    {
      "arxiv_id": "2602.17186",
      "authors": [
        "Seulbi Lee",
        "Sangheum Hwang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209608+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
          "url": "https://arxiv.org/abs/2602.17186"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
        "url": "https://arxiv.org/abs/2602.17186"
      },
      "published_at": "2026-02-19T09:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7667822784365148,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.566782278436516
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17186",
      "summary": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity",
      "summary_zh": "大型視覺語言模型（LVLMs）取得了顯著進展，但它們經常遭受 `language bias` 的困擾，在不依賴視覺證據的情況下產生答案。雖然先前的工作試圖透過 `decoding strategies`、架構修改或精選的指令數據來緩解這個問題，但它們通常缺乏一種量化指標來衡量單個訓練樣本或 `tokens` 從圖像中實際受益的程度。在這項工作中，我們引入了 `Visual Information Gain (VIG)`，這是一種 `perplexity`...",
      "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
      "title_zh": "透過視覺資訊增益對大型視覺語言模型進行選擇性訓練"
    },
    {
      "arxiv_id": "2602.17169",
      "authors": [
        "Yuhuan Xia",
        "Tun Li",
        "Hongji Zhou",
        "Xianfa Zhou",
        "Chong Chen",
        "Ruiyu Zhang"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209788+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
          "url": "https://arxiv.org/abs/2602.17169"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
        "url": "https://arxiv.org/abs/2602.17169"
      },
      "published_at": "2026-02-19T08:34:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7647588372717248,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.564758837271725
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17169",
      "summary": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-ali",
      "summary_zh": "本文介紹了 SimulatorCoder，這是一個由大型語言模型（LLMs）驅動的代理，旨在根據自然語言描述生成和最佳化深度神經網路（DNN）加速器模擬器。透過整合領域特定的 `prompt engineering`，包括 `In-Context Learning (ICL)`、`Chain-of-Thought (CoT)` 推理，以及多輪 `feedback-verification flow`，SimulatorCoder 系統性地將高階功能需求轉化為高效、可執行且與架構對齊（architecture-ali）的...",
      "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
      "title_zh": "SimulatorCoder: 透過大型語言模型進行 DNN 加速器模擬器程式碼生成與最佳化"
    },
    {
      "arxiv_id": "2602.17100",
      "authors": [
        "Siyu Wang",
        "Ruotian Lu",
        "Zhihao Yang",
        "Yuchao Wang",
        "Yanzhou Zhang",
        "Lei Xu",
        "Qimin Xu",
        "Guojun Yin",
        "Cailian Chen",
        "Xinping Guan"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
          "url": "https://arxiv.org/abs/2602.17100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
        "url": "https://arxiv.org/abs/2602.17100"
      },
      "published_at": "2026-02-19T05:51:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7561833841263125,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.556183384126314
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17100",
      "summary": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively ref",
      "summary_zh": "由大型語言模型 (LLM) 驅動的多智能體系統 (MAS) 透過預定義的互動拓撲協調專業化的代理，並已在諸如競爭級程式碼生成等複雜任務中展現出潛力。最近的研究表明，精心設計的多智能體工作流程和通訊圖譜可以透過利用協作推理顯著提高程式碼生成效能。然而，現有方法既無法使拓撲密度適應任務難度，也無法迭代地重新定義",
      "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
      "title_zh": "AgentConductor: 用於多智能體競爭級程式碼生成的拓撲演化"
    },
    {
      "arxiv_id": "2602.17066",
      "authors": [
        "Sumedh Rasal"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209902+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
          "url": "https://arxiv.org/abs/2602.17066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
        "url": "https://arxiv.org/abs/2602.17066"
      },
      "published_at": "2026-02-19T04:15:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7511450181387119,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.551145018138712
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17066",
      "summary": "We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor",
      "summary_zh": "我們引入了預測批次排程 (Predictive Batch Scheduling, PBS)，這是一種新穎的訓練優化技術，它透過在批次構建期間動態地優先考慮高損失樣本來加速語言模型收斂。與需要預定義難度指標的 curriculum learning 方法或需要昂貴的逐樣本損失追蹤的 hard example mining 方法不同，PBS 採用一個輕量級的線性預測器，該預測器在線訓練，用於從靜態 token-level features 估計樣本難度。我們的預測器",
      "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
      "title_zh": "預測批次排程：透過損失感知樣本優先級化加速語言模型訓練"
    },
    {
      "arxiv_id": "2602.17062",
      "authors": [
        "Yonghyeon Jo",
        "Sunwoo Lee",
        "Seungyul Han"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584068+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17062"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17062"
      },
      "published_at": "2026-02-19T04:07:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7507417337428066,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.550741733742807
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17062",
      "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax",
      "summary_zh": "價值分解是協作多智能體強化學習 (MARL) 的核心方法。然而，現有方法仍依賴單一的最優行動，並且在訓練期間底層價值函數發生變化時難以適應，常常收斂到次優策略。為了解決這個限制，我們提出了 Successive Sub-value Q-learning (S2Q)，它學習多個子價值函數以保留替代的高價值行動。將這些子價值函數整合到 Softmax 中",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
      "title_zh": "在多智能體強化學習中保留次優行動以追隨變化的最優解"
    },
    {
      "arxiv_id": "2602.17049",
      "authors": [
        "Seoyoung Lee",
        "Seobin Yoon",
        "Seongbeen Lee",
        "Yoojung Chun",
        "Dayoung Park",
        "Doyeon Kim",
        "Joo Yong Sim"
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584126+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
          "url": "https://arxiv.org/abs/2602.17049"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
        "url": "https://arxiv.org/abs/2602.17049"
      },
      "published_at": "2026-02-19T03:42:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7494047979516134,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.549404797951617
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17049",
      "summary": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate ",
      "summary_zh": "電腦使用代理在嘈雜感知、多視窗情境和不斷變化的環境狀態下長時間執行任務。現有方法，從基於 RL 的規劃器到軌跡檢索，經常偏離用戶意圖並重複解決常規子問題，導致錯誤累積和效率低下。我們提出了 IntentCUA，一個多智能體電腦使用框架，旨在透過意圖對齊的計畫記憶穩定長時間執行。一個 Planner、Plan-Optimizer 和 Critic 協調",
      "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
      "title_zh": "IntentCUA: 用於電腦使用代理中技能抽象和多智能體規劃的意圖級表示學習"
    },
    {
      "arxiv_id": "2602.17038",
      "authors": [
        "Shengtian Yang",
        "Yu Li",
        "Shuo He",
        "Yewen Li",
        "Qingpeng Cai",
        "Peng Jiang",
        "Lei Feng"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584186+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17038"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17038"
      },
      "published_at": "2026-02-19T03:18:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7481698191636298,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.548169819163633
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17038",
      "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different ",
      "summary_zh": "強化學習 (RL) 賦予 LLM agents 解決複雜任務的強大能力。然而，現有 RL 方法通常使用一個單一的策略網絡 (policy network)，導致簡潔偏誤 (simplicity bias)，即簡單任務佔用大部分參數並主導梯度更新，為複雜任務留下不足的容量。一個可行的補救措施是在策略網絡中採用專家混合模型 (Mixture-of-Experts, MoE) 架構，因為 MoE 允許不同的參數（專家）專注於不同的",
      "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
      "title_zh": "用於代理強化學習的階段感知專家混合模型"
    },
    {
      "arxiv_id": "2602.17037",
      "authors": [
        "Rahul Nanda",
        "Chandra Maddila",
        "Smriti Jha",
        "Euna Mehnaz Khan",
        "Matteo Paltenghi",
        "Satish Chandra"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584244+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Wink: Recovering from Misbehaviors in Coding Agents",
          "url": "https://arxiv.org/abs/2602.17037"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Wink: Recovering from Misbehaviors in Coding Agents",
        "url": "https://arxiv.org/abs/2602.17037"
      },
      "published_at": "2026-02-19T03:15:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.747987994430234,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.547987994430237
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17037",
      "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatica",
      "summary_zh": "由 large language models (LLMs) 驅動的自主編碼 Agent 正日益被軟體產業採用，以自動化複雜的工程任務。然而，這些 Agent 容易出現各種不當行為，例如偏離使用者的指令、陷入重複循環，或未能正確使用工具。這些失敗會中斷開發工作流程，並且通常需要耗費大量資源的手動干預。在本文中，我們提出了一個用於自動化",
      "title": "Wink: Recovering from Misbehaviors in Coding Agents",
      "title_zh": "Wink: 從編碼 Agent 的不當行為中恢復"
    },
    {
      "arxiv_id": "2602.17003",
      "authors": [
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584362+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
          "url": "https://arxiv.org/abs/2602.17003"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
        "url": "https://arxiv.org/abs/2602.17003"
      },
      "published_at": "2026-02-19T01:54:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7438147561865559,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.543814756186556
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17003",
      "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on",
      "summary_zh": "大型語言模型 (Large language models) 已經推動了 Web Agents 的發展，但目前的 Agent 缺乏個性化能力。由於使用者很少詳細說明其意圖，實用的 Web Agents 必須能夠透過推斷使用者偏好和上下文來解釋模糊的查詢。為了解決這個挑戰，我們提出了 Persona2Web，這是第一個用於評估真實開放 Web 上個性化 Web Agents 的基準測試，它建立在 'clarify-to-personalize' 原則之上，該原則要求 Agent 根據",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
      "title_zh": "Persona2Web: 用於基於使用者歷史的上下文推理之個性化 Web Agents 基準測試"
    },
    {
      "arxiv_id": "2602.16987",
      "authors": [
        "Josef A. Habdank"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584497+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
          "url": "https://arxiv.org/abs/2602.16987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
        "url": "https://arxiv.org/abs/2602.16987"
      },
      "published_at": "2026-02-19T01:21:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7420975307721988,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.5420975307722
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16987",
      "summary": "As artificial intelligence (AI) capabilities advance rapidly, frontier models increasingly demonstrate systematic deception and scheming, complying with safety protocols during oversight but defecting when unsupervised. This paper examines the ensuing alignment challenge through an analogy from forensic psychology, where internalized belief systems in psychopathic populations reduce antisocial behavior via perceived omnipresent monitoring and inevitable consequences. Adapting this mechanism to s",
      "summary_zh": "隨著人工智慧 (AI) 能力的迅速發展，前沿模型日益展現出系統性的欺騙和詭計，在監督下遵守安全協議，但在無人監督時則背叛。本文透過法醫心理學的一個類比來探討隨之而來的對齊挑戰，在法醫心理學中，精神病態人群內化的信仰系統透過感知到的無所不在的監控和不可避免的後果來減少反社會行為。將這種機制應用於",
      "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
      "title_zh": "AI 對齊的一個可測試框架：作為矽基 Agent 的工程化世界觀的 Simulation Theology"
    },
    {
      "arxiv_id": "2602.16873",
      "authors": [
        "Geunbin Yu"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.210921+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
          "url": "https://arxiv.org/abs/2602.16873"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
        "url": "https://arxiv.org/abs/2602.16873"
      },
      "published_at": "2026-02-18T21:00:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7287648012992005,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.528764801299204
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16873",
      "summary": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dyna",
      "summary_zh": "隨著來自不同供應商的 large language models 在基準測試性能上趨於收斂，為每個任務選擇單一最佳模型的傳統範式帶來了遞減的回報。我們認為，編排拓撲 (orchestration topology)——即多個 Agent 如何協調、並行化和合成的結構組成——現在在系統級性能上，其主導作用超越了單個模型的性能。我們提出了 AdaptOrch，這是一個用於任務自適應多 Agent 編排的正式框架，它動態地",
      "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
      "title_zh": "AdaptOrch: 在 LLM 性能收斂時代的任務自適應多 Agent 編排"
    },
    {
      "arxiv_id": "2602.16610",
      "authors": [
        "Mengjie Qian",
        "Guangzhi Sun",
        "Mark J. F. Gales",
        "Kate M. Knill"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.212541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
          "url": "https://arxiv.org/abs/2602.16610"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
        "url": "https://arxiv.org/abs/2602.16610"
      },
      "published_at": "2026-02-18T17:04:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7169160057017954,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.516916005701795
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16610",
      "summary": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavail",
      "summary_zh": "大型語言模型 (LLMs) 正日益被用作自然語言生成評估的自動評估器，通常採用 pairwise comparative judgements。現有方法通常依賴單一評審員或假定可靠性相等地聚合多個評審員。實際上，LLM 評審員在不同任務和方面上的表現差異很大，其判斷機率可能存在偏差和不一致。此外，用於評審員校準的人工標註監督可能無法獲得",
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "title_zh": "我們能相信誰？用於比較評估的 LLM-as-a-jury"
    },
    {
      "arxiv_id": "2602.16601",
      "authors": [
        "Nail B. Khelifa",
        "Richard E. Turner",
        "Ramji Venkataramanan"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566449+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
          "url": "https://arxiv.org/abs/2602.16601"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
        "url": "https://arxiv.org/abs/2602.16601"
      },
      "published_at": "2026-02-18T16:56:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.716546026504939,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.51654602650494
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16601",
      "summary": "Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target ",
      "summary_zh": "Machine learning models 越來越多地使用 synthetic data 進行訓練或 fine-tuning。觀察發現，遞歸地使用此類資料進行訓練會顯著降低各種任務的性能，其特點通常是逐漸偏離 target distribution。在這項工作中，我們在 score-based diffusion models 的背景下，從理論上分析了這種現象。對於一個現實的 pipeline，其中每個訓練回合都結合使用了 synthetic data 和來自目標的新樣本。",
      "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
      "title_zh": "Diffusion Models 中的錯誤傳播與模型崩潰：一項理論研究"
    },
    {
      "arxiv_id": "2602.16587",
      "authors": [
        "Luankang Zhang",
        "Yonghao Huang",
        "Hang Lv",
        "Mingjia Yin",
        "Liangyue Li",
        "Zulong Chen",
        "Hao Wang",
        "Enhong Chen"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536755+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models",
          "url": "https://arxiv.org/abs/2602.16587"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models",
        "url": "https://arxiv.org/abs/2602.16587"
      },
      "published_at": "2026-02-18T16:38:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7156384791532111,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.51563847915321
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16587",
      "summary": "Integrating Chain-of-Thought (CoT) reasoning into Semantic ID-based recommendation foundation models (such as OpenOneRec) often paradoxically degrades recommendation performance. We identify the root cause as textual inertia from the General Subspace, where verbose reasoning dominates inference and causes the model to neglect critical Semantic ID. To address this, we propose a training-free Inference-Time Subspace Alignment framework. By compressing reasoning chains and applying bias-subtracted ",
      "summary_zh": "將 Chain-of-Thought (CoT) reasoning 整合到基於 Semantic ID 的推薦基礎模型 (例如 OpenOneRec) 中，往往會矛盾地降低推薦性能。我們將根本原因歸結為來自 General Subspace 的 textual inertia，其中冗長的 reasoning 主導了 inference，並導致模型忽略關鍵的 Semantic ID。為了解決這個問題，我們提出了一個 training-free 的 Inference-Time Subspace Alignment 框架。透過壓縮 reasoning chains 並應用 bias-subtracted。",
      "title": "Why Thinking Hurts? Diagnosing and Rectifying the Reasoning Shift in Foundation Recommender Models",
      "title_zh": "為何思考會造成困擾？診斷與糾正 Foundation Recommender Models 中的推理轉變"
    },
    {
      "arxiv_id": "2602.16570",
      "authors": [
        "Ankur Moitra",
        "Andrej Risteski",
        "Dhruv Rohatgi"
      ],
      "categories": [
        "cs.LG",
        "cs.DS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566504+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
          "url": "https://arxiv.org/abs/2602.16570"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
        "url": "https://arxiv.org/abs/2602.16570"
      },
      "published_at": "2026-02-18T16:11:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7142946072500744,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.514294607250076
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16570",
      "summary": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.\n  In this paper, we consider the task of sampling from a re",
      "summary_zh": "Inference-time algorithms 是一種新興的範式，其中 pre-trained models 被用作 subroutines 來解決 downstream tasks。此類演算法已針對從 inverse problems 和 guided image generation 到 reasoning 的各種任務被提出。然而，目前實際部署的方法是具有多種 failure modes 的 heuristics —— 我們對這些 heuristics 何時能被有效改進知之甚少。在本文中，我們考慮從一個 re 進行採樣的任務。",
      "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
      "title_zh": "使用二次獎勵導引 diffusion models：一項細粒度分析"
    },
    {
      "arxiv_id": "2602.16198",
      "authors": [
        "Qijie Zhu",
        "Zeqi Ye",
        "Han Liu",
        "Zhaoran Wang",
        "Minshuo Chen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566846+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform",
          "url": "https://arxiv.org/abs/2602.16198"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform",
        "url": "https://arxiv.org/abs/2602.16198"
      },
      "published_at": "2026-02-18T05:44:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6838619984272832,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.483861998427287
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16198",
      "summary": "Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical just",
      "summary_zh": "Adaptation methods 一直是釋放 pre-trained diffusion models 在各種應用中變革性力量的主力。現有方法通常將 adaptation objectives 抽象為一個 reward function，並引導 diffusion models 生成 high-reward samples。然而，這些方法可能由於額外的訓練而產生高昂的 computational overhead，或者依賴於對 reward 的嚴格假設，例如 differentiability。此外，儘管它們取得了 empirical success，理論上的 justification。",
      "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform",
      "title_zh": "透過 Doob's $h$-Transform 對 Diffusion Models 進行免訓練適應"
    },
    {
      "arxiv_id": "2602.16165",
      "authors": [
        "Jiangweizhi Peng",
        "Yuanxin Liu",
        "Ruida Zhou",
        "Charles Fleming",
        "Zhaoran Wang",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213941+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
          "url": "https://arxiv.org/abs/2602.16165"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
        "url": "https://arxiv.org/abs/2602.16165"
      },
      "published_at": "2026-02-18T03:31:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.677586615664608,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.47758661566461
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16165",
      "summary": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajecto",
      "summary_zh": "將 LLMs 訓練成用於 multi-turn decision-making 的 interactive agents 仍然具有挑戰性，尤其是在 rewards 稀疏且延遲的 long-horizon tasks 中，agents 必須執行一系列長時間的動作才能接收到有意義的 feedback。大多數現有的 reinforcement learning (RL) 方法將 LLM agents 建模為在單一時間尺度上運作的 flat policies，在每個回合選擇一個 action。在 sparse-reward 環境中，此類 flat policies 必須在整個 trajectory 中傳播 credit。",
      "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
      "title_zh": "HiPER：用於 Large Language Model Agents 的帶有顯式信用分配的階層式強化學習"
    },
    {
      "arxiv_id": "2602.16990",
      "authors": [
        "Yan Wang",
        "Yi Han",
        "Lingfei Qian",
        "Yueru He",
        "Xueqing Peng",
        "Dongji Feng",
        "Zhuohan Xie",
        "Vincent Jim Zhang",
        "Rosie Guo",
        "Fengran Mo",
        "Jimin Huang",
        "Yankai Chen",
        "Xue Liu",
        "Jian-Yun Nie"
      ],
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-22T00:41:03.570438+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
          "url": "https://arxiv.org/abs/2602.16990"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
        "url": "https://arxiv.org/abs/2602.16990"
      },
      "published_at": "2026-02-19T01:29:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7425451574024653,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.442545157402463
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16990",
      "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an on",
      "summary_zh": "大多數推薦基準評估模型模仿用戶行為的程度。然而，在金融諮詢中，在市場波動下觀察到的行為可能具有噪音或短視，並可能與用戶的長期目標相衝突。因此，將用戶選擇的內容視為唯一的「ground truth」，會將行為模仿與決策品質混淆。我們介紹了 Conv-FinRe，這是一個對話式和縱向的股票推薦基準測試，它評估 LLMs 超越行為匹配的能力。鑑於一個",
      "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
      "title_zh": "Conv-FinRe: 一個對話式和縱向的基準測試，用於以效用為基礎的金融推薦"
    },
    {
      "arxiv_id": "2602.17584",
      "authors": [
        "Sharut Gupta",
        "Sanyam Kansal",
        "Stefanie Jegelka",
        "Phillip Isola",
        "Vikas Garg"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.564570+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Canonicalizing Multimodal Contrastive Representation Learning",
          "url": "https://arxiv.org/abs/2602.17584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Canonicalizing Multimodal Contrastive Representation Learning",
        "url": "https://arxiv.org/abs/2602.17584"
      },
      "published_at": "2026-02-19T18:09:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7959305454826482,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 26.39593054548265
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17584",
      "summary": "As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\\widetilde{f},\\widetilde{g})$) ",
      "summary_zh": "隨著模型和數據的擴展，獨立訓練的網絡通常會產生類似的相似性概念。然而，匹配相似性比在表示空間之間建立明確的對應關係要弱，特別是對於多模態模型而言，一致性不僅必須在每個模態內部成立，還必須在學習到的 image-text coupling 中成立。因此我們提出問題：給定兩個獨立訓練的多模態對比模型（帶有編碼器 $(f, g)$ 和 $(\\widetilde{f},\\widetilde{g})$）",
      "title": "Canonicalizing Multimodal Contrastive Representation Learning",
      "title_zh": "多模態對比表示學習的規範化"
    },
    {
      "arxiv_id": "2602.17108",
      "authors": [
        "Anton Dzega",
        "Aviad Elyashar",
        "Ortal Slobodin",
        "Odeya Cohen",
        "Rami Puzis"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.565339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
          "url": "https://arxiv.org/abs/2602.17108"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
        "url": "https://arxiv.org/abs/2602.17108"
      },
      "published_at": "2026-02-19T06:08:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7570573506108913,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 26.35705735061089
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17108",
      "summary": "Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the So",
      "summary_zh": "主題統覺測驗 (TAT) 是一個基於心理測量學的多維度評估框架，系統地區分人格類功能的認知表徵和情感關係成分。這項測驗是一種投射性心理框架，旨在揭示人格的無意識面向。本研究探討 Large Multimodal Models (LMMs) 的人格特質是否可以透過非語言模態進行評估，使用 So",
      "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
      "title_zh": "使用主題統覺測驗對大型多模態模型進行投射性心理評估"
    },
    {
      "arxiv_id": "2602.17103",
      "authors": [
        "Sajad Ashkezari",
        "Shai Ben-David"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583634+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners",
          "url": "https://arxiv.org/abs/2602.17103"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners",
        "url": "https://arxiv.org/abs/2602.17103"
      },
      "published_at": "2026-02-19T06:01:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7566867986650847,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 26.306686798665087
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17103",
      "summary": "We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.",
      "summary_zh": "我們研究了最近引入的「帶有改進的學習」模型，其中代理被允許對其特徵值進行微小更改，以獲得更理想的標籤。我們透過提供表徵此模型中線上可學習性的組合維度、分析多類別設置、在 bandit feedback 設置中的可學習性、建模代理進行改進的成本等方面，廣泛擴展了先前發表的結果。",
      "title": "Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners",
      "title_zh": "帶有改進型代理的線上學習：多類別、預算代理和 Bandit Learners"
    },
    {
      "arxiv_id": "2602.17623",
      "authors": [
        "Alireza Sakhaeirad",
        "Ali Ma'manpoosh",
        "Arshia Hemmat"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207689+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
          "url": "https://arxiv.org/abs/2602.17623"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
        "url": "https://arxiv.org/abs/2602.17623"
      },
      "published_at": "2026-02-19T18:42:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7977658779366871,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.157765877936686
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17623",
      "summary": "While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Per",
      "summary_zh": "儘管新興的波斯語 NLP 基準已擴展到語用學和禮儀領域，但它們很少區分記憶的文化事實和推斷隱含社會規範的能力。我們引入 DivanBench，這是一個診斷性基準測試，專注於迷信和習俗，這些是任意的、依賴於上下文的規則，難以進行簡單的邏輯推導。透過涵蓋三種任務類型（事實檢索、配對情境驗證和情境推理）的 315 個問題，我們評估了七個 Per",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
      "title_zh": "揭示波斯語語言模型中的事實-概念鴻溝"
    },
    {
      "arxiv_id": "2602.17433",
      "authors": [
        "Francesco Ortu",
        "Joeun Yook",
        "Punya Syon Pandey",
        "Keenan Samway",
        "Bernhard Schölkopf",
        "Alberto Cazzaniga",
        "Rada Mihalcea",
        "Zhijing Jin"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208856+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
          "url": "https://arxiv.org/abs/2602.17433"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
        "url": "https://arxiv.org/abs/2602.17433"
      },
      "published_at": "2026-02-19T15:05:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7858013752486279,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.145801375248627
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17433",
      "summary": "Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \\textsc{\\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each eve",
      "summary_zh": "大型語言模型（LLMs）越來越多地被用作歷史資訊的來源，這促使人們需要在模擬真實用戶互動的環境中，對有爭議的事件和帶有政治色彩的敘事進行可擴展的審計。我們引入了 \textsc{\texttt{HistoricalMisinfo}}，這是一個精心策劃的資料集，包含來自 $45$ 個國家的 $500$ 個有爭議事件，每個事件都配有一段事實參考敘事和一段有記載的修正主義參考敘事。為了近似真實世界的使用情況，我們為每個事件實例化了",
      "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
      "title_zh": "保護歷史真相：檢測大型語言模型中的歷史修正主義"
    },
    {
      "arxiv_id": "2602.17170",
      "authors": [
        "Chuting Yu",
        "Hang Li",
        "Joel Mackenzie",
        "Teerapong Leelanupab"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209732+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
          "url": "https://arxiv.org/abs/2602.17170"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
        "url": "https://arxiv.org/abs/2602.17170"
      },
      "published_at": "2026-02-19T08:37:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7649208345974803,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.124920834597482
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17170",
      "summary": "Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across ",
      "summary_zh": "人工相關性評估既耗時又耗費認知，限制了 Information Retrieval 評估的可擴展性。這導致人們對使用大型語言模型（LLMs）作為人類評審的代理產生了越來越濃厚的興趣。然而，LLM 主導的相關性判斷是否足夠可靠、穩定和嚴謹，以媲美人類在相關性評估方面的表現，仍然是一個懸而未決的問題。在這項工作中，我們系統性地研究了 LLM 主導的相關性判斷中的過度評分行為，跨越",
      "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
      "title_zh": "當 LLM 評審誇大分數時：探討相關性評估中的過度評分現象"
    },
    {
      "arxiv_id": "2602.16965",
      "authors": [
        "Sourav Chakraborty",
        "Amit Kiran Rege",
        "Claire Monteleoni",
        "Lijun Chen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584738+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Lipschitz Bandits",
          "url": "https://arxiv.org/abs/2602.16965"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Lipschitz Bandits",
        "url": "https://arxiv.org/abs/2602.16965"
      },
      "published_at": "2026-02-18T23:58:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7378555241710846,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.097855524171084
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16965",
      "summary": "We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed se",
      "summary_zh": "我們研究在連續、具有 Lipschitz 結構的行動空間上的去中心化多玩家隨機 Bandit 問題，其中硬碰撞會產生零獎勵。我們的目標是設計一種無通訊策略，以最大化集體獎勵，其協調成本與時間範圍 $T$ 無關。我們提出了一種模組化協議，該協議首先解決多智能體協調問題——透過一種新穎的最大值導向的...來識別並安排玩家進入不同的高價值區域",
      "title": "Multi-Agent Lipschitz Bandits",
      "title_zh": "多智能體 Lipschitz Bandits"
    },
    {
      "arxiv_id": "2602.16898",
      "authors": [
        "Iman Ahmadi",
        "Mehrshad Taji",
        "Arad Mahdinezhad Kashani",
        "AmirHossein Jadidi",
        "Saina Kashani",
        "Babak Khalaj"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.590555+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
          "url": "https://arxiv.org/abs/2602.16898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
        "url": "https://arxiv.org/abs/2602.16898"
      },
      "published_at": "2026-02-18T21:28:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7302263249072447,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.090226324907245
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16898",
      "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi g",
      "summary_zh": "使用大型語言模型（LLMs）進行機器人操作的任務規劃是一個新興領域。先前的方法依賴於專用模型、fine tuning 或 prompt tuning，並且通常以開迴路（open loop）方式操作，缺乏穩健的環境回饋，這使得它們在動態環境中顯得脆弱。我們提出了 MALLVi，這是一個 Multi Agent Large Language and Vision 框架，它能夠實現閉迴路（closed loop）回饋驅動的機器人操作。鑑於自然語言指令和環境圖像，MALLVi",
      "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
      "title_zh": "MALLVI：一個用於整合式通用機器人操作的多智能體框架"
    },
    {
      "arxiv_id": "2602.16154",
      "authors": [
        "Nithin Sivakumaran",
        "Shoubin Yu",
        "Hyunji Lee",
        "Yue Zhang",
        "Ali Payani",
        "Mohit Bansal",
        "Elias Stengel-Eskin"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.537143+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
          "url": "https://arxiv.org/abs/2602.16154"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
        "url": "https://arxiv.org/abs/2602.16154"
      },
      "published_at": "2026-02-18T02:55:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.6759111930175777,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.03591119301758
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16154",
      "summary": "Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that ",
      "summary_zh": "Chain-of-thought (CoT) 推理有時未能忠實反映大型語言模型（LLM）的真實計算過程，這阻礙了其在解釋 LLMs 如何得出答案方面的效用。此外，在推理中優化忠實性和可解釋性通常會降低任務性能。為了解決這種權衡並提高 CoT 的忠實性，我們提出了 Reasoning Execution by Multiple Listeners (REMUL)，這是一種多方 reinforcement learning 方法。REMUL 基於以下假設：",
      "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
      "title_zh": "透過多聽眾軟執行平衡推理中的忠實性與性能"
    },
    {
      "arxiv_id": "2602.16144",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Ziming Wang",
        "Chunlei Meng",
        "Jiaxuan Lu",
        "Jiekai Wu",
        "Kangan Qian",
        "Hao Zhang",
        "Simon Fong"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566969+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
          "url": "https://arxiv.org/abs/2602.16144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
        "url": "https://arxiv.org/abs/2602.16144"
      },
      "published_at": "2026-02-18T02:29:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.6746747194547372,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.03467471945474
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16144",
      "summary": "As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request",
      "summary_zh": "隨著多模態系統日益處理敏感的個人數據，選擇性撤銷特定數據模態的能力已成為隱私合規性和用戶自主權的關鍵要求。我們提出 Missing-by-Design (MBD)，這是一個用於可撤銷多模態情感分析的統一框架，它結合了結構化表示學習和可認證的參數修改管線 (pipeline)。可撤銷性在隱私敏感型應用中至關重要，在這些應用中，用戶或監管機構可能會要求",
      "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
      "title_zh": "Missing-by-Design：用於可撤銷多模態情感分析的可認證模態刪除"
    },
    {
      "arxiv_id": "2602.17452",
      "authors": [
        "Wyatt Benno",
        "Alberto Centelles",
        "Antoine Douchet",
        "Khalil Gibran"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773300+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
          "url": "https://arxiv.org/abs/2602.17452"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
        "url": "https://arxiv.org/abs/2602.17452"
      },
      "published_at": "2026-02-19T15:17:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7864637646896905,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.586463764689693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17452",
      "summary": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes",
      "summary_zh": "我們提出了 Jolt Atlas，這是一個 zero-knowledge machine learning (zkML) 框架，它將 Jolt 證明系統擴展到模型推論。與模擬 CPU 指令執行的 zkVMs (zero-knowledge virtual machines) 不同，Jolt Atlas 採用了 Jolt 以查閱為中心的方法，並將其直接應用於 ONNX tensor 操作。ONNX 計算模型消除了對 CPU 暫存器的需求，並簡化了記憶體一致性驗證。此外，ONNX 是一種開源、可攜帶的格式，這使得",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
      "title_zh": "Jolt Atlas：透過零知識中的查閱論證實現可驗證推論"
    },
    {
      "arxiv_id": "2602.17133",
      "authors": [
        "Linwei Zhai",
        "Han Ding",
        "Mingzhi Lin",
        "Cui Zhao",
        "Fei Wang",
        "Ge Wang",
        "Wang Zhi",
        "Wei Xi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.774047+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
          "url": "https://arxiv.org/abs/2602.17133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
        "url": "https://arxiv.org/abs/2602.17133"
      },
      "published_at": "2026-02-19T07:12:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7604383394648299,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.56043833946483
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17133",
      "summary": "Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and \"codebook collapse\" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the n",
      "summary_zh": "Vector Quantized Variational Autoencoders (VQ-VAEs) 對於現代生成模型至關重要，但由於表示學習和離散碼本 (codebook) 優化的內在耦合，它們經常遭受訓練不穩定和「碼本崩潰」(codebook collapse) 的問題。在本文中，我們提出了 VP-VAE (Vector Perturbation VAE)，這是一種新穎的範式，透過在訓練期間消除對顯式碼本的需求，將表示學習與離散化解耦。我們的主要見解是，從",
      "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
      "title_zh": "VP-VAE：透過自適應向量擾動重新思考向量量化"
    },
    {
      "arxiv_id": "2602.17033",
      "authors": [
        "Peize Li",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.772286+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing",
          "url": "https://arxiv.org/abs/2602.17033"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing",
        "url": "https://arxiv.org/abs/2602.17033"
      },
      "published_at": "2026-02-19T02:57:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.747083856760075,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.547083856760075
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17033",
      "summary": "Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastiv",
      "summary_zh": "具有部分級結構的單圖像 3D 生成仍然具有挑戰性：學習到的先驗知識難以涵蓋部分幾何體的長尾並保持多視角一致性，且現有系統對精確、局部化的編輯支援有限。我們提出了 PartRAG，這是一個檢索增強框架，它將外部部分數據庫與 diffusion transformer 相結合，以將生成與可編輯表示耦合起來。為了克服第一個挑戰，我們引入了一個 Hierarchical Contrastiv",
      "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing",
      "title_zh": "PartRAG：檢索增強的部分級 3D 生成與編輯"
    },
    {
      "arxiv_id": "2602.16456",
      "authors": [
        "Abdulla Jasem Almansoori",
        "Maria Ivanova",
        "Andrey Veprikov",
        "Aleksandr Beznosikov",
        "Samuel Horváth",
        "Martin Takáč"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.775410+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
          "url": "https://arxiv.org/abs/2602.16456"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
        "url": "https://arxiv.org/abs/2602.16456"
      },
      "published_at": "2026-02-18T13:41:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7069122933578401,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.506912293357843
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16456",
      "summary": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, wh",
      "summary_zh": "Low-Rank Adaptation (LoRA) 透過在凍結權重之上學習低秩更新來 fine-tune 大型模型，顯著減少了可訓練參數和記憶體。在這項工作中，我們解決了使用低秩投影 (SVDLoRA) 進行完整步驟訓練與 LoRA fine-tuning 之間的差距。我們提出了 LoRSum，這是一種記憶體效率高的子程序 (subroutine)，它透過將 LoRA 優化視為一個近端子問題並使用交替最小平方更新有效地解決它來彌補梯度下降的這一差距",
      "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
      "title_zh": "超越 SGD，無需 SVD：結合對角分式 K-FAC 的近端子空間迭代 LoRA"
    },
    {
      "arxiv_id": "2602.17665",
      "authors": [
        "Akashah Shabbir",
        "Muhammad Umer Sheikh",
        "Muhammad Akhtar Munir",
        "Hiyam Debary",
        "Mustansar Fiaz",
        "Muhammad Zaigham Zaheer",
        "Paolo Fraccaro",
        "Fahad Shahbaz Khan",
        "Muhammad Haris Khan",
        "Xiao Xiang Zhu",
        "Salman Khan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.582721+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
          "url": "https://arxiv.org/abs/2602.17665"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
        "url": "https://arxiv.org/abs/2602.17665"
      },
      "published_at": "2026-02-19T18:59:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7987156365033719,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.49871563650337
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17665",
      "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satelli",
      "summary_zh": "多模態推理 (multimodal reasoning) 的最新進展使得 Agent 能夠解釋圖像、將其與語言連結並執行結構化分析任務。將此類能力擴展到遙感領域 (remote sensing domain) 仍然具有挑戰性，因為模型必須對空間尺度 (spatial scale)、地理結構 (geographic structures) 和多光譜指數 (multispectral indices) 進行推理，同時保持連貫的多步驟邏輯 (multi-step logic)。為彌補這一差距，OpenEarthAgent 引入了一個統一框架，用於開發經衛星訓練的工具增強型地理空間 Agent。",
      "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
      "title_zh": "OpenEarthAgent: 一個用於工具增強型地理空間 Agent 的統一框架"
    },
    {
      "arxiv_id": "2602.17634",
      "authors": [
        "Xinghong Fu",
        "Yanhong Li",
        "Georgios Papaioannou",
        "Yoon Kim"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207612+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
          "url": "https://arxiv.org/abs/2602.17634"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
        "url": "https://arxiv.org/abs/2602.17634"
      },
      "published_at": "2026-02-19T18:48:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7980632488700691,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.498063248870068
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17634",
      "summary": "Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and exp",
      "summary_zh": "學習 Time Series Foundation Models 已被證明是實現跨多樣化 Time Series Domains 的 zero-shot Time Series Forecasting 的一種有前景的方法。由於 Scaling 在語言和視覺等其他 Modalities 中一直是 Foundation Models 性能的關鍵驅動力，近期許多關於 Time Series Foundation Modeling 的工作都集中在 Scaling 上。這導致了具有數億個 Parameters 的 Time Series Foundation Models，這些模型雖然性能良好，但卻效率低下且昂貴。",
      "title": "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting",
      "title_zh": "Reverso: 用於 Zero-shot 預測的高效 Time Series Foundation Models"
    },
    {
      "arxiv_id": "2602.17607",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.582944+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
          "url": "https://arxiv.org/abs/2602.17607"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
        "url": "https://arxiv.org/abs/2602.17607"
      },
      "published_at": "2026-02-19T18:31:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.797162241975681,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.49716224197568
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17607",
      "summary": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language d",
      "summary_zh": "PDEs 是科學和工程建模的核心，但設計準確的 Numerical Solvers 通常需要大量的數學專業知識和手動調整。最近基於 Neural Network 的方法提高了靈活性，但通常需要高計算成本 (computational cost) 並受到可解釋性 (interpretability) 有限的影響。我們引入了 \texttt{AutoNumerics}，這是一個 Multi-Agent Framework，可以直接從 Natural Language 描述中自主設計、實施、調試和驗證通用 PDEs 的 Numerical Solvers。",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "title_zh": "AutoNumerics: 一個用於科學計算的自主、PDE-Agnostic Multi-Agent Pipeline"
    },
    {
      "arxiv_id": "2602.17558",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583086+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
          "url": "https://arxiv.org/abs/2602.17558"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
        "url": "https://arxiv.org/abs/2602.17558"
      },
      "published_at": "2026-02-19T17:11:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7927522647257643,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.492752264725766
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17558",
      "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inher",
      "summary_zh": "多模態大型語言模型 (multimodal large language models, MLLMs) 的最新進展在將 Vision-Language Reasoning 擴展到專業工具型影像編輯方面展現出巨大潛力，實現了直觀和創新的編輯。一個有前景的方向是使用強化學習 (reinforcement learning, RL) 使 MLLMs 能夠在專業影像編輯軟體中推斷並執行最佳的工具使用計畫 (tool-use plans)。然而，由於缺乏可靠、可驗證的 Reward Signals 來反映其固有的特性，訓練仍然具有挑戰性。",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
      "title_zh": "RetouchIQ: 用於基於指令的影像修圖並具備通才獎勵的 MLLM Agents"
    },
    {
      "arxiv_id": "2602.17535",
      "authors": [
        "Behzad Bozorgtabar",
        "Dwarikanath Mahapatra",
        "Sudipta Roy",
        "Muzammal Naseer",
        "Imran Razzak",
        "Zongyuan Ge"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.564745+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
          "url": "https://arxiv.org/abs/2602.17535"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
        "url": "https://arxiv.org/abs/2602.17535"
      },
      "published_at": "2026-02-19T16:45:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7913029645991694,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.49130296459917
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17535",
      "summary": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and",
      "summary_zh": "醫學 Vision-Language Models (VLMs) 是醫學影像強大的 zero-shot 識別器，但它們在 Domain Shift 下的可靠性取決於具有保證的 calibrated uncertainty。Split Conformal Prediction (SCP) 提供了有限樣本覆蓋 (finite-sample coverage)，但預測集 (prediction sets) 通常會變得很大（效率低），且 Class-wise Coverage 不平衡 — Class-conditioned Coverage Gap (CCV) 很高，尤其是在 few-shot 和不平衡的狀況下；此外，天真地適應 Calibration Labels 會破壞 Exchangeability 和",
      "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
      "title_zh": "LATA: 運用 Laplacian-Assisted Transductive Adaptation 改善醫學 VLM 中的 Conformal Uncertainty"
    },
    {
      "arxiv_id": "2602.17518",
      "authors": [
        "Francesca Pezzuti",
        "Ophir Frieder",
        "Fabrizio Silvestri",
        "Sean MacAvaney",
        "Nicola Tonellotto"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583203+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A Picture of Agentic Search",
          "url": "https://arxiv.org/abs/2602.17518"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A Picture of Agentic Search",
        "url": "https://arxiv.org/abs/2602.17518"
      },
      "published_at": "2026-02-19T16:32:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7905852561038373,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.490585256103838
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17518",
      "summary": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effecti",
      "summary_zh": "隨著自動化系統越來越多地與人類一同發出搜尋查詢，資訊檢索 (IR) 面臨著重大轉變。然而，IR 仍然以人為中心，其系統、evaluation metrics、user models 和 datasets 都是圍繞人類查詢和行為設計的。因此，IR 在實踐中依賴的假設已不再適用，工作負載量、可預測性和查詢行為都發生了變化。這種不匹配影響了系統性能和最佳化：caching 可能會失去其效益。",
      "title": "A Picture of Agentic Search",
      "title_zh": "Agentic Search 的圖景"
    },
    {
      "arxiv_id": "2602.17475",
      "authors": [
        "Pietro Ferrazzi",
        "Mattia Franzin",
        "Alberto Lavelli",
        "Bernardo Magnini"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208734+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
          "url": "https://arxiv.org/abs/2602.17475"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
        "url": "https://arxiv.org/abs/2602.17475"
      },
      "published_at": "2026-02-19T15:38:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7876370525815763,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.487637052581576
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17475",
      "summary": "Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Reco",
      "summary_zh": "大型語言模型 (LLMs) 在各種醫療 Natural Language Processing (NLP) 任務中持續表現出色，然而，其龐大的計算需求經常限制了它們在實際醫療環境中的部署。在這項工作中，我們研究了「小型」LLMs（約十億參數）是否能在保持競爭性準確度的同時有效執行醫療任務。我們評估了來自 Llama-3、Gemma-3 和 Qwen3 這三個主要系列的模型，涵蓋了 20 項臨床 NLP 任務，其中包括 Named Entity Reco",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
      "title_zh": "用於醫療 NLP 的 Small LLMs：在義大利語中對 Few-Shot、Constraint Decoding、Fine-Tuning 和 Continual Pre-Training 的系統性分析"
    },
    {
      "arxiv_id": "2602.17431",
      "authors": [
        "Dylan Bouchard",
        "Mohit Singh Chauhan",
        "Viren Bajaj",
        "David Skarbrevik"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208911+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
          "url": "https://arxiv.org/abs/2602.17431"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
        "url": "https://arxiv.org/abs/2602.17431"
      },
      "published_at": "2026-02-19T15:02:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7856549606251653,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.485654960625165
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17431",
      "summary": "Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consis",
      "summary_zh": "Uncertainty quantification 已成為 LLMs closed-book hallucination 檢測的一種有效方法，但現有方法主要為短篇輸出設計，未能很好地泛化到長篇生成。我們引入了一種用於長篇 LLM 輸出中 fine-grained uncertainty quantification 的分類法，該分類法根據三個階段的設計選擇來區分方法：response decomposition、unit-level scoring 和 response-level aggregation。我們形式化了幾類 consis",
      "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
      "title_zh": "針對長篇語言模型輸出的 Fine-Grained Uncertainty Quantification：一項比較研究"
    },
    {
      "arxiv_id": "2602.17410",
      "authors": [
        "Bingqian Li",
        "Bowen Zheng",
        "Xiaolei Wang",
        "Long Zhang",
        "Jinpeng Wang",
        "Sheng Chen",
        "Wayne Xin Zhao",
        "Ji-rong Wen"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209146+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
          "url": "https://arxiv.org/abs/2602.17410"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
        "url": "https://arxiv.org/abs/2602.17410"
      },
      "published_at": "2026-02-19T14:37:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7843048681885179,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.484304868188516
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17410",
      "summary": "Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we pro",
      "summary_zh": "大型語言模型 (LLMs) 在推薦系統中展現了巨大的潛力，其中 supervised fine-tuning (SFT) 通常用於適應。隨後的研究進一步引入了 preference learning，將 negative samples 納入訓練過程。然而，現有方法依賴於序列級別的、離線生成的 negatives，這使得它們在將 LLMs 適應到具有大型 negative item spaces 的推薦任務時，區分度較低且資訊量不足。為了應對這些挑戰，我們提",
      "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
      "title_zh": "利用 Intermediate Layers 的 Self-Hard Negatives 改善基於 LLM 的推薦"
    },
    {
      "arxiv_id": "2602.17316",
      "authors": [
        "Bogdan Kostić",
        "Conor Fallon",
        "Julian Risch",
        "Alexander Löser"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209207+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
          "url": "https://arxiv.org/abs/2602.17316"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
        "url": "https://arxiv.org/abs/2602.17316"
      },
      "published_at": "2026-02-19T12:24:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7770933919696114,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.47709339196961
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17316",
      "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ",
      "summary_zh": "大型語言模型 (LLMs) 的快速發展已將標準化 evaluation benchmarks 確立為模型比較的主要工具。然而，由於對 input prompts 中淺層變化的敏感性，它們的可靠性受到越來越多的質疑。本文研究了受控的、在真值條件下等價的 lexical 和 syntactic perturbations 如何影響 23 種當代 LLMs 在 MMLU、SQuAD 和 AMEGA 這三個 benchmarks 上的絕對性能和相對排名。我們採用",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
      "title_zh": "相同意義，不同分數：LLM 評估中的詞彙和句法敏感性"
    },
    {
      "arxiv_id": "2602.17245",
      "authors": [
        "Linxi Jiang",
        "Rui Xi",
        "Zhijie Liu",
        "Shuo Chen",
        "Zhiqiang Lin",
        "Suman Nath"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583454+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
          "url": "https://arxiv.org/abs/2602.17245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
        "url": "https://arxiv.org/abs/2602.17245"
      },
      "published_at": "2026-02-19T10:50:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7720461554007229,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.472046155400722
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17245",
      "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web",
      "summary_zh": "網路正從人類瀏覽的媒介演變為軟體代理代表用戶執行操作的環境。大型語言模型 (LLMs) 的進步使得自然語言成為執行目標導向任務的實用介面，然而，大多數目前的網路代理仍舊在低階原語（例如點擊和鍵盤輸入）上操作。這些操作脆弱、效率低下且難以驗證。為了補充像 NLWeb 為檢索提供的 semantic layer 這類面向內容的工作，我們認為代理式網路",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
      "title_zh": "Web Verbs：為代理式網路提供可靠任務組合的類型化抽象"
    },
    {
      "arxiv_id": "2602.17203",
      "authors": [
        "Yuhong Luo",
        "Daniel Schoepflin",
        "Xintong Wang"
      ],
      "categories": [
        "cs.MA",
        "cs.GT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535744+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation",
          "url": "https://arxiv.org/abs/2602.17203"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation",
        "url": "https://arxiv.org/abs/2602.17203"
      },
      "published_at": "2026-02-19T09:47:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7686785004483077,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.468678500448306
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17203",
      "summary": "The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies wit",
      "summary_zh": "演算法共謀的威脅及其是否值得監管干預，仍然存在爭議，因為現有對其出現的評估通常依賴於長期的學習週期、關於對手在採用共謀策略方面的理性假設，以及參與者之間超參數和經濟環境的對稱性。為了研究共謀風險，我們引入了一種元遊戲設計，用於分析測試時限制下的演算法行為。我們將代理建模為具備預訓練策略",
      "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation",
      "title_zh": "測試時的演算法共謀：一種元遊戲設計與評估"
    },
    {
      "arxiv_id": "2602.17183",
      "authors": [
        "Kishan Maharaj",
        "Nandakishore Menon",
        "Ashita Saxena",
        "Srikanth Tamilselvam"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209664+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
          "url": "https://arxiv.org/abs/2602.17183"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
        "url": "https://arxiv.org/abs/2602.17183"
      },
      "published_at": "2026-02-19T09:05:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.766393660932428,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.46639366093243
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17183",
      "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three setti",
      "summary_zh": "大型語言模型 (LLMs) 越來越多地協助需要對長程式碼上下文進行推理的軟體工程任務，然而，它們在不同輸入條件下的魯棒性仍不明確。我們透過使用受控消融（controlled ablations）進行了長上下文程式碼問答的系統性研究，這些消融測試了對答案格式、干擾項和上下文規模的敏感性。我們將 LongCodeBench Python 資料集擴展至包含新的 COBOL 和 Java 問答集，並在三種設定下評估了最先進的模型",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
      "title_zh": "大型語言模型在長上下文程式碼問答中的魯棒性與推理忠實度"
    },
    {
      "arxiv_id": "2602.17078",
      "authors": [
        "Xuefeng Wang",
        "Lei Zhang",
        "Henglin Pu",
        "Husheng Li",
        "Ahmed H. Qureshi"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583945+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
          "url": "https://arxiv.org/abs/2602.17078"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
        "url": "https://arxiv.org/abs/2602.17078"
      },
      "published_at": "2026-02-19T04:42:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7525529942226076,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.452552994222607
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17078",
      "summary": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Be",
      "summary_zh": "近年來，Multi-agent reinforcement learning (MARL) 已取得顯著進展，但大多數演算法仍然依賴於具有固定決策間隔的 discrete-time Markov Decision Process (MDP)。這種公式化方法通常不適用於複雜的 multi-agent 動態，特別是在高頻或不規則時間間隔的設定中，這會導致性能下降，並促使了 continuous-time MARL (CT-MARL) 的發展。現有的 CT-MARL 方法主要建立在 Hamilton-Jacobi-Be",
      "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
      "title_zh": "透過 Epigraph Form 實現安全連續時間多代理強化學習"
    },
    {
      "arxiv_id": "2602.17054",
      "authors": [
        "Hussein S. Al-Olimat",
        "Ahmad Alshareef"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535918+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
          "url": "https://arxiv.org/abs/2602.17054"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
        "url": "https://arxiv.org/abs/2602.17054"
      },
      "published_at": "2026-02-19T03:51:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7498924165886867,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.44989241658869
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17054",
      "summary": "While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 53",
      "summary_zh": "雖然最近的阿拉伯語 NLP 基準測試側重於規模，但它們通常依賴於合成或翻譯的資料，這可能需要更深入的語言學驗證。我們推出了 ALPS (Arabic Linguistic & Pragmatic Suite)，這是一個原生的、由專家策劃的診斷挑戰集，旨在探究 Deep Semantics 和 Pragmatics，這些能力補充了專門的大規模基準測試。雖然廣泛覆蓋的基準測試優先考慮規模和 multi-task 覆蓋，ALPS 則透過 53",
      "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
      "title_zh": "ALPS：用於阿拉伯語語言學與語用推理的診斷挑戰集"
    },
    {
      "arxiv_id": "2602.16953",
      "authors": [
        "Hejia Zhang",
        "Zhongming Yu",
        "Chia-Tung Ho",
        "Haoxing Ren",
        "Brucek Khailany",
        "Jishen Zhao"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.584872+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
          "url": "https://arxiv.org/abs/2602.16953"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
        "url": "https://arxiv.org/abs/2602.16953"
      },
      "published_at": "2026-02-18T23:36:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7367376325823793,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.43673763258238
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16953",
      "summary": "Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Bu",
      "summary_zh": "執行感知型 LLM agents 為從工具回饋中學習提供了一個有前景的範式，但這種回饋的獲取成本通常很高且速度緩慢，使得 online reinforcement learning (RL) 不切實際。高覆蓋率的硬體驗證正是這種挑戰的例證，因為它依賴於工業模擬器和 non-differentiable execution signals。我們提出了 LLM4Cov，這是一個 offline agent-learning 框架，它將驗證建模為由 deterministic evaluators 引導的無記憶狀態轉換。Bu",
      "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
      "title_zh": "LLM4Cov: 用於高覆蓋率 Testbench 生成的執行感知 Agentic Learning"
    },
    {
      "arxiv_id": "2602.16915",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:12.406444+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
          "url": "https://arxiv.org/abs/2602.16915"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
        "url": "https://arxiv.org/abs/2602.16915"
      },
      "published_at": "2026-02-18T22:12:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7324202931889129,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.432420293188912
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16915",
      "summary": "Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
      "summary_zh": "立體深度估計對於水下機器人感知至關重要，但卻受到波長相關的光衰減、散射和折射引起的嚴重 domain shifts 影響。最近的方法利用 monocular foundation models 結合基於 GRU 的迭代細化進行水下適應；然而，GRU 中序列門控和局部卷積核需要多次迭代才能進行長距離 disparity propagation，這限制了在 large-disparity 和 textureless 水下區域的性能。在本文中，我們提出了 StereoAdapter-2，它用一種基於 selective state space models 的新型 ConvSS2D operator 取代了傳統的 ConvGRU updater。所提出的 operator 採用一種四方向掃描策略，該策略自然地與 epipolar geometry 對齊，同時捕獲垂直結構一致性，從而能夠在單個更新步驟中以 linear computational complexity 實現高效的長距離 spatial propagation。此外，我們構建了 UW-StereoDepth-80K，這是一個大型合成水下立體數據集，透過結合 semantic-aware style transfer 和 geometry-consistent novel view synthesis 的兩階段生成 pipeline，具有多樣的 baselines、attenuation coefficients 和 scattering parameters。結合從 StereoAdapter 繼承的 dynamic LoRA adaptation，我們的框架在水下基準測試中取得了 state-of-the-art 的 zero-shot 性能，在 TartanAir-UW 上提高了 17%，在 SQUID 上提高了 7.2%，並且在 BlueROV2 平台上進行的真實世界驗證證明了我們方法的穩健性。Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
      "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
      "title_zh": "StereoAdapter-2: 全局結構一致的水下立體深度估計"
    },
    {
      "arxiv_id": "2602.16872",
      "authors": [
        "Sean Man",
        "Roy Ganz",
        "Roi Ronen",
        "Shahar Tsiper",
        "Shai Mazor",
        "Niv Nayman"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.565996+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "DODO: Discrete OCR Diffusion Models",
          "url": "https://arxiv.org/abs/2602.16872"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "DODO: Discrete OCR Diffusion Models",
        "url": "https://arxiv.org/abs/2602.16872"
      },
      "published_at": "2026-02-18T20:59:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7287285326572173,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.428728532657217
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16872",
      "summary": "Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlik",
      "summary_zh": "光學字符識別 (Optical Character Recognition, OCR) 是一項將資訊數位化的基本任務，是視覺數據和文本理解之間的關鍵橋樑。雖然現代 Vision-Language Models (VLM) 在此領域取得了高準確度，但它們主要依賴於 autoregressive decoding，這對於長文件來說計算成本高且速度慢，因為它需要為每個生成的 token 進行 sequential forward pass。我們發現了一個克服此瓶頸的關鍵機會：unlik",
      "title": "DODO: Discrete OCR Diffusion Models",
      "title_zh": "DODO: 離散 OCR Diffusion Models"
    },
    {
      "arxiv_id": "2602.16844",
      "authors": [
        "Madeleine Grunde-McLaughlin",
        "Hussein Mozannar",
        "Maya Murad",
        "Jingya Chen",
        "Saleema Amershi",
        "Adam Fourney"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.590749+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
          "url": "https://arxiv.org/abs/2602.16844"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
        "url": "https://arxiv.org/abs/2602.16844"
      },
      "published_at": "2026-02-18T20:16:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7265573958862068,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.426557395886206
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16844",
      "summary": "To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are ",
      "summary_zh": "為了實現人類監督，agentic AI 系統通常會提供推理和行動步驟的 trace。設計具有資訊豐富但不至於過載的詳細程度的 trace 仍然是一個關鍵挑戰。在對 Computer User Agent 進行的三項用戶研究中，我們調查了基本 action traces 對於驗證的效用，透過 design probes 探索了三種替代方案，並測試了一個新型介面在問答任務中發現錯誤的影響。正如預期的那樣，我們發現當前的做法是",
      "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
      "title_zh": "無需持續監督地監管 Agents：挑戰與機會"
    },
    {
      "arxiv_id": "2602.16819",
      "authors": [
        "Yiqing Xie",
        "Emmy Liu",
        "Gaokai Zhang",
        "Nachiket Kotalwar",
        "Shubham Gandhi",
        "Sathwik Acharya",
        "Xingyao Wang",
        "Carolyn Rose",
        "Graham Neubig",
        "Daniel Fried"
      ],
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.590816+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
          "url": "https://arxiv.org/abs/2602.16819"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
        "url": "https://arxiv.org/abs/2602.16819"
      },
      "published_at": "2026-02-18T19:30:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.724266137708249,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.42426613770825
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16819",
      "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for de",
      "summary_zh": "在評估 coding agents 的品質時，主要的基準測試專注於解決 GitHub 上的單一問題，例如 SWE-Bench。相比之下，在實際使用中，這些 agents 解決更為多樣和複雜的任務，這些任務涉及探索 codebases、測試軟體和設計 architecture 等其他技能。在本文中，我們首先透過將 trajectories 分解為 fine-grained components，來描述跨不同任務共享的一些 transferable skills，並導出了一套設計原則",
      "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
      "title_zh": "Hybrid-Gym：訓練 Coding Agents 以跨任務泛化"
    },
    {
      "arxiv_id": "2602.16796",
      "authors": [
        "Zifan Wang",
        "Riccardo De Santi",
        "Xiaoyu Mo",
        "Michael M. Zavlanos",
        "Andreas Krause",
        "Karl H. Johansson"
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.594189+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning",
          "url": "https://arxiv.org/abs/2602.16796"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning",
        "url": "https://arxiv.org/abs/2602.16796"
      },
      "published_at": "2026-02-18T19:00:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7227621667019342,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.422762166701936
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16796",
      "summary": "Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a p",
      "summary_zh": "對預訓練的 diffusion 和 flow models 進行 fine-tuning 以最佳化下游應用（downstream utilities）是實際部署的關鍵。現有的 entropy-regularized methods 主要最大化預期獎勵（expected reward），但未能提供塑造 tail behavior 的機制。然而，tail control 通常至關重要：lower tail 透過限制低獎勵失敗（low-reward failures）來決定可靠性（reliability），而 upper tail 則透過優先考慮稀有、高獎勵的結果（rare, high-reward outcomes）來實現發現（discovery）。在這項工作中，我們提出了 Tail-aware Flow Fine-Tuning (TFFT)，一個p",
      "title": "Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning",
      "title_zh": "透過 Flow Model Fine-Tuning 實現高效的尾部感知生成式最佳化"
    },
    {
      "arxiv_id": "2602.16784",
      "authors": [
        "Victoria Lin",
        "Louis-Philippe Morency",
        "Eli Ben-Michael"
      ],
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211386+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
          "url": "https://arxiv.org/abs/2602.16784"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
        "url": "https://arxiv.org/abs/2602.16784"
      },
      "published_at": "2026-02-18T19:00:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7227169954781307,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.422716995478133
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16784",
      "summary": "Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify tha",
      "summary_zh": "儘管現代 language models 在各種任務上表現出色，但它們在 distribution shifts 下仍然容易受到影響，當在與其 training data 分佈不同的數據上進行評估時，會表現出脆弱的行為。在本文中，我們描述了 language models 中的 distribution shifts 如何被分為可觀察（observable）和不可觀察（unobservable）兩部分，並討論了現有處理 distribution shift 的方法僅處理前者。重要的是，我們發現了tha",
      "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
      "title_zh": "分佈偏移下語言模型中的遺漏變數偏差"
    },
    {
      "arxiv_id": "2602.16702",
      "authors": [
        "Mingjia Shi",
        "Yinhan He",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536582+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
          "url": "https://arxiv.org/abs/2602.16702"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
        "url": "https://arxiv.org/abs/2602.16702"
      },
      "published_at": "2026-02-18T18:49:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7222077598649528,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.422207759864953
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16702",
      "summary": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominate",
      "summary_zh": "Vision-language models (VLMs) 旨在透過共同利用視覺（visual）和文本（textual）模態（modalities）進行推理。雖然為 large language models (LLMs) 分配額外的 inference-time computation 已被證明是有效的，但在 VLMs 中實現類似的擴展仍然充滿挑戰。一個關鍵障礙是，視覺輸入（visual inputs）通常在生成開始時僅提供一次，而文本推理（textual reasoning）（例如，早期視覺摘要 early visual summaries）是 autoregressively 生成的，這導致推理越來越受到文本的主導",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
      "title_zh": "顯著性感知多路徑思維：重思視覺語言推理"
    },
    {
      "arxiv_id": "2602.16671",
      "authors": [
        "Jaid Monwar Chowdhury",
        "Chi-An Fu",
        "Reyhaneh Jabbarvand"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536638+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
          "url": "https://arxiv.org/abs/2602.16671"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
        "url": "https://arxiv.org/abs/2602.16671"
      },
      "published_at": "2026-02-18T18:09:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7201602330651622,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.42016023306516
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16671",
      "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in n",
      "summary_zh": "由於高階程式意圖（high-level program intent）與指標算術（pointer arithmetic）和手動記憶體管理（manual memory management）的嚴格語法限制（syntactic constraints）之間存在語義鴻溝（semantic gap），C 語言的自動化 unit test generation 仍然是一個艱鉅的挑戰。雖然 Large Language Models (LLMs) 展現出強大的生成能力（generative capabilities），但直接的意圖到程式碼合成（intent-to-code synthesis）經常遭遇「跳躍式程式碼生成失敗模式」（leap-to-code failure mode），即模型在沒有依據程式結構（program structure）、約束（constraints）和語義（semantics）的情況下過早地輸出程式碼。這將導致n",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "title_zh": "SPARC：用於自動化 C 語言單元測試生成的場景規劃與推理"
    },
    {
      "arxiv_id": "2602.16660",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211809+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
          "url": "https://arxiv.org/abs/2602.16660"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
        "url": "https://arxiv.org/abs/2602.16660"
      },
      "published_at": "2026-02-18T18:01:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7197769164719094,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.41977691647191
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16660",
      "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment.",
      "summary_zh": "Large language models (LLMs) 在各個語言社群的廣泛部署，使得可靠的 multilingual safety alignment 成為必要。然而，最近將 alignment 擴展到其他語言的努力，通常需要大量的資源，無論是透過在 target language 中進行大規模、高品質的 supervision，還是透過與 high-resource languages 進行 pairwise alignment，這限制了 scalability。在這項工作中，我們提出了一種 resource-efficient method，用於改進 multilingual safety alignment。",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "title_zh": "一次對齊，多語受益：強制 LLM 安全對齊的多語言一致性"
    },
    {
      "arxiv_id": "2602.16585",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591222+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
          "url": "https://arxiv.org/abs/2602.16585"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
        "url": "https://arxiv.org/abs/2602.16585"
      },
      "published_at": "2026-02-18T16:35:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7155109345877408,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.41551093458774
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16585",
      "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a ",
      "summary_zh": "操作的嚴謹性決定了人類與代理的協作能否成功。科學數據管線需要類似於 DevOps 的 SciOps，然而，常見的方法在不具備 transactional guarantees 的孤立系統中分散了 provenance。DataJoint 2.0 透過關係型工作流程模型解決了這個問題：表格代表 workflow steps，行代表 artifacts，foreign keys 規定了 execution order。其 schema 不僅指定了存在哪些 data，還指定了其衍生方式——",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "title_zh": "DataJoint 2.0：代理式科學工作流程的計算基質"
    },
    {
      "arxiv_id": "2602.16467",
      "authors": [
        "Saurabh Bharti",
        "Gaurav Azad",
        "Abhinaw Jagtap",
        "Nachiket Tapas"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213201+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
          "url": "https://arxiv.org/abs/2602.16467"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
        "url": "https://arxiv.org/abs/2602.16467"
      },
      "published_at": "2026-02-18T13:55:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7076130072225704,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.407613007222572
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16467",
      "summary": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realist",
      "summary_zh": "大型語言模型 (LLMs) 的快速進步，使得需要反映真實世界學術嚴謹性和多語言複雜性的評估框架。本文介紹了 IndicEval，一個可擴展的 benchmarking 平台，旨在利用來自 UPSC、JEE 和 NEET 的真實 high-stakes 考試題目，在 STEM 和人文領域，以英語和印地語雙語評估 LLM 的性能。與 synthetic benchmarks 不同，IndicEval 將評估建立在真實的考試標準上，實現了現實主義的",
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "title_zh": "IndicEval：一個用於大型語言模型的雙語印度教育評估框架"
    },
    {
      "arxiv_id": "2602.16429",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591693+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
          "url": "https://arxiv.org/abs/2602.16429"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
        "url": "https://arxiv.org/abs/2602.16429"
      },
      "published_at": "2026-02-18T13:01:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7049317911494739,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.404931791149473
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16429",
      "summary": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-t",
      "summary_zh": "Agentic systems，即自主執行 multi-step workflows 以實現 complex goals 的 AI architectures，通常透過重複呼叫 large language model (LLM) 來處理 closed-set decision tasks，例如 routing、shortlisting、gating 和 verification。儘管這種設計很方便，但由於 cumulative latency 和 token usage，使得 deployments 變得緩慢且昂貴。我們提出了 TabAgent，一個用於在 closed-set selection tasks 中用緊湊的 textual-t 替換 generative decision components 的框架。",
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "title_zh": "TabAgent：一個用表格-文本分類器替換代理式生成組件的框架"
    },
    {
      "arxiv_id": "2602.16412",
      "authors": [
        "Daichi Yashima",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Komei Sugiura"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213378+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
          "url": "https://arxiv.org/abs/2602.16412"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
        "url": "https://arxiv.org/abs/2602.16412"
      },
      "published_at": "2026-02-18T12:37:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7037725451344176,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.403772545134416
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16412",
      "summary": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating ",
      "summary_zh": "儘管 Multimodal Large Language Models (MLLMs) 在各種任務中取得了顯著成功，但 long-form video understanding 仍然是一個巨大挑戰。在本研究中，我們專注於 MLLMs 的 video understanding。這項任務之所以具有挑戰性，是因為處理完整的 RGB 幀流在計算上是 intractable 且高度 redundant 的，因為 self-attention 的複雜度與 sequence length 呈二次方關係。在本文中，我們提出了 ReMoRa，這是一個透過操作來處理影片的 video MLLM。",
      "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
      "title_zh": "ReMoRa：基於精煉運動表示的用於長影片理解的多模態大型語言模型"
    },
    {
      "arxiv_id": "2602.16241",
      "authors": [
        "Md. Najib Hasan",
        "Touseef Hasan",
        "Souvika Sarkar"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213696+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Are LLMs Ready to Replace Bangla Annotators?",
          "url": "https://arxiv.org/abs/2602.16241"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Are LLMs Ready to Replace Bangla Annotators?",
        "url": "https://arxiv.org/abs/2602.16241"
      },
      "published_at": "2026-02-18T07:36:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6892192128678211,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.389219212867822
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16241",
      "summary": "Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified ",
      "summary_zh": "大型語言模型 (LLMs) 越來越多地被用作 automated annotators，以擴展 dataset creation，然而，它們作為 unbiased annotators 的可靠性——尤其是在 low-resource 和 identity-sensitive settings 下——仍知之甚少。在本研究中，我們探討了 LLMs 作為 Bangla hate speech 的 zero-shot annotators 的行為，這是一項即使人類共識也充滿挑戰，且 annotator bias 可能導致嚴重 downstream consequences 的任務。我們使用一個統一的標準，對 17 個 LLMs 進行了系統性的 benchmark。",
      "title": "Are LLMs Ready to Replace Bangla Annotators?",
      "title_zh": "LLMs 準備好取代孟加拉語標註者了嗎？"
    },
    {
      "arxiv_id": "2602.16745",
      "authors": [
        "Zhangyi Liu",
        "Huaizhi Qu",
        "Xiaowei Yin",
        "He Sun",
        "Yanjun Han",
        "Tianlong Chen",
        "Zhun Deng"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.537089+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
          "url": "https://arxiv.org/abs/2602.16745"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
        "url": "https://arxiv.org/abs/2602.16745"
      },
      "published_at": "2026-02-18T03:28:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6774368416603863,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.377436841660387
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16745",
      "summary": "Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority",
      "summary_zh": "Test-time scaling 可以透過聚合隨機推理軌跡來提高模型性能。然而，在有限預算下實現樣本高效的 Test-Time Self-Consistency 仍然是一個開放的挑戰。我們引入了 PETS (Principled and Efficient Test-Time Self-Consistency)，它透過一個優化框架，啟動了對軌跡分配的原則性研究。我們方法的關鍵是 self-consistency rate，這是一個新測量指標，定義為與無限預算多數意見的一致性。",
      "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
      "title_zh": "PETS: 一個旨在實現高效 Test-Time Self-Consistency 的最佳軌跡分配原則性框架"
    },
    {
      "arxiv_id": "2602.16131",
      "authors": [
        "Chihiro Watanabe",
        "Jingyu Sun"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.214248+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis",
          "url": "https://arxiv.org/abs/2602.16131"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis",
        "url": "https://arxiv.org/abs/2602.16131"
      },
      "published_at": "2026-02-18T01:49:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6728047809317749,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.372804780931773
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16131",
      "summary": "Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework",
      "summary_zh": "大型語言模型 (LLMs) 正日益被用作 agent 來解決複雜任務，例如問答 (QA)、科學辯論和軟體開發。標準的評估程序會將來自 LLM agent 的多個響應彙總成單一最終答案，通常透過多數投票，並與參考答案進行比較。然而，這個過程可能會掩蓋原始響應的品質和分佈特性。在本文中，我們提出了一個新穎的評估框架。",
      "title": "Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis",
      "title_zh": "用於基於 LLM 的 Agent 系統分析的經驗累積分佈函數聚類"
    },
    {
      "arxiv_id": "2602.16503",
      "authors": [
        "Vasilis Gkolemis",
        "Loukas Kavouras",
        "Dimitrios Kyriakopoulos",
        "Konstantinos Tsopelas",
        "Dimitrios Rontogiannis",
        "Giuseppe Casalicchio",
        "Theodore Dalamagas",
        "Christos Diou"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.806596+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
          "url": "https://arxiv.org/abs/2602.16503"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
        "url": "https://arxiv.org/abs/2602.16503"
      },
      "published_at": "2026-02-18T14:45:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7100545433680158,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 25.310054543368015
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16503",
      "summary": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature",
      "summary_zh": "廣義可加模型 (GAMs) 透過獨立的單變量特徵效應提供解釋性，但在數據中存在交互作用時會出現欠擬合。GA$^2$Ms 增加了選定的成對交互作用，提高了準確性，但犧牲了解釋性並限制了模型審計。我們提出了 Conditionally Additive Local Models (CALMs)，這是一種新的模型類別，它平衡了 GAMs 的解釋性與 GA$^2$Ms 的準確性。CALMs 允許每個特徵具有多個單變量形狀函數。",
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "title_zh": "透過精準的局部可加模型和條件特徵效應實現設計即解釋性"
    },
    {
      "arxiv_id": "2602.16548",
      "authors": [
        "Tianmeng Hu",
        "Yongzheng Cui",
        "Biao Luo",
        "Ke Li"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566558+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
          "url": "https://arxiv.org/abs/2602.16548"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
        "url": "https://arxiv.org/abs/2602.16548"
      },
      "published_at": "2026-02-18T15:52:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7133601874918086,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 25.21336018749181
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16548",
      "summary": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose",
      "summary_zh": "RNA 三維 (3D) 結構的逆向設計對於合成生物學和治療學中功能性 RNA 的工程設計至關重要。儘管最近的深度學習方法已推動該領域的發展，但它們通常是使用原始序列恢復進行優化和評估的，這對於結構保真度來說是一個有限的替代指標，因為不同的序列可以摺疊成相似的 3D 結構，並且高恢復率不一定表示正確摺疊。為了解決這個限制，我們提出了一種方法。",
      "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
      "title_zh": "RIDER: 結合強化學習引導的 Diffusion 進行 3D RNA 逆向設計"
    },
    {
      "arxiv_id": "2602.16554",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.591278+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
          "url": "https://arxiv.org/abs/2602.16554"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
        "url": "https://arxiv.org/abs/2602.16554"
      },
      "published_at": "2026-02-18T15:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.713464226771831,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 25.16346422677183
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16554",
      "summary": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three p",
      "summary_zh": "我們介紹 MerLean，一個用於量子計算自動形式化的全自動 agentic 框架。MerLean 從 \\LaTeX{} 源文件中提取數學陳述，將其形式化為基於 Mathlib 的經驗證的 Lean~4 代碼，並將結果翻譯回人類可讀的 \\LaTeX{} 以進行語義審查。我們在三篇理論量子計算論文上評估了 MerLean，總共從 114 個陳述中產生了 2,050 個 Lean 宣告。MerLean 在所有三篇論文上實現了端到端形式化。",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "title_zh": "MerLean: 一個用於量子計算自動形式化的 Agentic 框架"
    },
    {
      "arxiv_id": "2602.17659",
      "authors": [
        "Yu Fang",
        "Yuchun Feng",
        "Dong Jing",
        "Jiaqi Liu",
        "Yue Yang",
        "Zhenyu Wei",
        "Daniel Szafir",
        "Mingyu Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.565268+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
          "url": "https://arxiv.org/abs/2602.17659"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
        "url": "https://arxiv.org/abs/2602.17659"
      },
      "published_at": "2026-02-19T18:59:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7986842061823972,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.158684206182397
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17659",
      "summary": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we intro",
      "summary_zh": "Vision-Language-Action models (VLAs) 承諾將語言指令實現在機器人控制中，然而在實踐中卻經常未能忠實地遵循語言。當提供缺乏強烈場景特定監督的指令時，VLAs 會遭遇 counterfactual failures：它們基於由 dataset biases 引起的視覺捷徑行事，重複執行訓練中常見的習得行為並選擇訓練中經常見到的物體，而無論語言意圖為何。為系統性地研究此現象，我們引入",
      "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
      "title_zh": "當視覺凌駕於語言：評估並緩解 VLA 中的反事實失敗"
    },
    {
      "arxiv_id": "2602.17510",
      "authors": [
        "Kasun Dewage",
        "Marianna Pensky",
        "Suranadi De Silva",
        "Shankadeep Mondal"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773175+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
          "url": "https://arxiv.org/abs/2602.17510"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
        "url": "https://arxiv.org/abs/2602.17510"
      },
      "published_at": "2026-02-19T16:22:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7900254564998628,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.15002545649986
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17510",
      "summary": "We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layer",
      "summary_zh": "我們介紹 CRAFT (Cross-layer Rank Adaptation via Frozen Tucker)，這是一種 parameter-efficient fine-tuning (PEFT) 方法，它將 Tucker tensor decomposition 應用於堆疊在 transformer layers 上的預訓練 attention weight matrices，並且僅在由此產生的 frozen Tucker factors 上訓練小的方形 adaptation matrices。現有的 tensor-based PEFT 方法分解 gradient updates：LoTR 應用帶有共享 factor matrices 的 Tucker decomposition，而 SuperLoRA 則跨層級對 $ΔW$ 進行分組和重塑。",
      "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
      "title_zh": "LORA-CRAFT: 透過預訓練 Attention Weights 的凍結 Tucker 分解實現跨層級秩適應"
    },
    {
      "arxiv_id": "2602.17386",
      "authors": [
        "Adrià Molina",
        "Oriol Ramos Terrades",
        "Josep Lladós"
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
          "url": "https://arxiv.org/abs/2602.17386"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
        "url": "https://arxiv.org/abs/2602.17386"
      },
      "published_at": "2026-02-19T14:10:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.782846547153511,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.14284654715351
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17386",
      "summary": "Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propos",
      "summary_zh": "Information retrieval 是現代數位產業的基石。儘管 natural language search 近年來在 embedding-based models 和 large-scale pretraining 的大力推動下取得了顯著進展，但該領域仍面臨重大挑戰。具體來說，涉及複雜關係、物體組成或精確約束（例如身份、計數和比例）的查詢在當前框架內通常仍無法解決或不可靠。在本文中，我們提出",
      "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
      "title_zh": "Visual Model Checking: 圖像檢索中視覺例程的圖形化推斷"
    },
    {
      "arxiv_id": "2602.16183",
      "authors": [
        "Subham Pokhriyal",
        "Shweta Jain",
        "Vaneet Aggarwal"
      ],
      "categories": [
        "cs.GT",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.592394+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Combinatorial-Multi-Armed-Bandit framework for the Submodular Welfare Problem under Bandit Feedback",
          "url": "https://arxiv.org/abs/2602.16183"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Combinatorial-Multi-Armed-Bandit framework for the Submodular Welfare Problem under Bandit Feedback",
        "url": "https://arxiv.org/abs/2602.16183"
      },
      "published_at": "2026-02-18T05:00:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.68180086067715,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 25.13180086067715
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16183",
      "summary": "We study the \\emph{Submodular Welfare Problem} (SWP), where items are partitioned among agents with monotone submodular utilities to maximize the total welfare under \\emph{bandit feedback}. Classical SWP assumes full value-oracle access, achieving $(1-1/e)$ approximations via continuous-greedy algorithms. We extend this to a \\emph{multi-agent combinatorial bandit} framework (\\textsc{MA-CMAB}), where actions are partitions under full-bandit feedback with non-communicating agents. Unlike prior sin",
      "summary_zh": "我們研究 \\emph{Submodular Welfare Problem} (SWP)，其中物品在具有 monotone submodular utilities 的 agent 之間分配，以在 \\emph{bandit feedback} 下最大化總福利。經典的 SWP 假設完全的 value-oracle access，透過 continuous-greedy algorithms 實現 $(1-1/e)$ 近似。我們將此擴展到一個 \\emph{multi-agent combinatorial bandit} 框架 (\textsc{MA-CMAB})，其中 actions 是在 full-bandit feedback 下的分割，並由不通訊的 agents 執行。與以往不同的是",
      "title": "Multi-Agent Combinatorial-Multi-Armed-Bandit framework for the Submodular Welfare Problem under Bandit Feedback",
      "title_zh": "在 Bandit Feedback 下用於次模福利問題的 Multi-Agent Combinatorial-Multi-Armed-Bandit 框架"
    },
    {
      "arxiv_id": "2602.16832",
      "authors": [
        "Priyaranjan Pattnayak",
        "Sanchari Chowdhuri"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.553339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-safety",
          "tier": 1,
          "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
          "url": "https://arxiv.org/abs/2602.16832"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-safety",
        "tier": 1,
        "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
        "url": "https://arxiv.org/abs/2602.16832"
      },
      "published_at": "2026-02-18T19:54:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7254473855027794,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 25.085447385502782
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16832",
      "summary": "Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.\n  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLa",
      "summary_zh": "large language models (LLMs) 的 Safety alignment 主要在 English 和 contract-bound 情況下進行評估，使得 multilingual vulnerabilities 缺乏深入研究。我們引入 \textbf{Indic Jailbreak Robustness (IJR)}，這是一個針對 12 種 Indic 和 South Asian languages (2.1 Billion 說話者) 的 adversarial safety 的 judge-free benchmark，涵蓋 JSON (contract-bound) 和 Free (naturalistic) 軌道中的 45216 個 prompts。\n  IJR 揭示了三種模式。(1) Contracts 誇大了拒絕率但未能阻止 jailbreaks：在 JSON 中，LLa",
      "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
      "title_zh": "IndicJR: 針對南亞語言 Jailbreak Robustness 的無評審基準"
    },
    {
      "arxiv_id": "2602.16634",
      "authors": [
        "Yu Xie",
        "Ludwig Winkler",
        "Lixin Sun",
        "Sarah Lewis",
        "Adam E. Foster",
        "José Jiménez Luna",
        "Tim Hempel",
        "Michael Gastegger",
        "Yaoyi Chen",
        "Iryna Zaporozhets",
        "Cecilia Clementi",
        "Christopher M. Bishop",
        "Frank Noé"
      ],
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "physics.bio-ph",
        "physics.chem-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566387+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
          "url": "https://arxiv.org/abs/2602.16634"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
        "url": "https://arxiv.org/abs/2602.16634"
      },
      "published_at": "2026-02-18T17:26:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7180229346532292,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.6000000000000005,
        "total_score": 25.01802293465323
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16634",
      "summary": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free ene",
      "summary_zh": "稀有事件採樣問題長期以來一直是 molecular dynamics (MD)，特別是 biomolecular simulation 中的核心限制因素。近來，諸如 BioEmu 等 diffusion models 已成為強大的 equilibrium samplers，能夠從複雜的 molecular distributions 生成獨立樣本，從而消除了採樣稀有過渡事件的成本。然而，當計算依賴於 equilibrium 中稀有狀態的 observables 時，例如 folding free energy，採樣問題依然存在。",
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "title_zh": "增強式 Diffusion 採樣：利用 Diffusion Models 進行高效稀有事件採樣與自由能計算"
    },
    {
      "arxiv_id": "2602.17525",
      "authors": [
        "Luca Ghafourpour",
        "Sinho Chewi",
        "Alessio Figalli",
        "Aram-Alexandre Pooladian"
      ],
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773106+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Variational inference via radial transport",
          "url": "https://arxiv.org/abs/2602.17525"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Variational inference via radial transport",
        "url": "https://arxiv.org/abs/2602.17525"
      },
      "published_at": "2026-02-19T16:36:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7908213688967878,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 24.94082136889679
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17525",
      "summary": "In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, s",
      "summary_zh": "在 variational inference (VI) 中，實踐者通常使用一個簡單的替代分佈，常是一個 (product) Gaussian distribution，來近似高維分佈 $π$。然而，在許多實際應用中，Gaussian distributions 可能無法捕捉到 $π$ 的正確 radial profile，導致 coverage 不佳。在這項工作中，我們從優化這些 radial profiles 的角度來解決 VI 問題。我們的演算法 radVI 是一個經濟且有效的 add-on，可應用於許多現有的 VI schemes，s",
      "title": "Variational inference via radial transport",
      "title_zh": "透過徑向傳輸的 Variational Inference"
    },
    {
      "arxiv_id": "2602.17434",
      "authors": [
        "Eleftherios E. Vlahakis",
        "Arash Bahari Kordabad",
        "Lars Lindemann",
        "Pantelis Sopasakis",
        "Sadegh Soudjani",
        "Dimos V. Dimarogonas"
      ],
      "categories": [
        "eess.SY",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:58.583266+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
          "url": "https://arxiv.org/abs/2602.17434"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
        "url": "https://arxiv.org/abs/2602.17434"
      },
      "published_at": "2026-02-19T15:05:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.7858068322215706,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.82580683222157
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17434",
      "summary": "Multi-agent planning under Signal Temporal Logic (STL) is often hindered by collaborative tasks that lead to computational challenges due to the inherent high-dimensionality of the problem, preventing scalable synthesis with satisfaction guarantees. To address this, we formulate STL planning as an optimization program under arbitrary multi-agent constraints and introduce a penalty-based unconstrained relaxation that can be efficiently solved via a Block-Coordinate Gradient Descent (BCGD) method,",
      "summary_zh": "在 Signal Temporal Logic (STL) 下的多智能體規劃常受限於協作任務，這些任務因問題固有的高維性而帶來計算挑戰，阻礙了具有 satisfaction guarantees 的可擴展合成。為了解決這個問題，我們將 STL 規劃 форму化為一個在任意 multi-agent 限制下的優化程式，並引入了一種基於懲罰的 unconstrained relaxation，它可透過 Block-Coordinate Gradient Descent (BCGD) 方法有效求解，",
      "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
      "title_zh": "透過懲罰函數與 Block-Coordinate Optimization 的多智能體時序邏輯規劃"
    },
    {
      "arxiv_id": "2602.17413",
      "authors": [
        "René Brinkhege",
        "Prahlad Menon"
      ],
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209089+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
          "url": "https://arxiv.org/abs/2602.17413"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
        "url": "https://arxiv.org/abs/2602.17413"
      },
      "published_at": "2026-02-19T14:43:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.7846362706810934,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.824636270681093
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17413",
      "summary": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over priv",
      "summary_zh": "在當前的跨組織數據空間中，使用策略主要在 asset level 執行：整個文件或 dataset 要麼被共享，要麼被保留。當文件只有部分內容敏感時，希望避免洩露受保護信息的提供者通常必須在共享之前手動編輯 (redact) 文件，這既昂貴又粗糙 (coarse-grained)，且難以隨著策略或合作夥伴的變化而維護。我們提出了 DAVE，這是一個強制執行使用策略的 LLM 發言人，它能回答關於私有",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
      "title_zh": "DAVE：用於安全多文件數據共享的策略強制性 LLM 發言人"
    },
    {
      "arxiv_id": "2602.17565",
      "authors": [
        "Hien Dang",
        "Pratik Patil",
        "Alessandro Rinaldo"
      ],
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.772918+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Optimal Unconstrained Self-Distillation in Ridge Regression: Strict Improvements, Precise Asymptotics, and One-Shot Tuning",
          "url": "https://arxiv.org/abs/2602.17565"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Optimal Unconstrained Self-Distillation in Ridge Regression: Strict Improvements, Precise Asymptotics, and One-Shot Tuning",
        "url": "https://arxiv.org/abs/2602.17565"
      },
      "published_at": "2026-02-19T17:21:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7932625796696604,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.49326257966966
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17565",
      "summary": "Self-distillation (SD) is the process of retraining a student on a mixture of ground-truth labels and the teacher's own predictions using the same architecture and training data. Although SD has been empirically shown to often improve generalization, its formal guarantees remain limited. We study SD for ridge regression in unconstrained setting in which the mixing weight $ξ$ may be outside the unit interval. Conditioned on the training data and without any distributional assumptions, we prove th",
      "summary_zh": "Self-distillation (SD) 是一種使用相同 architecture 和 training data，在 ground-truth labels 與 teacher 自身預測的混合物上重新訓練 student 的過程。儘管 SD 在實證上常被證明能提高 generalization，但其形式化的保證仍然有限。我們研究了在 unconstrained setting 中 ridge regression 的 SD，其中 mixing weight $ξ$ 可能位於 unit interval 之外。在訓練數據的條件下且沒有任何 distributional assumptions，我們證明了",
      "title": "Optimal Unconstrained Self-Distillation in Ridge Regression: Strict Improvements, Precise Asymptotics, and One-Shot Tuning",
      "title_zh": "Ridge Regression 中無約束 Self-Distillation 的最佳化：嚴格改進、精確漸近性與 One-Shot Tuning"
    },
    {
      "arxiv_id": "2602.17443",
      "authors": [
        "Adib Sakhawat",
        "Fardeen Sadab",
        "Rakin Shahriar"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.565878+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
          "url": "https://arxiv.org/abs/2602.17443"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
        "url": "https://arxiv.org/abs/2602.17443"
      },
      "published_at": "2026-02-19T15:09:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7860215032197522,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.486021503219753
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17443",
      "summary": "Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring c",
      "summary_zh": "評估 Large Language Models (LLMs) 的策略推理能力需要從靜態基準測試轉向動態的 multi-turn interactions。我們引入了 AIDG (Adversarial Information Deduction Game)，這是一個 game-theoretic 框架，旨在探究對話中 information extraction (主動推斷) 和 information containment (狀態維護) 之間的不對稱性。我們提出了兩項互補的任務：AIDG-I，用於衡量 social deduction 中的 pragmatic strategy；以及 AIDG-II，用於衡量 c",
      "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
      "title_zh": "AIDG：評估多輪對話中資訊提取與資訊抑制之間的不對稱性"
    },
    {
      "arxiv_id": "2602.17106",
      "authors": [
        "Xiaoran Cai",
        "Wang Yang",
        "Xiyu Ren",
        "Chekun Law",
        "Rohit Sharma",
        "Peng Qi"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.569481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction",
          "url": "https://arxiv.org/abs/2602.17106"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction",
        "url": "https://arxiv.org/abs/2602.17106"
      },
      "published_at": "2026-02-19T06:04:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7568427062176982,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.4568427062177
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17106",
      "summary": "Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluat",
      "summary_zh": "永續性或 ESG 評級機構利用公司披露資料和外部數據來產生評分或評級，以評估公司的環境、社會和治理績效。然而，單一公司在不同機構之間的永續性評級差異很大，這限制了它們的可比性、可信度和與決策的相關性。為了協調評級結果，我們建議採用一個通用的 human-AI collaboration framework 來生成可信賴的 benchmark datasets 以進行評估",
      "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction",
      "title_zh": "邁向永續性評級方法論的可信評估：一個用於基準數據集建構的人機協作框架"
    },
    {
      "arxiv_id": "2602.16974",
      "authors": [
        "Yongjie Zhou",
        "Shuai Wang",
        "Bevan Koopman",
        "Guido Zuccon"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.570546+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval",
          "url": "https://arxiv.org/abs/2602.16974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval",
        "url": "https://arxiv.org/abs/2602.16974"
      },
      "published_at": "2026-02-19T00:27:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.73932501057247,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.43932501057247
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16974",
      "summary": "Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal ove",
      "summary_zh": "Document chunking 是 dense retrieval systems 中一個關鍵的預處理步驟，然而，分塊策略的設計空間仍未被充分理解。最近的研究提出了幾種並行方法，包括 LLM-guided methods (例如：DenseX 和 LumberChunker) 和 contextualized strategies (例如：Late Chunking)，它們在 segmentation 之前生成 embeddings 以保留 contextual information。然而，這些方法是獨立出現的，並且在 minimal ove 的基準測試上進行了評估",
      "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval",
      "title_zh": "超越 Chunk-Then-Embed：資訊檢索中文檔分塊策略的綜合分類與評估"
    },
    {
      "arxiv_id": "2602.16938",
      "authors": [
        "Ofer Meshi",
        "Krisztian Balog",
        "Sally Goldman",
        "Avi Caciularu",
        "Guy Tennenholtz",
        "Jihwan Jeong",
        "Amir Globerson",
        "Craig Boutilier"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.570601+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
          "url": "https://arxiv.org/abs/2602.16938"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
        "url": "https://arxiv.org/abs/2602.16938"
      },
      "published_at": "2026-02-18T23:00:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7348768247839838,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.434876824783984
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16938",
      "summary": "The promise of LLM-based user simulators to improve conversational AI is hindered by a critical \"realism gap,\" leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both \"good\" and \"bad\" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, e",
      "summary_zh": "LLM-based user simulators 改善 conversational AI 的前景受阻於一個關鍵的「realism gap」，導致系統雖然為 simulated interactions 而優化，但在真實世界中可能表現不佳。我們引入了 ConvApparel，一個旨在解決此問題的新 human-AI conversations 數據集。其獨特的 dual-agent data collection protocol —— 同時使用「good」和「bad」recommenders —— 透過捕捉廣泛的使用者體驗來實現 counterfactual validation，e",
      "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
      "title_zh": "ConvApparel：用於對話式推薦系統中使用者模擬器的基準數據集與驗證框架"
    },
    {
      "arxiv_id": "2602.16449",
      "authors": [
        "Nicolas Salvy",
        "Hugues Talbot",
        "Bertrand Thirion"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571641+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
          "url": "https://arxiv.org/abs/2602.16449"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
        "url": "https://arxiv.org/abs/2602.16449"
      },
      "published_at": "2026-02-18T13:33:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7065303039614986,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.4065303039615
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16449",
      "summary": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a",
      "summary_zh": "Generative model evaluation 通常依賴於 high-dimensional embedding spaces 來計算樣本之間的距離。我們發現這些空間中的 dataset representations 會受到 hubness phenomenon 的影響，這會扭曲 nearest neighbor relationships 並偏差 distance-based metrics。基於經典的 Iterative Contextual Dissimilarity Measure (ICDM)，我們引入了 Generative ICDM (GICDM)，這是一種用於校正真實數據和生成數據的 neighborhood estimation 的方法。我們引入了一個",
      "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
      "title_zh": "GICDM：緩解中心性 (Hubness) 以實現可靠的基於距離的生成模型評估"
    },
    {
      "arxiv_id": "2602.17653",
      "authors": [
        "Iskar Deng",
        "Nathalia Xu",
        "Shane Steinert-Threlkeld"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.207520+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
          "url": "https://arxiv.org/abs/2602.17653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
        "url": "https://arxiv.org/abs/2602.17653"
      },
      "published_at": "2026-02-19T18:56:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7985307700219606,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.39853077002196
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17653",
      "summary": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing ",
      "summary_zh": "近期研究表明，在合成語料庫上訓練的語言模型 (LMs) 可以表現出與人類語言中的跨語言規律性相似的類型學偏好，特別是對於詞序等句法現象。在本文中，我們將此範式擴展到差異論元標記 (DAM)，這是一種語義許可系統，其中形態標記取決於語義顯著性。我們使用受控的合成學習方法，在 18 個實現了 GPT-2 模型的語料庫上進行訓練。",
      "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
      "title_zh": "語言模型處理差異論元標記時的類型學對齊差異"
    },
    {
      "arxiv_id": "2602.16320",
      "authors": [
        "Kavyansh Tyagi",
        "Vishwas Rathi",
        "Puneet Goyal"
      ],
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.775473+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
          "url": "https://arxiv.org/abs/2602.16320"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
        "url": "https://arxiv.org/abs/2602.16320"
      },
      "published_at": "2026-02-18T09:58:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6960638020651553,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.396063802065157
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16320",
      "summary": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The archi",
      "summary_zh": "準確且計算高效的 3D 醫學圖像分割在臨床工作流程中仍然是一個關鍵挑戰。基於 Transformer 的架構通常展現出優越的全局上下文建模能力，但代價是過多的參數數量和記憶體需求，這限制了它們的臨床部署。我們提出了 RefineFormer3D，這是一個輕量級的分層 Transformer 架構，可在體積醫學影像中平衡分割準確性和計算效率。該架構",
      "title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion",
      "title_zh": "RefineFormer3D：通過自適應多尺度 Transformer 與交叉注意力融合實現高效 3D 醫學圖像分割"
    },
    {
      "arxiv_id": "2602.17555",
      "authors": [
        "Zixu Cheng",
        "Da Li",
        "Jian Hu",
        "Ziquan Liu",
        "Wei Li",
        "Shaogang Gong"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535427+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
          "url": "https://arxiv.org/abs/2602.17555"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
        "url": "https://arxiv.org/abs/2602.17555"
      },
      "published_at": "2026-02-19T17:09:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7926155634492524,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.39261556344925
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17555",
      "summary": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video re",
      "summary_zh": "影片推理需要理解影片中事件之間的因果關係。然而，這些關係通常是隱含的，且手動註釋成本高昂。雖然現有的多模態大型語言模型 (MLLMs) 通常通過密集描述或影片摘要來推斷事件關係以進行影片推理，但這種建模仍然缺乏因果理解。如果沒有對影片事件內部和之間的明確因果結構建模，這些模型在影片推理過程中會出現幻覺問題。",
      "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
      "title_zh": "GraphThinker：以事件圖思維強化影片推理"
    },
    {
      "arxiv_id": "2602.17529",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        " Jianzhong",
        " Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208420+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
          "url": "https://arxiv.org/abs/2602.17529"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
        "url": "https://arxiv.org/abs/2602.17529"
      },
      "published_at": "2026-02-19T16:40:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7910090281733376,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.391009028173336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17529",
      "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowle",
      "summary_zh": "Large language models (LLMs) 已在各種任務中展現出強大潛力，但由於領域複雜性、不斷演進的標準和專業術語，它們在電信領域的應用仍然充滿挑戰。因此，通用領域的 LLMs 可能難以在此背景下提供準確可靠的輸出，導致幻覺增加並降低電信營運中的實用性。為了解決這些限制，本研究引入了 KG-RAG——一個整合了知識的新穎框架。",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "title_zh": "使用動態知識圖譜和可解釋的 Retrieval-Augmented Generation 增強電信領域的 Large Language Models (LLMs)"
    },
    {
      "arxiv_id": "2602.17465",
      "authors": [
        "Hongming Li",
        "Yang Liu",
        "Chao Huang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208793+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Entropy-Based Data Selection for Language Models",
          "url": "https://arxiv.org/abs/2602.17465"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Entropy-Based Data Selection for Language Models",
        "url": "https://arxiv.org/abs/2602.17465"
      },
      "published_at": "2026-02-19T15:29:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7871340007344338,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.387134000734434
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17465",
      "summary": "Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estim",
      "summary_zh": "現代語言模型 (LMs) 越來越需要兩種關鍵資源：計算資源和資料資源。資料選擇技術可以有效減少 fine-tuning LMs 所需的訓練資料量。然而，它們的有效性與計算資源密切相關，這總是需要高昂的計算預算。由於實際 fine-tuning 情境中的資源限制，我們系統地揭示了資料選擇與不確定性估計之間的關係。",
      "title": "Entropy-Based Data Selection for Language Models",
      "title_zh": "用於語言模型的基於熵的資料選擇"
    },
    {
      "arxiv_id": "2602.17419",
      "authors": [
        "Xiaomeng Peng",
        "Xilang Huang",
        "Seon Han Choi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208974+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17419"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17419"
      },
      "published_at": "2026-02-19T14:50:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7850268697183851,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.385026869718384
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17419",
      "summary": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial ",
      "summary_zh": "工業異常檢測對於智慧製造至關重要，但許多 deep learning 方法僅產生二元決策並提供有限的語義解釋。Multimodal large language models (MLLMs) 有潛力生成細粒度、基於語言的分析，然而現有方法通常需要耗費成本的 fine-tuning，並且與輕量級的 specialist detectors 相比，未能持續提高異常檢測的準確性。我們提出了一種用於工業的 expert-augmented attention guidance 方法。",
      "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
      "title_zh": "EAGLE：用於多模態大型語言模型中免調校工業異常檢測的專家增強注意力引導"
    },
    {
      "arxiv_id": "2602.17342",
      "authors": [
        "Luzhi Wang",
        "Xuanshuo Fu",
        "He Zhang",
        "Chuang Liu",
        "Xiaobao Wang",
        "Hongbo Liu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection",
          "url": "https://arxiv.org/abs/2602.17342"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection",
        "url": "https://arxiv.org/abs/2602.17342"
      },
      "published_at": "2026-02-19T13:19:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7800770638849458,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.380077063884944
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17342",
      "summary": "Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-p",
      "summary_zh": "Graph Out-of-Distribution (OOD) detection 旨在識別測試圖是否偏離訓練期間觀察到的圖的分佈，這對於確保 Graph Neural Networks (GNNs) 在開放世界場景中部署時的可靠性至關重要。圖 OOD detection 的最新進展側重於 test-time training 技術，這些技術無需訪問潛在的監督信息（例如 training data）即可促進 OOD detection。然而，這些方法大多採用 one-p",
      "title": "From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection",
      "title_zh": "從細微到顯著：Prompt 驅動的測試時圖 OOD 檢測中的自我改進優化"
    },
    {
      "arxiv_id": "2602.17271",
      "authors": [
        "Giuseppe Di Poce",
        "Mario Edoardo Pandolfo",
        "Emilio Calvanese Strinati",
        "Paolo Di Lorenzo"
      ],
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593509+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
          "url": "https://arxiv.org/abs/2602.17271"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
        "url": "https://arxiv.org/abs/2602.17271"
      },
      "published_at": "2026-02-19T11:18:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7735541885911482,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.373554188591147
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17271",
      "summary": "Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that sh",
      "summary_zh": "Semantic communication 旨在傳達意義以實現有效的任務執行，但 AI-native 設備中不同的 latent representations 可能導致語義不匹配，從而阻礙相互理解。本文提出了一種新穎的方法，用於緩解多代理 AI-native semantic communications 中的 latent space misalignment。在 downlink 情境中，我們考慮一個 access point (AP) 與多個用戶通訊以完成特定的 AI-driven 任務。我們的方法實施了一個協定，該協定 sh",
      "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
      "title_zh": "用於多用戶語義通訊的聯邦潛在空間對齊"
    },
    {
      "arxiv_id": "2602.17068",
      "authors": [
        "Xiaocai Zhang",
        "Neema Nassir",
        "Milad Haghani"
      ],
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.565580+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
          "url": "https://arxiv.org/abs/2602.17068"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
        "url": "https://arxiv.org/abs/2602.17068"
      },
      "published_at": "2026-02-19T04:18:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7513110882284962,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.351311088228496
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17068",
      "summary": "Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures sp",
      "summary_zh": "廊道網路中以人為本的 traffic signal control 必須越來越多地考慮多模態旅客，特別是高載客量的公共交通，而不是僅僅關注以車輛為中心的性能。本文提出了 STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning)，這是一個可擴展的 multi-agent deep reinforcement learning 框架，遵循 centralized training 和 decentralized execution 範式。所提出的方法捕捉了 sp",
      "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
      "title_zh": "用於以人為本多模態廊道交通號誌控制的時空雙階段超圖 MARL"
    },
    {
      "arxiv_id": "2602.17045",
      "authors": [
        "Jared Moore",
        "Rasmus Overmark",
        "Ned Cooper",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.209958+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large Language Models Persuade Without Planning Theory of Mind",
          "url": "https://arxiv.org/abs/2602.17045"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large Language Models Persuade Without Planning Theory of Mind",
        "url": "https://arxiv.org/abs/2602.17045"
      },
      "published_at": "2026-02-19T03:31:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7488464219441243,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.348846421944124
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17045",
      "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategicall",
      "summary_zh": "越來越多的研究試圖使用靜態、非互動式的問答基準來評估人類和 large language models (LLMs) 的 theory of mind (ToM) 能力。然而，該領域的理論研究表明，第一人稱互動是 ToM 的關鍵部分，並且此類預測性、旁觀性任務可能無法評估它。我們通過一個新穎的 ToM 任務來解決這一空白，該任務要求代理者通過策略性地說服目標選擇三項政策提案中的一項。",
      "title": "Large Language Models Persuade Without Planning Theory of Mind",
      "title_zh": "大型語言模型在不具備規劃心智理論的情況下進行說服"
    },
    {
      "arxiv_id": "2602.16936",
      "authors": [
        "Zikai Zhang",
        "Rui Hu",
        "Jiahao Xu"
      ],
      "categories": [
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.594011+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation",
          "url": "https://arxiv.org/abs/2602.16936"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation",
        "url": "https://arxiv.org/abs/2602.16936"
      },
      "published_at": "2026-02-18T22:57:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.734756056341478,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.334756056341476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16936",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and ",
      "summary_zh": "大型語言模型（LLMs）透過 fine-tuning 在適應下游任務方面展現出卓越的效能。Federated Learning (FL) 透過利用 Low-Rank Adaptation (LoRA) 實現分佈式客戶端之間的協同 fine-tuning，同時透過避免原始數據共享來保護數據隱私，從而擴展了此能力。然而，當客戶端擁有異質資源並因此採用不同的 LoRA ranks 時，實際部署會面臨挑戰，導致大量的初始化與",
      "title": "Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation",
      "title_zh": "異質聯邦式微調與並行單秩適應"
    },
    {
      "arxiv_id": "2602.16626",
      "authors": [
        "SungJun Cho",
        "Chetan Gohil",
        "Rukuang Huang",
        "Oiwi Parker Jones",
        "Mark W. Woolrich"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.212220+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
          "url": "https://arxiv.org/abs/2602.16626"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
        "url": "https://arxiv.org/abs/2602.16626"
      },
      "published_at": "2026-02-18T17:21:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7177628646588602,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.31776286465886
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16626",
      "summary": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNM",
      "summary_zh": "自然語言處理的近期成功激發了人們對神經影像數據大型 foundation models 的日益增長興趣。此類模型通常需要對連續神經時間序列數據進行離散化，這一過程稱為 'tokenization'。然而，對於神經數據的不同 tokenization 策略的影響目前尚不清楚。在這項工作中，我們提出對基於 transformer 的大型神經影像模型 (LNM) 的樣本級別 tokenization 策略進行系統性評估",
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "title_zh": "針對MEG Foundation Models的樣本級別Tokenization策略之系統性評估"
    },
    {
      "arxiv_id": "2602.17594",
      "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.565479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
          "url": "https://arxiv.org/abs/2602.17594"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
        "url": "https://arxiv.org/abs/2602.17594"
      },
      "published_at": "2026-02-19T18:17:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7963627130288894,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 24.29636271302889
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17594",
      "summary": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is th",
      "summary_zh": "在當今技術快速進步的時代，嚴格評估機器智慧與人類通用智慧的廣泛範圍變得日益重要和具有挑戰性。傳統的 AI benchmarks 通常僅評估人類活動有限範圍內的狹窄能力。大多數也都是靜態的，隨著開發人員明確或隱式地為其進行最佳化而迅速飽和。我們提出，評估 AI 系統中類人通用智慧的一個更有前景的方法是透過",
      "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
      "title_zh": "AI Gamestore：透過人類遊戲對機器通用智慧進行可擴展、開放式評估"
    },
    {
      "arxiv_id": "2602.16162",
      "authors": [
        "Peiqi Sui"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.214000+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers",
          "url": "https://arxiv.org/abs/2602.16162"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers",
        "url": "https://arxiv.org/abs/2602.16162"
      },
      "published_at": "2026-02-18T03:19:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6770049565877351,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.277004956587735
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16162",
      "summary": "We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the \"uncertainty gap\" between human-authored stories and model-generated continuation",
      "summary_zh": "我們認為不確定性是 LLMs 在創意寫作中一個關鍵且未被充分研究的限制，而這種寫作常被描述為陳腐和充滿 clichés。文學理論將不確定性視為創意表達的必要條件，而當前的 alignment strategies 則引導模型遠離不確定的輸出，以確保事實性並減少 hallucination。我們透過量化人類創作故事與模型生成續寫之間的「不確定性差距」（\"uncertainty gap\"）來形式化這種張力",
      "title": "LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers",
      "title_zh": "LLMs 在創意寫作中的不確定性顯著低於專業作家"
    },
    {
      "arxiv_id": "2602.16742",
      "authors": [
        "Haoxiang Sun",
        "Lizhen Xu",
        "Bing Zhao",
        "Wotao Yin",
        "Wei Wang",
        "Boyu Yang",
        "Rui Wang",
        "Hu Wei"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.537194+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
          "url": "https://arxiv.org/abs/2602.16742"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
        "url": "https://arxiv.org/abs/2602.16742"
      },
      "published_at": "2026-02-18T01:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6728873291745366,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.272887329174537
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16742",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \\textbf{DeepVision-103K}, a comprehensive dataset for RLVR traini",
      "summary_zh": "Reinforcement Learning with Verifiable Rewards (RLVR) 已被證明能有效增強大型多模態模型 (LMMs) 的視覺反思和推理能力。然而，現有 dataset 主要來源於小規模手動建構或先前資源的重新組合，這限制了數據的多樣性和覆蓋範圍，從而制約了模型性能的進一步提升。為此，我們介紹了 \textbf{DeepVision-103K}，一個用於 RLVR 訓練的綜合性 dataset",
      "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
      "title_zh": "DeepVision-103K：一個視覺多樣、覆蓋廣泛且可驗證的數學資料集，用於多模態推理"
    },
    {
      "arxiv_id": "2602.17215",
      "authors": [
        "Yi Shan",
        "Yixuan He",
        "Zekai Shao",
        "Kai Xu",
        "Siming Chen"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-22T00:41:05.772154+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom",
          "url": "https://arxiv.org/abs/2602.17215"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom",
        "url": "https://arxiv.org/abs/2602.17215"
      },
      "published_at": "2026-02-19T10:07:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7697076521422308,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.26970765214223
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17215",
      "summary": "High-quality exploratory data analysis (EDA) is essential in the data science pipeline, but remains highly dependent on analysts' expertise and effort. While recent LLM-based approaches partially reduce this burden, they struggle to generate effective analysis plans and appropriate insights and visualizations when user intent is abstract. Meanwhile, a vast collection of analysis notebooks produced across platforms and organizations contains rich analytical knowledge that can potentially guide au",
      "summary_zh": "高品質的 exploratory data analysis (EDA) 在 data science 流程中至關重要，但它仍然高度依賴分析師的專業知識和努力。儘管近期基於 LLM 的方法部分減輕了這一負擔，但當使用者意圖抽象時，它們在生成有效的分析計畫、適當的洞察和視覺化方面仍面臨困難。同時，跨平台和組織生成的大量分析 Notebook 包含豐富的分析知識，這些知識潛在地可以指導自動化生成。",
      "title": "NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom",
      "title_zh": "NotebookRAG：檢索多個 Notebook 以增強 EDA Notebooks 的生成，實現群體智慧"
    },
    {
      "arxiv_id": "2602.17206",
      "authors": [
        "Ron Shapira Weber",
        "Oren Freifeld"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "nvidia"
      ],
      "first_seen_at": "2026-02-22T00:41:04.773826+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
          "url": "https://arxiv.org/abs/2602.17206"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
        "url": "https://arxiv.org/abs/2602.17206"
      },
      "published_at": "2026-02-19T09:53:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7689525689462002,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.2689525689462
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17206",
      "summary": "We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length con",
      "summary_zh": "我們介紹了 softdtw-cuda-torch，這是一個用於在 GPU 上計算 Soft Dynamic Time Warping (SoftDTW) 的開源 PyTorch 函式庫。我們的實作解決了現有 SoftDTW GPU 實作的三個關鍵限制：1024 的硬性序列長度上限、在反向傳播 (backward pass) 中對小平滑參數的數值不穩定性，以及從實體化成對距離張量 (pairwise distance tensors) 產生過度的 GPU 記憶體消耗。我們引入了 (1) 瓦片式反對角核心執行 (tiled anti-diagonal kernel execution)，它消除了序列長度限制。",
      "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
      "title_zh": "SoftDTW-CUDA-Torch：適用於 PyTorch 的記憶體高效 GPU 加速 Soft Dynamic Time Warping"
    },
    {
      "arxiv_id": "2602.16851",
      "authors": [
        "Sujoy Mondal",
        "Taehyuk Park",
        "Sudipta Biswas",
        "Alan X. Wang",
        "Wenshan Cai"
      ],
      "categories": [
        "physics.optics"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566190+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "MxDiffusion: A Physics-Aware Maxwells Law-Guided Diffusion Model Strategy for Inverse Photonic Metasurface Design",
          "url": "https://arxiv.org/abs/2602.16851"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "MxDiffusion: A Physics-Aware Maxwells Law-Guided Diffusion Model Strategy for Inverse Photonic Metasurface Design",
        "url": "https://arxiv.org/abs/2602.16851"
      },
      "published_at": "2026-02-18T20:27:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7271235574788856,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.6000000000000005,
        "total_score": 23.927123557478886
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16851",
      "summary": "We introduce MxDiffusion, a hybrid physics- and data-driven diffusion-based framework that enables efficient and highly accurate generation of photonic structures from target optical properties. The improved accuracy is achieved through a two-stage generation strategy, in which the first diffusion model is explicitly trained with Maxwells equation-based loss to embed physical insight directly into the inverse design process, while the second model maps the physically consistent intermediate repr",
      "summary_zh": "我們介紹了 MxDiffusion，這是一個混合物理和資料驅動的基於 Diffusion 的框架，它能夠從目標光學特性高效且高度準確地生成光子結構。這種改進的準確性是透過兩階段生成策略實現的，其中第一個 diffusion model 透過基於 Maxwells equation 的 loss 進行明確訓練，以將物理洞察直接嵌入到 inverse design 過程中，而第二個模型則映射了物理上一致的中間表示。",
      "title": "MxDiffusion: A Physics-Aware Maxwells Law-Guided Diffusion Model Strategy for Inverse Photonic Metasurface Design",
      "title_zh": "MxDiffusion：一種物理感知並由 Maxwells Law 引導的 Diffusion Model 策略，用於逆向光子超表面設計"
    },
    {
      "arxiv_id": "2602.16793",
      "authors": [
        "Xingyu Dang",
        "Rohit Agarwal",
        "Rodrigo Porto",
        "Anirudh Goyal",
        "Liam H Fowl",
        "Sanjeev Arora"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.774838+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models",
          "url": "https://arxiv.org/abs/2602.16793"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models",
        "url": "https://arxiv.org/abs/2602.16793"
      },
      "published_at": "2026-02-18T19:00:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.7227429267614552,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.762742926761454
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16793",
      "summary": "In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only gene",
      "summary_zh": "在過去一年中，客製化和未發布的數學推理模型在 International Mathematical Olympiad (IMO) 中達到了金牌表現。隨後，使用公開可用模型進行大規模 inference 也報告了類似的性能，但成本過高（例如，每個問題 3000 美元）。在這項工作中，我們提出了一個 inference pipeline，它在 IMO 風格的數學問題上實現了一流的性能，其平均 inference 成本比競爭方法低幾個數量級，同時僅使用通用模型。",
      "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models",
      "title_zh": "逃離認知困境：使用現成模型進行高效的競技數學"
    },
    {
      "arxiv_id": "2602.17357",
      "authors": [
        "Pranav Aggarwal",
        "Ananya Basotia",
        "Debayan Gupta",
        "Rahul Kulkarni",
        "Shalini Kapoor",
        "Kashyap J.",
        "A. Mukundan",
        "Aishwarya Pokhriyal",
        "Anirban Sen",
        "Aryan Shah",
        "Aalok Thakkar"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.552675+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-safety",
          "tier": 1,
          "title": "Astra: AI Safety, Trust, & Risk Assessment",
          "url": "https://arxiv.org/abs/2602.17357"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-safety",
        "tier": 1,
        "title": "Astra: AI Safety, Trust, & Risk Assessment",
        "url": "https://arxiv.org/abs/2602.17357"
      },
      "published_at": "2026-02-19T13:37:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7810310742950731,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.0,
        "total_score": 23.481031074295075
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17357",
      "summary": "This paper argues that existing global AI safety frameworks exhibit contextual blindness towards India's unique socio-technical landscape. With a population of 1.5 billion and a massive informal economy, India's AI integration faces specific challenges such as caste-based discrimination, linguistic exclusion of vernacular speakers, and infrastructure failures in low-connectivity rural zones, that are frequently overlooked by Western, market-centric narratives.\n  We introduce ASTRA, an empiricall",
      "summary_zh": "本文認為，現有的全球 AI 安全框架對印度獨特的社會技術景觀表現出情境盲點。印度擁有 15 億人口和龐大的非正規經濟，其 AI 整合面臨著特定的挑戰，例如基於種姓的歧視、地方語言使用者的語言排斥，以及低連接性農村地區的基礎設施故障，這些問題常被西方以市場為中心的敘事所忽視。我們介紹了 ASTRA，這是一個基於經驗的框架。",
      "title": "Astra: AI Safety, Trust, & Risk Assessment",
      "title_zh": "Astra：AI 安全、信任與風險評估"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Yunfei Bai"
      ],
      "categories": [
        "Amazon Bedrock AgentCore",
        "Artificial Intelligence",
        "Best Practices",
        "Generative AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:54.153982+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
          "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
        "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
      },
      "published_at": "2026-02-18T11:21:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7000623027603309,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 26.80006230276033
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:328a53729df06635",
      "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon&nbsp;through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with&nbsp;Amazon use case-specific evaluation approaches and metrics.&nbsp;",
      "summary_zh": "在本文中，我們提出了一個針對 Amazon agentic AI 系統的綜合評估框架，該框架透過兩個核心組成部分解決了 Amazon agentic AI 應用程序的複雜性：一個通用的 evaluation workflow，用於標準化不同 agent 實作的評估程序；以及一個 agent evaluation library，它在 Amazon Bedrock AgentCore Evaluations 中提供系統性的測量和 metrics，並結合了 Amazon 特定 use case 的評估方法和 metrics。",
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
      "title_zh": "評估 AI agents：在 Amazon 構建 agentic 系統的實務經驗"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Eric Horvitz, Andrew Jenks, Jessica Young"
      ],
      "categories": [
        "Research Blog"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:53.490211+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "microsoft-research-blog",
          "tier": 0,
          "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
          "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "microsoft-research-blog",
        "tier": 0,
        "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
        "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions"
      },
      "published_at": "2026-02-19T08:00:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7629844279230545,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 1.9500000000000002,
        "total_score": 23.712984427923054
      },
      "section": null,
      "source_name": "Microsoft Research Blog",
      "story_id": "fallback:70b04b512e5b8d61",
      "summary": "<p>As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/\">Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
      "summary_zh": "隨著 synthetic media 的增長，驗證內容的真實性和來源變得比以往任何時候都更加重要。我們最新的報告探討了 media integrity 和 authentication methods、它們的局限性，以及在圖像、音訊和視訊中實現可信 provenance 的實用途徑。",
      "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
      "title_zh": "媒體真實性方法實踐：能力、限制與方向"
    },
    {
      "arxiv_id": "2602.17442",
      "authors": [
        "Marco Avolio",
        "Potito Aghilar",
        "Sabino Roccotelli",
        "Vito Walter Anelli",
        "Chiara Mallamaci",
        "Vincenzo Paparella",
        "Marco Valentini",
        "Alejandro Bellogín",
        "Michelantonio Trizio",
        "Joseph Trotta",
        "Antonio Ferrara",
        "Tommaso Di Noia"
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773365+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
          "url": "https://arxiv.org/abs/2602.17442"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
        "url": "https://arxiv.org/abs/2602.17442"
      },
      "published_at": "2026-02-19T15:09:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7860142252765647,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.38601422527656
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17442",
      "summary": "Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly t",
      "summary_zh": "Recommender Systems 的創新目前受到支離破碎的生態系統的阻礙，研究人員必須在便捷的 in-memory 實驗與分散式工業引擎所需的昂貴且複雜的重寫之間做出選擇。為彌合這一差距，我們提出了 WarpRec，這是一個高性能框架，它通過一種新穎的 backend-agnostic 架構消除了這種權衡。它包含了 50 多種 state-of-the-art algorithms、40 種 metrics 以及 19 種 filtering 和 splitting strategies，這些策略無縫地...",
      "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
      "title_zh": "WarpRec：整合學術嚴謹性與工業規模，實現負責任、可重現且高效的推薦"
    },
    {
      "arxiv_id": "2602.17414",
      "authors": [
        "David Yallup"
      ],
      "categories": [
        "stat.CO",
        "astro-ph.IM",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773423+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
          "url": "https://arxiv.org/abs/2602.17414"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
        "url": "https://arxiv.org/abs/2602.17414"
      },
      "published_at": "2026-02-19T14:43:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7846462603268346,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.384646260326832
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17414",
      "summary": "We present Nested Sampling with Slice-within-Gibbs (NS-SwiG), an algorithm for Bayesian inference and evidence estimation in high-dimensional models whose likelihood admits a factorization, such as hierarchical Bayesian models. We construct a procedure to sample from the likelihood-constrained prior using a Slice-within-Gibbs kernel: an outer update of hyperparameters followed by inner block updates over local parameters. A likelihood-budget decomposition caches per-block contributions so that e",
      "summary_zh": "我們提出了帶有 Slice-within-Gibbs (NS-SwiG) 的 Nested Sampling，這是一種用於高維模型中貝葉斯推斷和證據估計的演算法，其 likelihood 允許因式分解，例如 hierarchical Bayesian models。我們構建了一個程序，使用 Slice-within-Gibbs kernel 從 likelihood-constrained prior 中採樣：一個 hyperparameters 的外部更新，接著是 local parameters 的內部 block updates。一個 likelihood-budget decomposition 緩存了每個 block 的貢獻，以便...",
      "title": "Nested Sampling with Slice-within-Gibbs: Efficient Evidence Calculation for Hierarchical Bayesian Models",
      "title_zh": "使用 Slice-within-Gibbs 的 Nested Sampling：分層貝葉斯模型的有效證據計算"
    },
    {
      "arxiv_id": "2602.17284",
      "authors": [
        "Vitaly Feldman",
        "Moshe Shenfeld"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773656+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Efficient privacy loss accounting for subsampling and random allocation",
          "url": "https://arxiv.org/abs/2602.17284"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Efficient privacy loss accounting for subsampling and random allocation",
        "url": "https://arxiv.org/abs/2602.17284"
      },
      "published_at": "2026-02-19T11:44:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7749225470169555,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.374922547016954
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17284",
      "summary": "We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling.",
      "summary_zh": "我們考慮了一種採樣方案的 privacy amplification 特性，其中用戶的數據在從 $t$ 個步驟序列（或集合）中隨機且均勻選擇的 $k$ 個步驟中使用。這種採樣方案最近已被應用於 differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) 和 communication-efficient high-dimensional private aggregation (Asi et al., 2025) 的背景下，並顯示出其優於標準 Poisson sampling 的 utility 優勢。",
      "title": "Efficient privacy loss accounting for subsampling and random allocation",
      "title_zh": "子採樣和隨機分配的有效隱私損失核算"
    },
    {
      "arxiv_id": "2602.17264",
      "authors": [
        "Michael Müller",
        "Amir Reza Mohammadi",
        "Andreas Peintner",
        "Beatriz Barroso Gstrein",
        "Günther Specht",
        "Eva Zangerle"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.566236+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems",
          "url": "https://arxiv.org/abs/2602.17264"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems",
        "url": "https://arxiv.org/abs/2602.17264"
      },
      "published_at": "2026-02-19T11:10:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7730825002174608,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.37308250021746
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17264",
      "summary": "User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability ",
      "summary_zh": "「使用者中心評估」(User-centric evaluation) 已成為評估「對話式推薦系統」(Conversational Recommender Systems, CRS) 的關鍵範式，旨在捕捉滿意度、信任和融洽關係等主觀品質。為了實現可擴展的評估，近期研究越來越依賴於由 crowd workers 或 large language models 對靜態對話日誌進行第三方註釋。然而，這種做法的可靠性在很大程度上仍未被檢視。在本文中，我們提出了一項大規模實證研究，旨在調查其可靠性。",
      "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems",
      "title_zh": "關於對話式推薦系統之使用者中心評估的可靠性"
    },
    {
      "arxiv_id": "2602.17599",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Levé",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593134+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
          "url": "https://arxiv.org/abs/2602.17599"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
        "url": "https://arxiv.org/abs/2602.17599"
      },
      "published_at": "2026-02-19T18:23:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7967250298530548,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.296725029853057
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17599",
      "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifi",
      "summary_zh": "音樂生成透過 multimodal deep learning 已取得顯著進展，使模型能夠從文字，以及最近從圖像合成音訊。然而，現有的 image-conditioned 系統存在兩個根本性限制：(i) 它們通常在 natural photographs 上進行訓練，限制了其捕捉藝術作品中更豐富的語義、風格和文化內容的能力；(ii) 大多數依賴於 image-to-text 轉換階段，將 language 作為 semantic shortcut，簡化了。",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
      "title_zh": "Art2Mus：透過視覺條件化和大規模跨模態對齊的藝術作品到音樂生成"
    },
    {
      "arxiv_id": "2602.17481",
      "authors": [
        "Yanni Mei",
        "Samuel Wendt",
        "Florian Mueller",
        "Jan Gugenheimer"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.208679+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
          "url": "https://arxiv.org/abs/2602.17481"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
        "url": "https://arxiv.org/abs/2602.17481"
      },
      "published_at": "2026-02-19T15:50:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7882809171792292,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.28828091717923
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17481",
      "summary": "Augmented Reality (AR) can simulate various visual perceptions, such as how individuals with colorblindness see the world. However, these simulations require developers to predefine each visual effect, limiting flexibility. We present ShadAR, an AR application enabling real-time transformation of visual perception through shader generation using large language models (LLMs). ShadAR allows users to express their visual intent via natural language, which is interpreted by an LLM to generate corres",
      "summary_zh": "Augmented Reality (AR) 可以模擬各種視覺感知，例如色盲人士如何看待世界。然而，這些模擬要求開發者預先定義每個視覺效果，這限制了靈活性。我們提出了 ShadAR，一個 AR 應用程式，它透過使用 large language models (LLMs) 進行 shader 生成，實現視覺感知的即時轉換。ShadAR 允許使用者透過 natural language 表達他們的視覺意圖，LLM 會解釋這些意圖以生成對應的。",
      "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
      "title_zh": "ShadAR：LLM 驅動的著色器生成，以轉變擴增實境中的視覺感知"
    },
    {
      "arxiv_id": "2602.16231",
      "authors": [
        "Yiming Ju",
        "Hanyu Zhao",
        "Quanyue Ma",
        "Donglin Hao",
        "Chengwei Wu",
        "Ming Li",
        "Songjing Wang",
        "Tengfei Pan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.773005+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling",
          "url": "https://arxiv.org/abs/2602.16231"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling",
        "url": "https://arxiv.org/abs/2602.16231"
      },
      "published_at": "2026-02-18T07:12:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6880396164737183,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.288039616473718
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16231",
      "summary": "Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matchi",
      "summary_zh": "大規模影片儲存庫越來越多地用於現代 video understanding 和 generation 任務。然而，將原始影片轉換為高品質、任務特定的資料集仍然成本高昂且效率低下。我們提出了 DataCube，一個用於自動影片處理、多維度分析和查詢驅動檢索的智慧平台。DataCube 構建影片片段的結構化 semantic representations，並支援使用 neural re-ranking 和 deep semantic matching 的混合檢索。",
      "title": "DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling",
      "title_zh": "DataCube：一種透過自然語言語義分析實現的影片檢索平台"
    },
    {
      "arxiv_id": "2602.17354",
      "authors": [
        "Daniele Malitesta",
        "Emanuele Rossi",
        "Claudio Pomo",
        "Tommaso Di Noia",
        "Fragkiskos D. Malliaros"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.564949+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
          "url": "https://arxiv.org/abs/2602.17354"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
        "url": "https://arxiv.org/abs/2602.17354"
      },
      "published_at": "2026-02-19T13:37:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7810075713987855,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.28100757139879
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17354",
      "summary": "Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisati",
      "summary_zh": "Multimodal recommender systems (RSs) 透過多模態資料（例如，產品圖片和描述）來表示目錄中的項目，這些資料在某些情況下可能帶有雜訊，甚至更糟的是遺失。在這些情境下，常見的做法是丟棄具有遺失模態的項目，並在原始資料集的 subsample 上訓練 multimodal RSs。迄今為止，multimodal recommendation 中遺失模態的問題在文獻中仍受到有限的關注，缺乏精確的形式化。",
      "title": "Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation",
      "title_zh": "Multimodal 推薦中遺失模態的免訓練基於圖形歸因法"
    },
    {
      "arxiv_id": "2602.16124",
      "authors": [
        "Jiang Zhang",
        "Yubo Wang",
        "Wei Chang",
        "Lu Han",
        "Xingying Cheng",
        "Feng Zhang",
        "Min Li",
        "Songhao Jiang",
        "Wei Zheng",
        "Harry Tran",
        "Zhen Wang",
        "Lei Chen",
        "Yueming Wang",
        "Benyu Zhang",
        "Xiangjun Fan",
        "Bi Xue",
        "Qifan Wang"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.773127+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
          "url": "https://arxiv.org/abs/2602.16124"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
        "url": "https://arxiv.org/abs/2602.16124"
      },
      "published_at": "2026-02-18T01:31:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.671959633962852,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.27195963396285
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16124",
      "summary": "Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, wh",
      "summary_zh": "近似最近鄰 (ANN) 搜尋廣泛應用於大規模推薦系統的檢索階段。在此階段，候選項目使用其學習到的 embedding 向量建立索引，並針對每個使用者（或項目）查詢執行 ANN 搜尋，以檢索一組相關項目。然而，基於 ANN 的檢索存在兩個主要限制。首先，項目 embedding 及其索引通常在不同階段學習：索引通常在 embedding 訓練之後離線執行，而",
      "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
      "title_zh": "重新思考基於 ANN 的檢索：用於大規模推薦系統的多面向可學習索引"
    },
    {
      "arxiv_id": "2602.17072",
      "authors": [
        "Yunseung Lee",
        "Subin Kim",
        "Youngjun Kwak",
        "Jaegul Choo"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.535852+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
          "url": "https://arxiv.org/abs/2602.17072"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
        "url": "https://arxiv.org/abs/2602.17072"
      },
      "published_at": "2026-02-19T04:27:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7517781940841782,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.251778194084178
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17072",
      "summary": "Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and context",
      "summary_zh": "基於大型語言模型 (LLMs) 的聊天機器人正日益被金融領域採用，特別是在數位銀行中，以處理客戶關於存款、儲蓄和貸款等產品的查詢。然而，這些模型在核心銀行計算方面仍表現出低準確性，包括總支付估計、比較具有不同利率的產品以及在提前還款條件下的利息計算。此類任務需要多步驟的數值推理和 context",
      "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
      "title_zh": "BankMathBench：銀行情境中數值推理的基準"
    },
    {
      "arxiv_id": "2602.16811",
      "authors": [
        "Charalampos Mastrokostas",
        "Nikolaos Giarelis",
        "Nikos Karacapilidis"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211214+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
          "url": "https://arxiv.org/abs/2602.16811"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
        "url": "https://arxiv.org/abs/2602.16811"
      },
      "published_at": "2026-02-18T19:15:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7234911519645943,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.223491151964595
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16811",
      "summary": "Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small ",
      "summary_zh": "自然語言處理和深度學習的最新進展促成了大型語言模型 (LLMs) 的發展，這些模型在包括 Question Answering (QA) 在內的廣泛任務中顯著推進了最先進水平。儘管有這些進展，關於 LLMs 的研究主要針對高資源語言（例如，English），直到最近才將注意力轉向多語模型。然而，這些模型表現出對一小部分訓練資料的偏差",
      "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
      "title_zh": "評估用於希臘語問答的單語和多語大型語言模型：DemosQA 基準"
    },
    {
      "arxiv_id": "2602.16669",
      "authors": [
        "Bo Lang",
        "Nirav Savaliya",
        "Zhihao Zheng",
        "Jinglun Feng",
        "Zheng-Hang Yeh",
        "Mooi Choo Chuah"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:59.536702+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction",
          "url": "https://arxiv.org/abs/2602.16669"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction",
        "url": "https://arxiv.org/abs/2602.16669"
      },
      "published_at": "2026-02-18T18:08:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7201293935303382,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.22012939353034
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16669",
      "summary": "High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, whic",
      "summary_zh": "高精地圖 (HD) 對於自動駕駛至關重要，它們提供道路元素的結構化表示，以支持導航和規劃。然而，現有的基於查詢的方法通常採用隨機查詢初始化，並依賴隱式時間建模，這導致在建構全局地圖期間的時間不一致性和不穩定性。為了克服這些挑戰，我們引入了一種新穎的 end-to-end 框架，用於一致的線上 HD 向量化地圖建構，該框架",
      "title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction",
      "title_zh": "PredMapNet：用於一致線上 HD 向量化地圖建構的未來與歷史推理"
    },
    {
      "arxiv_id": "2602.16653",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211864+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
          "url": "https://arxiv.org/abs/2602.16653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
        "url": "https://arxiv.org/abs/2602.16653"
      },
      "published_at": "2026-02-18T17:52:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.719322201140852,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.219322201140855
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16653",
      "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on pu",
      "summary_zh": "Agent Skill 框架現在得到主要參與者（如 GitHub Copilot, LangChain 和 OpenAI）的廣泛和官方支持，它透過改進 context engineering、減少 hallucinations 和提升任務準確性，在與專有模型搭配時表現出色。基於這些觀察，我們進行了一項調查，以確定 Agent Skill 範式是否為小型語言模型 (SLMs) 帶來類似的好處。這個問題在工業情境中很重要，在這些情境中，持續依賴公共",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "title_zh": "Agent Skill 框架：小型語言模型在工業環境中的潛力展望"
    },
    {
      "arxiv_id": "2602.16640",
      "authors": [
        "Subrit Dikshit"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.211927+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
          "url": "https://arxiv.org/abs/2602.16640"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
        "url": "https://arxiv.org/abs/2602.16640"
      },
      "published_at": "2026-02-18T17:29:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7181958128349623,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.218195812834963
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16640",
      "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered t",
      "summary_zh": "大型語言模型 (LLMs) 的迅速普及徹底改變了 Natural Language Processing (NLP)，但也同時造成了「資源鴻溝」。最先進的法律智慧系統通常依賴於大量的參數（7B+）和基於雲端的推論，這使得資源受限環境中的從業者難以使用它們，並帶來重大的資料主權風險。本文介紹了 Quecto-V1，這是一種為領域特定 Small Language Model (SLM) 而設計的。",
      "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
      "title_zh": "Quecto-V1: 8位元量化小型語言模型用於裝置端法律檢索的實證分析"
    },
    {
      "arxiv_id": "2602.16551",
      "authors": [
        "Rui Hu",
        "Yue Wu",
        "Tianhao Su",
        "Yin Wang",
        "Shunbo Hu",
        "Jizhong Huang"
      ],
      "categories": [
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.212793+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
          "url": "https://arxiv.org/abs/2602.16551"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
        "url": "https://arxiv.org/abs/2602.16551"
      },
      "published_at": "2026-02-18T15:53:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7134006454089344,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.213400645408935
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16551",
      "summary": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constit",
      "summary_zh": "文化遺產的保護正日益轉向資料驅動的預測性維護和「Digital Twin」建構。然而，高擬真模擬所需的機械本構模型仍分散在數十年非結構化的科學文獻中，形成了阻礙保護工程的「Data Silo」。為了解決這個問題，我們提出了一個自動化、兩階段的 agentic 框架，利用 Large Language Models (LLMs) 來擷取機械本構模型。",
      "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
      "title_zh": "使用大型語言模型從科學文獻中自動擷取機械本構模型：在文化遺產保護中的應用"
    },
    {
      "arxiv_id": "2602.16379",
      "authors": [
        "Mohammad H. A. Monfared",
        "Lucie Flek",
        "Akbar Karimi"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213459+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
          "url": "https://arxiv.org/abs/2602.16379"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
        "url": "https://arxiv.org/abs/2602.16379"
      },
      "published_at": "2026-02-18T11:38:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7008754627261763,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.200875462726177
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16379",
      "summary": "We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE",
      "summary_zh": "我們提出了一種用於 Aspect-Based Sentiment Analysis (ABSA) 的 agentic 資料增強方法，該方法使用迭代生成和驗證來產生高品質的合成訓練範例。為了隔離 agentic 結構的影響，我們還使用相同的模型和指令開發了一個密切匹配的 prompting-based 基準線。這兩種方法都在三個 ABSA 子任務（Aspect Term Extraction (ATE)、Aspect Sentiment Classification (ATSC) 和 Aspect Sentiment Pair Extraction (ASPE)）上進行了評估。",
      "title": "Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents",
      "title_zh": "使用 LLM Agents 進行基於方面的情緒分析的標籤一致資料生成"
    },
    {
      "arxiv_id": "2602.16304",
      "authors": [
        "Ahmed Ryan",
        "Ibrahim Khalil",
        "Abdullah Al Jahid",
        "Md Erfan",
        "Akond Ashfaque Ur Rahman",
        "Md Rayhanur Rahman"
      ],
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213578+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification",
          "url": "https://arxiv.org/abs/2602.16304"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification",
        "url": "https://arxiv.org/abs/2602.16304"
      },
      "published_at": "2026-02-18T09:36:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.6949907260708303,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.19499072607083
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16304",
      "summary": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we e",
      "summary_zh": "在 PyPI 等開源儲存庫中惡意套件的普遍存在，對軟體供應鏈構成了關鍵威脅。儘管 Large Language Models (LLMs) 已成為自動化安全任務的有前景工具，但它們在偵測惡意套件和指標方面的有效性仍未充分探索。本文對 13 個 LLMs 在偵測惡意軟體套件方面的表現進行了系統性評估。我們使用了一個包含 4,070 個套件（3,700 個良性套件和 370 個惡意套件）的精心策劃資料集進行評估。",
      "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification",
      "title_zh": "關注差距：評估 LLMs 在高層次惡意套件偵測與細粒度指標識別上的表現"
    },
    {
      "arxiv_id": "2602.16238",
      "authors": [
        "Hiroki Nakamura",
        "Hiroto Iino",
        "Masashi Okada",
        "Tadahiro Taniguchi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:57.213751+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection",
          "url": "https://arxiv.org/abs/2602.16238"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection",
        "url": "https://arxiv.org/abs/2602.16238"
      },
      "published_at": "2026-02-18T07:28:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.6888109076588728,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.188810907658873
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16238",
      "summary": "We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for hig",
      "summary_zh": "我們提出了 EasyControlEdge，它將圖像生成基礎模型應用於邊緣偵測。在真實世界的邊緣偵測中（例如，平面圖牆壁、衛星道路/建築物和醫療器官邊界），清晰度和資料效率至關重要，然而在有限訓練樣本下產生清晰原始邊緣圖仍然具有挑戰性。儘管圖像生成基礎模型在許多下游任務上表現良好，但它們用於資料高效傳輸和高品質迭代細化的預訓練先驗知識。",
      "title": "EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection",
      "title_zh": "EasyControlEdge: 針對邊緣偵測的基礎模型微調"
    },
    {
      "arxiv_id": "2602.16505",
      "authors": [
        "Sophie Hanna Langbein",
        "Hubert Baniecki",
        "Fabian Fumagalli",
        "Niklas Koenen",
        "Marvin N. Wright",
        "Julia Herbinger"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.806534+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
          "url": "https://arxiv.org/abs/2602.16505"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
        "url": "https://arxiv.org/abs/2602.16505"
      },
      "published_at": "2026-02-18T14:47:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7101424838088223,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 23.06014248380882
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16505",
      "summary": "Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanat",
      "summary_zh": "在事件時間預測中，危害函數 (hazard function) 和生存函數 (survival function) 是自然且可解釋的目標，但其固有的非加性 (non-additivity) 從根本上限制了標準的加性解釋方法 (additive explanation methods)。我們引入了 Survival Functional Decomposition (SurvFD)，這是一種用於分析機器學習生存模型中特徵交互作用 (feature interactions) 的原則性方法。透過將高階效應分解為時間相關 (time-dependent) 和時間獨立 (time-independent) 的組件，SurvFD 提供了一個以前未被認可的生存解釋視角。",
      "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
      "title_zh": "用於解釋生存模型的函數分解與 Shapley 交互作用"
    },
    {
      "arxiv_id": "2602.16133",
      "authors": [
        "Ren Okubo",
        "Yu Fujikata",
        "Izumi Takahara",
        "Teruyasu Mizoguchi"
      ],
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.567102+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Generative Inverse Estimation of 3D Atomic Coordination from Near-Edge Spectra via Equivariant Diffusion Models",
          "url": "https://arxiv.org/abs/2602.16133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Generative Inverse Estimation of 3D Atomic Coordination from Near-Edge Spectra via Equivariant Diffusion Models",
        "url": "https://arxiv.org/abs/2602.16133"
      },
      "published_at": "2026-02-18T01:54:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6730571300371243,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 1.2000000000000002,
        "total_score": 22.573057130037125
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16133",
      "summary": "Extracting 3D atomic coordinates from spectroscopic data is a longstanding inverse problem. We present an equivariant diffusion model that generates site-specific 3D structures directly from near-edge spectra (ELNES/XANES). Trained on Si-O crystals, the model achieves radial accuracy comparable to Extended X-ray Absorption Fine Structure (EXAFS) (RMSD ~0.06 Å) but with superior coordination number precision (errors < 4.3% vs. EXAFS ~20%). Crucially, it reconstructs full 3D geometries including b",
      "summary_zh": "從光譜數據中提取 3D atomic coordinates 是一個長期存在的逆問題 (inverse problem)。我們提出了一個 equivariant diffusion model，可以直接從 near-edge spectra (ELNES/XANES) 生成位點特異性 (site-specific) 的 3D 結構。該模型在 Si-O 晶體上訓練，其徑向準確度 (radial accuracy) 可與 Extended X-ray Absorption Fine Structure (EXAFS) 相媲美 (RMSD 約 0.06 Å)，但在 coordination number precision 方面表現更優 (誤差小於 4.3% 對比 EXAFS 的約 20%)。關鍵的是，它能重建完整的 3D 幾何結構，包括 b",
      "title": "Generative Inverse Estimation of 3D Atomic Coordination from Near-Edge Spectra via Equivariant Diffusion Models",
      "title_zh": "透過 Equivariant Diffusion Models 從近邊緣光譜生成式逆向估計三維原子配位"
    },
    {
      "arxiv_id": "2602.17654",
      "authors": [
        "Jiaqi Xi",
        "Raghav Saboo",
        "Luming Chen",
        "Martin Wang",
        "Sudeep Das"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.771620+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval",
          "url": "https://arxiv.org/abs/2602.17654"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval",
        "url": "https://arxiv.org/abs/2602.17654"
      },
      "published_at": "2026-02-19T18:56:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7985326184749566,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.298532618474958
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17654",
      "summary": "We propose a two-stage \"Mine and Refine\" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear sep",
      "summary_zh": "我們提出了一個兩階段的「Mine and Refine」對比訓練框架 (contrastive training framework)，用於 semantic text embeddings，以增強多類別電子商務搜尋檢索 (multi-category e-commerce search retrieval)。大規模電子商務搜尋需要能夠泛化到長尾 (long tail)、嘈雜查詢 (noisy queries) 的 embeddings，同時要符合可擴展監督 (scalable supervision) 以及產品和政策限制。一個實際的挑戰是相關性 (relevance) 通常是分級的 (graded)：用戶接受的替代品 (substitutes) 或補充品 (complements) 超出了精確匹配的範圍，而生產系統則受益於清晰的分隔。",
      "title": "Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval",
      "title_zh": "挖掘與精煉：優化電子商務搜尋檢索中的分級相關性"
    },
    {
      "arxiv_id": "2602.17528",
      "authors": [
        "Felipe Hawthorne",
        "Leandro Seixas",
        "James M. Almeida",
        "Cristiano F. Woellner",
        "Raphael M. Tromer"
      ],
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.dis-nn"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:06.805581+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-interpretability",
          "tier": 1,
          "title": "Interpretable Machine Learning of Nanoparticle Stability through Topological Layer Embeddings",
          "url": "https://arxiv.org/abs/2602.17528"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-interpretability",
        "tier": 1,
        "title": "Interpretable Machine Learning of Nanoparticle Stability through Topological Layer Embeddings",
        "url": "https://arxiv.org/abs/2602.17528"
      },
      "published_at": "2026-02-19T16:37:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7908799504271063,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.290879950427108
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17528",
      "summary": "The stability of chemically complex nanoparticles is governed by an immense configurational space arising from heterogeneous local atomic environments across surface and interior regions. Efficiently identifying low-energy configurations within this space remains a central challenge for first-principles-based materials discovery, particularly when the available reference data are limited. Here, we introduce a data-efficient and physically interpretable machine-learning framework based on a fragm",
      "summary_zh": "化學複雜奈米粒子 (chemically complex nanoparticles) 的穩定性受到巨大的組態空間 (configurational space) 支配，該空間源於表面和內部區域異質的局部原子環境 (heterogeneous local atomic environments)。在這個空間中有效地識別低能量組態 (low-energy configurations) 仍然是基於第一性原理的材料發現 (first-principles-based materials discovery) 的核心挑戰，特別是在可用參考數據有限的情況下。在此，我們引入了一個基於片段的數據高效且物理可解釋的機器學習框架。",
      "title": "Interpretable Machine Learning of Nanoparticle Stability through Topological Layer Embeddings",
      "title_zh": "透過拓撲層嵌入實現奈米粒子穩定性的可解釋機器學習"
    },
    {
      "arxiv_id": "2602.17508",
      "authors": [
        "Pranay Jain",
        "Maximilian Kasper",
        "Göran Köber",
        "Axel Plinge",
        "Dominik Seuß"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.565766+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems",
          "url": "https://arxiv.org/abs/2602.17508"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems",
        "url": "https://arxiv.org/abs/2602.17508"
      },
      "published_at": "2026-02-19T16:21:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7899934538020312,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.289993453802033
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17508",
      "summary": "This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point op",
      "summary_zh": "這項工作提出了一個實用的基準測試框架 (benchmarking framework)，用於在 ARM Cortex 處理器 (M0+, M4, M7) 上優化 artificial intelligence (AI) 模型，重點關注嵌入式系統 (embedded systems) 中的能源效率 (energy efficiency)、準確度 (accuracy) 和資源利用率 (resource utilization)。透過自動化測試平台 (automated test bench) 的設計，我們提供了一種系統性方法，用於評估關鍵績效指標 (KPIs) 並識別處理器和 AI 模型的最佳組合。該研究強調了浮點運算 (floating-point op) 之間幾乎線性的相關性。",
      "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems",
      "title_zh": "Pareto 最優基準測試 AI 模型在 ARM Cortex 處理器上以實現永續嵌入式系統"
    },
    {
      "arxiv_id": "2602.17425",
      "authors": [
        "Sanjeev Kumar",
        "Preethi Jyothi",
        "Pushpak Bhattacharyya"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.565933+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics",
          "url": "https://arxiv.org/abs/2602.17425"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics",
        "url": "https://arxiv.org/abs/2602.17425"
      },
      "published_at": "2026-02-19T14:56:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7853394889430629,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.285339488943066
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17425",
      "summary": "Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, sour",
      "summary_zh": "在極低資源語言 (ELRL) 情境中評估機器翻譯 (MT) 品質帶來了獨特的挑戰，因為廣泛使用的指標，例如在高資源設定中有效的 BLEU，在數據稀缺的情境中常常錯誤地表示品質。本研究對 BLEU（一種基於 n-gram 的指標）和 ChrF++（一種基於字元的指標）在 ELRL 設定中的 MT 評估進行了比較分析。我們研究了每個指標如何響應翻譯中的人工痕跡，包括幻覺、重複、源...",
      "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics",
      "title_zh": "評估極低資源機器翻譯：ChrF++ 和 BLEU 指標的比較研究"
    },
    {
      "arxiv_id": "2602.17327",
      "authors": [
        "Michael Dinzinger",
        "Laura Caspari",
        "Ali Salman",
        "Irvin Topi",
        "Jelena Mitrović",
        "Michael Granitzer"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.772026+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
          "url": "https://arxiv.org/abs/2602.17327"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
        "url": "https://arxiv.org/abs/2602.17327"
      },
      "published_at": "2026-02-19T12:45:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7782418916578006,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.2782418916578
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17327",
      "summary": "We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more di",
      "summary_zh": "我們介紹了 WebFAQ 2.0，WebFAQ 資料集的一個新版本，包含橫跨 108 種語言的 1.98 億個基於 FAQ 的自然問答對。與之前版本相比，它顯著擴展了多語言覆蓋範圍和雙語對齊 QA 對的數量，達到超過 1430 萬個，使其成為最大的基於 FAQ 的資源。與原始版本不同，WebFAQ 2.0 採用了一種新穎的數據收集策略，直接爬取並提取相關的網路內容，從而產生了實質上更多樣化...",
      "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
      "title_zh": "WebFAQ 2.0：一個用於 Dense Retrieval 的多語言 QA 資料集，帶有挖掘的困難負樣本"
    },
    {
      "arxiv_id": "2602.17145",
      "authors": [
        "Joseph Bingham",
        "Sam Helmich"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.773993+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning",
          "url": "https://arxiv.org/abs/2602.17145"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning",
        "url": "https://arxiv.org/abs/2602.17145"
      },
      "published_at": "2026-02-19T07:46:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7622050632219768,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.262205063221977
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17145",
      "summary": "As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demo",
      "summary_zh": "隨著對更準確、更強大的卷積神經網路 (CNNs) 的需求增加，其大小、執行時間、記憶體佔用和功耗也隨之增加。為了解決這個問題，已經提出了諸如剪枝 (pruning) 之類的解決方案，它們各自擁有用於如何移除權重的指標和方法，或者說準則。這些解決方案沒有共同的實現方式，並且難以實施和比較。在這項工作中，我們介紹了 Combine，一個基於準則的剪枝解決方案和演示...",
      "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning",
      "title_zh": "Bonsai：一個使用基於準則剪枝的卷積神經網路加速框架"
    },
    {
      "arxiv_id": "2602.16858",
      "authors": [
        "Kathiravan Palaniappan"
      ],
      "categories": [
        "cs.PF",
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.570836+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation",
          "url": "https://arxiv.org/abs/2602.16858"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation",
        "url": "https://arxiv.org/abs/2602.16858"
      },
      "published_at": "2026-02-18T20:34:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7274838421746729,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.227483842174674
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16858",
      "summary": "The deployment of deep learning inference in production environments continues to grow, where throughput, latency, and hardware efficiency are critical. Although specialized accelerators are increasingly adopted, many inference workloads still run on CPU-only systems, particularly in legacy data centers and cost-sensitive environments. This study investigates the scalability limits of CPU-based inference for convolutional neural networks by benchmarking ResNet models across varying batch sizes o",
      "summary_zh": "深度學習推論在生產環境中的部署持續增長，其中吞吐量 (throughput)、延遲 (latency) 和硬體效率 (hardware efficiency) 至關重要。儘管專用加速器越來越普及，但許多推論工作負載仍然運行在僅限 CPU 的系統上，特別是在傳統數據中心和成本敏感的環境中。本研究通過對不同批次大小的 ResNet 模型進行基準測試，調查了基於 CPU 的卷積神經網路推論的可擴展性限制...",
      "title": "GDEV-AI: A Generalized Evaluation of Deep Learning Inference Scaling and Architectural Saturation",
      "title_zh": "GDEV-AI：深度學習推論擴展性和架構飽和度的一項通用評估"
    },
    {
      "arxiv_id": "2602.16843",
      "authors": [
        "Ahmed Rafid",
        "Rumman Adib",
        "Fariya Ahmed",
        "Ajwad Abrar",
        "Mohammed Saidul Islam"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.570896+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
          "url": "https://arxiv.org/abs/2602.16843"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
        "url": "https://arxiv.org/abs/2602.16843"
      },
      "published_at": "2026-02-18T20:13:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7263917529573398,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.22639175295734
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16843",
      "summary": "Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverag",
      "summary_zh": "評估事實一致性對於可靠的文本摘要至關重要，特別是在醫療保健和新聞等高風險領域。然而，大多數現有的評估指標都忽略了孟加拉語 (Bangla) 這種廣泛使用但資源不足的語言，並且通常依賴於參考摘要。我們介紹了 BanglaSummEval，一個無參考、基於問答的框架，用於評估孟加拉語摘要中的事實一致性。所提出的方法評估了事實準確性和內容覆蓋...",
      "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
      "title_zh": "BanglaSummEval：用於孟加拉語摘要的無參考事實一致性評估"
    },
    {
      "arxiv_id": "2602.16571",
      "authors": [
        "Zhuqian Zhou",
        "Kirk Vanacore",
        "Bakhtawar Ahtisham",
        "Jinsook Lee",
        "Doug Pietrzak",
        "Daryl Hedley",
        "Jorge Dias",
        "Chris Shaw",
        "Ruth Schäfer",
        "René F. Kizilcec"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571385+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
          "url": "https://arxiv.org/abs/2602.16571"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
        "url": "https://arxiv.org/abs/2602.16571"
      },
      "published_at": "2026-02-18T16:12:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7143681899982474,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.214368189998247
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16571",
      "summary": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcrip",
      "summary_zh": "大規模共享對話式數據對於推動教學科學至關重要，然而嚴格的去識別化仍然是一大障礙。在數學輔導文本中，數字表達式經常類似於結構化識別符（例如，日期或 IDs），導致通用 Personally Identifiable Information (PII) 檢測系統過度刪減核心教學內容，並降低 dataset 的效用。本研究探討如何在數學輔導文本中檢測 PII。",
      "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
      "title_zh": "數學輔導中的效用保留去識別化：探討 MathEd-PII Benchmark Dataset 中的數字歧義性"
    },
    {
      "arxiv_id": "2602.16511",
      "authors": [
        "Osher Azulay",
        "Zhengjie Xu",
        "Andrew Scheffer",
        "Stella X. Yu"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:02.553521+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-safety",
          "tier": 1,
          "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
          "url": "https://arxiv.org/abs/2602.16511"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-safety",
        "tier": 1,
        "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
        "url": "https://arxiv.org/abs/2602.16511"
      },
      "published_at": "2026-02-18T14:57:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.710646502102705,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.210646502102705
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16511",
      "summary": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning o",
      "summary_zh": "對於在雜亂環境中運作的 humanoids 而言，可靠的跌倒恢復至關重要。與四足或輪式機器人不同，humanoids 在跌倒時會經歷高能量衝擊、複雜的全身接觸以及視角的大幅變化，這使得恢復對於持續運作來說至關重要。現有方法將跌倒安全分解為獨立的問題，例如跌倒避免、衝擊緩解和站立恢復，或者依賴於透過 reinforcement learning 進行無視覺訓練的 end-to-end 策略。",
      "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
      "title_zh": "VIGOR：用於統一類人機器人跌倒安全的視覺情境目標推斷"
    },
    {
      "arxiv_id": "2602.16298",
      "authors": [
        "Martin Hyben",
        "Sebastian Kula",
        "Jan Cegin",
        "Jakub Simko",
        "Ivan Srba",
        "Robert Moro"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571924+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
          "url": "https://arxiv.org/abs/2602.16298"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
        "url": "https://arxiv.org/abs/2602.16298"
      },
      "published_at": "2026-02-18T09:28:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.6946103549510922,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.194610354951095
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16298",
      "summary": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) tex",
      "summary_zh": "Large Language Models (LLMs) 開始重塑媒體專業人士驗證資訊的方式，然而，對於檢測「可查證性」聲明（fact-checking 過程中的關鍵步驟）的自動化支援仍然有限。我們介紹了 Multi-Check-Worthy (MultiCW) dataset，這是一個用於「可查證性」聲明檢測的平衡多語言基準，涵蓋 16 種語言、7 個主題領域和 2 種寫作風格。它包含 123,722 個樣本，在嘈雜（非正式）和結構化（正式）文本之間均勻分佈。",
      "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
      "title_zh": "MultiCW：用於訓練穩健「可查證性」檢測模型的大規模平衡基準 Dataset"
    },
    {
      "arxiv_id": "2602.16326",
      "authors": [
        "Fabrizio Corriera",
        "Frank W. Takes",
        "Akrati Saxena"
      ],
      "categories": [
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.571705+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Individual Fairness in Community Detection: Quantitative Measure and Comparative Evaluation",
          "url": "https://arxiv.org/abs/2602.16326"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Individual Fairness in Community Detection: Quantitative Measure and Comparative Evaluation",
        "url": "https://arxiv.org/abs/2602.16326"
      },
      "published_at": "2026-02-18T10:06:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.6964433569584757,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 21.096443356958474
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16326",
      "summary": "Community detection is a fundamental task in complex network analysis. Fairness-aware community detection seeks to prevent biased node partitions, typically framed in terms of individual fairness, which requires similar nodes to be treated similarly, and group fairness, which aims to avoid disadvantaging specific groups of nodes. While existing literature on fair community detection has primarily focused on group fairness, we introduce a novel measure to quantify individual fairness in community",
      "summary_zh": "社群檢測是複雜網路分析中的一項基本任務。考慮公平性的社群檢測旨在防止偏頗的節點劃分，通常分為個體公平性（要求相似節點被相似對待）和群體公平性（旨在避免特定節點群體處於劣勢）。儘管關於公平社群檢測的現有文獻主要關注 group fairness，但我們引入了一種新穎的測量方法來量化社群中的 individual fairness。",
      "title": "Individual Fairness in Community Detection: Quantitative Measure and Comparative Evaluation",
      "title_zh": "社群檢測中的個體公平性：量化測量與比較評估"
    },
    {
      "arxiv_id": "2602.16299",
      "authors": [
        "Mathias Vast",
        "Victor Morand",
        "Basile van Cooten",
        "Laure Soulier",
        "Josiane Mothe",
        "Benjamin Piwowarski"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.775580+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking",
          "url": "https://arxiv.org/abs/2602.16299"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking",
        "url": "https://arxiv.org/abs/2602.16299"
      },
      "published_at": "2026-02-18T09:30:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.6946875381672972,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 21.094687538167296
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16299",
      "summary": "Cross-encoders deliver state-of-the-art ranking effectiveness in information retrieval, but have a high inference cost. This prevents them from being used as first-stage rankers, but also incurs a cost when re-ranking documents. Prior work has addressed this bottleneck from two largely separate directions: accelerating cross-encoder inference by sparsifying the attention process or improving first-stage retrieval effectiveness using more complex models, e.g. late-interaction ones. In this work, ",
      "summary_zh": "Cross-encoders 在 information retrieval 中提供了最先進的 ranking 效果，但其 inference 成本很高。這使得它們無法用作 first-stage rankers，同時在 re-ranking 文件時也會產生額外成本。先前的工作從兩個主要獨立的方向解決了這個瓶頸：透過稀疏化 attention 過程來加速 cross-encoder inference，或使用更複雜的模型（例如 late-interaction 模型）來提高 first-stage retrieval 的效率。在本研究中，",
      "title": "MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking",
      "title_zh": "MICE：用於高效 Re-ranking 的最小互動 Cross-Encoders"
    },
    {
      "arxiv_id": "2602.16117",
      "authors": [
        "Vicente Chomalí-Castro",
        "Nick Clarisse",
        "Nicki Mullins",
        "Jorge Noronha"
      ],
      "categories": [
        "nucl-th",
        "astro-ph.HE",
        "gr-qc"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.567211+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Solving BDNK diffusion using physics-informed neural networks",
          "url": "https://arxiv.org/abs/2602.16117"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Solving BDNK diffusion using physics-informed neural networks",
        "url": "https://arxiv.org/abs/2602.16117"
      },
      "published_at": "2026-02-18T00:58:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.6704137301355214,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 0.44999999999999996,
        "total_score": 20.72041373013552
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16117",
      "summary": "In this work, we reformulate the relativistic BDNK (Bemfica-Disconzi-Noronha-Kovtun) diffusion equation in flux-conservative form, and solve the resulting equations in $(1+1)$D using both a second-order Kurganov-Tadmor finite volume scheme and physics-informed neural networks (PINNs). In particular, we introduce the SA-PINN-ACTO framework, which combines the self-adaptive PINN technique with an exact enforcement of initial and periodic boundary conditions through an algebraic transform of the ne",
      "summary_zh": "在這項工作中，我們將相對論性 BDNK (Bemfica-Disconzi-Noronha-Kovtun) 擴散方程重新表述為流量守恆形式，並使用二階 Kurganov-Tadmor 有限體積方案和 physics-informed neural networks (PINNs) 在 $(1+1)$D 中求解所得方程。特別地，我們引入了 SA-PINN-ACTO 框架，該框架將自適應 PINN 技術與透過對 ne 的代數變換精確實施初始和週期性邊界條件相結合。",
      "title": "Solving BDNK diffusion using physics-informed neural networks",
      "title_zh": "使用 physics-informed neural networks 解決 BDNK 擴散問題"
    },
    {
      "arxiv_id": "2602.17639",
      "authors": [
        "Pourya Shamsolmoali",
        "Masoumeh Zareapoor",
        "Eric Granger",
        "Yue Lu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.771754+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "IntRec: Intent-based Retrieval with Contrastive Refinement",
          "url": "https://arxiv.org/abs/2602.17639"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "IntRec: Intent-based Retrieval with Contrastive Refinement",
        "url": "https://arxiv.org/abs/2602.17639"
      },
      "published_at": "2026-02-19T18:50:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.55,
        "llm_relevance_score": 12.100000000000001,
        "recency_score": 0.798215671335945,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 20.098215671335947
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17639",
      "summary": "Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anch",
      "summary_zh": "從複雜場景中檢索使用者指定的物件仍然是一項艱鉅的任務，尤其當查詢模糊或涉及多個相似物件時。現有的 open-vocabulary 檢測器以 one-shot 方式運作，缺乏根據使用者回饋精修預測的能力。為了解決這個問題，我們提出了 IntRec，一個基於使用者回饋精修預測的互動式物件檢索框架。其核心是一個 Intent State (IS)，它為 positive anch 維護雙記憶體集。",
      "title": "IntRec: Intent-based Retrieval with Contrastive Refinement",
      "title_zh": "IntRec: 具對比式精修的基於意圖檢索"
    },
    {
      "arxiv_id": "2602.16249",
      "authors": [
        "David Smerkous",
        "Zian Wang",
        "Behzad Najafian"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.775700+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards",
          "url": "https://arxiv.org/abs/2602.16249"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards",
        "url": "https://arxiv.org/abs/2602.16249"
      },
      "published_at": "2026-02-18T07:58:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.5,
        "llm_relevance_score": 11.0,
        "recency_score": 0.6902777850091086,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 18.890277785009108
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16249",
      "summary": "Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introd",
      "summary_zh": "自監督預訓練透過實現資料高效的 fine-tuning 改變了電腦視覺領域，然而高解析度訓練通常需要伺服器規模的基礎設施，這限制了許多研究實驗室的領域內 foundation model 開發。Masked Autoencoders (MAE) 透過僅編碼可見 token 來減少計算，但由於密集網格先驗和對遮罩敏感的設計權衡，將 MAE 與層次下採樣架構結合在結構上仍然具有挑戰性。我們引入",
      "title": "AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards",
      "title_zh": "AFFMAE: 適用於桌上型顯示卡的可擴展高效視覺預訓練"
    },
    {
      "arxiv_id": "2602.16989",
      "authors": [
        "Chentong Hao",
        "Minmao Wang"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:05.772343+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline",
          "url": "https://arxiv.org/abs/2602.16989"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline",
        "url": "https://arxiv.org/abs/2602.16989"
      },
      "published_at": "2026-02-19T01:28:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.4,
        "llm_relevance_score": 8.8,
        "recency_score": 0.7424660943034909,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 16.74246609430349
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16989",
      "summary": "We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwi",
      "summary_zh": "我們為 WSDM Cup 2026 多語言檢索任務提出了一種低成本檢索系統，該系統使用英文查詢從約一千萬篇中文、波斯語和俄語新聞文章的集合中檢索相關文件，並為每個查詢輸出前 1000 個排名結果。我們遵循一個四階段管線，該管線結合了基於 LLM 的 GRF 風格查詢擴展與 BM25 候選檢索、使用來自 jina-embeddings-v4 的長文本表示進行密集排名，以及 pointwi。",
      "title": "WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline",
      "title_zh": "WSDM Cup 2026 多語言檢索：一種低成本多階段檢索管線"
    },
    {
      "arxiv_id": "2602.16245",
      "authors": [
        "J. Dhar",
        "M. K. Pandey",
        "D. Chakladar",
        "M. Haghighat",
        "A. Alavi",
        "S. Mistry",
        "N. Zaidi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:01.566785+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
          "url": "https://arxiv.org/abs/2602.16245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
        "url": "https://arxiv.org/abs/2602.16245"
      },
      "published_at": "2026-02-18T07:47:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 7.699999999999999,
        "recency_score": 0.6897522874721853,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 16.589752287472184
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16245",
      "summary": "Multimodal fusion frameworks, which integrate diverse medical imaging modalities (e.g., MRI, CT), have shown great potential in applications such as skin cancer detection, dementia diagnosis, and brain tumor prediction. However, existing multimodal fusion methods face significant challenges. First, they often rely on computationally expensive models, limiting their applicability in low-resource environments. Second, they often employ cascaded attention modules, which potentially increase risk of",
      "summary_zh": "多模態融合框架，整合了多樣化的醫學影像模態（例如 MRI、CT），在皮膚癌檢測、失智症診斷和腦腫瘤預測等應用中展現出巨大潛力。然而，現有的多模態融合方法面臨重大挑戰。首先，它們通常依賴計算成本高昂的模型，限制了它們在低資源環境中的適用性。其次，它們通常採用級聯 attention modules，這可能會增加的風險。",
      "title": "HyPCA-Net: Advancing Multimodal Fusion in Medical Image Analysis",
      "title_zh": "HyPCA-Net: 推動醫學影像分析中的多模態融合進展"
    },
    {
      "arxiv_id": "2602.17101",
      "authors": [
        "Varun Burde",
        "Pavel Burget",
        "Torsten Sattler"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:03.570050+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-evaluation",
          "tier": 1,
          "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success",
          "url": "https://arxiv.org/abs/2602.17101"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-evaluation",
        "tier": 1,
        "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success",
        "url": "https://arxiv.org/abs/2602.17101"
      },
      "published_at": "2026-02-19T05:55:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 7.699999999999999,
        "recency_score": 0.7563461911285758,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.656346191128575
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17101",
      "summary": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based ",
      "summary_zh": "3D 重建是眾多機器人感知任務的基礎層，包括 6D 物體姿態估計和抓取姿態生成。現代物體 3D 重建方法可以從多視角圖像生成視覺上和幾何上令人印象深刻的網格，然而，標準的幾何評估未能反映重建品質如何影響下游任務，例如機器人操作性能。本文透過引入一個大規模、基於物理的評估來解決這一空白",
      "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success",
      "title_zh": "基準測試物體姿態估計和重建對機器人抓取成功率的影響"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Monica Jain"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence",
        "Technical How-to"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-22T00:40:54.153925+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build unified intelligence with Amazon Bedrock AgentCore",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build unified intelligence with Amazon Bedrock AgentCore",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore"
      },
      "published_at": "2026-02-18T15:54:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 4.4,
        "recency_score": 0.7134617494697889,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 15.363461749469788
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:7033c681de64957b",
      "summary": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
      "summary_zh": "在這篇文章中，我們將透過我們對 Customer Agent and Knowledge Engine (CAKE) 的實際實施，展示如何使用 Amazon Bedrock AgentCore 構建統一智慧系統。",
      "title": "Build unified intelligence with Amazon Bedrock AgentCore",
      "title_zh": "透過 Amazon Bedrock AgentCore 構建統一智慧"
    },
    {
      "arxiv_id": "2602.17636",
      "authors": [
        "Jiyoung Kim",
        "Youngjin Shin",
        "Siyoon Jin",
        "Dahyun Chung",
        "Jisu Nam",
        "Tongmin Kim",
        "Jongjae Park",
        "Hyeonwoo Kang",
        "Seungryong Kim"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:00.593071+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
          "url": "https://arxiv.org/abs/2602.17636"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
        "url": "https://arxiv.org/abs/2602.17636"
      },
      "published_at": "2026-02-19T18:50:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.25,
        "llm_relevance_score": 5.5,
        "recency_score": 0.7981777939447041,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 14.498177793944704
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17636",
      "summary": "Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garme",
      "summary_zh": "現有的 Virtual Try-On (VTON) 方法在保留精細服裝細節方面常遇到困難，尤其是在需要精確個人-服裝對應關係的非配對設定中。這些方法沒有明確地強制執行個人-服裝對齊，也未能解釋 Diffusion Transformers (DiTs) 中對應關係是如何出現的。在本文中，我們首先分析了基於 DiT 架構中的完整 3D attention，並揭示了個人-服裝對應關係關鍵地取決於精確的個人-服裝",
      "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On",
      "title_zh": "CORAL：透過對應對齊改進 Virtual Try-On"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Myriam Hamed Torres"
      ],
      "categories": [
        "Gemini App",
        "AI"
      ],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-22T00:40:51.039906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 2,
      "links": [
        {
          "link_type": "blog",
          "source_id": "deepmind-blog",
          "tier": 0,
          "title": "A new way to express yourself: Gemini can now create music",
          "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music"
        },
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "A new way to express yourself: Gemini can now create music",
          "url": "https://blog.google/innovation-and-ai/products/gemini-app/lyria-3"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "deepmind-blog",
        "tier": 0,
        "title": "A new way to express yourself: Gemini can now create music",
        "url": "https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music"
      },
      "published_at": "2026-02-18T08:01:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.15,
        "llm_relevance_score": 3.3,
        "recency_score": 0.6904144160077216,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 14.49041441600772
      },
      "section": null,
      "source_name": "DeepMind Blog",
      "story_id": "fallback:7a4f73fdfe61f776",
      "summary": "The Gemini app now features our most advanced music generation model Lyria 3, empowering anyone to make 30-second tracks using text or images.",
      "summary_zh": "Gemini app 現在搭載了我們最先進的音樂生成模型 Lyria 3，讓任何人都能使用文字或圖像創作 30 秒的音軌。",
      "title": "A new way to express yourself: Gemini can now create music",
      "title_zh": "一種表達自己的新方式：Gemini 現在可以創作音樂"
    },
    {
      "arxiv_id": "2602.16641",
      "authors": [
        "Xihan Ma",
        "Haichong Zhang"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.774969+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting",
          "url": "https://arxiv.org/abs/2602.16641"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting",
        "url": "https://arxiv.org/abs/2602.16641"
      },
      "published_at": "2026-02-18T17:31:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.3,
        "llm_relevance_score": 6.6,
        "recency_score": 0.7182689661338281,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.9000000000000004,
        "total_score": 14.418268966133828
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16641",
      "summary": "Medical ultrasound (US) imaging is a frontline tool for the diagnosis of kidney diseases. However, traditional freehand imaging procedure suffers from inconsistent, operator-dependent outcomes, lack of 3D localization information, and risks of work-related musculoskeletal disorders. While robotic ultrasound (RUS) systems offer the potential for standardized, operator-independent 3D kidney data acquisition, the existing scanning methods lack the ability to determine the optimal imaging window for",
      "summary_zh": "醫學超聲 (US) 成像是用於診斷腎臟疾病的一線工具。然而，傳統的徒手成像程序存在結果不一致、依賴操作者、缺乏 3D 定位資訊以及工作相關肌肉骨骼疾病風險的問題。儘管機器人超聲 (RUS) 系統提供了標準化、獨立於操作者的 3D 腎臟數據採集的潛力，但現有的掃描方法缺乏確定最佳成像視窗的能力",
      "title": "Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting",
      "title_zh": "邁向自主機器人腎臟超聲：透過模板引導的最佳樞轉實現空間高效體積成像"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dan Ferguson"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Amazon SageMaker Data & AI Governance",
        "Foundational (100)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-22T00:40:54.153500+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-1-flexible-training-plans-and-improvements-to-price-performance-for-inference-workloads"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-1-flexible-training-plans-and-improvements-to-price-performance-for-inference-workloads"
      },
      "published_at": "2026-02-20T12:26:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.8589452771479542,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 12.458945277147954
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:cf7b1d292be16c83",
      "summary": "In 2025, Amazon SageMaker AI saw dramatic improvements to core infrastructure offerings along four dimensions: capacity, price performance, observability, and usability. In this series of posts, we discuss these various improvements and their benefits. In Part 1, we discuss capacity improvements with the launch of Flexible Training Plans. We also describe improvements to price performance for inference workloads. In Part 2, we discuss enhancements made to observability, model customization, and model hosting.",
      "title": "Amazon SageMaker AI in 2025, a year in review part 1: Flexible Training Plans and improvements to price performance for inference workloads"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dan Ferguson"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Amazon SageMaker Data & AI Governance",
        "Foundational (100)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-22T00:40:54.153649+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-2-improved-observability-and-enhanced-features-for-sagemaker-ai-model-customization-and-hosting"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-ai-in-2025-a-year-in-review-part-2-improved-observability-and-enhanced-features-for-sagemaker-ai-model-customization-and-hosting"
      },
      "published_at": "2026-02-20T12:26:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.8589283767705724,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 12.458928376770572
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9a3c94c00ad8b85e",
      "summary": "In 2025, Amazon SageMaker AI made several improvements designed to help you train, tune, and host generative AI workloads. In Part 1 of this series, we discussed Flexible Training Plans and price performance improvements made to inference components. In this post, we discuss enhancements made to observability, model customization, and model hosting. These improvements facilitate a whole new class of customer use cases to be hosted on SageMaker AI.",
      "title": "Amazon SageMaker AI in 2025, a year in review part 2: Improved observability and enhanced features for SageMaker AI model customization and hosting"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-22T00:40:51.039700+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "deepmind-blog",
          "tier": 0,
          "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
          "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "deepmind-blog",
        "tier": 0,
        "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
        "url": "https://deepmind.google/blog/gemini-3-1-pro-a-smarter-model-for-your-most-complex-tasks"
      },
      "published_at": "2026-02-19T08:06:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.7632697173230388,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 12.113269717323039
      },
      "section": null,
      "source_name": "DeepMind Blog",
      "story_id": "fallback:fcbf948f2e957168",
      "summary": "3.1 Pro is designed for tasks where a simple answer isn’t enough.",
      "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks"
    },
    {
      "arxiv_id": null,
      "authors": [
        "ND Ngoka"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Elastic Kubernetes Service",
        "Amazon Simple Storage Service (S3)",
        "Customer Solutions",
        "Technical How-to",
        "AI/ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:54.153800+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte"
      },
      "published_at": "2026-02-19T08:28:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 4.4,
        "recency_score": 0.7644429084433375,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 2.25,
        "total_score": 11.914442908443338
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:1be4363aa25716fa",
      "summary": "In this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora, AWS Identity and Access Management (IAM), and Amazon CloudWatch. We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.",
      "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Ebbey Thomas"
      ],
      "categories": [
        "Amazon Quick Suite",
        "Artificial Intelligence",
        "Intermediate (200)",
        "Technical How-to"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:54.153733+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)",
          "url": "https://aws.amazon.com/blogs/machine-learning/integrate-external-tools-with-amazon-quick-agents-using-model-context-protocol-mcp"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)",
        "url": "https://aws.amazon.com/blogs/machine-learning/integrate-external-tools-with-amazon-quick-agents-using-model-context-protocol-mcp"
      },
      "published_at": "2026-02-20T08:26:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.1,
        "llm_relevance_score": 2.2,
        "recency_score": 0.8447227401901573,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 11.294722740190156
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:6c24855fb78864a1",
      "summary": "In this post, you’ll use a six-step checklist to build a new MCP server or validate and adjust an existing MCP server for Amazon&nbsp;Quick integration.&nbsp;The Amazon&nbsp;Quick User Guide describes the MCP client behavior and constraints. This is a “How to” guide for detailed implementation required by 3P partners to integrate with Amazon Quick with MCP.",
      "title": "Integrate external tools with Amazon Quick Agents using Model Context Protocol (MCP)"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Richard Black"
      ],
      "categories": [
        "Research Blog"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:53.490434+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "microsoft-research-blog",
          "tier": 0,
          "title": "Project Silica’s advances in glass storage technology",
          "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "microsoft-research-blog",
        "tier": 0,
        "title": "Project Silica’s advances in glass storage technology",
        "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology"
      },
      "published_at": "2026-02-18T08:11:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.6908996345228753,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.0,
        "total_score": 8.190899634522875
      },
      "section": null,
      "source_name": "Microsoft Research Blog",
      "story_id": "fallback:6bbe35b78a036ef7",
      "summary": "<p>Project Silica introduces new techniques for encoding data in borosilicate glass, as described in the journal Nature. These advances lower media cost and simplify writing and reading systems while supporting 10,000-year data preservation.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/\">Project Silica’s advances in glass storage technology</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
      "summary_zh": "Project Silica 在《Nature》期刊中介紹了在 borosilicate glass 中編碼數據的新技術。這些進展降低了媒體成本，簡化了寫入和讀取系統，同時支持 10,000 年的數據保存。",
      "title": "Project Silica’s advances in glass storage technology",
      "title_zh": "Project Silica 在玻璃儲存技術方面的進展"
    },
    {
      "arxiv_id": "2602.16933",
      "authors": [
        "Dan M. Kluger",
        "Stephen Bates"
      ],
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:41:04.774539+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference",
          "url": "https://arxiv.org/abs/2602.16933"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference",
        "url": "https://arxiv.org/abs/2602.16933"
      },
      "published_at": "2026-02-18T22:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.7345800421221914,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 7.9345800421221915
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16933",
      "summary": "In two-phase multiwave sampling, inexpensive measurements are collected on a large sample and expensive, more informative measurements are adaptively obtained on subsets of units across multiple waves. Adaptively collecting the expensive measurements can increase efficiency but complicates statistical inference. We give valid estimators and confidence intervals for M-estimation under adaptive two-phase multiwave sampling. We focus on the case where proxies for the expensive variables -- such as ",
      "summary_zh": "在雙階段多波抽樣中，會在大型樣本上收集低成本測量，並在多波次的單位子集上自適應地獲取高成本、資訊更豐富的測量。自適應地收集高成本測量可以提高效率，但會使統計推斷複雜化。我們針對自適應雙階段多波抽樣下的 M-estimation 提供了有效的估計量和置信區間。我們重點關注高成本變量的 proxies ——例如...",
      "title": "M-estimation under Two-Phase Multiwave Sampling with Applications to Prediction-Powered Inference",
      "title_zh": "在雙階段多波抽樣下的 M-estimation 及其在 Prediction-Powered Inference 中的應用"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Sundar Pichai"
      ],
      "categories": [
        "A message from our CEO",
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:51.505549+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "“No technology has me dreaming bigger than AI”",
          "url": "https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "“No technology has me dreaming bigger than AI”",
        "url": "https://blog.google/company-news/inside-google/message-ceo/sundar-pichai-ai-impact-summit-2026"
      },
      "published_at": "2026-02-18T20:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.7272439131199856,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 2.25,
        "total_score": 7.4772439131199855
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:64676d972524e1dd",
      "summary": "a stylized design resembling the Ashoka Chakra with colorful network lines and text reading \"भारत 2026 INDIA.\" A vertical line separates it from the Google logo on the right, all set against a light blue gradient background with a faint grid pattern.",
      "summary_zh": "一個風格化的設計，類似於 Ashoka Chakra，帶有彩色網絡線條，文字顯示為「भारत 2026 INDIA」。一條垂直線將其與右側的 Google logo 分隔開來，所有這些都設置在帶有微弱網格圖案的淺藍色漸變背景上。",
      "title": "“No technology has me dreaming bigger than AI”",
      "title_zh": "沒有任何技術能比 AI 更讓我懷抱遠大夢想"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Vignessh Baskaran"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Quick Sight",
        "Amazon Quick Suite"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:54.153861+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
          "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source"
      },
      "published_at": "2026-02-19T08:06:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.05,
        "llm_relevance_score": 1.1,
        "recency_score": 0.7632935698743994,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 6.363293569874399
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9c836f2795128c3a",
      "summary": "In this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.",
      "summary_zh": "在這篇部落格文章中，我們將引導您如何通過安全的 key pair authentication 在 Amazon Quick Sight 和 Snowflake 之間建立數據源連接。",
      "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
      "title_zh": "Amazon Quick 現已支持使用 key pair authentication 連接到 Snowflake 數據源"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:51.505687+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "AI Impact Summit 2026",
          "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "AI Impact Summit 2026",
        "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-collection"
      },
      "published_at": "2026-02-18T20:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.7272439131199856,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 5.2272439131199855
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:a6435c936e61839f",
      "summary": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Collection_Hero-2.max-600x600.format-webp.webp\" />A look at the partnerships and investments Google announced at the AI Impact Summit 2026.",
      "summary_zh": "回顧 Google 在 AI Impact Summit 2026 上宣布的合作夥伴關係和投資。",
      "title": "AI Impact Summit 2026",
      "title_zh": "AI Impact Summit 2026"
    },
    {
      "arxiv_id": null,
      "authors": [
        "James Manyika"
      ],
      "categories": [
        "Google.org",
        "Google in Asia",
        "AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-22T00:40:51.505839+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "google-ai-blog",
          "tier": 0,
          "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
          "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "google-ai-blog",
        "tier": 0,
        "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
        "url": "https://blog.google/innovation-and-ai/technology/ai/ai-impact-summit-2026-india"
      },
      "published_at": "2026-02-18T02:30:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.6746958033691547,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 0.0,
        "total_score": 5.174695803369155
      },
      "section": null,
      "source_name": "Google AI Blog",
      "story_id": "fallback:290081df3883fdd9",
      "summary": "four people seated on a conference stage",
      "summary_zh": "四人坐在會議舞台上",
      "title": "AI Impact Summit 2026: How we’re partnering to make AI work for everyone",
      "title_zh": "AI Impact Summit 2026：我們如何合作讓 AI 造福所有人"
    }
  ],
  "run_date": "2026-02-22",
  "run_id": "27e06276-703f-4be3-839d-b7d14f133ea8",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-22T01:09:03.408853+00:00",
    "items_total": 466,
    "run_id": "27e06276-703f-4be3-839d-b7d14f133ea8",
    "started_at": "2026-02-22T00:40:50.318639+00:00",
    "stories_total": 345,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 56,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": "2026-02-19T18:59:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-agents",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 18,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": "2026-02-19T18:59:03+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-alignment",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 30,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Efficiency",
      "newest_item_date": "2026-02-19T18:59:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-efficiency",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 28,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Evaluation",
      "newest_item_date": "2026-02-19T18:59:44+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-evaluation",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 10,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Interpretability",
      "newest_item_date": "2026-02-19T16:43:12+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-interpretability",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 95,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": "2026-02-19T18:59:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-llm",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 31,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": "2026-02-19T18:59:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-multimodal",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 24,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": "2026-02-19T18:47:38+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-reasoning",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 15,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Retrieval",
      "newest_item_date": "2026-02-19T18:56:36+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-retrieval",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reinforcement Learning",
      "newest_item_date": "2026-02-19T04:42:37+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-api-rl",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 8,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Safety",
      "newest_item_date": "2026-02-19T16:59:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-safety",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CL",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CV",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.IR",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ir",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.LG",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.MA",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ma",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.RO",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ro",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.SE",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-se",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv stat.ML",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 7,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": "2026-02-20T12:26:47+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": "2026-02-19T08:06:14+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 4,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": "2026-02-18T20:30:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 11,
      "items_updated": 12,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": "2026-02-19T18:11:28+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": "2026-02-20T15:55:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": "2026-02-21T00:14:28+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": "2026-02-19T00:28:31+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": "2026-02-20T05:27:33+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": "2026-02-19T08:00:51+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "NO_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.16802",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "qwen"
      ],
      "first_seen_at": "2026-02-22T00:40:57.211268+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "References Improve LLM Alignment in Non-Verifiable Domains",
          "url": "https://arxiv.org/abs/2602.16802"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "References Improve LLM Alignment in Non-Verifiable Domains",
        "url": "https://arxiv.org/abs/2602.16802"
      },
      "published_at": "2026-02-18T19:03:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7228918405274664,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.28289184052747
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16802",
      "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains"
    },
    {
      "arxiv_id": "2602.16682",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "deepmind"
      ],
      "first_seen_at": "2026-02-22T00:41:12.407600+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Situated Awareness in the Real World",
          "url": "https://arxiv.org/abs/2602.16682"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Situated Awareness in the Real World",
        "url": "https://arxiv.org/abs/2602.16682"
      },
      "published_at": "2026-02-18T18:22:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7208515517445369,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.52085155174454
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16682",
      "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "title": "Learning Situated Awareness in the Real World"
    },
    {
      "arxiv_id": "2602.16968",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:01.565695+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
          "url": "https://arxiv.org/abs/2602.16968"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
        "url": "https://arxiv.org/abs/2602.16968"
      },
      "published_at": "2026-02-19T00:15:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7387134379854283,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.298713437985427
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16968",
      "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers"
    },
    {
      "arxiv_id": "2602.17365",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.406999+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Computer-Using World Model",
          "url": "https://arxiv.org/abs/2602.17365"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Computer-Using World Model",
        "url": "https://arxiv.org/abs/2602.17365"
      },
      "published_at": "2026-02-19T13:48:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.87,
        "llm_relevance_score": 19.14,
        "recency_score": 0.7816279231887001,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.1216279231887
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17365",
      "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "title": "Computer-Using World Model"
    },
    {
      "arxiv_id": "2602.17363",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-22T00:41:12.406921+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
          "url": "https://arxiv.org/abs/2602.17363"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
        "url": "https://arxiv.org/abs/2602.17363"
      },
      "published_at": "2026-02-19T13:45:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7814596741771334,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.681459674177134
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17363",
      "summary": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy"
    }
  ]
}