{
  "archive_dates": [
    "2026-02-20"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-20T16:57:26.037711+00:00",
  "model_releases_by_entity": {
    "huggingface": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "huggingface"
        ],
        "first_seen_at": "2026-02-20T16:19:38.113744+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 11,
          "likes": 4,
          "pipeline_tag": "time-series-forecasting"
        },
        "hf_model_id": "google/timesfm-2.5-200m-transformers",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-google",
            "tier": 1,
            "title": "google/timesfm-2.5-200m-transformers",
            "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-google",
          "tier": 1,
          "title": "google/timesfm-2.5-200m-transformers",
          "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
        },
        "published_at": "2026-02-20T15:55:54+00:00",
        "section": null,
        "source_name": null,
        "story_id": "hf:google/timesfm-2.5-200m-transformers",
        "summary": "TimesFM (Time Series Foundation Model) is a pretrained decoder-only model for time-series forecasting. This repository contains the **Transformers** port of the official TimesFM 2.5 PyTorch release. **Resources and Technical Documentation**: * Original model: google/timesfm-2.5-200m-pytorch * Transformers model: google/timesfm-2.5-200m-transformers * Paper: A decoder-only foundation model for time-series forecasting * Transformers docs: TimesFM 2.5 This model is converted from the official TimesFM 2.5 PyTorch checkpoint and integrated into `transformers` as `Timesfm2P5ModelForPrediction`. The converted checkpoint preserves the original architecture and forecasting behavior, including: * patch-based inputs for time-series contexts * decoder-only self-attention stack",
        "title": "google/timesfm-2.5-200m-transformers"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-20T16:19:39.375352+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 105189,
          "likes": 759,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        },
        "published_at": "2026-02-20T05:27:33+00:00",
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
        "title": "Qwen/Qwen3.5-397B-A17B"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.16839",
      "authors": [
        "Zeliang Zhang, Xiaodong Liu, Hao Cheng, Hao Sun, Chenliang Xu, Jianfeng Gao"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.003822+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
          "url": "https://arxiv.org/abs/2602.16839"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
        "url": "https://arxiv.org/abs/2602.16839"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16839",
      "summary": "arXiv:2602.16839v1 Announce Type: cross \nAbstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consiste",
      "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding"
    },
    {
      "arxiv_id": "2602.17183",
      "authors": [
        "Kishan Maharaj, Nandakishore Menon, Ashita Saxena, Srikanth Tamilselvam"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.388630+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
          "url": "https://arxiv.org/abs/2602.17183"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
        "url": "https://arxiv.org/abs/2602.17183"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17183",
      "summary": "arXiv:2602.17183v1 Announce Type: cross \nAbstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for asse",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering"
    },
    {
      "arxiv_id": "2602.06275",
      "authors": [
        "Isaac Picov, Ritesh Goru"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.639999+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution",
          "url": "https://arxiv.org/abs/2602.06275"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution",
        "url": "https://arxiv.org/abs/2602.06275"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.06275",
      "summary": "arXiv:2602.06275v2 Announce Type: replace \nAbstract: Explaining closed-source Large Language Model (LLM) outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce \\textbf{Rotary Positional Embedding Linear Local Interpretable Model-agnostic Explanations (RoPE-LIME)}, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in \\textbf{RoPE embedding space} for stable similarity under masking, and (ii) \\textbf{Sparse-$K$} sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on Hotpot",
      "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution"
    },
    {
      "arxiv_id": "2602.17623",
      "authors": [
        "Alireza Sakhaeirad, Ali Ma'manpoosh, Arshia Hemmat"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.660795+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
          "url": "https://arxiv.org/abs/2602.17623"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
        "url": "https://arxiv.org/abs/2602.17623"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17623",
      "summary": "arXiv:2602.17623v1 Announce Type: new \nAbstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings dem",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models"
    },
    {
      "arxiv_id": "2602.16727",
      "authors": [
        "Hua Yan, Heng Tan, Yingxue Zhang, Yu Yang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.362281+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
          "url": "https://arxiv.org/abs/2602.16727"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
        "url": "https://arxiv.org/abs/2602.16727"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16727",
      "summary": "arXiv:2602.16727v1 Announce Type: cross \nAbstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments",
      "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation"
    },
    {
      "arxiv_id": "2602.17046",
      "authors": [
        "Uria Franko"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.382945+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs",
          "url": "https://arxiv.org/abs/2602.17046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs",
        "url": "https://arxiv.org/abs/2602.17046"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17046",
      "summary": "arXiv:2602.17046v1 Announce Type: new \nAbstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evalua",
      "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs"
    },
    {
      "arxiv_id": "2511.17673",
      "authors": [
        "Myung Ho Kim"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.354315+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
          "url": "https://arxiv.org/abs/2511.17673"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
        "url": "https://arxiv.org/abs/2511.17673"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2511.17673",
      "summary": "arXiv:2511.17673v5 Announce Type: replace-cross \nAbstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Soft Symbolic Control constitutes a dedicated governance layer within SCL, applying symbolic constraints to probabilistic inference while preserving the flexibility of neural reasoning and restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approa",
      "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer"
    },
    {
      "arxiv_id": "2602.16738",
      "authors": [
        "Rebin Saleh, Khanh Pham Dinh, Bal\\'azs Vill\\'anyi, Truong-Son Hy"
      ],
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.999731+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
          "url": "https://arxiv.org/abs/2602.16738"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
        "url": "https://arxiv.org/abs/2602.16738"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16738",
      "summary": "arXiv:2602.16738v1 Announce Type: cross \nAbstract: Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainab",
      "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance"
    },
    {
      "arxiv_id": "2602.17222",
      "authors": [
        "Ben Yellin, Ehud Ezra, Mark Foreman, Shula Grinapol"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.389765+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
          "url": "https://arxiv.org/abs/2602.17222"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
        "url": "https://arxiv.org/abs/2602.17222"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17222",
      "summary": "arXiv:2602.17222v1 Announce Type: new \nAbstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset lin",
      "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight"
    },
    {
      "arxiv_id": "2602.17288",
      "authors": [
        "Anuj Gupta"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.390806+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
          "url": "https://arxiv.org/abs/2602.17288"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
        "url": "https://arxiv.org/abs/2602.17288"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17288",
      "summary": "arXiv:2602.17288v1 Announce Type: cross \nAbstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraint",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training"
    },
    {
      "arxiv_id": "2602.16898",
      "authors": [
        "Iman Ahmadi, Mehrshad Taji, Arad Mahdinezhad Kashani, AmirHossein Jadidi, Saina Kashani, Babak Khalaj"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.367310+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
          "url": "https://arxiv.org/abs/2602.16898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
        "url": "https://arxiv.org/abs/2602.16898"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16898",
      "summary": "arXiv:2602.16898v1 Announce Type: cross \nAbstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides vi",
      "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation"
    },
    {
      "arxiv_id": "2602.17092",
      "authors": [
        "Aadi Joshi, Kavya Bhand"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.019850+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Locality Radius Framework for Understanding Relational Inductive Bias in Database Learning",
          "url": "https://arxiv.org/abs/2602.17092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Locality Radius Framework for Understanding Relational Inductive Bias in Database Learning",
        "url": "https://arxiv.org/abs/2602.17092"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17092",
      "summary": "arXiv:2602.17092v1 Announce Type: new \nAbstract: Foreign key discovery and related schema-level prediction tasks are often modeled using graph neural networks (GNNs), implicitly assuming that relational inductive bias improves performance. However, it remains unclear when multi-hop structural reasoning is actually necessary. In this work, we introduce locality radius, a formal measure of the minimum structural neighborhood required to determine a prediction in relational schemas. We hypothesize that model performance depends critically on alignment between task locality radius and architectural aggregation depth. We conduct a controlled empirical study across foreign key prediction, join cost estimation, blast radius regression, cascade impact classification, and additional graph-derived schema tasks. Our evaluation includes multi-seed experiments, capacity-matched comparisons, statistical significance testing, scaling analysis, and synthetic radius-controlled benchmarks. Results revea",
      "title": "A Locality Radius Framework for Understanding Relational Inductive Bias in Database Learning"
    },
    {
      "arxiv_id": "2602.17127",
      "authors": [
        "Dusan Bosnjakovic"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.655912+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
          "url": "https://arxiv.org/abs/2602.17127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
        "url": "https://arxiv.org/abs/2602.17127"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17127",
      "summary": "arXiv:2602.17127v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.\n  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions in",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI"
    },
    {
      "arxiv_id": "2602.17475",
      "authors": [
        "Pietro Ferrazzi, Mattia Franzin, Alberto Lavelli, Bernardo Magnini"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.659290+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
          "url": "https://arxiv.org/abs/2602.17475"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
        "url": "https://arxiv.org/abs/2602.17475"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17475",
      "summary": "arXiv:2602.17475v1 Announce Type: new \nAbstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternativ",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian"
    },
    {
      "arxiv_id": "2602.15531",
      "authors": [
        "Javier Irigoyen, Roberto Daza, Aythami Morales, Julian Fierrez, Francisco Jurado, Alvaro Ortigosa, Ruben Tolosana"
      ],
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.359498+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "EduEVAL-DB: A Role-Based Dataset for Pedagogical Risk Evaluation in Educational Explanations",
          "url": "https://arxiv.org/abs/2602.15531"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "EduEVAL-DB: A Role-Based Dataset for Pedagogical Risk Evaluation in Educational Explanations",
        "url": "https://arxiv.org/abs/2602.15531"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.15531",
      "summary": "arXiv:2602.15531v2 Announce Type: replace \nAbstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations ",
      "title": "EduEVAL-DB: A Role-Based Dataset for Pedagogical Risk Evaluation in Educational Explanations"
    },
    {
      "arxiv_id": "2602.16749",
      "authors": [
        "Romiyal George, Sathiyamohan Nishankar, Selvarajah Thuseethan, Chathrie Wimalasooriya, Yakub Sebastian, Roshan G. Ragel, Zhongwei Liang"
      ],
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.001011+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition",
          "url": "https://arxiv.org/abs/2602.16749"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition",
        "url": "https://arxiv.org/abs/2602.16749"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16749",
      "summary": "arXiv:2602.16749v1 Announce Type: cross \nAbstract: Federated learning has emerged as a privacy-preserving and efficient approach for deploying intelligent agricultural solutions. Accurate edge-based diagnosis across geographically dispersed farms is crucial for recognising tomato diseases in sustainable farming. Traditional centralised training aggregates raw data on a central server, leading to communication overhead, privacy risks and latency. Meanwhile, edge devices require lightweight networks to operate effectively within limited resources. In this paper, we propose U-FedTomAtt, an ultra-lightweight federated learning framework with attention for tomato disease recognition in resource-constrained and distributed environments. The model comprises only 245.34K parameters and 71.41 MFLOPS. First, we propose an ultra-lightweight neural network with dilated bottleneck (DBNeck) modules and a linear transformer to minimise computational and memory overhead. To mitigate potential accuracy",
      "title": "U-FedTomAtt: Ultra-lightweight Federated Learning with Attention for Tomato Disease Recognition"
    },
    {
      "arxiv_id": "2602.16918",
      "authors": [
        "Shlok Mishra, Tsung-Yu Lin, Linda Wang, Hongli Xu, Yimin Liu, Michael Hsu, Chaitanya Ahuja, Hao Yuan, Jianpeng Cheng, Hong-You Chen, Haoyuan Xu, Chao Li, Abhijeet Awasthi, Jihye Moon, Don Husa, Michael Ge, Sumedha Singla, Arkabandhu Chowdhury, Phong Dingh, Satya Narayan Shukla, Yonghuan Yang, David Jacobs, Qi Guo, Jun Xiao, Xiangjun Fan, Aashu Singh"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.367846+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
          "url": "https://arxiv.org/abs/2602.16918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
        "url": "https://arxiv.org/abs/2602.16918"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.16918",
      "summary": "arXiv:2602.16918v1 Announce Type: new \nAbstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, ",
      "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data"
    },
    {
      "arxiv_id": "2601.12815",
      "authors": [
        "Zhaolu Kang, Junhao Gong, Qingxi Chen, Hao Zhang, Jiaxin Liu, Rong Fu, Zhiyuan Feng, Yuan Wang, Simon Fong, Kaiyue Zhou"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.639673+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
          "url": "https://arxiv.org/abs/2601.12815"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
        "url": "https://arxiv.org/abs/2601.12815"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2601.12815",
      "summary": "arXiv:2601.12815v5 Announce Type: replace \nAbstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives f",
      "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction"
    },
    {
      "arxiv_id": "2602.17659",
      "authors": [
        "Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:32.134130+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
          "url": "https://arxiv.org/abs/2602.17659"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
        "url": "https://arxiv.org/abs/2602.17659"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17659",
      "summary": "arXiv:2602.17659v1 Announce Type: new \nAbstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in",
      "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs"
    },
    {
      "arxiv_id": "2506.15733",
      "authors": [
        "Mert Cemri, Nived Rajaraman, Rishabh Tiwari, Xiaoxuan Liu, Kurt Keutzer, Ion Stoica, Kannan Ramchandran, Ahmad Beirami, Ziteng Sun"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.350644+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
          "url": "https://arxiv.org/abs/2506.15733"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
        "url": "https://arxiv.org/abs/2506.15733"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2506.15733",
      "summary": "arXiv:2506.15733v2 Announce Type: replace-cross \nAbstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, ",
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts"
    }
  ],
  "radar": [
    {
      "arxiv_id": "2602.16720",
      "authors": [
        "Bowen Cao, Weibin Liao, Yushi Sun, Dong Fang, Haitao Li, Wai Lam"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.361999+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL",
          "url": "https://arxiv.org/abs/2602.16720"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL",
        "url": "https://arxiv.org/abs/2602.16720"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16720",
      "summary": "arXiv:2602.16720v1 Announce Type: cross \nAbstract: Text-to-SQL systems powered by Large Language Models have excelled on academic benchmarks but struggle in complex enterprise environments. The primary limitation lies in their reliance on static schema representations, which fails to resolve semantic ambiguity and scale effectively to large, complex databases. To address this, we propose APEX-SQL, an Agentic Text-to-SQL Framework that shifts the paradigm from passive translation to agentic exploration. Our framework employs a hypothesis-verification loop to ground model reasoning in real data. In the schema linking phase, we use logical planning to verbalize hypotheses, dual-pathway pruning to reduce the search space, and parallel data profiling to validate column roles against real data, followed by global synthesis to ensure topological connectivity. For SQL generation, we introduce a deterministic mechanism to retrieve exploration directives, allowing the agent to effectively explor",
      "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL"
    },
    {
      "arxiv_id": "2503.19356",
      "authors": [
        "Reza Pourreza, Rishit Dagli, Apratim Bhattacharyya, Sunny Panchal, Guillaume Berger, Roland Memisevic"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:32.106437+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?",
          "url": "https://arxiv.org/abs/2503.19356"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?",
        "url": "https://arxiv.org/abs/2503.19356"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2503.19356",
      "summary": "arXiv:2503.19356v2 Announce Type: replace \nAbstract: AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users a",
      "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?"
    },
    {
      "arxiv_id": "2602.17655",
      "authors": [
        "Clara Meister, Ahmetcan Yavuz, Pietro Lesci, Tiago Pimentel"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.661215+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "What Language is This? Ask Your Tokenizer",
          "url": "https://arxiv.org/abs/2602.17655"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "What Language is This? Ask Your Tokenizer",
        "url": "https://arxiv.org/abs/2602.17655"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17655",
      "summary": "arXiv:2602.17655v1 Announce Type: new \nAbstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empi",
      "title": "What Language is This? Ask Your Tokenizer"
    },
    {
      "arxiv_id": "2512.08646",
      "authors": [
        "Maximilian Kreutner, Jens Rupprecht, Georg Ahnert, Ahmed Salem, Markus Strohmaier"
      ],
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.639007+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
          "url": "https://arxiv.org/abs/2512.08646"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
        "url": "https://arxiv.org/abs/2512.08646"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2512.08646",
      "summary": "arXiv:2512.08646v2 Announce Type: replace \nAbstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation (>40 million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers. We also find that answers can be obtained for a fraction of the compute cost, by changing the presentation method. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs \\emph{without coding knowledge}. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.",
      "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models"
    },
    {
      "arxiv_id": "2602.16938",
      "authors": [
        "Ofer Meshi, Krisztian Balog, Sally Goldman, Avi Caciularu, Guy Tennenholtz, Jihwan Jeong, Amir Globerson, Craig Boutilier"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.653184+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
          "url": "https://arxiv.org/abs/2602.16938"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
        "url": "https://arxiv.org/abs/2602.16938"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16938",
      "summary": "arXiv:2602.16938v1 Announce Type: new \nAbstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical \"realism gap,\" leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both \"good\" and \"bad\" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt",
      "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders"
    },
    {
      "arxiv_id": "2602.17497",
      "authors": [
        "Wen-Tse Chen, Jiayu Chen, Fahim Tajwar, Hao Zhu, Xintong Duan, Ruslan Salakhutdinov, Jeff Schneider"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.027260+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
          "url": "https://arxiv.org/abs/2602.17497"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
        "url": "https://arxiv.org/abs/2602.17497"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17497",
      "summary": "arXiv:2602.17497v1 Announce Type: new \nAbstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the en",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models"
    },
    {
      "arxiv_id": "2602.17366",
      "authors": [
        "Yiming Zhang, Siyue Zhang, Junbo Zhao, Chen Zhao"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.657548+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
          "url": "https://arxiv.org/abs/2602.17366"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
        "url": "https://arxiv.org/abs/2602.17366"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17366",
      "summary": "arXiv:2602.17366v1 Announce Type: new \nAbstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 an",
      "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering"
    },
    {
      "arxiv_id": "2602.16876",
      "authors": [
        "Yaroslav Solovko"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.004439+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "ML-driven detection and reduction of ballast information in multi-modal datasets",
          "url": "https://arxiv.org/abs/2602.16876"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "ML-driven detection and reduction of ballast information in multi-modal datasets",
        "url": "https://arxiv.org/abs/2602.16876"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.16876",
      "summary": "arXiv:2602.16876v1 Announce Type: cross \nAbstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals",
      "title": "ML-driven detection and reduction of ballast information in multi-modal datasets"
    },
    {
      "arxiv_id": "2602.17558",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:32.133001+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
          "url": "https://arxiv.org/abs/2602.17558"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
        "url": "https://arxiv.org/abs/2602.17558"
      },
      "published_at": "2026-02-19T17:11:59+00:00",
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17558",
      "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inher",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward"
    },
    {
      "arxiv_id": "2602.17443",
      "authors": [
        "Adib Sakhawat, Fardeen Sadab, Rakin Shahriar"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.658489+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
          "url": "https://arxiv.org/abs/2602.17443"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
        "url": "https://arxiv.org/abs/2602.17443"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17443",
      "summary": "arXiv:2602.17443v1 Announce Type: new \nAbstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured \"20 Questions\" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint A",
      "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue"
    },
    {
      "arxiv_id": "2602.15868",
      "authors": [
        "Magnus Boman"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.641185+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning",
          "url": "https://arxiv.org/abs/2602.15868"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning",
        "url": "https://arxiv.org/abs/2602.15868"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.15868",
      "summary": "arXiv:2602.15868v2 Announce Type: replace \nAbstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.",
      "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning"
    },
    {
      "arxiv_id": "2602.17068",
      "authors": [
        "Xiaocai Zhang, Neema Nassir, Milad Haghani"
      ],
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.018965+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
          "url": "https://arxiv.org/abs/2602.17068"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
        "url": "https://arxiv.org/abs/2602.17068"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17068",
      "summary": "arXiv:2602.17068v1 Announce Type: new \nAbstract: Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH",
      "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control"
    },
    {
      "arxiv_id": "2602.06530",
      "authors": [
        "Haipeng Li, Rongxuan Peng, Anwei Luo, Shunquan Tan, Changsheng Chen, Anastasia Antsiferova"
      ],
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:32.110524+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance",
          "url": "https://arxiv.org/abs/2602.06530"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance",
        "url": "https://arxiv.org/abs/2602.06530"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.06530",
      "summary": "arXiv:2602.06530v2 Announce Type: replace \nAbstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forg",
      "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance"
    },
    {
      "arxiv_id": "2602.16977",
      "authors": [
        "Zachary Coalson, Beth Sohler, Aiden Gabriel, Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:31.007370+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Fail-Closed Alignment for Large Language Models",
          "url": "https://arxiv.org/abs/2602.16977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Fail-Closed Alignment for Large Language Models",
        "url": "https://arxiv.org/abs/2602.16977"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16977",
      "summary": "arXiv:2602.16977v1 Announce Type: new \nAbstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small compu",
      "title": "Fail-Closed Alignment for Large Language Models"
    },
    {
      "arxiv_id": "2602.17518",
      "authors": [
        "Francesca Pezzuti",
        "Ophir Frieder",
        "Fabrizio Silvestri",
        "Sean MacAvaney",
        "Nicola Tonellotto"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:34.094083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A Picture of Agentic Search",
          "url": "https://arxiv.org/abs/2602.17518"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A Picture of Agentic Search",
        "url": "https://arxiv.org/abs/2602.17518"
      },
      "published_at": "2026-02-19T16:32:34+00:00",
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17518",
      "summary": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effecti",
      "title": "A Picture of Agentic Search"
    }
  ],
  "run_date": "2026-02-20",
  "run_id": "e58ec82f-d677-4e5a-94ae-6bdcd2d3738b",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-20T16:57:26.035540+00:00",
    "items_total": 798,
    "run_id": "e58ec82f-d677-4e5a-94ae-6bdcd2d3738b",
    "started_at": "2026-02-20T16:19:23.180467+00:00",
    "stories_total": 531,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 4,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": "2026-02-19T18:59:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-agents",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 3,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": "2026-02-19T18:56:34+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-alignment",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 9,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": "2026-02-19T18:48:08+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-llm",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 2,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": "2026-02-19T18:36:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-api-multimodal",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 1,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": "2026-02-19T16:59:11+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-reasoning",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 260,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.AI",
      "newest_item_date": "2026-02-19T21:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 46,
      "items_updated": 57,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CL",
      "newest_item_date": "2026-02-19T21:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 69,
      "items_updated": 29,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CV",
      "newest_item_date": "2026-02-19T21:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 145,
      "items_updated": 82,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.LG",
      "newest_item_date": "2026-02-19T21:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 5,
      "items_updated": 22,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv stat.ML",
      "newest_item_date": "2026-02-19T21:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": "2026-02-20T15:55:54+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": "2026-02-20T05:27:33+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "NO_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.16346",
      "authors": [
        "Nivya Talokar, Ayush K Tarun, Murari Mandal, Maksym Andriushchenko, Antoine Bosselut"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.998264+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
          "url": "https://arxiv.org/abs/2602.16346"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
        "url": "https://arxiv.org/abs/2602.16346"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16346",
      "summary": "arXiv:2602.16346v2 Announce Type: replace \nAbstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenari",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents"
    },
    {
      "arxiv_id": "2602.17330",
      "authors": [
        "Rong Fu, Zijian Zhang, Wenxin Zhang, Kun Liu, Jiekai Wu, Xianda Li, Simon Fong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.391605+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework",
          "url": "https://arxiv.org/abs/2602.17330"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework",
        "url": "https://arxiv.org/abs/2602.17330"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17330",
      "summary": "arXiv:2602.17330v1 Announce Type: new \nAbstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving reca",
      "title": "SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework"
    },
    {
      "arxiv_id": "2505.16928",
      "authors": [
        "Bosung Kim, Prithviraj Ammanabrolu"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.349787+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
          "url": "https://arxiv.org/abs/2505.16928"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
        "url": "https://arxiv.org/abs/2505.16928"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2505.16928",
      "summary": "arXiv:2505.16928v3 Announce Type: replace-cross \nAbstract: We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provid",
      "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning"
    },
    {
      "arxiv_id": "2601.15599",
      "authors": [
        "Cecil Pang, Hiroki Sayama"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.356492+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Autonomous Business System via Neuro-symbolic AI",
          "url": "https://arxiv.org/abs/2601.15599"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Autonomous Business System via Neuro-symbolic AI",
        "url": "https://arxiv.org/abs/2601.15599"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2601.15599",
      "summary": "arXiv:2601.15599v2 Announce Type: replace \nAbstract: Current business environments demand continuous reconfiguration of cross-functional processes, yet enterprise systems remain organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile, large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic and verifiable execution of complex business logic. We introduce Autonomous Business System (AUTOBUS), a system that combines LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic architecture for executing end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre- and post-conditions, required data, evaluation rules, and API-level actions. Enterprise data is represented as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, p",
      "title": "Autonomous Business System via Neuro-symbolic AI"
    },
    {
      "arxiv_id": "2602.16931",
      "authors": [
        "Idhant Gulati, Shivam Raval"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T16:19:30.375938+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
          "url": "https://arxiv.org/abs/2602.16931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
        "url": "https://arxiv.org/abs/2602.16931"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16931",
      "summary": "arXiv:2602.16931v1 Announce Type: new \nAbstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \\pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \\pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the ma",
      "title": "Narrow fine-tuning erodes safety alignment in vision-language agents"
    }
  ]
}