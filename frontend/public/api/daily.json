{
  "archive_dates": [
    "2026-02-21",
    "2026-02-20"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-21T03:08:23.127432+00:00",
  "model_releases_by_entity": {
    "huggingface": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "huggingface"
        ],
        "first_seen_at": "2026-02-20T20:28:35.192601+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 11,
          "likes": 6,
          "pipeline_tag": "time-series-forecasting"
        },
        "hf_model_id": "google/timesfm-2.5-200m-transformers",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-google",
            "tier": 1,
            "title": "google/timesfm-2.5-200m-transformers",
            "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-google",
          "tier": 1,
          "title": "google/timesfm-2.5-200m-transformers",
          "url": "https://huggingface.co/google/timesfm-2.5-200m-transformers"
        },
        "published_at": "2026-02-20T15:55:54+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9543894635975325,
          "semantic_score": 3.5083012521266936,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.262690715724226
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:google/timesfm-2.5-200m-transformers",
        "summary": "TimesFM (Time Series Foundation Model) is a pretrained decoder-only model for time-series forecasting. This repository contains the **Transformers** port of the official TimesFM 2.5 PyTorch release. **Resources and Technical Documentation**: * Original model: google/timesfm-2.5-200m-pytorch * Transformers model: google/timesfm-2.5-200m-transformers * Paper: A decoder-only foundation model for time-series forecasting * Transformers docs: TimesFM 2.5 This model is converted from the official TimesFM 2.5 PyTorch checkpoint and integrated into `transformers` as `Timesfm2P5ModelForPrediction`. The converted checkpoint preserves the original architecture and forecasting behavior, including: * patch-based inputs for time-series contexts * decoder-only self-attention stack",
        "title": "google/timesfm-2.5-200m-transformers"
      }
    ],
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-20T20:28:35.990832+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 63264,
          "likes": 592,
          "pipeline_tag": "automatic-speech-recognition"
        },
        "hf_model_id": "mistralai/voxtral-mini-4b-realtime-2602",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
            "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Voxtral-Mini-4B-Realtime-2602",
          "url": "https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"
        },
        "published_at": "2026-02-19T00:28:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8097051835881824,
          "semantic_score": 3.0389719903469086,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 11.648677173935091
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/voxtral-mini-4b-realtime-2602",
        "summary": "Voxtral Mini 4B Realtime 2602 is a **multilingual, realtime speech-transcription model** and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of **= 3600 / 0.8 = 45000`. In theory, you should be able to record with no limit; in practice, pre-allocations of RoPE parameters among other things limits `--max-model-len`. For the best user experience, we recommend to simply instantiate vLLM with the default parameters which will automatically set a maximum model length of 131072 (~ca. 3h). - We strongly recommend using websockets to set up audio streaming sessions. For more info on how to do so, check Usage. - We recommend using a delay of 480ms as we found it to be the sweet spot of performance and low latency. If, however, you want to adapt...",
        "title": "mistralai/Voxtral-Mini-4B-Realtime-2602"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-20T23:44:43.147032+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 16,
          "pipeline_tag": "image-feature-extraction"
        },
        "hf_model_id": "microsoft/latent-zoning-networks",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/latent-zoning-networks",
            "url": "https://huggingface.co/microsoft/latent-zoning-networks"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/latent-zoning-networks",
          "url": "https://huggingface.co/microsoft/latent-zoning-networks"
        },
        "published_at": "2026-02-21T00:14:28+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.988011676590804,
          "semantic_score": 1.8290148854255674,
          "tier_score": 2.0,
          "topic_score": 1.2000000000000002,
          "total_score": 7.8170265620163715
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/latent-zoning-networks",
        "summary": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label...",
        "title": "microsoft/latent-zoning-networks"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-20T20:28:36.910760+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 105189,
          "likes": 765,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        },
        "published_at": "2026-02-20T05:27:33+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9136398112945214,
          "semantic_score": 3.9173970222473145,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.631036833541836
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
        "title": "Qwen/Qwen3.5-397B-A17B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-20T20:28:36.910952+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 26823,
          "likes": 40,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        },
        "published_at": "2026-02-18T16:13:24+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.7823381849881171,
          "semantic_score": 3.957234025001526,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 14.539572209989643
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a...",
        "title": "Qwen/Qwen3.5-397B-A17B-FP8"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.17363",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.474223+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
          "url": "https://arxiv.org/abs/2602.17363"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
        "url": "https://arxiv.org/abs/2602.17363"
      },
      "published_at": "2026-02-19T13:45:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8557755839587832,
        "semantic_score": 3.338787281513214,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.094562865472
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17363",
      "summary": "Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy"
    },
    {
      "arxiv_id": "2602.16891",
      "authors": [
        "Hongwei Li",
        "Zhun Wang",
        "Qinrun Dai",
        "Yuzhou Nie",
        "Jinjun Peng",
        "Ruitong Liu",
        "Jingyang Zhang",
        "Kaijie Zhu",
        "Jingxuan He",
        "Lun Wang",
        "Yangruibo Ding",
        "Yueqi Chen",
        "Wenbo Guo",
        "Dawn Song"
      ],
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934187+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenSage: Self-programming Agent Generation Engine",
          "url": "https://arxiv.org/abs/2602.16891"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenSage: Self-programming Agent Generation Engine",
        "url": "https://arxiv.org/abs/2602.16891"
      },
      "published_at": "2026-02-18T21:16:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7989789199573009,
        "semantic_score": 4.165587991476059,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.96456691143336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16891",
      "summary": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with",
      "title": "OpenSage: Self-programming Agent Generation Engine"
    },
    {
      "arxiv_id": "2602.17259",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:33.177984+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
          "url": "https://arxiv.org/abs/2602.17259"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
        "url": "https://arxiv.org/abs/2602.17259"
      },
      "published_at": "2026-02-19T11:00:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.84604830876431,
        "semantic_score": 3.1939113974571227,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.93995970622143
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17259",
      "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
      "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment"
    },
    {
      "arxiv_id": "2602.15547",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.632505+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
          "url": "https://arxiv.org/abs/2602.15547"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
        "url": "https://arxiv.org/abs/2602.15547"
      },
      "published_at": "2026-02-17T12:50:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6980006116509437,
        "semantic_score": 4.399275106191635,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.897275717842575
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15547",
      "summary": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation"
    },
    {
      "arxiv_id": "2602.17469",
      "authors": [
        "Nusrat Jahan Lia",
        "Shubhashis Roy Dipta"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.909882+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
          "url": "https://arxiv.org/abs/2602.17469"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers",
        "url": "https://arxiv.org/abs/2602.17469"
      },
      "published_at": "2026-02-19T15:35:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8623278099284498,
        "semantic_score": 4.200939857959748,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.863267667888195
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17469",
      "summary": "The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7%",
      "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers"
    },
    {
      "arxiv_id": "2602.16835",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.459482+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NeST: Neuron Selective Tuning for LLM Safety",
          "url": "https://arxiv.org/abs/2602.16835"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NeST: Neuron Selective Tuning for LLM Safety",
        "url": "https://arxiv.org/abs/2602.16835"
      },
      "published_at": "2026-02-18T20:01:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7948026315614604,
        "semantic_score": 4.057654619216919,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.85245725077838
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16835",
      "summary": "Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.\n  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.",
      "title": "NeST: Neuron Selective Tuning for LLM Safety"
    },
    {
      "arxiv_id": "2602.17245",
      "authors": [
        "Linxi Jiang",
        "Rui Xi",
        "Zhijie Liu",
        "Shuo Chen",
        "Zhiqiang Lin",
        "Suman Nath"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942649+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
          "url": "https://arxiv.org/abs/2602.17245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
        "url": "https://arxiv.org/abs/2602.17245"
      },
      "published_at": "2026-02-19T10:50:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8454668504512323,
        "semantic_score": 4.262393414974213,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.667860265425446
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17245",
      "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web"
    },
    {
      "arxiv_id": "2602.16928",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934862+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
          "url": "https://arxiv.org/abs/2602.16928"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
        "url": "https://arxiv.org/abs/2602.16928"
      },
      "published_at": "2026-02-18T22:41:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8036820858314253,
        "semantic_score": 3.8141231536865234,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.61780523951795
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16928",
      "summary": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models"
    },
    {
      "arxiv_id": "2602.16704",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.631115+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Reinforced Fast Weights with Next-Sequence Prediction",
          "url": "https://arxiv.org/abs/2602.16704"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Reinforced Fast Weights with Next-Sequence Prediction",
        "url": "https://arxiv.org/abs/2602.16704"
      },
      "published_at": "2026-02-18T18:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7910738097403233,
        "semantic_score": 3.9596444725990296,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.55071828233935
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16704",
      "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction"
    },
    {
      "arxiv_id": "2602.17544",
      "authors": [
        "Shashank Aggarwal",
        "Ram Vikas Mishra",
        "Amit Awekar"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.946554+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
          "url": "https://arxiv.org/abs/2602.17544"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
        "url": "https://arxiv.org/abs/2602.17544"
      },
      "published_at": "2026-02-19T16:59:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8673707478005939,
        "semantic_score": 4.098165392875671,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.525536140676266
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17544",
      "summary": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusa",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability"
    },
    {
      "arxiv_id": "2602.17004",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.467924+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Arcee Trinity Large Technical Report",
          "url": "https://arxiv.org/abs/2602.17004"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Arcee Trinity Large Technical Report",
        "url": "https://arxiv.org/abs/2602.17004"
      },
      "published_at": "2026-02-19T01:58:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8147996063883935,
        "semantic_score": 3.721488332748413,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.33628793913681
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17004",
      "summary": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
      "title": "Arcee Trinity Large Technical Report"
    },
    {
      "arxiv_id": "2602.17560",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.947030+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
          "url": "https://arxiv.org/abs/2602.17560"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
        "url": "https://arxiv.org/abs/2602.17560"
      },
      "published_at": "2026-02-19T17:13:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8682475965770587,
        "semantic_score": 4.546582609415054,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.31483020599211
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17560",
      "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment"
    },
    {
      "arxiv_id": "2602.17365",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.630353+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Computer-Using World Model",
          "url": "https://arxiv.org/abs/2602.17365"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Computer-Using World Model",
        "url": "https://arxiv.org/abs/2602.17365"
      },
      "published_at": "2026-02-19T13:48:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8559598332564524,
        "semantic_score": 3.657119929790497,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.31307976304695
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17365",
      "summary": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "title": "Computer-Using World Model"
    },
    {
      "arxiv_id": "2602.16138",
      "authors": [
        "Parsa Madinei",
        "Srijita Karmakar",
        "Russell Cohen Hoffing",
        "Felix Gervitz",
        "Miguel P. Eckstein"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119995+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.16138"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.16138"
      },
      "published_at": "2026-02-18T02:06:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7376486773020487,
        "semantic_score": 3.5535216450691225,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.29117032237117
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16138",
      "summary": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%",
      "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models"
    },
    {
      "arxiv_id": "2602.16682",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "deepmind"
      ],
      "first_seen_at": "2026-02-20T20:28:38.631684+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Situated Awareness in the Real World",
          "url": "https://arxiv.org/abs/2602.16682"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Situated Awareness in the Real World",
        "url": "https://arxiv.org/abs/2602.16682"
      },
      "published_at": "2026-02-18T18:22:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7894036992904976,
        "semantic_score": 2.7860475063323973,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.2754512056229
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16682",
      "summary": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "title": "Learning Situated Awareness in the Real World"
    },
    {
      "arxiv_id": "2602.16756",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.629134+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
          "url": "https://arxiv.org/abs/2602.16756"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
        "url": "https://arxiv.org/abs/2602.16756"
      },
      "published_at": "2026-02-18T09:41:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7613522560154142,
        "semantic_score": 4.559647023677826,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.22099927969324
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16756",
      "summary": "We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.",
      "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist"
    },
    {
      "arxiv_id": "2602.16346",
      "authors": [
        "Nivya Talokar, Ayush K Tarun, Murari Mandal, Maksym Andriushchenko, Antoine Bosselut"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.454887+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
          "url": "https://arxiv.org/abs/2602.16346"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",
        "url": "https://arxiv.org/abs/2602.16346"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.162283420562744,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.04428150702468
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16346",
      "summary": "arXiv:2602.16346v2 Announce Type: replace \nAbstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenari",
      "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents"
    },
    {
      "arxiv_id": "2601.01569",
      "authors": [
        "Maohao Ran, Zhenglin Wan, Cooper Lin, Yanting Zhang, Hongyu Xin, Hongwei Fan, Yibo Xu, Beier Luo, Yaxin Zhou, Wangbo Zhao, Lijie Yang, Lang Feng, Fuchao Yang, Jingxuan Wu, Yiqiao Huang, Chendong Ma, Dailing Jiang, Jianbo Deng, Sirui Han, Yang You, Bo An, Yike Guo, Jun Song"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.915542+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
          "url": "https://arxiv.org/abs/2601.01569"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
        "url": "https://arxiv.org/abs/2601.01569"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.04437130689621,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.926369393358144
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2601.01569",
      "summary": "arXiv:2601.01569v3 Announce Type: replace-cross \nAbstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ``LLM-as-Text-Generator'' to ``LLM-as-Runtime-Operator.'' CaveAgent introduces a dual-stream architecture that inverts the conventional paradigm: rather than treating the LLM's text context as the primary workspace with tools as auxiliary, CaveAgent elevates the persistent Python runtime as the central locus of state, with a lightweight semantic stream serving as its orchestrator. Beyond leveraging code generation to resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, CaveAgent introduces \\textit{Stateful Runtime Management}: it injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, dat",
      "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators"
    },
    {
      "arxiv_id": "2602.16493",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119399+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "MMA: Multimodal Memory Agent",
          "url": "https://arxiv.org/abs/2602.16493"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "MMA: Multimodal Memory Agent",
        "url": "https://arxiv.org/abs/2602.16493"
      },
      "published_at": "2026-02-18T14:30:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.7767721486529802,
        "semantic_score": 3.4415493249893188,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.7783214736423
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16493",
      "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "title": "MMA: Multimodal Memory Agent"
    },
    {
      "arxiv_id": "2602.17223",
      "authors": [
        "Arka Pal",
        "Louai Zahran",
        "William Gvozdjak",
        "Akilesh Potti",
        "Micah Goldblum"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.471979+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
          "url": "https://arxiv.org/abs/2602.17223"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
        "url": "https://arxiv.org/abs/2602.17223"
      },
      "published_at": "2026-02-19T10:15:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8434134154453646,
        "semantic_score": 3.949293547868729,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.692706963314095
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17223",
      "summary": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely ",
      "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs"
    },
    {
      "arxiv_id": "2602.15927",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.631442+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
          "url": "https://arxiv.org/abs/2602.15927"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
        "url": "https://arxiv.org/abs/2602.15927"
      },
      "published_at": "2026-02-17T18:34:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7148832799967164,
        "semantic_score": 4.031131148338318,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.646014428335036
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15927",
      "summary": "Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations"
    },
    {
      "arxiv_id": "2602.16008",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.631334+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "MAEB: Massive Audio Embedding Benchmark",
          "url": "https://arxiv.org/abs/2602.16008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "MAEB: Massive Audio Embedding Benchmark",
        "url": "https://arxiv.org/abs/2602.16008"
      },
      "published_at": "2026-02-17T21:00:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7221615838777303,
        "semantic_score": 3.51650093793869,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.598662521816422
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16008",
      "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "title": "MAEB: Massive Audio Embedding Benchmark"
    },
    {
      "arxiv_id": "2508.08800",
      "authors": [
        "David Mguni, Yaqi Sun, Haojun Chen, Wanrong Yang, Amir Darabi, Larry Olanrewaju Orimoloye, Yaodong Yang"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.001073+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints",
          "url": "https://arxiv.org/abs/2508.08800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints",
        "url": "https://arxiv.org/abs/2508.08800"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.06682613492012,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.508824221382056
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2508.08800",
      "summary": "arXiv:2508.08800v2 Announce Type: replace \nAbstract: We study robustness to agent malfunctions in cooperative multi-agent reinforcement learning (MARL), a failure mode that is critical in practice yet underexplored in existing theory. We introduce MARTA, a plug-and-play robustness layer that augments standard MARL algorithms with a Switcher-Adversary mechanism which selectively induces malfunctions in performance-critical states. This formulation defines a fault-switching $(N+2)$-player Markov game in which the Switcher chooses when and which agent fails, and the Adversary controls the resulting faulty behaviour via random or worst-case policies. We develop a Q-learning-type scheme and show that the associated Bellman operator is a contraction, yielding existence and uniqueness of the minimax value, convergence to a Markov perfect equilibrium. MARTA integrates seamlessly with MARL algorithms without architectural modification and consistently improves robustness across Traffic Junction",
      "title": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints"
    },
    {
      "arxiv_id": "2602.15922",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.632153+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "World Action Models are Zero-shot Policies",
          "url": "https://arxiv.org/abs/2602.15922"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "World Action Models are Zero-shot Policies",
        "url": "https://arxiv.org/abs/2602.15922"
      },
      "published_at": "2026-02-17T15:04:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.92,
        "llm_relevance_score": 20.240000000000002,
        "recency_score": 0.7044870709331551,
        "semantic_score": 2.327925592660904,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.47241266359406
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15922",
      "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "title": "World Action Models are Zero-shot Policies"
    },
    {
      "arxiv_id": "2602.16699",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.919908+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16699"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16699"
      },
      "published_at": "2026-02-18T18:46:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7906856928331291,
        "semantic_score": 3.7389176189899445,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.429603311823072
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16699",
      "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents"
    },
    {
      "arxiv_id": "2602.16837",
      "authors": [
        "Hanna Herasimchyk, Robin Labryga, Tomislav Prusina, S\\\"oren Laue"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.459590+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Residual-Aware Theory of Position Bias in Transformers",
          "url": "https://arxiv.org/abs/2602.16837"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Residual-Aware Theory of Position Bias in Transformers",
        "url": "https://arxiv.org/abs/2602.16837"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7403719902038572,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.422370076665793
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16837",
      "summary": "arXiv:2602.16837v1 Announce Type: new \nAbstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.",
      "title": "A Residual-Aware Theory of Position Bias in Transformers"
    },
    {
      "arxiv_id": "2602.17053",
      "authors": [
        "Yunseok Han, Yejoon Lee, Jaeyoung Do"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938535+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
          "url": "https://arxiv.org/abs/2602.17053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models",
        "url": "https://arxiv.org/abs/2602.17053"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9238059639930727,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.365804050455008
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17053",
      "summary": "arXiv:2602.17053v1 Announce Type: cross \nAbstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-famil",
      "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models"
    },
    {
      "arxiv_id": "2602.16438",
      "authors": [
        "Eva Paraschou",
        "Line Harder Clemmensen",
        "Sneha Das"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.179604+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
          "url": "https://arxiv.org/abs/2602.16438"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
        "url": "https://arxiv.org/abs/2602.16438"
      },
      "published_at": "2026-02-18T13:19:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.7729301861525145,
        "semantic_score": 4.32703275680542,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.339962942957932
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16438",
      "summary": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In ",
      "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment"
    },
    {
      "arxiv_id": "2602.16485",
      "authors": [
        "Jeffrey T. H. Wong",
        "Zixi Zhang",
        "Junyi Liu",
        "Yiren Zhao"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.436490+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
          "url": "https://arxiv.org/abs/2602.16485"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
        "url": "https://arxiv.org/abs/2602.16485"
      },
      "published_at": "2026-02-18T14:19:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7761484641348497,
        "semantic_score": 3.6500952541828156,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.326243718317663
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16485",
      "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models wit",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling"
    },
    {
      "arxiv_id": "2602.16301",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.631802+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Multi-agent cooperation through in-context co-player inference",
          "url": "https://arxiv.org/abs/2602.16301"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Multi-agent cooperation through in-context co-player inference",
        "url": "https://arxiv.org/abs/2602.16301"
      },
      "published_at": "2026-02-18T09:31:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7608166780792232,
        "semantic_score": 3.6619945764541626,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.322811254533384
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16301",
      "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.",
      "title": "Multi-agent cooperation through in-context co-player inference"
    },
    {
      "arxiv_id": "2411.02317",
      "authors": [
        "Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.907967+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Defining and Evaluating Physical Safety for Large Language Models",
          "url": "https://arxiv.org/abs/2411.02317"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Defining and Evaluating Physical Safety for Large Language Models",
        "url": "https://arxiv.org/abs/2411.02317"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.454601615667343,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.236599702129276
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2411.02317",
      "summary": "arXiv:2411.02317v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better",
      "title": "Defining and Evaluating Physical Safety for Large Language Models"
    },
    {
      "arxiv_id": "2602.15868",
      "authors": [
        "Magnus Boman"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.904121+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning",
          "url": "https://arxiv.org/abs/2602.15868"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning",
        "url": "https://arxiv.org/abs/2602.15868"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.39552920460701,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.177527291068944
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.15868",
      "summary": "arXiv:2602.15868v2 Announce Type: replace \nAbstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.",
      "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning"
    },
    {
      "arxiv_id": "2602.12414",
      "authors": [
        "Maximilian Idahl, Benedikt Droste, Bj\\\"orn Pl\\\"uster, Jan Philipp Harries"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.903766+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale",
          "url": "https://arxiv.org/abs/2602.12414"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale",
        "url": "https://arxiv.org/abs/2602.12414"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.393800842761993,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.175798929223927
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.12414",
      "summary": "arXiv:2602.12414v2 Announce Type: replace \nAbstract: Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from Fin",
      "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale"
    },
    {
      "arxiv_id": "2602.17433",
      "authors": [
        "Francesco Ortu",
        "Joeun Yook",
        "Punya Syon Pandey",
        "Keenan Samway",
        "Bernhard Schlkopf",
        "Alberto Cazzaniga",
        "Rada Mihalcea",
        "Zhijing Jin"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:30.514248+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
          "url": "https://arxiv.org/abs/2602.17433"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
        "url": "https://arxiv.org/abs/2602.17433"
      },
      "published_at": "2026-02-19T15:05:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8605301757728075,
        "semantic_score": 4.445303362607956,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.105833538380764
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17433",
      "summary": "Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \\textsc{\\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each eve",
      "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models"
    },
    {
      "arxiv_id": "2510.19771",
      "authors": [
        "Gil Pasternak, Dheeraj Rajagopal, Julia White, Dhruv Atreja, Matthew Thomas, George Hurn-Maloney, Ash Lewis"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913020+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
          "url": "https://arxiv.org/abs/2510.19771"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
        "url": "https://arxiv.org/abs/2510.19771"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.663826495409012,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.105824581870948
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2510.19771",
      "summary": "arXiv:2510.19771v3 Announce Type: replace \nAbstract: LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Addi",
      "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents"
    },
    {
      "arxiv_id": "2510.27118",
      "authors": [
        "Andy Yang, Anej Svete, Jiaoda Li, Anthony Widjaja Lin, Jonathan Rawski, Ryan Cotterell, David Chiang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.890668+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Probability Distributions Computed by Hard-Attention Transformers",
          "url": "https://arxiv.org/abs/2510.27118"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Probability Distributions Computed by Hard-Attention Transformers",
        "url": "https://arxiv.org/abs/2510.27118"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8598942339420317,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.101892320403966
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2510.27118",
      "summary": "arXiv:2510.27118v2 Announce Type: replace \nAbstract: Most expressivity results for transformers treat them as language recognizers (which accept or reject strings), and not as they are used in practice, as language models (which generate strings autoregressively and probabilistically). We characterize the probability distributions that transformer language models can express. We show that making transformer language recognizers autoregressive can sometimes increase their expressivity, and that making them probabilistic can break equivalences that hold in the non-probabilistic case. Our overall contribution is to tease apart what functions transformers are capable of expressing, in their most common use-case as language models.",
      "title": "Probability Distributions Computed by Hard-Attention Transformers"
    },
    {
      "arxiv_id": "2602.17170",
      "authors": [
        "Chuting Yu",
        "Hang Li",
        "Joel Mackenzie",
        "Teerapong Leelanupab"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:30.515843+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
          "url": "https://arxiv.org/abs/2602.17170"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment",
        "url": "https://arxiv.org/abs/2602.17170"
      },
      "published_at": "2026-02-19T08:37:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8376639198934791,
        "semantic_score": 4.4447356402874,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.082399560180882
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17170",
      "summary": "Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across ",
      "title": "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment"
    },
    {
      "arxiv_id": "2506.20642",
      "authors": [
        "Chao Wan, Albert Gong, Mihir Mishra, Carl-Leander Henneking, Claas Beger, Kilian Q. Weinberger"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.889581+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "$\\pi$-CoT: Prolog-Initialized Chain-of-Thought Prompting for Multi-Hop Question-Answering",
          "url": "https://arxiv.org/abs/2506.20642"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "$\\pi$-CoT: Prolog-Initialized Chain-of-Thought Prompting for Multi-Hop Question-Answering",
        "url": "https://arxiv.org/abs/2506.20642"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.6122371673583986,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.054235253820334
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2506.20642",
      "summary": "arXiv:2506.20642v2 Announce Type: replace \nAbstract: Chain-of-Thought (CoT) prompting significantly enhances large language models' (LLMs) problem-solving capabilities, but still struggles with complex multi-hop questions, often falling into circular reasoning patterns or deviating from the logical path entirely. This limitation is particularly acute in retrieval-augmented generation (RAG) settings, where obtaining the right context is critical. We introduce Prolog-Initialized Chain-of-Thought ($\\pi$-CoT), a novel prompting strategy that combines logic programming's structural rigor with language models' flexibility. $\\pi$-CoT reformulates multi-hop questions into Prolog queries decomposed as single-hop sub-queries. These are resolved sequentially, producing intermediate artifacts, with which we initialize the subsequent CoT reasoning procedure. Extensive experiments demonstrate that $\\pi$-CoT significantly outperforms standard RAG and in-context CoT on multi-hop question-answering ben",
      "title": "$\\pi$-CoT: Prolog-Initialized Chain-of-Thought Prompting for Multi-Hop Question-Answering"
    },
    {
      "arxiv_id": "2602.17622",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Ruozhao Yang",
        "Xiaofei Xie",
        "Jie Zhang",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:30.512552+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
          "url": "https://arxiv.org/abs/2602.17622"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
        "url": "https://arxiv.org/abs/2602.17622"
      },
      "published_at": "2026-02-19T18:42:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8736264219357608,
        "semantic_score": 3.472192168235779,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.04581859017154
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17622",
      "summary": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of toolin",
      "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?"
    },
    {
      "arxiv_id": "2602.17483",
      "authors": [
        "Dimitri Staufer",
        "Kirsten Morehouse"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945413+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
          "url": "https://arxiv.org/abs/2602.17483"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data",
        "url": "https://arxiv.org/abs/2602.17483"
      },
      "published_at": "2026-02-19T15:53:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8634223828205927,
        "semantic_score": 4.342266637086868,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.00568901990746
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17483",
      "summary": "Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative stud",
      "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data"
    },
    {
      "arxiv_id": "2505.16928",
      "authors": [
        "Bosung Kim, Prithviraj Ammanabrolu"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.909918+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
          "url": "https://arxiv.org/abs/2505.16928"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning",
        "url": "https://arxiv.org/abs/2505.16928"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.111594593524933,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.993592679986868
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2505.16928",
      "summary": "arXiv:2505.16928v3 Announce Type: replace-cross \nAbstract: We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provid",
      "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning"
    },
    {
      "arxiv_id": "2510.15297",
      "authors": [
        "Luca Belli, Kate Bentley, Will Alexander, Emily Ward, Matt Hawrilenko, Kelly Johnston, Mill Brown, Adam Chekroud"
      ],
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912885+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "VERA-MH Concept Paper",
          "url": "https://arxiv.org/abs/2510.15297"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "VERA-MH Concept Paper",
        "url": "https://arxiv.org/abs/2510.15297"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1020828008651735,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.98408088732711
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2510.15297",
      "summary": "arXiv:2510.15297v3 Announce Type: replace-cross \nAbstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.\n  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.\n  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-",
      "title": "VERA-MH Concept Paper"
    },
    {
      "arxiv_id": "2602.15338",
      "authors": [
        "Edward Chen",
        "Sanmi Koyejo",
        "Carlos Guestrin"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.180997+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Discovering Implicit Large Language Model Alignment Objectives",
          "url": "https://arxiv.org/abs/2602.15338"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Discovering Implicit Large Language Model Alignment Objectives",
        "url": "https://arxiv.org/abs/2602.15338"
      },
      "published_at": "2026-02-17T03:58:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6726877950548038,
        "semantic_score": 4.510782331228256,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.983470126283063
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15338",
      "summary": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes a",
      "title": "Discovering Implicit Large Language Model Alignment Objectives"
    },
    {
      "arxiv_id": "2602.16317",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.631227+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
          "url": "https://arxiv.org/abs/2602.16317"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
        "url": "https://arxiv.org/abs/2602.16317"
      },
      "published_at": "2026-02-18T09:54:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7620451902226849,
        "semantic_score": 2.4205544233322143,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.982599613554903
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16317",
      "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
      "title": "CADEvolve: Creating Realistic CAD via Program Evolution"
    },
    {
      "arxiv_id": "2602.16173",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.630928+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Personalized Agents from Human Feedback",
          "url": "https://arxiv.org/abs/2602.16173"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Personalized Agents from Human Feedback",
        "url": "https://arxiv.org/abs/2602.16173"
      },
      "published_at": "2026-02-18T04:18:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.7444613608205549,
        "semantic_score": 3.326549208164215,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.971010568984767
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16173",
      "summary": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "title": "Learning Personalized Agents from Human Feedback"
    },
    {
      "arxiv_id": "2602.17588",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.911360+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Modeling Distinct Human Interaction in Web Agents",
          "url": "https://arxiv.org/abs/2602.17588"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Modeling Distinct Human Interaction in Web Agents",
        "url": "https://arxiv.org/abs/2602.17588"
      },
      "published_at": "2026-02-19T18:11:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8717356138034011,
        "semantic_score": 3.1736801862716675,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.945415800075068
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17588",
      "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "title": "Modeling Distinct Human Interaction in Web Agents"
    },
    {
      "arxiv_id": "2602.17046",
      "authors": [
        "Uria Franko"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938206+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs",
          "url": "https://arxiv.org/abs/2602.17046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs",
        "url": "https://arxiv.org/abs/2602.17046"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.150485038757324,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.932483125219257
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17046",
      "summary": "arXiv:2602.17046v1 Announce Type: new \nAbstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evalua",
      "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs"
    },
    {
      "arxiv_id": "2602.15823",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.628874+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
          "url": "https://arxiv.org/abs/2602.15823"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
        "url": "https://arxiv.org/abs/2602.15823"
      },
      "published_at": "2026-02-17T18:58:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7160301635006893,
        "semantic_score": 4.371560156345367,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.88759031984606
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15823",
      "summary": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.",
      "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing"
    },
    {
      "arxiv_id": "2602.17262",
      "authors": [
        "Kensuke Okada",
        "Yui Furukawa",
        "Kyosuke Bunji"
      ],
      "categories": [
        "cs.CL",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.907876+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
          "url": "https://arxiv.org/abs/2602.17262"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study",
        "url": "https://arxiv.org/abs/2602.17262"
      },
      "published_at": "2026-02-19T11:07:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8464381291291075,
        "semantic_score": 3.773148161172867,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.859586290301973
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17262",
      "summary": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-b",
      "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study"
    },
    {
      "arxiv_id": "2602.16554",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.436140+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
          "url": "https://arxiv.org/abs/2602.16554"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
        "url": "https://arxiv.org/abs/2602.16554"
      },
      "published_at": "2026-02-18T15:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7813138482702674,
        "semantic_score": 2.2892195105552675,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 30.820533358825536
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16554",
      "summary": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three p",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation"
    },
    {
      "arxiv_id": "2602.17127",
      "authors": [
        "Dusan Bosnjakovic"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.907494+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
          "url": "https://arxiv.org/abs/2602.17127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
        "url": "https://arxiv.org/abs/2602.17127"
      },
      "published_at": "2026-02-19T06:56:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8317899398121766,
        "semantic_score": 4.141481012105942,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.77327095191812
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17127",
      "summary": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individu",
      "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI"
    },
    {
      "arxiv_id": "2602.17162",
      "authors": [
        "Ariel Larey",
        "Elay Dahan",
        "Amit Bleiweiss",
        "Raizy Kellerman",
        "Guy Leib",
        "Omri Nayshool",
        "Dan Ofer",
        "Tal Zinger",
        "Dan Dominissini",
        "Gideon Rechavi",
        "Nicole Bussola",
        "Simon Lee",
        "Shane O'Connell",
        "Dung Hoang",
        "Marissa Wirth",
        "Alexander W. Charney",
        "Nati Daniel",
        "Yoli Shavit"
      ],
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.940899+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
          "url": "https://arxiv.org/abs/2602.17162"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures",
        "url": "https://arxiv.org/abs/2602.17162"
      },
      "published_at": "2026-02-19T08:20:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.836704646340696,
        "semantic_score": 4.097121894359589,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.733826540700285
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17162",
      "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with",
      "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures"
    },
    {
      "arxiv_id": "2602.17234",
      "authors": [
        "Zeyu Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942428+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
          "url": "https://arxiv.org/abs/2602.17234"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
        "url": "https://arxiv.org/abs/2602.17234"
      },
      "published_at": "2026-02-19T10:28:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8441253458180366,
        "semantic_score": 3.6456489801406864,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.729774325958722
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17234",
      "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes ",
      "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting"
    },
    {
      "arxiv_id": "2602.16977",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.467463+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Fail-Closed Alignment for Large Language Models",
          "url": "https://arxiv.org/abs/2602.16977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Fail-Closed Alignment for Large Language Models",
        "url": "https://arxiv.org/abs/2602.16977"
      },
      "published_at": "2026-02-19T00:33:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8099901299830007,
        "semantic_score": 4.5284746289253235,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.698464758908322
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16977",
      "summary": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial ",
      "title": "Fail-Closed Alignment for Large Language Models"
    },
    {
      "arxiv_id": "2602.16855",
      "authors": [
        "Haiyang Xu, Xi Zhang, Haowei Liu, Junyang Wang, Zhaozai Zhu, Shengjie Zhou, Xuhao Hu, Feiyu Gao, Junjie Cao, Zihua Wang, Zhiyuan Chen, Jitong Liao, Qi Zheng, Jiahui Zeng, Ze Xu, Shuai Bai, Junyang Lin, Jingren Zhou, Ming Yan"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.933706+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
          "url": "https://arxiv.org/abs/2602.16855"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
        "url": "https://arxiv.org/abs/2602.16855"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.242189335823059,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.684187422284992
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16855",
      "summary": "arXiv:2602.16855v1 Announce Type: cross \nAbstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order ",
      "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents"
    },
    {
      "arxiv_id": "2602.17443",
      "authors": [
        "Adib Sakhawat, Fardeen Sadab, Rakin Shahriar"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.909504+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
          "url": "https://arxiv.org/abs/2602.17443"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue",
        "url": "https://arxiv.org/abs/2602.17443"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.877696490287781,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.659694576749715
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17443",
      "summary": "arXiv:2602.17443v1 Announce Type: new \nAbstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured \"20 Questions\" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint A",
      "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue"
    },
    {
      "arxiv_id": "2602.15799",
      "authors": [
        "Max Springer",
        "Chung Peng Lee",
        "Blossom Metevier",
        "Jane Castleman",
        "Bohdan Turbal",
        "Hayoung Jung",
        "Zeyu Shen",
        "Aleksandra Korolova"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.180277+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
          "url": "https://arxiv.org/abs/2602.15799"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
        "url": "https://arxiv.org/abs/2602.15799"
      },
      "published_at": "2026-02-17T18:39:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7150951286482992,
        "semantic_score": 4.1113327741622925,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.626427902810594
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15799",
      "summary": "Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this thro",
      "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety"
    },
    {
      "arxiv_id": "2602.16943",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.935548+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
          "url": "https://arxiv.org/abs/2602.16943"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
        "url": "https://arxiv.org/abs/2602.16943"
      },
      "published_at": "2026-02-18T23:17:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.805707792507839,
        "semantic_score": 4.0147406458854675,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.62044843839331
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16943",
      "summary": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and too",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents"
    },
    {
      "arxiv_id": "2602.16839",
      "authors": [
        "Zeliang Zhang, Xiaodong Liu, Hao Cheng, Hao Sun, Chenliang Xu, Jianfeng Gao"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.459703+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
          "url": "https://arxiv.org/abs/2602.16839"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding",
        "url": "https://arxiv.org/abs/2602.16839"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.797883480787277,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.57988156724921
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16839",
      "summary": "arXiv:2602.16839v1 Announce Type: cross \nAbstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consiste",
      "title": "Training Large Reasoning Models Efficiently via Progressive Thought Encoding"
    },
    {
      "arxiv_id": "2602.16984",
      "authors": [
        "Vishal Srivastava"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.936716+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
          "url": "https://arxiv.org/abs/2602.16984"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning",
        "url": "https://arxiv.org/abs/2602.16984"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7708295583724976,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.55282764483443
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16984",
      "summary": "arXiv:2602.16984v1 Announce Type: new \nAbstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a su",
      "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning"
    },
    {
      "arxiv_id": "2412.18899",
      "authors": [
        "Masahiro Sato"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.908407+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "GAI: Generative Agents for Innovation",
          "url": "https://arxiv.org/abs/2412.18899"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "GAI: Generative Agents for Innovation",
        "url": "https://arxiv.org/abs/2412.18899"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.661802977323532,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.543801063785466
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2412.18899",
      "summary": "arXiv:2412.18899v3 Announce Type: replace \nAbstract: This study examines whether collective reasoning among generative agents can facilitate novel and coherent thinking that leads to innovation. To achieve this, it proposes GAI, a new LLM-empowered framework designed for reflection and interaction among multiple generative agents to replicate the process of innovation. The core of the GAI framework lies in an architecture that dynamically processes the internal states of agents and a dialogue scheme specifically tailored to facilitate analogy-driven innovation. The framework's functionality is evaluated using Dyson's invention of the bladeless fan as a case study, assessing the extent to which the core ideas of the innovation can be replicated through a set of fictional technical documents. The experimental results demonstrate that models with internal states significantly outperformed those without, achieving higher average scores and lower variance. Notably, the model with five heter",
      "title": "GAI: Generative Agents for Innovation"
    },
    {
      "arxiv_id": "2602.17546",
      "authors": [
        "Jyotin Goel, Souvik Maji, Pratik Mazumder"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.476443+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
          "url": "https://arxiv.org/abs/2602.17546"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning",
        "url": "https://arxiv.org/abs/2602.17546"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.75814688205719,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.540144968519122
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17546",
      "summary": "arXiv:2602.17546v1 Announce Type: new \nAbstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that h",
      "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning"
    },
    {
      "arxiv_id": "2512.03870",
      "authors": [
        "Hongzhan Lin, Zhiqi Bai, Xinmiao Zhang, Sen Yang, Xiang Li, Siran Yang, Yunlong Xu, Jiaheng Liu, Yongchi Zhao, Jiamang Wang, Yuchi Xu, Wenbo Su, Bo Zheng"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.891350+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
          "url": "https://arxiv.org/abs/2512.03870"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
        "url": "https://arxiv.org/abs/2512.03870"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.382254832983017,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.50425291944495
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2512.03870",
      "summary": "arXiv:2512.03870v3 Announce Type: replace \nAbstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-",
      "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
    },
    {
      "arxiv_id": "2602.17419",
      "authors": [
        "Xiaomeng Peng",
        "Xilang Huang",
        "Seon Han Choi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.298217+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17419"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17419"
      },
      "published_at": "2026-02-19T14:50:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8596820156638156,
        "semantic_score": 4.281809931993484,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.5014919476573
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17419",
      "summary": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial ",
      "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models"
    },
    {
      "arxiv_id": "2602.16901",
      "authors": [
        "Tanqiu Jiang",
        "Yuhui Wang",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934415+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
          "url": "https://arxiv.org/abs/2602.16901"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks",
        "url": "https://arxiv.org/abs/2602.16901"
      },
      "published_at": "2026-02-18T21:30:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7997477519954818,
        "semantic_score": 3.8975977897644043,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.497345541759888
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16901",
      "summary": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack ",
      "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks"
    },
    {
      "arxiv_id": "2602.17616",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.948198+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
          "url": "https://arxiv.org/abs/2602.17616"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
        "url": "https://arxiv.org/abs/2602.17616"
      },
      "published_at": "2026-02-19T18:40:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8735162144432188,
        "semantic_score": 3.7883182764053345,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.461834490848553
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17616",
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplifi",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs"
    },
    {
      "arxiv_id": "2602.16520",
      "authors": [
        "Doron Shavit"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.436316+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
          "url": "https://arxiv.org/abs/2602.16520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
        "url": "https://arxiv.org/abs/2602.16520"
      },
      "published_at": "2026-02-18T15:07:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7787471530013175,
        "semantic_score": 3.8662858664989472,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.445033019500265
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16520",
      "summary": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries work",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents"
    },
    {
      "arxiv_id": "2602.16935",
      "authors": [
        "Justin Albrethsen",
        "Yash Datta",
        "Kunal Kumar",
        "Sharath Rajasekar"
      ],
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.935327+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
          "url": "https://arxiv.org/abs/2602.16935"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
        "url": "https://arxiv.org/abs/2602.16935"
      },
      "published_at": "2026-02-18T22:57:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8046156057408511,
        "semantic_score": 4.248390400409699,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.41300600615055
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16935",
      "summary": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext disca",
      "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs"
    },
    {
      "arxiv_id": "2602.16813",
      "authors": [
        "Chanhyuk Lee, Jaehoon Yoo, Manan Agarwal, Sheel Shah, Jerry Huang, Aditi Raghunathan, Seunghoon Hong, Nicholas M. Boffi, Jinwoo Kim"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.923338+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "One-step Language Modeling via Continuous Denoising",
          "url": "https://arxiv.org/abs/2602.16813"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "One-step Language Modeling via Continuous Denoising",
        "url": "https://arxiv.org/abs/2602.16813"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.6053466737270354,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.38734476018897
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16813",
      "summary": "arXiv:2602.16813v1 Announce Type: new \nAbstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the",
      "title": "One-step Language Modeling via Continuous Denoising"
    },
    {
      "arxiv_id": "2505.17508",
      "authors": [
        "Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.910038+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
          "url": "https://arxiv.org/abs/2505.17508"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
        "url": "https://arxiv.org/abs/2505.17508"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.6394970417022705,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.32149512816421
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2505.17508",
      "summary": "arXiv:2505.17508v4 Announce Type: replace-cross \nAbstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to ",
      "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning"
    },
    {
      "arxiv_id": "2602.17497",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.475424+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
          "url": "https://arxiv.org/abs/2602.17497"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
        "url": "https://arxiv.org/abs/2602.17497"
      },
      "published_at": "2026-02-19T16:13:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8646214130165994,
        "semantic_score": 3.6488131284713745,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.313434541487975
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17497",
      "summary": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large languag",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models"
    },
    {
      "arxiv_id": "2511.00794",
      "authors": [
        "Yan Sun, Jia Guo, Stanley Kok, Zihao Wang, Zujie Wen, Zhiqiang Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913519+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration",
          "url": "https://arxiv.org/abs/2511.00794"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration",
        "url": "https://arxiv.org/abs/2511.00794"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.626101344823837,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.308099431285775
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2511.00794",
      "summary": "arXiv:2511.00794v3 Announce Type: replace \nAbstract: Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mat",
      "title": "Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration"
    },
    {
      "arxiv_id": "2602.16832",
      "authors": [
        "Priyaranjan Pattnayak, Sanchari Chowdhuri"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.933275+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
          "url": "https://arxiv.org/abs/2602.16832"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
        "url": "https://arxiv.org/abs/2602.16832"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.62108656167984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.303084648141777
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16832",
      "summary": "arXiv:2602.16832v1 Announce Type: cross \nAbstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.\n  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comp",
      "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages"
    },
    {
      "arxiv_id": "2412.06106",
      "authors": [
        "Kaleel Mahmood, Shaoyi Huang"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.445704+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Efficient Context Propagating Perceiver Architectures for Auto-Regressive Language Modeling",
          "url": "https://arxiv.org/abs/2412.06106"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Efficient Context Propagating Perceiver Architectures for Auto-Regressive Language Modeling",
        "url": "https://arxiv.org/abs/2412.06106"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.4623122692108153,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.24431035567275
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2412.06106",
      "summary": "arXiv:2412.06106v2 Announce Type: replace \nAbstract: One of the key challenges in Transformer architectures is the quadratic complexity of the attention mechanism, which limits the efficient processing of long sequences. Many recent research works have attempted to provide a reduction from the $O(n^2)$ time complexity of attention to semi-linear complexity. However, it remains an unsolved problem in the sense of maintaining high performance when complexity is reduced. One of the important works in this respect is the Perceiver class of architectures that have demonstrated excellent performance, while reducing the computation complexity. In this paper, we use the PerceiverAR as a basis and explore the design space of different trade-offs between preserving context and reducing attention complexity. To this end, we develop four new architectural paradigms, the best performing of which we denote as the Efficient Context propagating Perceiver (ECP). ECP has two major advantages over the Pe",
      "title": "Efficient Context Propagating Perceiver Architectures for Auto-Regressive Language Modeling"
    },
    {
      "arxiv_id": "2602.17554",
      "authors": [
        "Corinna Cortes, Mehryar Mohri, Yutao Zhong"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.476674+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "A Theoretical Framework for Modular Learning of Robust Generative Models",
          "url": "https://arxiv.org/abs/2602.17554"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "A Theoretical Framework for Modular Learning of Robust Generative Models",
        "url": "https://arxiv.org/abs/2602.17554"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.556770890951157,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.238768977413095
      },
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.17554",
      "summary": "arXiv:2602.17554v1 Announce Type: cross \nAbstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can ",
      "title": "A Theoretical Framework for Modular Learning of Robust Generative Models"
    },
    {
      "arxiv_id": "2602.16736",
      "authors": [
        "Raymond Jay Martin II"
      ],
      "categories": [
        "cs.OS",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921212+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "The Compute ICE-AGE: Invariant Compute Envelope under Addressable Graph Evolution",
          "url": "https://arxiv.org/abs/2602.16736"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "The Compute ICE-AGE: Invariant Compute Envelope under Addressable Graph Evolution",
        "url": "https://arxiv.org/abs/2602.16736"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.352548396587372,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.234546483049307
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16736",
      "summary": "arXiv:2602.16736v1 Announce Type: cross \nAbstract: This paper presents empirical results from a production-grade C++ implementation of a deterministic semantic state substrate derived from prior formal work on Bounded Local Generator Classes (Martin, 2026). The system was mathematically specified prior to implementation and realized as a CPU-resident graph engine operating under bounded local state evolution.\n  Contemporary inference-driven AI architectures reconstruct semantic state through probabilistic recomposition, producing compute cost that scales with token volume and context horizon. In contrast, the substrate described here represents semantic continuity as a persistent, addressable memory graph evolved under a time-modulated local operator g(t). Work is bounded by local semantic change Delta s, independent of total memory cardinality M.\n  Empirical measurements on Apple M2-class silicon demonstrate invariant traversal latency (approximately 0.25 to 0.32 ms), stable CPU utili",
      "title": "The Compute ICE-AGE: Invariant Compute Envelope under Addressable Graph Evolution"
    },
    {
      "arxiv_id": "2602.17520",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:30.513642+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17520"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17520"
      },
      "published_at": "2026-02-19T16:33:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8658411485785015,
        "semantic_score": 4.644842916727066,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.210684065305568
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17520",
      "summary": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local de",
      "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning"
    },
    {
      "arxiv_id": "2602.16958",
      "authors": [
        "Xinhao Deng",
        "Jiaqing Wu",
        "Miao Chen",
        "Yue Xiao",
        "Ke Xu",
        "Qi Li"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.936001+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Automating Agent Hijacking via Structural Template Injection",
          "url": "https://arxiv.org/abs/2602.16958"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Automating Agent Hijacking via Structural Template Injection",
        "url": "https://arxiv.org/abs/2602.16958"
      },
      "published_at": "2026-02-18T23:52:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8076675570872526,
        "semantic_score": 3.5952957451343535,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.202963302221608
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16958",
      "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Te",
      "title": "Automating Agent Hijacking via Structural Template Injection"
    },
    {
      "arxiv_id": "2602.16412",
      "authors": [
        "Daichi Yashima",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Komei Sugiura"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119516+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
          "url": "https://arxiv.org/abs/2602.16412"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
        "url": "https://arxiv.org/abs/2602.16412"
      },
      "published_at": "2026-02-18T12:37:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7707004989358524,
        "semantic_score": 4.0619225859642025,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.192623084900056
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16412",
      "summary": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating ",
      "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding"
    },
    {
      "arxiv_id": "2510.09201",
      "authors": [
        "Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912629+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
          "url": "https://arxiv.org/abs/2510.09201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
        "url": "https://arxiv.org/abs/2510.09201"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.497374510765075,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.17937259722701
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2510.09201",
      "summary": "arXiv:2510.09201v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors ",
      "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs"
    },
    {
      "arxiv_id": "2602.16708",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.920023+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Policy Compiler for Secure Agentic Systems",
          "url": "https://arxiv.org/abs/2602.16708"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Policy Compiler for Secure Agentic Systems",
        "url": "https://arxiv.org/abs/2602.16708"
      },
      "published_at": "2026-02-18T18:57:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7912880879126539,
        "semantic_score": 3.5699635446071625,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.161251632519818
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16708",
      "summary": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture",
      "title": "Policy Compiler for Secure Agentic Systems"
    },
    {
      "arxiv_id": "2602.16444",
      "authors": [
        "Yixue Zhang, Kun Wu, Zhi Gao, Zhen Zhao, Pei Ren, Zhiyuan Xu, Fei Liao, Xinhua Wang, Shichao Fan, Di Wu, Qiuxuan Feng, Meng Li, Zhengping Che, Chang Liu, Jian Tang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.919779+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
          "url": "https://arxiv.org/abs/2602.16444"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
        "url": "https://arxiv.org/abs/2602.16444"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3683537006378175,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.15035178709975
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.16444",
      "summary": "arXiv:2602.16444v2 Announce Type: replace \nAbstract: The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We ",
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation"
    },
    {
      "arxiv_id": "2602.17550",
      "authors": [
        "Xiaoliang Fu",
        "Jiaye Lin",
        "Yangyi Fang",
        "Binbin Zheng",
        "Chaowen Hu",
        "Zekai Shao",
        "Cong Qin",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.946758+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.17550"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.17550"
      },
      "published_at": "2026-02-19T17:05:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8677412665063504,
        "semantic_score": 3.89488685131073,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.12262811781708
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17550",
      "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the t",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning"
    },
    {
      "arxiv_id": "2512.19941",
      "authors": [
        "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:27.914832+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Block-Recurrent Dynamics in Vision Transformers",
          "url": "https://arxiv.org/abs/2512.19941"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Block-Recurrent Dynamics in Vision Transformers",
        "url": "https://arxiv.org/abs/2512.19941"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8727425813674925,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.114740667829427
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2512.19941",
      "summary": "arXiv:2512.19941v5 Announce Type: replace \nAbstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent ",
      "title": "Block-Recurrent Dynamics in Vision Transformers"
    },
    {
      "arxiv_id": "2601.19245",
      "authors": [
        "Yongxin Deng, Zhen Fang, Sharon Li, Ling Chen"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.916267+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
          "url": "https://arxiv.org/abs/2601.19245"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
        "url": "https://arxiv.org/abs/2601.19245"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.983367145061493,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.10536523152343
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2601.19245",
      "summary": "arXiv:2601.19245v5 Announce Type: replace-cross \nAbstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs' initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both th",
      "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection"
    },
    {
      "arxiv_id": "2602.17316",
      "authors": [
        "Bogdan Kosti",
        "Conor Fallon",
        "Julian Risch",
        "Alexander Lser"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943317+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
          "url": "https://arxiv.org/abs/2602.17316"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation",
        "url": "https://arxiv.org/abs/2602.17316"
      },
      "published_at": "2026-02-19T12:24:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8509940733711695,
        "semantic_score": 4.5442147076129915,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.095208780984162
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17316",
      "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ",
      "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation"
    },
    {
      "arxiv_id": "2602.13110",
      "authors": [
        "Sher Badshah, Ali Emami, Hassan Sajjad"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917805+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
          "url": "https://arxiv.org/abs/2602.13110"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
        "url": "https://arxiv.org/abs/2602.13110"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.395969808101654,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.077967894563592
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.13110",
      "summary": "arXiv:2602.13110v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\\alpha$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves un",
      "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging"
    },
    {
      "arxiv_id": "2602.16800",
      "authors": [
        "Simon Lermen",
        "Daniel Paleka",
        "Joshua Swanson",
        "Michael Aerni",
        "Nicholas Carlini",
        "Florian Tramr"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.922579+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large-scale online deanonymization with LLMs",
          "url": "https://arxiv.org/abs/2602.16800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large-scale online deanonymization with LLMs",
        "url": "https://arxiv.org/abs/2602.16800"
      },
      "published_at": "2026-02-18T19:02:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7915977033007366,
        "semantic_score": 3.9234218895435333,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.07501959284427
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16800",
      "summary": "We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that",
      "title": "Large-scale online deanonymization with LLMs"
    },
    {
      "arxiv_id": "2602.00307",
      "authors": [
        "Udayan Khurana"
      ],
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.916528+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Autonomous Data Processing using Meta-Agents",
          "url": "https://arxiv.org/abs/2602.00307"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Autonomous Data Processing using Meta-Agents",
        "url": "https://arxiv.org/abs/2602.00307"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.630110090970993,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.072108177432927
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.00307",
      "summary": "arXiv:2602.00307v2 Announce Type: replace-cross \nAbstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integra",
      "title": "Autonomous Data Processing using Meta-Agents"
    },
    {
      "arxiv_id": "2602.17526",
      "authors": [
        "Peter Balogh"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945978+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
          "url": "https://arxiv.org/abs/2602.17526"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads",
        "url": "https://arxiv.org/abs/2602.17526"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9346740901470185,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.056672176608952
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17526",
      "summary": "arXiv:2602.17526v1 Announce Type: cross \nAbstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question \"has this token appeared before in the context?\" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\\% even at 180 unique context tokens -- well above the $d_\\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \\approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \\approx 5$ bits, saturating by $n \\approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after ",
      "title": "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads"
    },
    {
      "arxiv_id": "2602.07287",
      "authors": [
        "Juefei Pu, Xingyu Li, Zhengchuan Liang, Jonathan Cox, Yifan Wu, Kareem Shehada, Arrdya Srivastav, Zhiyun Qian"
      ],
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.594999+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction",
          "url": "https://arxiv.org/abs/2602.07287"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction",
        "url": "https://arxiv.org/abs/2602.07287"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.373663604259491,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.05566169072143
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2602.07287",
      "summary": "arXiv:2602.07287v2 Announce Type: replace-cross \nAbstract: Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches.\n  In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities",
      "title": "Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction"
    },
    {
      "arxiv_id": "2511.17673",
      "authors": [
        "Myung Ho Kim"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.914025+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
          "url": "https://arxiv.org/abs/2511.17673"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
        "url": "https://arxiv.org/abs/2511.17673"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.260868239402771,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.042866325864704
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2511.17673",
      "summary": "arXiv:2511.17673v5 Announce Type: replace-cross \nAbstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Soft Symbolic Control constitutes a dedicated governance layer within SCL, applying symbolic constraints to probabilistic inference while preserving the flexibility of neural reasoning and restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approa",
      "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer"
    },
    {
      "arxiv_id": "2602.17196",
      "authors": [
        "Yahong Wang",
        "Juncheng Wu",
        "Zhangkai Ni",
        "Chengmei Yang",
        "Yihang Liu",
        "Longzhen Yang",
        "Yuyin Zhou",
        "Ying Wen",
        "Lianghua He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.296179+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.17196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.17196"
      },
      "published_at": "2026-02-19T09:29:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8407156924225573,
        "semantic_score": 4.500897604227066,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.041613296649622
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17196",
      "summary": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse ",
      "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models"
    },
    {
      "arxiv_id": "2602.15650",
      "authors": [
        "Marco Salm",
        "Federico Siciliano",
        "Fabrizio Silvestri",
        "Paolo Soda",
        "Rosa Sicilia",
        "Valerio Guarrasi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.121207+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
          "url": "https://arxiv.org/abs/2602.15650"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
        "url": "https://arxiv.org/abs/2602.15650"
      },
      "published_at": "2026-02-17T15:18:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7051764028431059,
        "semantic_score": 3.9704021215438843,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.03557852438699
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15650",
      "summary": "Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transpare",
      "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation"
    },
    {
      "arxiv_id": "2602.17100",
      "authors": [
        "Siyu Wang",
        "Ruotian Lu",
        "Zhihao Yang",
        "Yuchao Wang",
        "Yanzhou Zhang",
        "Lei Xu",
        "Qimin Xu",
        "Guojun Yin",
        "Cailian Chen",
        "Xinping Guan"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.082937+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
          "url": "https://arxiv.org/abs/2602.17100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
        "url": "https://arxiv.org/abs/2602.17100"
      },
      "published_at": "2026-02-19T05:51:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8280955480038508,
        "semantic_score": 3.406295472383499,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.034391020387353
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17100",
      "summary": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively ref",
      "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation"
    },
    {
      "arxiv_id": "2602.17229",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942315+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
          "url": "https://arxiv.org/abs/2602.17229"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
        "url": "https://arxiv.org/abs/2602.17229"
      },
      "published_at": "2026-02-19T10:19:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8436018378658132,
        "semantic_score": 3.383018034696579,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.026619872562392
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17229",
      "summary": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residu",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy"
    },
    {
      "arxiv_id": "2602.17312",
      "authors": [
        "Hsin-Jung Yang, Zhanhong Jiang, Prajwal Koirala, Qisai Liu, Cody Fleming, Soumik Sarkar"
      ],
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.473275+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy",
          "url": "https://arxiv.org/abs/2602.17312"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy",
        "url": "https://arxiv.org/abs/2602.17312"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.314087986946106,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.996086073408044
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17312",
      "summary": "arXiv:2602.17312v1 Announce Type: new \nAbstract: Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task perf",
      "title": "LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy"
    },
    {
      "arxiv_id": "2602.16931",
      "authors": [
        "Idhant Gulati",
        "Shivam Raval"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.935094+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
          "url": "https://arxiv.org/abs/2602.16931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
        "url": "https://arxiv.org/abs/2602.16931"
      },
      "published_at": "2026-02-18T22:47:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8040430796703902,
        "semantic_score": 3.830564665794373,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.994607745464762
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16931",
      "summary": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimod",
      "title": "Narrow fine-tuning erodes safety alignment in vision-language agents"
    },
    {
      "arxiv_id": "2602.15758",
      "authors": [
        "Manav Nitin Kapadnis",
        "Lawanya Baghel",
        "Atharva Naik",
        "Carolyn Ros"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.120888+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
          "url": "https://arxiv.org/abs/2602.15758"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
        "url": "https://arxiv.org/abs/2602.15758"
      },
      "published_at": "2026-02-17T17:45:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.712434210433117,
        "semantic_score": 4.554370594024658,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.966804804457777
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15758",
      "summary": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-contr",
      "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models"
    },
    {
      "arxiv_id": "2511.00040",
      "authors": [
        "Seonggyun Lee, Sungjun Lim, Seojin Park, Soeun Cheon, Kyungwoo Song"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913381+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Semi-Supervised Preference Optimization with Limited Feedback",
          "url": "https://arxiv.org/abs/2511.00040"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Semi-Supervised Preference Optimization with Limited Feedback",
        "url": "https://arxiv.org/abs/2511.00040"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.814779430627823,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.93677751708976
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2511.00040",
      "summary": "arXiv:2511.00040v3 Announce Type: replace \nAbstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive e",
      "title": "Semi-Supervised Preference Optimization with Limited Feedback"
    },
    {
      "arxiv_id": "2510.13749",
      "authors": [
        "Ivan Vykopal, Mat\\'u\\v{s} Pikuliak, Simon Ostermann, Mari\\'an \\v{S}imko"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.890449+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
          "url": "https://arxiv.org/abs/2510.13749"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
        "url": "https://arxiv.org/abs/2510.13749"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.801838141679764,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.9238362281417
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2510.13749",
      "summary": "arXiv:2510.13749v2 Announce Type: replace \nAbstract: Chat assistants increasingly integrate web search functionality, enabling them to retrieve and cite external sources. While this promises more reliable answers, it also raises the risk of amplifying misinformation from low-credibility sources. In this paper, we introduce a novel methodology for evaluating assistants' web search behavior, focusing on source credibility and the groundedness of responses with respect to cited sources. Using 100 claims across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity, and Qwen Chat. Our findings reveal differences between the assistants, with Perplexity achieving the highest source credibility, whereas GPT-4o exhibits elevated citation of non-credibility sources on sensitive topics. This work provides the first systematic comparison of commonly used chat assistants for fact-checking behavior, offering a foundation for evaluating AI systems in high-stakes information environmen",
      "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants"
    },
    {
      "arxiv_id": "2602.17049",
      "authors": [
        "Seoyoung Lee",
        "Seobin Yoon",
        "Seongbeen Lee",
        "Yoojung Chun",
        "Dayoung Park",
        "Doyeon Kim",
        "Joo Yong Sim"
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938314+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
          "url": "https://arxiv.org/abs/2602.17049"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
        "url": "https://arxiv.org/abs/2602.17049"
      },
      "published_at": "2026-02-19T03:42:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8206723261361627,
        "semantic_score": 3.2849718570709228,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.905644183207087
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17049",
      "summary": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate ",
      "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents"
    },
    {
      "arxiv_id": "2602.16951",
      "authors": [
        "Mingzhe Cui",
        "Tao Chen",
        "Yang Jiao",
        "Yiqin Wang",
        "Lei Xie",
        "Yi Pan",
        "Luca Mainardi"
      ],
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:28.466382+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression",
          "url": "https://arxiv.org/abs/2602.16951"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression",
        "url": "https://arxiv.org/abs/2602.16951"
      },
      "published_at": "2026-02-18T23:30:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8064550971271138,
        "semantic_score": 3.4908538818359376,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.89730897896305
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16951",
      "summary": "Developing foundation models for electroencephalography (EEG) remains challenging due to the signal's low signal-to-noise ratio and complex spectro-temporal non-stationarity. Existing approaches often overlook the hierarchical latent structure inherent in neural dynamics, leading to suboptimal reconstruction of fine-grained information. In this work, we propose BrainRVQ, a general-purpose EEG foundation model pre-trained on a large-scale corpus of clinical EEG data. Unlike standard masked modeli",
      "title": "BrainRVQ: A High-Fidelity EEG Foundation Model via Dual-Domain Residual Quantization and Hierarchical Autoregression"
    },
    {
      "arxiv_id": "2602.16902",
      "authors": [
        "Juliusz Ziomek",
        "William Bankes",
        "Lorenz Wolf",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934523+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
          "url": "https://arxiv.org/abs/2602.16902"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
        "url": "https://arxiv.org/abs/2602.16902"
      },
      "published_at": "2026-02-18T21:33:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7999504915287318,
        "semantic_score": 4.391245281696319,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.89119577322505
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16902",
      "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest r",
      "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs"
    },
    {
      "arxiv_id": "2602.16666",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.632049+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Towards a Science of AI Agent Reliability",
          "url": "https://arxiv.org/abs/2602.16666"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Towards a Science of AI Agent Reliability",
        "url": "https://arxiv.org/abs/2602.16666"
      },
      "published_at": "2026-02-18T18:05:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.7884650136160114,
        "semantic_score": 3.3026547372341155,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.89111975085013
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16666",
      "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "title": "Towards a Science of AI Agent Reliability"
    },
    {
      "arxiv_id": "2602.15645",
      "authors": [
        "Lucas Elbert Suryana",
        "Farah Bierenga",
        "Sanne van Buuren",
        "Pepijn Kooij",
        "Elsefien Tulleners",
        "Federico Scari",
        "Simeon Calvert",
        "Bart van Arem",
        "Arkady Zgonnikov"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:32.112518+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
          "url": "https://arxiv.org/abs/2602.15645"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving",
        "url": "https://arxiv.org/abs/2602.15645"
      },
      "published_at": "2026-02-17T15:13:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7049552537251678,
        "semantic_score": 4.472788715362549,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.877743969087717
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15645",
      "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive",
      "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving"
    },
    {
      "arxiv_id": "2602.16873",
      "authors": [
        "Geunbin Yu"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934074+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
          "url": "https://arxiv.org/abs/2602.16873"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
        "url": "https://arxiv.org/abs/2602.16873"
      },
      "published_at": "2026-02-18T21:00:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7980694897112057,
        "semantic_score": 3.681317448616028,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.839386938327234
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16873",
      "summary": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dyna",
      "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence"
    },
    {
      "arxiv_id": "2602.16986",
      "authors": [
        "Qin Ding, Kevin Course, Linjian Ma, Jianhui Sun, Rouchen Liu, Zhao Zhu, Chunxing Yin, Wei Li, Dai Li, Yu Shi, Xuan Cao, Ze Yang, Han Li, Xing Liu, Bi Xue, Hongwei Li, Rui Jian, Daisy Shi He, Jing Qian, Matt Ma, Qunshu Zhang, Rui Li"
      ],
      "categories": [
        "cs.IR",
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T02:04:37.270333+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Bending the Scaling Law Curve in Large-Scale Recommendation Systems",
          "url": "https://arxiv.org/abs/2602.16986"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Bending the Scaling Law Curve in Large-Scale Recommendation Systems",
        "url": "https://arxiv.org/abs/2602.16986"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.129429376125335,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.81142746258727
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.16986",
      "summary": "arXiv:2602.16986v1 Announce Type: new \nAbstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster",
      "title": "Bending the Scaling Law Curve in Large-Scale Recommendation Systems"
    },
    {
      "arxiv_id": "2602.17095",
      "authors": [
        "Chuiyang Meng",
        "Ming Tang",
        "Vincent W. S. Wong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.939462+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
          "url": "https://arxiv.org/abs/2602.17095"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
        "url": "https://arxiv.org/abs/2602.17095"
      },
      "published_at": "2026-02-19T05:35:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.827145317241152,
        "semantic_score": 4.281585395336151,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.8087307125773
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17095",
      "summary": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggrega",
      "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment"
    },
    {
      "arxiv_id": "2602.17186",
      "authors": [
        "Seulbi Lee",
        "Sangheum Hwang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.295884+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
          "url": "https://arxiv.org/abs/2602.17186"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Selective Training for Large Vision Language Models via Visual Information Gain",
        "url": "https://arxiv.org/abs/2602.17186"
      },
      "published_at": "2026-02-19T09:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8397023848853337,
        "semantic_score": 4.263519692420959,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.803222077306295
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17186",
      "summary": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity",
      "title": "Selective Training for Large Vision Language Models via Visual Information Gain"
    },
    {
      "arxiv_id": "2602.17288",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:27.942983+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
          "url": "https://arxiv.org/abs/2602.17288"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
        "url": "https://arxiv.org/abs/2602.17288"
      },
      "published_at": "2026-02-19T11:47:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8487985091730782,
        "semantic_score": 3.342172032594681,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.790970541767756
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17288",
      "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training"
    },
    {
      "arxiv_id": "2602.17078",
      "authors": [
        "Xuefeng Wang",
        "Lei Zhang",
        "Henglin Pu",
        "Husheng Li",
        "Ahmed H. Qureshi"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.090357+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
          "url": "https://arxiv.org/abs/2602.17078"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form",
        "url": "https://arxiv.org/abs/2602.17078"
      },
      "published_at": "2026-02-19T04:42:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8241199122256992,
        "semantic_score": 3.1481004297733306,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.77222034199903
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17078",
      "summary": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Be",
      "title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form"
    },
    {
      "arxiv_id": "2602.16584",
      "authors": [
        "Akhil Ramidi",
        "Kevin Scharp"
      ],
      "categories": [
        "q-bio.NC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.179368+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
          "url": "https://arxiv.org/abs/2602.16584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities",
        "url": "https://arxiv.org/abs/2602.16584"
      },
      "published_at": "2026-02-18T16:29:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7832106522185303,
        "semantic_score": 3.6157516121864317,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.758962264404964
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16584",
      "summary": "There is growing evidence that independently trained AI systems come to represent the world in the same way. In other words, independently trained embeddings from text, vision, audio, and neural signals share an underlying geometry. We call this the Representational Alignment Hypothesis (RAH) and investigate evidence for and consequences of this claim. The evidence is of two kinds: (i) internal structure comparison techniques, such as representational similarity analysis and topological data ana",
      "title": "The Representational Alignment Hypothesis: Evidence for and Consequences of Invariant Semantic Structure Across Embedding Modalities"
    },
    {
      "arxiv_id": "2602.16053",
      "authors": [
        "Mehrab Beikzadeh",
        "Yasaman Asadollah Salmanpour",
        "Ashima Suvarna",
        "Sriram Sankararaman",
        "Matteo Malgaroli",
        "Majid Sarrafzadeh",
        "Saadia Gabriel"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.904235+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
          "url": "https://arxiv.org/abs/2602.16053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
        "url": "https://arxiv.org/abs/2602.16053"
      },
      "published_at": "2026-02-17T22:08:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7255487843049688,
        "semantic_score": 4.32948225736618,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.75503104167115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16053",
      "summary": "Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct ",
      "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy"
    },
    {
      "arxiv_id": "2602.16729",
      "authors": [
        "Shahriar Golchin, Marc Wetter"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921088+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
          "url": "https://arxiv.org/abs/2602.16729"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
        "url": "https://arxiv.org/abs/2602.16729"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.046334338188172,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.72833242465011
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16729",
      "summary": "arXiv:2602.16729v1 Announce Type: cross \nAbstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-",
      "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem"
    },
    {
      "arxiv_id": "2506.15733",
      "authors": [
        "Mert Cemri, Nived Rajaraman, Rishabh Tiwari, Xiaoxuan Liu, Kurt Keutzer, Ion Stoica, Kannan Ramchandran, Ahmad Beirami, Ziteng Sun"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.910797+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
          "url": "https://arxiv.org/abs/2506.15733"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
        "url": "https://arxiv.org/abs/2506.15733"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.0274158954620365,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.709413981923973
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2506.15733",
      "summary": "arXiv:2506.15733v2 Announce Type: replace-cross \nAbstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, ",
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts"
    },
    {
      "arxiv_id": "2512.08646",
      "authors": [
        "Maximilian Kreutner, Jens Rupprecht, Georg Ahnert, Ahmed Salem, Markus Strohmaier"
      ],
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.891482+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
          "url": "https://arxiv.org/abs/2512.08646"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models",
        "url": "https://arxiv.org/abs/2512.08646"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.448148399591446,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.69014648605338
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2512.08646",
      "summary": "arXiv:2512.08646v2 Announce Type: replace \nAbstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation (>40 million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers. We also find that answers can be obtained for a fraction of the compute cost, by changing the presentation method. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs \\emph{without coding knowledge}. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.",
      "title": "QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models"
    },
    {
      "arxiv_id": "2602.17558",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.299296+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
          "url": "https://arxiv.org/abs/2602.17558"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
        "url": "https://arxiv.org/abs/2602.17558"
      },
      "published_at": "2026-02-19T17:11:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8681420867874008,
        "semantic_score": 4.121445143222809,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.68958723001021
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17558",
      "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inher",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward"
    },
    {
      "arxiv_id": "2602.16702",
      "authors": [
        "Mingjia Shi",
        "Yinhan He",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119286+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
          "url": "https://arxiv.org/abs/2602.16702"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
        "url": "https://arxiv.org/abs/2602.16702"
      },
      "published_at": "2026-02-18T18:49:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7908888812321515,
        "semantic_score": 4.192357754707336,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.68324663593949
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16702",
      "summary": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominate",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning"
    },
    {
      "arxiv_id": "2602.17016",
      "authors": [
        "Zichen Wang, Wanli Ma, Zhenyu Ming, Gong Zhang, Kun Yuan, Zaiwen Wen"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937391+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "M2F: Automated Formalization of Mathematical Literature at Scale",
          "url": "https://arxiv.org/abs/2602.17016"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "M2F: Automated Formalization of Mathematical Literature at Scale",
        "url": "https://arxiv.org/abs/2602.17016"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.2379188895225526,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.679916975984487
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17016",
      "summary": "arXiv:2602.17016v1 Announce Type: new \nAbstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F convert",
      "title": "M2F: Automated Formalization of Mathematical Literature at Scale"
    },
    {
      "arxiv_id": "2602.02377",
      "authors": [
        "Haotong Yang, Zitong Wang, Shijia Kang, Siqi Yang, Wenkai Yu, Xu Niu, Yike Sun, Yi Hu, Zhouchen Lin, Muhan Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.903126+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof",
          "url": "https://arxiv.org/abs/2602.02377"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof",
        "url": "https://arxiv.org/abs/2602.02377"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.894829899072647,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.67682798553458
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.02377",
      "summary": "arXiv:2602.02377v2 Announce Type: replace \nAbstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality ``**question-proof-check**'' triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incor",
      "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof"
    },
    {
      "arxiv_id": "2502.10361",
      "authors": [
        "Bettina Messmer, Vinko Sabol\\v{c}ec, Martin Jaggi"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.446305+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
          "url": "https://arxiv.org/abs/2502.10361"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
        "url": "https://arxiv.org/abs/2502.10361"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.431679517030716,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.67367760349265
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2502.10361",
      "summary": "arXiv:2502.10361v2 Announce Type: replace \nAbstract: Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we develop a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseli",
      "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection"
    },
    {
      "arxiv_id": "2602.17410",
      "authors": [
        "Bingqian Li",
        "Bowen Zheng",
        "Xiaolei Wang",
        "Long Zhang",
        "Jinpeng Wang",
        "Sheng Chen",
        "Wayne Xin Zhao",
        "Ji-rong Wen"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.944644+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
          "url": "https://arxiv.org/abs/2602.17410"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers",
        "url": "https://arxiv.org/abs/2602.17410"
      },
      "published_at": "2026-02-19T14:37:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.858891352624815,
        "semantic_score": 4.096697807312012,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.655589159936827
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17410",
      "summary": "Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we pro",
      "title": "Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers"
    },
    {
      "arxiv_id": "2509.22075",
      "authors": [
        "Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Ammar Ali, Stamatios Lefkimmiatis"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912091+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
          "url": "https://arxiv.org/abs/2509.22075"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
        "url": "https://arxiv.org/abs/2509.22075"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.354927897453308,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.59692598391524
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2509.22075",
      "summary": "arXiv:2509.22075v4 Announce Type: replace \nAbstract: Post-training compression of large language models (LLMs) often relies on low-rank weight approximations that represent each column of the weight matrix in a shared low-dimensional subspace. This strategy is computationally efficient but the underlying constraint can be overly rigid for heterogeneous projection weights and may incur avoidable accuracy loss. We propose CoSpaDi (Compression via Sparse Dictionary Learning), a training-free framework that replaces low-rank factorization with a structured sparse decomposition in which each weight matrix is represented as a dense dictionary multiplied by a column-sparse coefficient matrix. This yields a union-of-subspaces model: the columns of the weight matrix are represented as linear combinations of different subsets of dictionary atoms, improving expressiveness at a fixed parameter budget. CoSpaDi is calibration-guided: using a small calibration set, we optimize the factorization to mi",
      "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning"
    },
    {
      "arxiv_id": "2602.17658",
      "authors": [
        "Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.953957+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
          "url": "https://arxiv.org/abs/2602.17658"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
        "url": "https://arxiv.org/abs/2602.17658"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.914537191390991,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.59653527785293
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17658",
      "summary": "arXiv:2602.17658v1 Announce Type: new \nAbstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvatur",
      "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement"
    },
    {
      "arxiv_id": "2503.17338",
      "authors": [
        "Andr\\'e Barreto, Vincent Dumoulin, Yiran Mao, Mark Rowland, Nicolas Perez-Nieves, Bobak Shahriari, Yann Dauphin, Doina Precup, Hugo Larochelle"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.909037+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Capturing Individual Human Preferences with Reward Features",
          "url": "https://arxiv.org/abs/2503.17338"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Capturing Individual Human Preferences with Reward Features",
        "url": "https://arxiv.org/abs/2503.17338"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.863236963748932,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.54523505021087
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2503.17338",
      "summary": "arXiv:2503.17338v2 Announce Type: replace-cross \nAbstract: Reinforcement learning from human feedback usually models preferences using a reward function that does not distinguish between people. We argue that this is unlikely to be a good design choice in contexts with high potential for disagreement, like in the training of large language models. We formalise and analyse the problem of learning a reward model that can be specialised to a user. Using the principle of empirical risk minimisation, we derive a probably approximately correct (PAC) bound showing the dependency of the approximation error on the number of training examples, as usual, and also on the number of human raters who provided feedback on them. Based on our theoretical findings, we discuss how to best collect pairwise preference data and argue that adaptive reward models should be beneficial when there is considerable disagreement among users. We also propose a concrete architecture for an adaptive reward model. Our a",
      "title": "Capturing Individual Human Preferences with Reward Features"
    },
    {
      "arxiv_id": "2602.16980",
      "authors": [
        "Leo Marchyok, Zachary Coalson, Sungho Keum, Sooel Son, Sanghyun Hong"
      ],
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.467693+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
          "url": "https://arxiv.org/abs/2602.16980"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
        "url": "https://arxiv.org/abs/2602.16980"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.82,
        "llm_relevance_score": 18.04,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.421662747859955,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.54366083432189
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16980",
      "summary": "arXiv:2602.16980v1 Announce Type: new \nAbstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer",
      "title": "Discovering Universal Activation Directions for PII Leakage in Language Models"
    },
    {
      "arxiv_id": "2602.16942",
      "authors": [
        "Hexi Jin, Stephen Liu, Yuheng Li, Simran Malik, Yiying Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.935435+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
          "url": "https://arxiv.org/abs/2602.16942"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
        "url": "https://arxiv.org/abs/2602.16942"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.299190622568131,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.541188709030067
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16942",
      "summary": "arXiv:2602.16942v1 Announce Type: new \nAbstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI an",
      "title": "SourceBench: Can AI Answers Reference Quality Web Sources?"
    },
    {
      "arxiv_id": "2602.17072",
      "authors": [
        "Yunseung Lee, Subin Kim, Youngjun Kwak, Jaegul Choo"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.907324+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
          "url": "https://arxiv.org/abs/2602.17072"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
        "url": "https://arxiv.org/abs/2602.17072"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.833780121803284,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.51577820826522
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17072",
      "summary": "arXiv:2602.17072v1 Announce Type: new \nAbstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking sc",
      "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios"
    },
    {
      "arxiv_id": "2602.07666",
      "authors": [
        "Cen Zhang, Younggi Park, Fabian Fleischer, Yu-Fu Fu, Jiho Kim, Dongkwan Kim, Youngjoon Kim, Qingxiao Xu, Andrew Chin, Ze Sheng, Hanqing Zhao, Brian J. Lee, Joshua Wang, Michael Pelican, David J. Musliner, Jeff Huang, Jon Silliman, Mikel Mcdaniel, Jefferson Casavant, Isaac Goldthwaite, Nicholas Vidovich, Matthew Lehman, Taesoo Kim"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917041+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
          "url": "https://arxiv.org/abs/2602.07666"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
        "url": "https://arxiv.org/abs/2602.07666"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.7247535228729247,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.50675160933486
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.07666",
      "summary": "arXiv:2602.07666v2 Announce Type: replace-cross \nAbstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward dep",
      "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned"
    },
    {
      "arxiv_id": "2602.17271",
      "authors": [
        "Giuseppe Di Poce",
        "Mario Edoardo Pandolfo",
        "Emilio Calvanese Strinati",
        "Paolo Di Lorenzo"
      ],
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942765+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
          "url": "https://arxiv.org/abs/2602.17271"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Federated Latent Space Alignment for Multi-user Semantic Communications",
        "url": "https://arxiv.org/abs/2602.17271"
      },
      "published_at": "2026-02-19T11:18:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8471182958511813,
        "semantic_score": 3.2593857526779173,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.4665040485291
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17271",
      "summary": "Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that sh",
      "title": "Federated Latent Space Alignment for Multi-user Semantic Communications"
    },
    {
      "arxiv_id": "2602.17598",
      "authors": [
        "Jayadev Billa"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.947628+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
          "url": "https://arxiv.org/abs/2602.17598"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
        "url": "https://arxiv.org/abs/2602.17598"
      },
      "published_at": "2026-02-19T18:22:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8724128843956866,
        "semantic_score": 2.787013304233551,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.45942618862924
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17598",
      "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade (${=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text represen",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?"
    },
    {
      "arxiv_id": "2602.16745",
      "authors": [
        "Zhangyi Liu, Huaizhi Qu, Xiaowei Yin, He Sun, Yanjun Han, Tianlong Chen, Zhun Deng"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921728+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
          "url": "https://arxiv.org/abs/2602.16745"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
        "url": "https://arxiv.org/abs/2602.16745"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.767110675573349,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.449108762035287
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16745",
      "summary": "arXiv:2602.16745v1 Announce Type: new \nAbstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarante",
      "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency"
    },
    {
      "arxiv_id": "2602.17665",
      "authors": [
        "Akashah Shabbir",
        "Muhammad Umer Sheikh",
        "Muhammad Akhtar Munir",
        "Hiyam Debary",
        "Mustansar Fiaz",
        "Muhammad Zaigham Zaheer",
        "Paolo Fraccaro",
        "Fahad Shahbaz Khan",
        "Muhammad Haris Khan",
        "Xiao Xiang Zhu",
        "Salman Khan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.300341+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
          "url": "https://arxiv.org/abs/2602.17665"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
        "url": "https://arxiv.org/abs/2602.17665"
      },
      "published_at": "2026-02-19T18:59:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8746725683131674,
        "semantic_score": 2.729754459857941,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.40442702817111
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17665",
      "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satelli",
      "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents"
    },
    {
      "arxiv_id": "2602.17413",
      "authors": [
        "Ren Brinkhege",
        "Prahlad Menon"
      ],
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.909056+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
          "url": "https://arxiv.org/abs/2602.17413"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
        "url": "https://arxiv.org/abs/2602.17413"
      },
      "published_at": "2026-02-19T14:43:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.77,
        "llm_relevance_score": 16.94,
        "recency_score": 0.8592542711105422,
        "semantic_score": 3.34985927939415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.349113550504693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17413",
      "summary": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over priv",
      "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing"
    },
    {
      "arxiv_id": "2602.16966",
      "authors": [
        "Sourav Chakraborty, Amit Kiran Rege, Claire Monteleoni, Lijun Chen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.936214+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Unified Framework for Locality in Scalable MARL",
          "url": "https://arxiv.org/abs/2602.16966"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Unified Framework for Locality in Scalable MARL",
        "url": "https://arxiv.org/abs/2602.16966"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.6660175025463104,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.34801558900825
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16966",
      "summary": "arXiv:2602.16966v1 Announce Type: new \nAbstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \\emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^\\pi$, which decouples the environment's sensitivity to state ($E^{\\mathrm{s}}$) and action ($E^{\\mathrm{a}}$) from the policy's sensitivity to state ($\\Pi(\\pi)$). This decomposition reveals that locality can be induced by a smooth policy (small $\\Pi(\\pi)$) even when the environment is strongly action-c",
      "title": "A Unified Framework for Locality in Scalable MARL"
    },
    {
      "arxiv_id": "2602.17434",
      "authors": [
        "Eleftherios E. Vlahakis",
        "Arash Bahari Kordabad",
        "Lars Lindemann",
        "Pantelis Sopasakis",
        "Sadegh Soudjani",
        "Dimos V. Dimarogonas"
      ],
      "categories": [
        "eess.SY",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.082168+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
          "url": "https://arxiv.org/abs/2602.17434"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
        "url": "https://arxiv.org/abs/2602.17434"
      },
      "published_at": "2026-02-19T15:05:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8605361516975556,
        "semantic_score": 3.787213695049286,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.34774984674684
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17434",
      "summary": "Multi-agent planning under Signal Temporal Logic (STL) is often hindered by collaborative tasks that lead to computational challenges due to the inherent high-dimensionality of the problem, preventing scalable synthesis with satisfaction guarantees. To address this, we formulate STL planning as an optimization program under arbitrary multi-agent constraints and introduce a penalty-based unconstrained relaxation that can be efficiently solved via a Block-Coordinate Gradient Descent (BCGD) method,",
      "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization"
    },
    {
      "arxiv_id": "2602.16144",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Ziming Wang",
        "Chunlei Meng",
        "Jiaxuan Lu",
        "Jiekai Wu",
        "Kangan Qian",
        "Hao Zhang",
        "Simon Fong"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
          "url": "https://arxiv.org/abs/2602.16144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
        "url": "https://arxiv.org/abs/2602.16144"
      },
      "published_at": "2026-02-18T02:29:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7388355037405725,
        "semantic_score": 3.901291477680206,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.34012698142078
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16144",
      "summary": "As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request",
      "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis"
    },
    {
      "arxiv_id": "2602.17062",
      "authors": [
        "Yonghyeon Jo",
        "Sunwoo Lee",
        "Seungyul Han"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938782+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17062"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17062"
      },
      "published_at": "2026-02-19T04:07:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8221364029724096,
        "semantic_score": 3.8135067522525787,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.33564315522499
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17062",
      "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.15829",
      "authors": [
        "Toms Vergara-Browne",
        "Darshan Patil",
        "Ivan Titov",
        "Siva Reddy",
        "Tiago Pimentel",
        "Marius Mosbach"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.180158+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
          "url": "https://arxiv.org/abs/2602.15829"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
        "url": "https://arxiv.org/abs/2602.15829"
      },
      "published_at": "2026-02-17T18:59:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7161088979976251,
        "semantic_score": 3.917252314090729,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.333361212088352
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15829",
      "summary": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SA",
      "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity"
    },
    {
      "arxiv_id": "2602.17486",
      "authors": [
        "Yuma Fujimoto, Kenshi Abe, Kaito Ariu"
      ],
      "categories": [
        "cs.LG",
        "cs.GT",
        "cs.MA",
        "math.OC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.475186+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Linear Convergence in Games with Delayed Feedback via Extra Prediction",
          "url": "https://arxiv.org/abs/2602.17486"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Linear Convergence in Games with Delayed Feedback via Extra Prediction",
        "url": "https://arxiv.org/abs/2602.17486"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.6402463912963867,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.322244477758325
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.17486",
      "summary": "arXiv:2602.17486v1 Announce Type: cross \nAbstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\\exp(-\\Theta(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\\exp(-\\Theta(t/(m^{2}\\log m)))$.",
      "title": "Linear Convergence in Games with Delayed Feedback via Extra Prediction"
    },
    {
      "arxiv_id": "2602.16662",
      "authors": [
        "Richard Willis",
        "Jianing Zhao",
        "Yali Du",
        "Joel Z. Leibo"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.435881+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
          "url": "https://arxiv.org/abs/2602.16662"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
        "url": "https://arxiv.org/abs/2602.16662"
      },
      "published_at": "2026-02-18T18:02:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7883071539030082,
        "semantic_score": 3.169440972805023,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.317748126708032
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16662",
      "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise i",
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents"
    },
    {
      "arxiv_id": "2602.15772",
      "authors": [
        "Sen Ye",
        "Mengde Xu",
        "Shuyang Gu",
        "Di He",
        "Liwei Wang",
        "Han Hu"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.120618+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
          "url": "https://arxiv.org/abs/2602.15772"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
        "url": "https://arxiv.org/abs/2602.15772"
      },
      "published_at": "2026-02-17T18:04:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7133575094559985,
        "semantic_score": 3.8939067363739017,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.3072642458299
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15772",
      "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step p",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models"
    },
    {
      "arxiv_id": "2602.17375",
      "authors": [
        "David Tolpin"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.474479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MDP Planning as Policy Inference",
          "url": "https://arxiv.org/abs/2602.17375"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MDP Planning as Policy Inference",
        "url": "https://arxiv.org/abs/2602.17375"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.599424034357071,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.28142212081901
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17375",
      "summary": "arXiv:2602.17375v1 Announce Type: new \nAbstract: We cast episodic Markov decision process (MDP) planning as Bayesian inference over _policies_. A policy is treated as the latent variable and is assigned an unnormalized probability of optimality that is monotone in its expected return, yielding a posterior distribution whose modes coincide with return-maximizing solutions while posterior dispersion represents uncertainty over optimal behavior. To approximate this posterior in discrete domains, we adapt variational sequential Monte Carlo (VSMC) to inference over deterministic policies under stochastic dynamics, introducing a sweep that enforces policy consistency across revisited states and couples transition randomness across particles to avoid confounding from simulator noise. Acting is performed by posterior predictive sampling, which induces a stochastic control policy through a Thompson-sampling interpretation rather than entropy regularization. Across grid worlds, Blackjack, Triang",
      "title": "MDP Planning as Policy Inference"
    },
    {
      "arxiv_id": "2602.16787",
      "authors": [
        "Victoria Lin, Xinnuo Xu, Rachel Lawrence, Risa Ueno, Amit Sharma, Javier Gonzalez, Niranjani Prasad"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.457578+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency",
          "url": "https://arxiv.org/abs/2602.16787"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency",
        "url": "https://arxiv.org/abs/2602.16787"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5980593860149384,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.280057472476877
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16787",
      "summary": "arXiv:2602.16787v1 Announce Type: cross \nAbstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate t",
      "title": "Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency"
    },
    {
      "arxiv_id": "2602.16844",
      "authors": [
        "Madeleine Grunde-McLaughlin",
        "Hussein Mozannar",
        "Maya Murad",
        "Jingya Chen",
        "Saleema Amershi",
        "Adam Fourney"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.933580+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
          "url": "https://arxiv.org/abs/2602.16844"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities",
        "url": "https://arxiv.org/abs/2602.16844"
      },
      "published_at": "2026-02-18T20:16:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.7956521626004657,
        "semantic_score": 3.110388898849487,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.266041061449954
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16844",
      "summary": "To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are ",
      "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities"
    },
    {
      "arxiv_id": "2602.16953",
      "authors": [
        "Hejia Zhang",
        "Zhongming Yu",
        "Chia-Tung Ho",
        "Haoxing Ren",
        "Brucek Khailany",
        "Jishen Zhao"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.935780+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
          "url": "https://arxiv.org/abs/2602.16953"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
        "url": "https://arxiv.org/abs/2602.16953"
      },
      "published_at": "2026-02-18T23:36:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.806800528013792,
        "semantic_score": 3.746326744556427,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.253127272570218
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16953",
      "summary": "Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Bu",
      "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation"
    },
    {
      "arxiv_id": "2602.17633",
      "authors": [
        "Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.948420+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
          "url": "https://arxiv.org/abs/2602.17633"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
        "url": "https://arxiv.org/abs/2602.17633"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5239590406417847,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.205957127103723
      },
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.17633",
      "summary": "arXiv:2602.17633v1 Announce Type: cross \nAbstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verif",
      "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning"
    },
    {
      "arxiv_id": "2509.16072",
      "authors": [
        "Clemence Grislain, Hamed Rahimi, Olivier Sigaud, Mohamed Chetouani"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.310822+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models",
          "url": "https://arxiv.org/abs/2509.16072"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models",
        "url": "https://arxiv.org/abs/2509.16072"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.520058023929596,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.20205611039153
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2509.16072",
      "summary": "arXiv:2509.16072v3 Announce Type: replace \nAbstract: Language-conditioned robotic manipulation in open-world settings requires not only accurate task execution but also the ability to detect failures for robust deployment in real-world environments. Although recent advances in vision-language models (VLMs) have significantly improved the spatial reasoning and task-planning capabilities of robots, they remain limited in their ability to recognize their own failures. In particular, a critical yet underexplored challenge lies in detecting semantic misalignment errors, where the robot executes a task that is semantically meaningful but inconsistent with the given instruction. To address this, we propose a method for building datasets targeting Semantic Misalignment Failures detection, from existing language-conditioned manipulation datasets. We also present I-FailSense, an open-source VLM framework with grounded arbitration designed specifically for failure detection. Our approach relies o",
      "title": "I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models"
    },
    {
      "arxiv_id": "2602.08351",
      "authors": [
        "Zhiliang Chen, Alfred Wei Lun Leong, Shao Yong Ong, Apivich Hemachandra, Gregory Kang Ruey Lau, Chuan-Sheng Foo, Zhengyuan Liu, Nancy F. Chen, Bryan Kian Hsiang Low"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917163+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
          "url": "https://arxiv.org/abs/2602.08351"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
        "url": "https://arxiv.org/abs/2602.08351"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.513222599029541,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.195220685491478
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.08351",
      "summary": "arXiv:2602.08351v2 Announce Type: replace \nAbstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing th",
      "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs"
    },
    {
      "arxiv_id": "2501.09135",
      "authors": [
        "Yu Shi, Abdul Ali Bangash, Emad Fallahzadeh, Bram Adams, Ahmed E. Hassan"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.594473+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "HAFix: History-Augmented Large Language Models for Bug Fixing",
          "url": "https://arxiv.org/abs/2501.09135"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "HAFix: History-Augmented Large Language Models for Bug Fixing",
        "url": "https://arxiv.org/abs/2501.09135"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.607371300458908,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.189369386920845
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2501.09135",
      "summary": "arXiv:2501.09135v4 Announce Type: replace \nAbstract: Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and",
      "title": "HAFix: History-Augmented Large Language Models for Bug Fixing"
    },
    {
      "arxiv_id": "2602.17345",
      "authors": [
        "Boyang Ma, Hechuan Guo, Peizhuo Lv, Minghui Xu, Xuelong Dai, YeChao Zhang, Yijun Yang, Yue Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943754+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
          "url": "https://arxiv.org/abs/2602.17345"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
        "url": "https://arxiv.org/abs/2602.17345"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.503141903877258,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.185139990339195
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17345",
      "summary": "arXiv:2602.17345v1 Announce Type: cross \nAbstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to",
      "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?"
    },
    {
      "arxiv_id": "2602.16763",
      "authors": [
        "Mubashara Akhtar, Anka Reuel, Prajna Soni, Sanchit Ahuja, Pawan Sasanka Ammanamanchi, Ruchit Rawal, Vil\\'em Zouhar, Srishti Yadav, Chenxi Whitehouse, Dayeon Ki, Jennifer Mickel, Leshem Choshen, Marek \\v{S}uppa, Jan Batzner, Jenny Chim, Jeba Sania, Yanan Long, Hossein A. Rahmani, Christina Knight, Yiyang Nan, Jyoutir Raj, Yu Fan, Shubham Singh, Subramanyam Sahoo, Eliya Habba, Usman Gohar, Siddhesh Pawar, Robert Scholz, Arjun Subramonian, Jingwei Ni, Mykel Kochenderfer, Sanmi Koyejo, Mrinmaya Sachan, Stella Biderman, Zeerak Talat, Avijit Ghosh, Irene Solaiman"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.922424+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
          "url": "https://arxiv.org/abs/2602.16763"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation",
        "url": "https://arxiv.org/abs/2602.16763"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.489357513189316,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.171355599651253
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16763",
      "summary": "arXiv:2602.16763v1 Announce Type: new \nAbstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better t",
      "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation"
    },
    {
      "arxiv_id": "2602.17088",
      "authors": [
        "Haoyu Wang, Zhuo Huang, Xiaolong Wang, Bo Han, Zhiwei Lin, Tongliang Liu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.469608+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MeGU: Machine-Guided Unlearning with Target Feature Disentanglement",
          "url": "https://arxiv.org/abs/2602.17088"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MeGU: Machine-Guided Unlearning with Target Feature Disentanglement",
        "url": "https://arxiv.org/abs/2602.17088"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.48378741145134,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.165785497913276
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17088",
      "summary": "arXiv:2602.17088v1 Announce Type: new \nAbstract: The growing concern over training data privacy has elevated the \"Right to be Forgotten\" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignme",
      "title": "MeGU: Machine-Guided Unlearning with Target Feature Disentanglement"
    },
    {
      "arxiv_id": "2602.15569",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:38.629731+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
          "url": "https://arxiv.org/abs/2602.15569"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
        "url": "https://arxiv.org/abs/2602.15569"
      },
      "published_at": "2026-02-17T13:27:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.6997963915351076,
        "semantic_score": 2.638125467300415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.137921858835526
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15569",
      "summary": "Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.",
      "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing"
    },
    {
      "arxiv_id": "2602.16819",
      "authors": [
        "Yiqing Xie",
        "Emmy Liu",
        "Gaokai Zhang",
        "Nachiket Kotalwar",
        "Shubham Gandhi",
        "Sathwik Acharya",
        "Xingyao Wang",
        "Carolyn Rose",
        "Graham Neubig",
        "Daniel Fried"
      ],
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.458584+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
          "url": "https://arxiv.org/abs/2602.16819"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks",
        "url": "https://arxiv.org/abs/2602.16819"
      },
      "published_at": "2026-02-18T19:30:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7931430084239472,
        "semantic_score": 3.6380723416805267,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.131215350104476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16819",
      "summary": "When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for de",
      "title": "Hybrid-Gym: Training Coding Agents to Generalize Across Tasks"
    },
    {
      "arxiv_id": "2602.17283",
      "authors": [
        "Yukun Chen, Xinyu Zhang, Jialong Tang, Yu Wan, Baosong Yang, Yiming Li, Zhan Qin, Kui Ren"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942876+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
          "url": "https://arxiv.org/abs/2602.17283"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective",
        "url": "https://arxiv.org/abs/2602.17283"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.540095764398575,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.12209385086051
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17283",
      "summary": "arXiv:2602.17283v1 Announce Type: new \nAbstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the co",
      "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective"
    },
    {
      "arxiv_id": "2602.17047",
      "authors": [
        "Chaojie Yang, Tian Li, Yue Zhang, Jun Gao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:29.294561+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
          "url": "https://arxiv.org/abs/2602.17047"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers",
        "url": "https://arxiv.org/abs/2602.17047"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.535123360157013,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.11712144661895
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17047",
      "summary": "arXiv:2602.17047v1 Announce Type: new \nAbstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approa",
      "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers"
    },
    {
      "arxiv_id": "2602.17529",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        " Jianzhong",
        " Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.946105+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
          "url": "https://arxiv.org/abs/2602.17529"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
        "url": "https://arxiv.org/abs/2602.17529"
      },
      "published_at": "2026-02-19T16:40:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8662330704581804,
        "semantic_score": 4.643930691480636,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.110163761938814
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17529",
      "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowle",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation"
    },
    {
      "arxiv_id": "2506.11798",
      "authors": [
        "Maximilian Kreutner, Marlene Lutz, Markus Strohmaier"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.910648+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
          "url": "https://arxiv.org/abs/2506.11798"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
        "url": "https://arxiv.org/abs/2506.11798"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.50523915886879,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.087237245330726
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2506.11798",
      "summary": "arXiv:2506.11798v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse but have been found to consistently exhibit a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups with which the base model is not aligned. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict the positions of European groups on a diverse set of policies. We evaluate whether predictions are stable in response to counterfactual arguments, different persona prompts, and generation methods. Finally, we find that we can simulate the voting behavior of Members of the European Parliament reasonably well, achieving a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the",
      "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models"
    },
    {
      "arxiv_id": "2602.16741",
      "authors": [
        "Scott Thornton"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921459+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis",
          "url": "https://arxiv.org/abs/2602.16741"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis",
        "url": "https://arxiv.org/abs/2602.16741"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.494441986083984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.07644007254592
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16741",
      "summary": "arXiv:2602.16741v1 Announce Type: cross \nAbstract: AI-assisted code review is widely used to detect vulnerabilities before production release. Prior work shows that adversarial prompt manipulation can degrade large language model (LLM) performance in code generation. We test whether similar comment-based manipulation misleads LLMs during vulnerability detection. We build a 100-sample benchmark across Python, JavaScript, and Java, each paired with eight comment variants ranging from no comments to adversarial strategies such as authority spoofing and technical deception. Eight frontier models, five commercial and three open-source, are evaluated in 9,366 trials. Adversarial comments produce small, statistically non-significant effects on detection accuracy (McNemar exact p > 0.21; all 95 percent confidence intervals include zero). This holds for commercial models with 89 to 96 percent baseline detection and open-source models with 53 to 72 percent, despite large absolute performance gap",
      "title": "Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis"
    },
    {
      "arxiv_id": "2602.17038",
      "authors": [
        "Shengtian Yang",
        "Yu Li",
        "Shuo He",
        "Yewen Li",
        "Qingpeng Cai",
        "Peng Jiang",
        "Lei Feng"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938099+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17038"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17038"
      },
      "published_at": "2026-02-19T03:18:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.819319902296025,
        "semantic_score": 2.875776356458664,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.055096258754688
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17038",
      "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different ",
      "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.15571",
      "authors": [
        "Davide Casnici",
        "Martin Lefebvre",
        "Justin Dauwels",
        "Charlotte Frenkel"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.180743+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
          "url": "https://arxiv.org/abs/2602.15571"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
        "url": "https://arxiv.org/abs/2602.15571"
      },
      "published_at": "2026-02-17T13:29:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.6998644306027969,
        "semantic_score": 3.6325578093528748,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.03242223995567
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15571",
      "summary": "Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which",
      "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment"
    },
    {
      "arxiv_id": "2602.16898",
      "authors": [
        "Iman Ahmadi",
        "Mehrshad Taji",
        "Arad Mahdinezhad Kashani",
        "AmirHossein Jadidi",
        "Saina Kashani",
        "Babak Khalaj"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934301+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
          "url": "https://arxiv.org/abs/2602.16898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
        "url": "https://arxiv.org/abs/2602.16898"
      },
      "published_at": "2026-02-18T21:28:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7996700025213651,
        "semantic_score": 3.52805449962616,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.027724502147528
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16898",
      "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi g",
      "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation"
    },
    {
      "arxiv_id": "2602.17168",
      "authors": [
        "Siyuan Liang",
        "Yongcheng Jing",
        "Yingjie Wang",
        "Jiaxing Huang",
        "Ee-chien Chang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.295660+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
          "url": "https://arxiv.org/abs/2602.17168"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning",
        "url": "https://arxiv.org/abs/2602.17168"
      },
      "published_at": "2026-02-19T08:31:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.837310120405275,
        "semantic_score": 3.4865437924861906,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.023853912891465
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17168",
      "summary": "Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both ch",
      "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning"
    },
    {
      "arxiv_id": "2602.16660",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:33.179248+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
          "url": "https://arxiv.org/abs/2602.16660"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
        "url": "https://arxiv.org/abs/2602.16660"
      },
      "published_at": "2026-02-18T18:01:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7882268674482866,
        "semantic_score": 4.616972184181213,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.005199051629496
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16660",
      "summary": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment.",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment"
    },
    {
      "arxiv_id": "2602.17452",
      "authors": [
        "Wyatt Benno, Alberto Centelles, Antoine Douchet, Khalil Gibran"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945306+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
          "url": "https://arxiv.org/abs/2602.17452"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
        "url": "https://arxiv.org/abs/2602.17452"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.310042691230774,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.99204077769271
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17452",
      "summary": "arXiv:2602.17452v1 Announce Type: cross \nAbstract: We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.\n  Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verif",
      "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge"
    },
    {
      "arxiv_id": "2507.19634",
      "authors": [
        "Sara Papi, Maike Z\\\"ufle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, Jan Niehues"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.911160+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
          "url": "https://arxiv.org/abs/2507.19634"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
        "url": "https://arxiv.org/abs/2507.19634"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.392255425453186,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.974253511915123
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2507.19634",
      "summary": "arXiv:2507.19634v3 Announce Type: replace-cross \nAbstract: Recent advances in large language models have laid the foundation for multimodal LLMs (MLLMs), which unify text, speech, and vision within a single framework. As these models are rapidly evolving toward general-purpose instruction following across diverse and complex tasks, a key frontier is evaluating their crosslingual and multimodal capabilities over both short- and long-form inputs. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on a single modality at a time, rely on short-form inputs, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first crosslingual human-annotated benchmark based on scientific talks on NLP and beyond. MCIF evaluates instruction following in crossl",
      "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks"
    },
    {
      "arxiv_id": "2602.06275",
      "authors": [
        "Isaac Picov, Ritesh Goru"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.903252+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution",
          "url": "https://arxiv.org/abs/2602.06275"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution",
        "url": "https://arxiv.org/abs/2602.06275"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.391531121730805,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.97352920819274
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.06275",
      "summary": "arXiv:2602.06275v2 Announce Type: replace \nAbstract: Explaining closed-source Large Language Model (LLM) outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce \\textbf{Rotary Positional Embedding Linear Local Interpretable Model-agnostic Explanations (RoPE-LIME)}, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in \\textbf{RoPE embedding space} for stable similarity under masking, and (ii) \\textbf{Sparse-$K$} sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on Hotpot",
      "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution"
    },
    {
      "arxiv_id": "2502.03752",
      "authors": [
        "Sanghyeon Lee, Sangjun Bae, Yisak Park, Seungyul Han"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.908522+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning",
          "url": "https://arxiv.org/abs/2502.03752"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning",
        "url": "https://arxiv.org/abs/2502.03752"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8407942354679108,
        "tier_score": 2.0,
        "topic_score": 3.45,
        "total_score": 28.972792321929848
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2502.03752",
      "summary": "arXiv:2502.03752v4 Announce Type: replace \nAbstract: Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks. Our c",
      "title": "Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.17431",
      "authors": [
        "Dylan Bouchard, Mohit Singh Chauhan, Viren Bajaj, David Skarbrevik"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.944976+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
          "url": "https://arxiv.org/abs/2602.17431"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study",
        "url": "https://arxiv.org/abs/2602.17431"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.3816238164901735,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.96362190295211
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17431",
      "summary": "arXiv:2602.17431v1 Announce Type: new \nAbstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. ",
      "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study"
    },
    {
      "arxiv_id": "2602.17646",
      "authors": [
        "Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.479078+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements",
          "url": "https://arxiv.org/abs/2602.17646"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements",
        "url": "https://arxiv.org/abs/2602.17646"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.367195987701416,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.94919407416335
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17646",
      "summary": "arXiv:2602.17646v1 Announce Type: new \nAbstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual ",
      "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements"
    },
    {
      "arxiv_id": "2601.07463",
      "authors": [
        "Sijia li, Xinran Li, Shibo Chen, Jun Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.915880+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2601.07463"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2601.07463"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.2600701689720153,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.942068255433952
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2601.07463",
      "summary": "arXiv:2601.07463v2 Announce Type: replace-cross \nAbstract: Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implici",
      "title": "Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.16968",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:27.936496+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
          "url": "https://arxiv.org/abs/2602.16968"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
        "url": "https://arxiv.org/abs/2602.16968"
      },
      "published_at": "2026-02-19T00:15:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.808964230221924,
        "semantic_score": 2.0888734340667723,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.937837664288697
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16968",
      "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers"
    },
    {
      "arxiv_id": "2602.16784",
      "authors": [
        "Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael"
      ],
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ME"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.457452+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
          "url": "https://arxiv.org/abs/2602.16784"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Omitted Variable Bias in Language Models Under Distribution Shift",
        "url": "https://arxiv.org/abs/2602.16784"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.32543420791626,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.907432294378197
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16784",
      "summary": "arXiv:2602.16784v1 Announce Type: cross \nAbstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization prov",
      "title": "Omitted Variable Bias in Language Models Under Distribution Shift"
    },
    {
      "arxiv_id": "2602.17532",
      "authors": [
        "Ihor Kendiukhov"
      ],
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.946324+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
          "url": "https://arxiv.org/abs/2602.17532"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
        "url": "https://arxiv.org/abs/2602.17532"
      },
      "published_at": "2026-02-19T16:43:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8664085405281376,
        "semantic_score": 3.3358509957790377,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.902259536307174
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17532",
      "summary": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incre",
      "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal"
    },
    {
      "arxiv_id": "2602.16964",
      "authors": [
        "Prasham Titiya, Rohit Khoja, Tomer Wolfson, Vivek Gupta, Dan Roth"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T02:04:37.270176+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data",
          "url": "https://arxiv.org/abs/2602.16964"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data",
        "url": "https://arxiv.org/abs/2602.16964"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.6512888431549073,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.893286929616842
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.16964",
      "summary": "arXiv:2602.16964v1 Announce Type: new \nAbstract: Retrieval-augmented question answering over heterogeneous corpora requires connected evidence across text, tables, and graph nodes. While entity-level knowledge graphs support structured access, they are costly to construct and maintain, and inefficient to traverse at query time. In contrast, standard retriever-reader pipelines use flat similarity search over independently chunked text, missing multi-hop evidence chains across modalities. We propose SAGE (Structure Aware Graph Expansion) framework that (i) constructs a chunk-level graph offline using metadata-driven similarities with percentile-based pruning, and (ii) performs online retrieval by running an initial baseline retriever to obtain k seed chunks, expanding first-hop neighbors, and then filtering the neighbors using dense+sparse retrieval, selecting k' additional chunks. We instantiate the initial retriever using hybrid dense+sparse retrieval for implicit cross-modal corpora a",
      "title": "SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data"
    },
    {
      "arxiv_id": "2505.20650",
      "authors": [
        "Yan Wang, Lingfei Qian, Xueqing Peng, Yang Ren, Keyi Wang, Yi Han, Dongji Feng, Fengran Mo, Shengyuan Lin, Qinchuan Zhang, Kaiwen He, Chenri Luo, Jianxing Chen, Junwei Wu, Chen Xu, Ziyang Xu, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, Qianqian Xie, Jian-Yun Nie"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.910292+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information",
          "url": "https://arxiv.org/abs/2505.20650"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information",
        "url": "https://arxiv.org/abs/2505.20650"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.303859043121338,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.885857129583272
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2505.20650",
      "summary": "arXiv:2505.20650v4 Announce Type: replace \nAbstract: Accurate interpretation of numerical data in financial reports is critical for markets and regulators. Although XBRL (eXtensible Business Reporting Language) provides a standard for tagging financial figures, mapping thousands of facts to over 10k US GAAP concepts remains costly and error prone. Existing benchmarks oversimplify this task as flat, single step classification over small subsets of concepts, ignoring the hierarchical semantics of the taxonomy and the structured nature of financial documents. Consequently, these benchmarks fail to evaluate Large Language Models (LLMs) under realistic reporting conditions. To bridge this gap, we introduce FinTagging, the first comprehensive benchmark for structure aware and full scope XBRL tagging. We decompose the complex tagging process into two subtasks: (1) FinNI (Financial Numeric Identification), which extracts entities and types from heterogeneous contexts including text and tables;",
      "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information"
    },
    {
      "arxiv_id": "2602.17327",
      "authors": [
        "Michael Dinzinger, Laura Caspari, Ali Salman, Irvin Topi, Jelena Mitrovi\\'c, Michael Granitzer"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943411+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
          "url": "https://arxiv.org/abs/2602.17327"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval",
        "url": "https://arxiv.org/abs/2602.17327"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.295653593540192,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.877651680002128
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17327",
      "summary": "arXiv:2602.17327v1 Announce Type: new \nAbstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for den",
      "title": "WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval"
    },
    {
      "arxiv_id": "2602.17054",
      "authors": [
        "Hussein S. Al-Olimat, Ahmad Alshareef"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.938655+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
          "url": "https://arxiv.org/abs/2602.17054"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning",
        "url": "https://arxiv.org/abs/2602.17054"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.287454378604889,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.869452465066825
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17054",
      "summary": "arXiv:2602.17054v1 Announce Type: new \nAbstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but ",
      "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning"
    },
    {
      "arxiv_id": "2602.10117",
      "authors": [
        "Iv\\'an Arcuschin, David Chanin, Adri\\`a Garriga-Alonso, Oana-Maria Camburu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917416+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
          "url": "https://arxiv.org/abs/2602.10117"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
        "url": "https://arxiv.org/abs/2602.10117"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.263626855611801,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.845624942073737
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.10117",
      "summary": "arXiv:2602.10117v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across seven LLMs o",
      "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention"
    },
    {
      "arxiv_id": "2602.17623",
      "authors": [
        "Alireza Sakhaeirad, Ali Ma'manpoosh, Arshia Hemmat"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.911636+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
          "url": "https://arxiv.org/abs/2602.17623"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models",
        "url": "https://arxiv.org/abs/2602.17623"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.262940192222596,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.84493827868453
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17623",
      "summary": "arXiv:2602.17623v1 Announce Type: new \nAbstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings dem",
      "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models"
    },
    {
      "arxiv_id": "2602.17009",
      "authors": [
        "Nikunj Gupta",
        "James Zachary Hare",
        "Jesse Milzman",
        "Rajgopal Kannan",
        "Viktor Prasanna"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.468052+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.17009"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.17009"
      },
      "published_at": "2026-02-19T02:13:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8156289736327378,
        "semantic_score": 3.3070092678070067,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.822638241439744
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17009",
      "summary": "Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \\textit{coordination cont",
      "title": "Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.16727",
      "authors": [
        "Hua Yan, Heng Tan, Yingxue Zhang, Yu Yang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.920952+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
          "url": "https://arxiv.org/abs/2602.16727"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
        "url": "https://arxiv.org/abs/2602.16727"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.236287552118301,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.818285638580235
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16727",
      "summary": "arXiv:2602.16727v1 Announce Type: cross \nAbstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments",
      "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation"
    },
    {
      "arxiv_id": "2602.17068",
      "authors": [
        "Xiaocai Zhang",
        "Neema Nassir",
        "Milad Haghani"
      ],
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.469115+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
          "url": "https://arxiv.org/abs/2602.17068"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control",
        "url": "https://arxiv.org/abs/2602.17068"
      },
      "published_at": "2026-02-19T04:18:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8227599024101556,
        "semantic_score": 3.275314301252365,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.798074203662523
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17068",
      "summary": "Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures sp",
      "title": "Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control"
    },
    {
      "arxiv_id": "2505.16723",
      "authors": [
        "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\\'c, Martin Vechev"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.447424+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LLM Fingerprinting via Semantically Conditioned Watermarks",
          "url": "https://arxiv.org/abs/2505.16723"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LLM Fingerprinting via Semantically Conditioned Watermarks",
        "url": "https://arxiv.org/abs/2505.16723"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.199236363172531,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.781234449634468
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2505.16723",
      "summary": "arXiv:2505.16723v3 Announce Type: replace-cross \nAbstract: Most LLM fingerprinting methods teach the model to respond to a few fixed queries with predefined atypical responses (keys). This memorization often does not survive common deployment steps such as finetuning or quantization, and such keys can be easily detected and filtered from LLM responses, ultimately breaking the fingerprint. To overcome these limitations we introduce LLM fingerprinting via semantically conditioned watermarks, replacing fixed query sets with a broad semantic domain, and replacing brittle atypical keys with a statistical watermarking signal diffused throughout each response. After teaching the model to watermark its responses only to prompts from a predetermined domain e.g., French language, the model owner can use queries from that domain to reliably detect the fingerprint and verify ownership. As we confirm in our thorough experimental evaluation, our fingerprint is both stealthy and robust to all common ",
      "title": "LLM Fingerprinting via Semantically Conditioned Watermarks"
    },
    {
      "arxiv_id": "2510.00167",
      "authors": [
        "Diego Ortiz Barbosa, Mohit Agrawal, Yash Malegaonkar, Luis Burbano, Axel Andersson, Gy\\\"orgy D\\'an, Henrik Sandberg, Alvaro A. Cardenas"
      ],
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912386+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI",
          "url": "https://arxiv.org/abs/2510.00167"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI",
        "url": "https://arxiv.org/abs/2510.00167"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.0909749269485474,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.772973013410486
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2510.00167",
      "summary": "arXiv:2510.00167v2 Announce Type: replace-cross \nAbstract: Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous",
      "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI"
    },
    {
      "arxiv_id": "2602.08801",
      "authors": [
        "Thanh Le, Hai Duong, ThanhVu Nguyen, Takeshi Matsumura"
      ],
      "categories": [
        "cs.LO",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.595082+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Verifying DNN-based Semantic Communication Against Generative Adversarial Noise",
          "url": "https://arxiv.org/abs/2602.08801"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Verifying DNN-based Semantic Communication Against Generative Adversarial Noise",
        "url": "https://arxiv.org/abs/2602.08801"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.181576901674271,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.763574988136206
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2602.08801",
      "summary": "arXiv:2602.08801v2 Announce Type: replace-cross \nAbstract: Safety-critical applications like autonomous vehicles and industrial IoT are adopting semantic communication (SemCom) systems using deep neural networks to reduce bandwidth and increase transmission speed by transmitting only task-relevant semantic features.\n  However, adversarial attacks against these DNN-based SemCom systems can cause catastrophic failures by manipulating transmitted semantic features.\n  Existing defense mechanisms rely on empirical approaches provide no formal guarantees against the full spectrum of adversarial perturbations.\n  We present VSCAN, a neural network verification framework that provides mathematical robustness guarantees by formulating adversarial noise generation as mixed integer programming and verifying end-to-end properties across multiple interconnected networks (encoder, decoder, and task model).\n  Our key insight is that realistic adversarial constraints (power limitations and statistical ",
      "title": "Verifying DNN-based Semantic Communication Against Generative Adversarial Noise"
    },
    {
      "arxiv_id": "2602.16979",
      "authors": [
        "Divyam Madaan, Sumit Chopra, Kyunghyun Cho"
      ],
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.467581+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling",
          "url": "https://arxiv.org/abs/2602.16979"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling",
        "url": "https://arxiv.org/abs/2602.16979"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.177017259597778,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.759015346059712
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.16979",
      "summary": "arXiv:2602.16979v1 Announce Type: new \nAbstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of predi",
      "title": "Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling"
    },
    {
      "arxiv_id": "2602.15438",
      "authors": [
        "Beatrix M. G. Nielsen, Emanuele Marconato, Luigi Gresele, Andrea Dittadi, Simon Buchholz"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.918641+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Logit Distance Bounds Representational Similarity",
          "url": "https://arxiv.org/abs/2602.15438"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Logit Distance Bounds Representational Similarity",
        "url": "https://arxiv.org/abs/2602.15438"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.3464542627334595,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 28.678452349195396
      },
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.15438",
      "summary": "arXiv:2602.15438v2 Announce Type: replace-cross \nAbstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; y",
      "title": "Logit Distance Bounds Representational Similarity"
    },
    {
      "arxiv_id": "2509.24368",
      "authors": [
        "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\\'c, Martin Vechev"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912231+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Watermarking Diffusion Language Models",
          "url": "https://arxiv.org/abs/2509.24368"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Watermarking Diffusion Language Models",
        "url": "https://arxiv.org/abs/2509.24368"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.9332177340984344,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.615215820560373
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2509.24368",
      "summary": "arXiv:2509.24368v2 Announce Type: replace \nAbstract: We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and ",
      "title": "Watermarking Diffusion Language Models"
    },
    {
      "arxiv_id": "2602.17607",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.947965+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
          "url": "https://arxiv.org/abs/2602.17607"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
        "url": "https://arxiv.org/abs/2602.17607"
      },
      "published_at": "2026-02-19T18:31:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8729714477653249,
        "semantic_score": 1.919625848531723,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.59259729629705
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17607",
      "summary": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language d",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing"
    },
    {
      "arxiv_id": "2505.02819",
      "authors": [
        "Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.909402+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
          "url": "https://arxiv.org/abs/2505.02819"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
        "url": "https://arxiv.org/abs/2505.02819"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.9009675860404966,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.582965672502432
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2505.02819",
      "summary": "arXiv:2505.02819v4 Announce Type: replace \nAbstract: We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\\% pruning while retaining approx",
      "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization"
    },
    {
      "arxiv_id": "2602.16823",
      "authors": [
        "Itamar Hadad, Guy Katz, Shahaf Bassan"
      ],
      "categories": [
        "cs.LG",
        "cs.LO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.458886+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
          "url": "https://arxiv.org/abs/2602.16823"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
        "url": "https://arxiv.org/abs/2602.16823"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.891884815692902,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.57388290215484
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16823",
      "summary": "arXiv:2602.16823v1 Announce Type: new \nAbstract: *Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yield circuits with *provable guarantees*. We focus on three types of guarantees: (1) *input domain robustness*, ensuring the circuit agrees with the model across a continuous input region; (2) *robust patching*, certifying circuit alignment under continuous patching perturbations; and (3) *minimality*, formalizing and capturing a wide array of various notions of succinctness. Interestingly, we uncover a diverse set of novel theoretical connections a",
      "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees"
    },
    {
      "arxiv_id": "2602.17659",
      "authors": [
        "Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.300226+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
          "url": "https://arxiv.org/abs/2602.17659"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
        "url": "https://arxiv.org/abs/2602.17659"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9862498700618745,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.56824795652381
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.17659",
      "summary": "arXiv:2602.17659v1 Announce Type: cross \nAbstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning ",
      "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs"
    },
    {
      "arxiv_id": "2602.09725",
      "authors": [
        "Liang Mi, Weijun Wang, Jinghan Chen, Ting Cao, Haipeng Dai, Yunxin Liu"
      ],
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.453481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs",
          "url": "https://arxiv.org/abs/2602.09725"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs",
        "url": "https://arxiv.org/abs/2602.09725"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3114639222621918,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.553462008724125
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.09725",
      "summary": "arXiv:2602.09725v2 Announce Type: cross \nAbstract: Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving min",
      "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs"
    },
    {
      "arxiv_id": "2505.21862",
      "authors": [
        "Chenhui Zhao, Yiwei Lyu, Asadur Chowdury, Edward Harake, Akhil Kondepudi, Akshay Rao, Xinhai Hou, Honglak Lee, Todd Hollon"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.279613+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging",
          "url": "https://arxiv.org/abs/2505.21862"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging",
        "url": "https://arxiv.org/abs/2505.21862"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8502393722534176,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.532237458715354
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2505.21862",
      "summary": "arXiv:2505.21862v3 Announce Type: replace \nAbstract: The scalability of current language-image pre-training for 3D medical imaging, such as CT and MRI, is constrained by the need for radiologists to manually curate raw clinical studies. In this work, we pioneer pre-training directly on uncurated studies, which both aligns more closely with the radiologist's workflow and provides a natural path to scalability. However, the unique structure of such data presents new challenges for existing model architectures, which were originally designed for 2D slices or single 3D scans. To address this, we introduce a novel hierarchical attention mechanism inspired by the intrinsic hierarchy of radiology data: slice, scan, and study. We denote our framework as Hierarchical attention for Language-Image Pre-training (HLIP). Trained on 220K studies with 3.13 million scans for brain MRI and 240K studies with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +10.5% balanced",
      "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging"
    },
    {
      "arxiv_id": "2602.17171",
      "authors": [
        "Ayush Goel, Arjun Kohli, Sarvagya Somvanshi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.941010+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks",
          "url": "https://arxiv.org/abs/2602.17171"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks",
        "url": "https://arxiv.org/abs/2602.17171"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.2806777596473693,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.522675846109305
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17171",
      "summary": "arXiv:2602.17171v1 Announce Type: new \nAbstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.",
      "title": "In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks"
    },
    {
      "arxiv_id": "2602.17386",
      "authors": [
        "Adri\\`a Molina, Oriol Ramos Terrades, Josep Llad\\'os"
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.944094+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
          "url": "https://arxiv.org/abs/2602.17386"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
        "url": "https://arxiv.org/abs/2602.17386"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.927306890487671,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.509304976949608
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17386",
      "summary": "arXiv:2602.17386v1 Announce Type: cross \nAbstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that of",
      "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval"
    },
    {
      "arxiv_id": "2602.16944",
      "authors": [
        "Philip Sosnin, Jodie Knapp, Fraser Kennedy, Josh Collyer, Calvin Tsay"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.466082+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming",
          "url": "https://arxiv.org/abs/2602.16944"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming",
        "url": "https://arxiv.org/abs/2602.16944"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8006612956523895,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.482659382114328
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16944",
      "summary": "arXiv:2602.16944v1 Announce Type: new \nAbstract: This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.",
      "title": "Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Yunfei Bai"
      ],
      "categories": [
        "Amazon Bedrock AgentCore",
        "Artificial Intelligence",
        "Best Practices",
        "Generative AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:25.867949+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
          "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
        "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon"
      },
      "published_at": "2026-02-18T11:21:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7666374168098858,
        "semantic_score": 3.480947208404541,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 29.247584625214426
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:328a53729df06635",
      "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon&nbsp;through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with&nbsp;Amazon use case-specific evaluation approaches and metrics.&nbsp;",
      "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon"
    },
    {
      "arxiv_id": "2602.17022",
      "authors": [
        "Takyoung Kim, Jinseok Nam, Chandrayee Basu, Xing Fan, Chengyuan Ma, Heng Ji, Gokhan Tur, Dilek Hakkani-T\\\"ur"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937629+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
          "url": "https://arxiv.org/abs/2602.17022"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
        "url": "https://arxiv.org/abs/2602.17022"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8987216472625734,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.48071973372451
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17022",
      "summary": "arXiv:2602.17022v1 Announce Type: new \nAbstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context",
      "title": "ReIn: Conversational Error Recovery with Reasoning Inception"
    },
    {
      "arxiv_id": "2602.11337",
      "authors": [
        "Yejin Kim, Wilbert Pumacay, Omar Rayyan, Max Argus, Winson Han, Eli VanderBilt, Jordi Salvador, Abhay Deshpande, Rose Hendrix, Snehal Jauhri, Shuo Liu, Nur Muhammad Mahi Shafiullah, Maya Guru, Ainaz Eftekhar, Karen Farley, Donovan Clay, Jiafei Duan, Arjun Guru, Piper Wolters, Alvaro Herrasti, Ying-Chun Lee, Georgia Chalvatzaki, Yuchen Cui, Ali Farhadi, Dieter Fox, Ranjay Krishna"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917678+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
          "url": "https://arxiv.org/abs/2602.11337"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
        "url": "https://arxiv.org/abs/2602.11337"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.794997888803482,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.476995975265417
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.11337",
      "summary": "arXiv:2602.11337v2 Announce Type: replace \nAbstract: Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: stati",
      "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation"
    },
    {
      "arxiv_id": "2602.17510",
      "authors": [
        "Kasun Dewage, Marianna Pensky, Suranadi De Silva, Shankadeep Mondal"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945864+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
          "url": "https://arxiv.org/abs/2602.17510"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights",
        "url": "https://arxiv.org/abs/2602.17510"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.200970768928528,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.44296885539046
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17510",
      "summary": "arXiv:2602.17510v1 Announce Type: new \nAbstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $\\Delta W$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. ",
      "title": "LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights"
    },
    {
      "arxiv_id": "2510.21193",
      "authors": [
        "Helena Grete Lillepalu, Tanel Alum\\\"ae"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.890552+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Estonian Native Large Language Model Benchmark",
          "url": "https://arxiv.org/abs/2510.21193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Estonian Native Large Language Model Benchmark",
        "url": "https://arxiv.org/abs/2510.21193"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.507667607069015,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.42966569353095
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2510.21193",
      "summary": "arXiv:2510.21193v2 Announce Type: replace \nAbstract: The availability of LLM benchmarks for the Estonian language is limited, and a comprehensive evaluation comparing the performance of different LLMs on Estonian tasks has yet to be conducted. We introduce a new benchmark for evaluating LLMs in Estonian, based on seven diverse datasets. These datasets assess general and domain-specific knowledge, understanding of Estonian grammar and vocabulary, summarization abilities, contextual comprehension, and more. The datasets are all generated from native Estonian sources without using machine translation. We compare the performance of base models, instruction-tuned open-source models, and commercial models. Our evaluation includes 6 base models and 26 instruction-tuned models. To assess the results, we employ both human evaluation and LLM-as-a-judge methods. Human evaluation scores showed moderate to high correlation with benchmark evaluations, depending on the dataset. Claude 3.7 Sonnet, use",
      "title": "Estonian Native Large Language Model Benchmark"
    },
    {
      "arxiv_id": "2602.17518",
      "authors": [
        "Francesca Pezzuti, Ophir Frieder, Fabrizio Silvestri, Sean MacAvaney, Nicola Tonellotto"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.082049+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "A Picture of Agentic Search",
          "url": "https://arxiv.org/abs/2602.17518"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "A Picture of Agentic Search",
        "url": "https://arxiv.org/abs/2602.17518"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8296133875846863,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.411611474046623
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17518",
      "summary": "arXiv:2602.17518v1 Announce Type: new \nAbstract: With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data pr",
      "title": "A Picture of Agentic Search"
    },
    {
      "arxiv_id": "2602.13294",
      "authors": [
        "Jiarong Liang, Max Ku, Ka-Hei Hui, Ping Nie, Wenhu Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.918094+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
          "url": "https://arxiv.org/abs/2602.13294"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
        "url": "https://arxiv.org/abs/2602.13294"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1589873671531676,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.400985453615103
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.13294",
      "summary": "arXiv:2602.13294v2 Announce Type: replace \nAbstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline p",
      "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction"
    },
    {
      "arxiv_id": "2602.17149",
      "authors": [
        "Tong Guan, Sheng Pan, Johan Barthelemy, Zhao Li, Yujun Cai, Cesare Alippi, Ming Jin, Shirui Pan"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.940785+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "TimeOmni-VL: Unified Models for Time Series Understanding and Generation",
          "url": "https://arxiv.org/abs/2602.17149"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "TimeOmni-VL: Unified Models for Time Series Understanding and Generation",
        "url": "https://arxiv.org/abs/2602.17149"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.805962061882019,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.387960148343954
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17149",
      "summary": "arXiv:2602.17149v1 Announce Type: new \nAbstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. W",
      "title": "TimeOmni-VL: Unified Models for Time Series Understanding and Generation"
    },
    {
      "arxiv_id": "2512.18957",
      "authors": [
        "Debamita Ghosh, George K. Atia, Yue Wang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.451513+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Online Robust Reinforcement Learning with General Function Approximation",
          "url": "https://arxiv.org/abs/2512.18957"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Online Robust Reinforcement Learning with General Function Approximation",
        "url": "https://arxiv.org/abs/2512.18957"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.803326904773712,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.38532499123565
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2512.18957",
      "summary": "arXiv:2512.18957v2 Announce Type: replace \nAbstract: In many real-world settings, reinforcement learning systems suffer performance degradation when the environment encountered at deployment differs from that observed during training. Distributionally robust reinforcement learning (DR-RL) mitigates this issue by seeking policies that maximize performance under the most adverse transition dynamics within a prescribed uncertainty set. Most existing DR-RL approaches, however, rely on strong data availability assumptions, such as access to a generative model or large offline datasets, and are largely restricted to tabular settings.\n  In this work, we propose a fully online DR-RL algorithm with general function approximation that learns robust policies solely through interaction, without requiring prior knowledge or pre-collected data. Our approach is based on a dual-driven fitted robust Bellman procedure that simultaneously estimates the value function and the corresponding worst-case back",
      "title": "Online Robust Reinforcement Learning with General Function Approximation"
    },
    {
      "arxiv_id": "2602.17664",
      "authors": [
        "Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.954230+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Sink-Aware Pruning for Diffusion Language Models",
          "url": "https://arxiv.org/abs/2602.17664"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Sink-Aware Pruning for Diffusion Language Models",
        "url": "https://arxiv.org/abs/2602.17664"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8017354607582092,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.383733547220146
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17664",
      "summary": "arXiv:2602.17664v1 Announce Type: new \nAbstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is ",
      "title": "Sink-Aware Pruning for Diffusion Language Models"
    },
    {
      "arxiv_id": "2602.17415",
      "authors": [
        "Yi Zhang, Omar Faris, Chapa Sirithunge, Kai-Fung Chu, Fumiya Iida, Fulvio Forni"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.313990+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace",
          "url": "https://arxiv.org/abs/2602.17415"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace",
        "url": "https://arxiv.org/abs/2602.17415"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.138479322195053,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.380477408656986
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.17415",
      "summary": "arXiv:2602.17415v1 Announce Type: new \nAbstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control ",
      "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace"
    },
    {
      "arxiv_id": "2602.16833",
      "authors": [
        "Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.933448+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study",
          "url": "https://arxiv.org/abs/2602.16833"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study",
        "url": "https://arxiv.org/abs/2602.16833"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7979471385478973,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.379945225009834
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16833",
      "summary": "arXiv:2602.16833v1 Announce Type: new \nAbstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by ",
      "title": "VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study"
    },
    {
      "arxiv_id": "2509.07825",
      "authors": [
        "Zhuoxu Huang, Mingqi Gao, Jungong Han"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.280659+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model",
          "url": "https://arxiv.org/abs/2509.07825"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model",
        "url": "https://arxiv.org/abs/2509.07825"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7933852434158326,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.375383329877767
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2509.07825",
      "summary": "arXiv:2509.07825v2 Announce Type: replace \nAbstract: 3D object segmentation with Large Language Models (LLMs) has become a prevailing paradigm due to its broad semantics, task flexibility, and strong generalization. However, this paradigm is hindered by representation misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds convey only dense geometric structures. In prior methods, misalignment limits both input and output. At the input stage, dense point patches require heavy pre-alignment, weakening object-level semantics and confusing similar distractors. At the output stage, predictions depend only on dense features without explicit geometric cues, leading to a loss of fine-grained accuracy. To address these limitations, we present the Point Linguist Model (PLM), a general framework that bridges the representation gap between LLMs and dense 3D point clouds without requiring large-scale pre-alignment between 3D-text or 3D-images. Specifically, we introduce Objec",
      "title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model"
    },
    {
      "arxiv_id": "2602.16705",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:38.631927+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
          "url": "https://arxiv.org/abs/2602.16705"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
        "url": "https://arxiv.org/abs/2602.16705"
      },
      "published_at": "2026-02-18T18:55:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7911690373189665,
        "semantic_score": 3.0773704290390014,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.36853946635797
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16705",
      "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation"
    },
    {
      "arxiv_id": "2507.05411",
      "authors": [
        "Mark Lee, Chang Lan, Tom Gunter, John Peebles, Hanzhi Zhou, Kelvin Zou, Sneha Bangalore, Chung-Cheng Chiu, Nan Du, Xianzhi Du, Philipp Dufter, Ruixuan Hou, Haoshuo Huang, Dongseong Hwang, Xiang Kong, Jinhao Lei, Tao Lei, Meng Li, Li Li, Jiarui Lu, Zhiyun Lu, Yiping Ma, David Qiu, Vivek Rathod, Senyu Tong, Zhucheng Tu, Jianyu Wang, Yongqiang Wang, Zirui Wang, Floris Weers, Sam Wiseman, Guoli Yin, Bowen Zhang, Xiyou Zhou, Danyang Zhuo, Cheng Leong, Ruoming Pang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.448679+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "AXLearn: Modular, Hardware-Agnostic Large Model Training",
          "url": "https://arxiv.org/abs/2507.05411"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "AXLearn: Modular, Hardware-Agnostic Large Model Training",
        "url": "https://arxiv.org/abs/2507.05411"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1020403504371643,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.344038436899098
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2507.05411",
      "summary": "arXiv:2507.05411v3 Announce Type: replace \nAbstract: AXLearn is a production system which facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-art deep learning systems, AXLearn has a unique focus on modularity and support for hardware-agnostic training. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on different hardware infrastructure. AXLearn maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in state-of-the-art training systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. ",
      "title": "AXLearn: Modular, Hardware-Agnostic Large Model Training"
    },
    {
      "arxiv_id": "2504.21730",
      "authors": [
        "Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbin Li, Yiming Li"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.909287+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises",
          "url": "https://arxiv.org/abs/2504.21730"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises",
        "url": "https://arxiv.org/abs/2504.21730"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.6560543179512024,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.33805240441314
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2504.21730",
      "summary": "arXiv:2504.21730v2 Announce Type: replace-cross \nAbstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To addre",
      "title": "Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises"
    },
    {
      "arxiv_id": "2602.10993",
      "authors": [
        "Ivan Vuli\\'c, Adam Grycner, Quentin de Laroussilhe, Jonas Pfeiffer"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.917536+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules",
          "url": "https://arxiv.org/abs/2602.10993"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules",
        "url": "https://arxiv.org/abs/2602.10993"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.75449076294899,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.336488849410927
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.10993",
      "summary": "arXiv:2602.10993v2 Announce Type: replace \nAbstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Rand",
      "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules"
    },
    {
      "arxiv_id": "2602.17242",
      "authors": [
        "Takao Inou\\'e"
      ],
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942540+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "TAPO-Structured Description Logic for Information Behavior: Procedural and Oracle-Based Extensions",
          "url": "https://arxiv.org/abs/2602.17242"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "TAPO-Structured Description Logic for Information Behavior: Procedural and Oracle-Based Extensions",
        "url": "https://arxiv.org/abs/2602.17242"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8473376512527464,
        "tier_score": 2.0,
        "topic_score": 3.9000000000000004,
        "total_score": 28.329335737714683
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17242",
      "summary": "arXiv:2602.17242v1 Announce Type: cross \nAbstract: We introduce \\emph{TAPO-Structured Description Logic} (TAPO--DL), a formal extension of classical description logic designed to model \\emph{information behavior} as a structured, dynamic process.\n  TAPO--DL extends the standard T--Box/A--Box architecture with two additional layers: a \\emph{Procedural Box} (P--Box), which supports concept-driven, imperative-style programs such as conditional and iterative actions, and an \\emph{Oracle Box} (O--Box), which formalizes controlled interaction with external information sources. While the terminological and assertional components capture static conceptual and factual knowledge, the procedural and oracle-based components enable the explicit representation of information-generating actions and external validation.\n  We provide a unified semantic framework for TAPO--DL based on a co-generative, sheaf-theoretic interpretation, in which local informational states are modeled as sections and informa",
      "title": "TAPO-Structured Description Logic for Information Behavior: Procedural and Oracle-Based Extensions"
    },
    {
      "arxiv_id": "2602.16994",
      "authors": [
        "Rahul Thomas, Teo Kitanovski, Micah Goldblum, Arka Pal"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.467807+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding",
          "url": "https://arxiv.org/abs/2602.16994"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding",
        "url": "https://arxiv.org/abs/2602.16994"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7438249588012695,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.325823045263206
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16994",
      "summary": "arXiv:2602.16994v1 Announce Type: new \nAbstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path",
      "title": "Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding"
    },
    {
      "arxiv_id": "2512.11108",
      "authors": [
        "Jonathan Kamp, Roos Bakker, Dominique Blok"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.914715+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
          "url": "https://arxiv.org/abs/2512.11108"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
        "url": "https://arxiv.org/abs/2512.11108"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7435292422771456,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.32552732873908
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2512.11108",
      "summary": "arXiv:2512.11108v2 Announce Type: replace \nAbstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find a trade-off between lexical and position biases in our mod",
      "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution"
    },
    {
      "arxiv_id": "2506.21039",
      "authors": [
        "Jaebak Hwang, Sanghyeon Lee, Jeongmo Kim, Seungyul Han"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.911050+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning",
          "url": "https://arxiv.org/abs/2506.21039"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning",
        "url": "https://arxiv.org/abs/2506.21039"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7236287593841553,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.305626845846092
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2506.21039",
      "summary": "arXiv:2506.21039v2 Announce Type: replace \nAbstract: Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, their reliance on conventional hindsight relabeling often fails to correct subgoal infeasibility, leading to inefficient high-level planning. To address this, we propose Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that integrates Frontier Experience Replay (FER) to separate unreachable from admissible subgoals and streamline high-level decision making. FER delineates the reachability frontier using failure and partial-success transitions, which identifies unreliable subgoals, increases subgoal reliability, and reduces unnecessary high-level decisions. Additionally, SSE employs a decoupled exploration policy to cover underexplored regions of the goal space and a path refinement that adj",
      "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.15852",
      "authors": [
        "Ha Na Cho, Sairam Sutari, Alexander Lopez, Hansen Bow, Kai Zheng"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.918996+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints",
          "url": "https://arxiv.org/abs/2602.15852"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints",
        "url": "https://arxiv.org/abs/2602.15852"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.717494928836823,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.299493015298758
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.15852",
      "summary": "arXiv:2602.15852v2 Announce Type: replace \nAbstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evalu",
      "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints"
    },
    {
      "arxiv_id": "2503.19356",
      "authors": [
        "Reza Pourreza, Rishit Dagli, Apratim Bhattacharyya, Sunny Panchal, Guillaume Berger, Roland Memisevic"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.279114+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?",
          "url": "https://arxiv.org/abs/2503.19356"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?",
        "url": "https://arxiv.org/abs/2503.19356"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7155015230178834,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.297499609479818
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2503.19356",
      "summary": "arXiv:2503.19356v2 Announce Type: replace \nAbstract: AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users a",
      "title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?"
    },
    {
      "arxiv_id": "2602.17315",
      "authors": [
        "Sourav Chakraborty, Amit Kiran Rege, Claire Monteleoni, Lijun Chen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943202+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Flickering Multi-Armed Bandits",
          "url": "https://arxiv.org/abs/2602.17315"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Flickering Multi-Armed Bandits",
        "url": "https://arxiv.org/abs/2602.17315"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.0429092943668365,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.28490738082877
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17315",
      "summary": "arXiv:2602.17315v1 Announce Type: new \nAbstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erd\\H{o}s--R\\'enyi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this pr",
      "title": "Flickering Multi-Armed Bandits"
    },
    {
      "arxiv_id": "2602.17559",
      "authors": [
        "Yaoyue Zheng, Yin Zhang, Joost van de Weijer, Gido M van de Ven, Shaoyi Du, Xuetao Zhang, Zhiqiang Tian"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.476807+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Revisiting Weight Regularization for Low-Rank Continual Learning",
          "url": "https://arxiv.org/abs/2602.17559"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Revisiting Weight Regularization for Low-Rank Continual Learning",
        "url": "https://arxiv.org/abs/2602.17559"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7028406262397766,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.284838712701713
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17559",
      "summary": "arXiv:2602.17559v1 Announce Type: new \nAbstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank re",
      "title": "Revisiting Weight Regularization for Low-Rank Continual Learning"
    },
    {
      "arxiv_id": "2602.16793",
      "authors": [
        "Xingyu Dang, Rohit Agarwal, Rodrigo Porto, Anirudh Goyal, Liam H Fowl, Sanjeev Arora"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.457696+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models",
          "url": "https://arxiv.org/abs/2602.16793"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models",
        "url": "https://arxiv.org/abs/2602.16793"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.5976565778255463,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.279654664287484
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16793",
      "summary": "arXiv:2602.16793v1 Announce Type: new \nAbstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently veri",
      "title": "Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models"
    },
    {
      "arxiv_id": "2602.16179",
      "authors": [
        "Sushant Mehta, Logan Ritchie, Suhaas Garre, Nick Heiner, Edwin Chen"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.919515+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
          "url": "https://arxiv.org/abs/2602.16179"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
        "url": "https://arxiv.org/abs/2602.16179"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.693214237689972,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.27521232415191
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16179",
      "summary": "arXiv:2602.16179v2 Announce Type: replace-cross \nAbstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI's suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37% to 36.76% task pass rate on held-out evaluation tasks. More importantly, these",
      "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments"
    },
    {
      "arxiv_id": "2601.12815",
      "authors": [
        "Zhaolu Kang, Junhao Gong, Qingxi Chen, Hao Zhang, Jiaxin Liu, Rong Fu, Zhiyuan Feng, Yuan Wang, Simon Fong, Kaiyue Zhou"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.902985+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
          "url": "https://arxiv.org/abs/2601.12815"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
        "url": "https://arxiv.org/abs/2601.12815"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.690969252586365,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.2729673390483
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2601.12815",
      "summary": "arXiv:2601.12815v5 Announce Type: replace \nAbstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives f",
      "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction"
    },
    {
      "arxiv_id": "2602.16720",
      "authors": [
        "Bowen Cao, Weibin Liao, Yushi Sun, Dong Fang, Haitao Li, Wai Lam"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.920699+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL",
          "url": "https://arxiv.org/abs/2602.16720"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL",
        "url": "https://arxiv.org/abs/2602.16720"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.589583432674408,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.271581519136344
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16720",
      "summary": "arXiv:2602.16720v1 Announce Type: cross \nAbstract: Text-to-SQL systems powered by Large Language Models have excelled on academic benchmarks but struggle in complex enterprise environments. The primary limitation lies in their reliance on static schema representations, which fails to resolve semantic ambiguity and scale effectively to large, complex databases. To address this, we propose APEX-SQL, an Agentic Text-to-SQL Framework that shifts the paradigm from passive translation to agentic exploration. Our framework employs a hypothesis-verification loop to ground model reasoning in real data. In the schema linking phase, we use logical planning to verbalize hypotheses, dual-pathway pruning to reduce the search space, and parallel data profiling to validate column roles against real data, followed by global synthesis to ensure topological connectivity. For SQL generation, we introduce a deterministic mechanism to retrieve exploration directives, allowing the agent to effectively explor",
      "title": "APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL"
    },
    {
      "arxiv_id": "2602.17445",
      "authors": [
        "Mateusz Nowak, Xavier Cadet, Peter Chin"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.474936+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ABCD: All Biases Come Disguised",
          "url": "https://arxiv.org/abs/2602.17445"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ABCD: All Biases Come Disguised",
        "url": "https://arxiv.org/abs/2602.17445"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.661843013763428,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.243841100225364
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17445",
      "summary": "arXiv:2602.17445v1 Announce Type: new \nAbstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any ",
      "title": "ABCD: All Biases Come Disguised"
    },
    {
      "arxiv_id": "2602.17037",
      "authors": [
        "Rahul Nanda",
        "Chandra Maddila",
        "Smriti Jha",
        "Euna Mehnaz Khan",
        "Matteo Paltenghi",
        "Satish Chandra"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937977+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Wink: Recovering from Misbehaviors in Coding Agents",
          "url": "https://arxiv.org/abs/2602.17037"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Wink: Recovering from Misbehaviors in Coding Agents",
        "url": "https://arxiv.org/abs/2602.17037"
      },
      "published_at": "2026-02-19T03:15:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8191207862411067,
        "semantic_score": 3.8188463926315306,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.237967178872637
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17037",
      "summary": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatica",
      "title": "Wink: Recovering from Misbehaviors in Coding Agents"
    },
    {
      "arxiv_id": "2602.16849",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.459946+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
          "url": "https://arxiv.org/abs/2602.16849"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
        "url": "https://arxiv.org/abs/2602.16849"
      },
      "published_at": "2026-02-18T20:25:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7961394645359111,
        "semantic_score": 2.732889688014984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.229029152550893
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16849",
      "summary": "We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.",
      "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking"
    },
    {
      "arxiv_id": "2602.17537",
      "authors": [
        "Qilong Cheng, Matthew Mackay, Ali Bereyhi"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.476109+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control",
          "url": "https://arxiv.org/abs/2602.17537"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control",
        "url": "https://arxiv.org/abs/2602.17537"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.982534337043762,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.224532423505696
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.17537",
      "summary": "arXiv:2602.17537v1 Announce Type: new \nAbstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse ci",
      "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control"
    },
    {
      "arxiv_id": "2602.17025",
      "authors": [
        "Gagan Mundada, Zihan Huang, Rohan Surana, Sheldon Yu, Jennifer Yuntong Zhang, Xintong Li, Tong Yu, Lina Yao, Jingbo Shang, Julian McAuley, Junda Wu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.468287+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
          "url": "https://arxiv.org/abs/2602.17025"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning",
        "url": "https://arxiv.org/abs/2602.17025"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.608599305152893,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.19059739161483
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17025",
      "summary": "arXiv:2602.17025v1 Announce Type: new \nAbstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial t",
      "title": "WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning"
    },
    {
      "arxiv_id": "2505.11235",
      "authors": [
        "Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.447203+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation",
          "url": "https://arxiv.org/abs/2505.11235"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation",
        "url": "https://arxiv.org/abs/2505.11235"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5981847643852234,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.18018285084716
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2505.11235",
      "summary": "arXiv:2505.11235v3 Announce Type: replace \nAbstract: Driven by the rapid growth of model parameters, parameter-efficient fine-tuning (PEFT) has become essential for adapting large models to diverse downstream tasks under constrained computational resources. Within this paradigm, orthogonal fine-tuning and its variants preserve semantic representations of pre-trained models, but struggle to achieve both expressiveness and efficiency in terms of parameter counts, memory, and computation. To overcome this limitation, we propose efficient Orthogonal Fine-Tuning with Principal Subspace adaptation (PSOFT), which confines orthogonal transformations to the principal subspace of pre-trained weights. Specifically, PSOFT constructs this subspace via matrix decomposition to enable compatible transformations with higher effective rank, establishes a theoretical condition that strictly maintains the geometry of this subspace for essential semantic preservation, and introduces efficient tunable vecto",
      "title": "Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation"
    },
    {
      "arxiv_id": "2510.14190",
      "authors": [
        "Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.450281+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation",
          "url": "https://arxiv.org/abs/2510.14190"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation",
        "url": "https://arxiv.org/abs/2510.14190"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.931852197647095,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.17385028410903
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2510.14190",
      "summary": "arXiv:2510.14190v2 Announce Type: replace \nAbstract: Diffusion models excel at generation, but their latent spaces are high dimensional and not explicitly organized for interpretation or control. We introduce ConDA (Contrastive Diffusion Alignment), a plug-and-play geometry layer that applies contrastive learning to pretrained diffusion latents using auxiliary variables (e.g., time, stimulation parameters, facial action units). ConDA learns a low-dimensional embedding whose directions align with underlying dynamical factors, consistent with recent contrastive learning results on structured and disentangled representations. In this embedding, simple nonlinear trajectories support smooth interpolation, extrapolation, and counterfactual editing while rendering remains in the original diffusion space. ConDA separates editing and rendering by lifting embedding trajectories back to diffusion latents with a neighborhood-preserving kNN decoder and is robust across inversion solvers. Across flu",
      "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation"
    },
    {
      "arxiv_id": "2602.16826",
      "authors": [
        "Nigel Doering, Rahath Malladi, Arshia Sangwan, David Danks, Tauhidur Rahman"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.923744+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind",
          "url": "https://arxiv.org/abs/2602.16826"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind",
        "url": "https://arxiv.org/abs/2602.16826"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5771989285945893,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.159197015056524
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16826",
      "summary": "arXiv:2602.16826v1 Announce Type: new \nAbstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.",
      "title": "HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind"
    },
    {
      "arxiv_id": "2602.17206",
      "authors": [
        "Ron Shapira Weber, Oren Freifeld"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "nvidia"
      ],
      "first_seen_at": "2026-02-20T20:28:28.471738+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
          "url": "https://arxiv.org/abs/2602.17206"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch",
        "url": "https://arxiv.org/abs/2602.17206"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7645116329193113,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.146509719381246
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17206",
      "summary": "arXiv:2602.17206v1 Announce Type: new \nAbstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.",
      "title": "SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch"
    },
    {
      "arxiv_id": "2602.17001",
      "authors": [
        "Zhao Tan, Yiji Zhao, Shiyu Wang, Chang Xu, Yuxuan Liang, Xiping Liu, Shirui Pan, Ming Jin"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937037+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases",
          "url": "https://arxiv.org/abs/2602.17001"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases",
        "url": "https://arxiv.org/abs/2602.17001"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.222683614492416,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.14468170095435
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17001",
      "summary": "arXiv:2602.17001v1 Announce Type: cross \nAbstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex tempora",
      "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases"
    },
    {
      "arxiv_id": "2509.22860",
      "authors": [
        "Artavazd Maranjyan, Peter Richt\\'arik"
      ],
      "categories": [
        "math.OC",
        "cs.DC",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.449365+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity",
          "url": "https://arxiv.org/abs/2509.22860"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity",
        "url": "https://arxiv.org/abs/2509.22860"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1124437153339386,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 28.14444180179587
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2509.22860",
      "summary": "arXiv:2509.22860v3 Announce Type: replace-cross \nAbstract: Asynchronous stochastic gradient methods are central to scalable distributed optimization, particularly when devices differ in computational capabilities. Such settings arise naturally in federated learning, where training takes place on smartphones and other heterogeneous edge devices. In addition to varying computation speeds, these devices often hold data from different distributions. However, existing asynchronous SGD methods struggle in such heterogeneous settings and face two key limitations. First, many rely on unrealistic assumptions of similarity across workers' data distributions. Second, methods that relax this assumption still fail to achieve theoretically optimal performance under heterogeneous computation times. We introduce Ringleader ASGD, the first asynchronous SGD algorithm that attains the theoretical lower bounds for parallel first-order stochastic methods in the smooth nonconvex regime, thereby achieving op",
      "title": "Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity"
    },
    {
      "arxiv_id": "2602.16585",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.436053+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
          "url": "https://arxiv.org/abs/2602.16585"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
        "url": "https://arxiv.org/abs/2602.16585"
      },
      "published_at": "2026-02-18T16:35:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.7835551956285908,
        "semantic_score": 2.656840354204178,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.140395549832768
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16585",
      "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a ",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows"
    },
    {
      "arxiv_id": "2602.16805",
      "authors": [
        "Yonatan Gideoni, Sebastian Risi, Yarin Gal"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.922841+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Simple Baselines are Competitive with Code Evolution",
          "url": "https://arxiv.org/abs/2602.16805"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Simple Baselines are Competitive with Code Evolution",
        "url": "https://arxiv.org/abs/2602.16805"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5570805370807648,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.1390786235427
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16805",
      "summary": "arXiv:2602.16805v1 Announce Type: cross \nAbstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done b",
      "title": "Simple Baselines are Competitive with Code Evolution"
    },
    {
      "arxiv_id": "2602.17608",
      "authors": [
        "Baihe Huang, Eric Xu, Kannan Ramchandran, Jiantao Jiao, Michael I. Jordan"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.948083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Towards Anytime-Valid Statistical Watermarking",
          "url": "https://arxiv.org/abs/2602.17608"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Towards Anytime-Valid Statistical Watermarking",
        "url": "https://arxiv.org/abs/2602.17608"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.551771914958954,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.13377000142089
      },
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.17608",
      "summary": "arXiv:2602.17608v1 Announce Type: cross \nAbstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case",
      "title": "Towards Anytime-Valid Statistical Watermarking"
    },
    {
      "arxiv_id": "2602.17645",
      "authors": [
        "Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.953762+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
          "url": "https://arxiv.org/abs/2602.17645"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting",
        "url": "https://arxiv.org/abs/2602.17645"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.539388132095337,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.121386218557273
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17645",
      "summary": "arXiv:2602.17645v1 Announce Type: cross \nAbstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment ",
      "title": "Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting"
    },
    {
      "arxiv_id": "2602.17013",
      "authors": [
        "Kevin D. Oden"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.468169+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Malliavin Calculus as Stochastic Backpropogation",
          "url": "https://arxiv.org/abs/2602.17013"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Malliavin Calculus as Stochastic Backpropogation",
        "url": "https://arxiv.org/abs/2602.17013"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.5335786938667297,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.115576780328666
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17013",
      "summary": "arXiv:2602.17013v1 Announce Type: new \nAbstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as",
      "title": "Malliavin Calculus as Stochastic Backpropogation"
    },
    {
      "arxiv_id": "2602.16738",
      "authors": [
        "Rebin Saleh, Khanh Pham Dinh, Bal\\'azs Vill\\'anyi, Truong-Son Hy"
      ],
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.455975+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
          "url": "https://arxiv.org/abs/2602.16738"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
        "url": "https://arxiv.org/abs/2602.16738"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.3839329063892363,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.06593099285117
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.16738",
      "summary": "arXiv:2602.16738v1 Announce Type: new \nAbstract: Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainabil",
      "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance"
    },
    {
      "arxiv_id": "2602.17133",
      "authors": [
        "Linwei Zhai, Han Ding, Mingzhi Lin, Cui Zhao, Fei Wang, Ge Wang, Wang Zhi, Wei Xi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.940570+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
          "url": "https://arxiv.org/abs/2602.17133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation",
        "url": "https://arxiv.org/abs/2602.17133"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.46666374206543,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.048661828527365
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17133",
      "summary": "arXiv:2602.17133v1 Announce Type: new \nAbstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and \"codebook collapse\" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assum",
      "title": "VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation"
    },
    {
      "arxiv_id": "2602.17222",
      "authors": [
        "Ben Yellin, Ehud Ezra, Mark Foreman, Shula Grinapol"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.942209+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
          "url": "https://arxiv.org/abs/2602.17222"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight",
        "url": "https://arxiv.org/abs/2602.17222"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.4639773607254027,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.045975447187338
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17222",
      "summary": "arXiv:2602.17222v1 Announce Type: new \nAbstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset lin",
      "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight"
    },
    {
      "arxiv_id": "2502.14762",
      "authors": [
        "Murat Onur Yildirim, Elif Ceren Gok Yildirim, Joaquin Vanschoren"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.446433+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Unlocking [CLS] Features for Continual Post-Training",
          "url": "https://arxiv.org/abs/2502.14762"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Unlocking [CLS] Features for Continual Post-Training",
        "url": "https://arxiv.org/abs/2502.14762"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.442898243665695,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.024896330127632
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2502.14762",
      "summary": "arXiv:2502.14762v2 Announce Type: replace-cross \nAbstract: Continual learning requires models to integrate new classes or domains over time while preserving previously acquired knowledge. Within this paradigm, foundation models often achieve strong performance, but they still remain subject to the stability-plasticity trade-off, where excessive plasticity leads to forgetting of prior knowledge, and excessive stability constrains the adaptation. This necessitates an effective post-training strategy that introduces minimal yet functional modifications. To address this challenge, we first introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate', or LuCA, designed to acquire task-specific knowledge through an adapter-calibrator couple, enabling well-refined feature representations. Then, for each task, we deploy a sparse LuCA module on top of the last classification token [CLS] just before the classifier, which we refer to as 'Token-level Sparse Calibration and Adaptati",
      "title": "Unlocking [CLS] Features for Continual Post-Training"
    },
    {
      "arxiv_id": "2508.11603",
      "authors": [
        "Zhe Zhu, Honghua Chen, Peng Li, Mingqiang Wei"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.280426+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "CoreEditor: Correspondence-constrained Diffusion for Consistent 3D Editing",
          "url": "https://arxiv.org/abs/2508.11603"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "CoreEditor: Correspondence-constrained Diffusion for Consistent 3D Editing",
        "url": "https://arxiv.org/abs/2508.11603"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.77155442237854,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.013552508840476
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2508.11603",
      "summary": "arXiv:2508.11603v3 Announce Type: replace \nAbstract: Text-driven 3D editing seeks to modify 3D scenes according to textual descriptions, and most existing approaches tackle this by adapting pre-trained 2D image editors to multi-view inputs. However, without explicit control over multi-view information exchange, they often fail to maintain cross-view consistency, leading to insufficient edits and blurry details. We introduce CoreEditor, a novel framework for consistent text-to-3D editing. The key innovation is a correspondence-constrained attention mechanism that enforces precise interactions between pixels expected to remain consistent throughout the diffusion denoising process. Beyond relying solely on geometric alignment, we further incorporate semantic similarity estimated during denoising, enabling more reliable correspondence modeling and robust multi-view editing. In addition, we design a selective editing pipeline that allows users to choose preferred results from multiple candi",
      "title": "CoreEditor: Correspondence-constrained Diffusion for Consistent 3D Editing"
    },
    {
      "arxiv_id": "2602.17653",
      "authors": [
        "Iskar Deng",
        "Nathalia Xu",
        "Shane Steinert-Threlkeld"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.911880+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
          "url": "https://arxiv.org/abs/2602.17653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
        "url": "https://arxiv.org/abs/2602.17653"
      },
      "published_at": "2026-02-19T18:56:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8744701212435209,
        "semantic_score": 4.6226924657821655,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.997162587025688
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17653",
      "summary": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing ",
      "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking"
    },
    {
      "arxiv_id": "2602.17066",
      "authors": [
        "Sumedh Rasal"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.939008+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
          "url": "https://arxiv.org/abs/2602.17066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization",
        "url": "https://arxiv.org/abs/2602.17066"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3961324989795685,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.978130585441505
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17066",
      "summary": "arXiv:2602.17066v1 Announce Type: new \nAbstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statis",
      "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization"
    },
    {
      "arxiv_id": "2602.16740",
      "authors": [
        "Karan Bali, Jack Stanley, Praneet Suresh, Danilo Bzdok"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921346+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality",
          "url": "https://arxiv.org/abs/2602.16740"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality",
        "url": "https://arxiv.org/abs/2602.16740"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.7325314342975617,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.974529520759496
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16740",
      "summary": "arXiv:2602.16740v1 Announce Type: new \nAbstract: In mechanistic interpretability, recent work scrutinizes transformer \"circuits\" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-dep",
      "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality"
    },
    {
      "arxiv_id": "2410.13957",
      "authors": [
        "Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.907737+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Goal Inference from Open-Ended Dialog",
          "url": "https://arxiv.org/abs/2410.13957"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Goal Inference from Open-Ended Dialog",
        "url": "https://arxiv.org/abs/2410.13957"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3810874700546263,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.96308555651656
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2410.13957",
      "summary": "arXiv:2410.13957v2 Announce Type: replace-cross \nAbstract: Embodied AI Agents are quickly becoming important and common tools in society. These embodied agents should be able to learn about and accomplish a wide range of user goals and preferences efficiently and robustly. Large Language Models (LLMs) are often used as they allow for opportunities for rich and open-ended dialog type interaction between the human and agent to accomplish tasks according to human preferences. In this thesis, we argue that for embodied agents that deal with open-ended dialog during task assistance: 1) AI Agents should extract goals from conversations in the form of Natural Language (NL) to be better at capturing human preferences as it is intuitive for humans to communicate their preferences on tasks to agents through natural language. 2) AI Agents should quantify/maintain uncertainty about these goals to ensure that actions are being taken according to goals that the agent is extremely certain about. We p",
      "title": "Goal Inference from Open-Ended Dialog"
    },
    {
      "arxiv_id": "2602.16863",
      "authors": [
        "Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.933848+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
          "url": "https://arxiv.org/abs/2602.16863"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
        "url": "https://arxiv.org/abs/2602.16863"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3658772349357604,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.947875321397696
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.16863",
      "summary": "arXiv:2602.16863v1 Announce Type: new \nAbstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipu",
      "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation"
    },
    {
      "arxiv_id": "2602.17475",
      "authors": [
        "Pietro Ferrazzi",
        "Mattia Franzin",
        "Alberto Lavelli",
        "Bernardo Magnini"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.909977+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
          "url": "https://arxiv.org/abs/2602.17475"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian",
        "url": "https://arxiv.org/abs/2602.17475"
      },
      "published_at": "2026-02-19T15:38:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8625404238936949,
        "semantic_score": 3.483192265033722,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.945732688927414
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17475",
      "summary": "Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Reco",
      "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian"
    },
    {
      "arxiv_id": "2602.15769",
      "authors": [
        "Yahia Alqurnawi",
        "Preetom Biswas",
        "Anmol Rao",
        "Tejas Anvekar",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.120737+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
          "url": "https://arxiv.org/abs/2602.15769"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
        "url": "https://arxiv.org/abs/2602.15769"
      },
      "published_at": "2026-02-17T18:01:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7132270694312092,
        "semantic_score": 4.727681910991668,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.940908980422876
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15769",
      "summary": "Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our resu",
      "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution"
    },
    {
      "arxiv_id": "2602.17555",
      "authors": [
        "Zixu Cheng, Da Li, Jian Hu, Ziquan Liu, Wei Li, Shaogang Gong"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.298951+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
          "url": "https://arxiv.org/abs/2602.17555"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
        "url": "https://arxiv.org/abs/2602.17555"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3560508251190186,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.938048911580953
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17555",
      "summary": "arXiv:2602.17555v1 Announce Type: new \nAbstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate ",
      "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking"
    },
    {
      "arxiv_id": "2506.02529",
      "authors": [
        "Nguyen-Khang Le, Quan Minh Bui, Minh Ngoc Nguyen, Hiep Nguyen, Trung Vo, Son T. Luu, Shoshin Nomura, Minh Le Nguyen"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.910426+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
          "url": "https://arxiv.org/abs/2506.02529"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs",
        "url": "https://arxiv.org/abs/2506.02529"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.421544170379638,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.903542256841572
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2506.02529",
      "summary": "arXiv:2506.02529v2 Announce Type: replace \nAbstract: Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehen",
      "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs"
    },
    {
      "arxiv_id": "2509.26522",
      "authors": [
        "Xi Wang, James McInerney, Lequn Wang, Nathan Kallus"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.449582+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting",
          "url": "https://arxiv.org/abs/2509.26522"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting",
        "url": "https://arxiv.org/abs/2509.26522"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3151970684528353,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.89719515491477
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2509.26522",
      "summary": "arXiv:2509.26522v2 Announce Type: replace \nAbstract: Reasoning LLMs show improved performance with longer chains of thought. However, recent work has highlighted their tendency to overthink, continuing to revise answers even after reaching the correct solution. We quantitatively confirm this inefficiency from the distribution dynamics perspective by tracking Pass@1 for answers averaged over a large number of rollouts and find the model often begins to always produce the correct answer early in the reasoning, making extra reasoning tokens wasteful. To detect and prevent overthinking, we propose a simple and inexpensive novel signal, Entropy After  (EAT), for monitoring and deciding whether to exit reasoning early. By appending a stop thinking token () and monitoring the entropy of the following token as the model reasons, we obtain a trajectory that decreases and stabilizes when Pass@1 plateaus; thresholding its variance under an exponential moving average yields a practical stopping ru",
      "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting"
    },
    {
      "arxiv_id": "2602.17033",
      "authors": [
        "Peize Li, Zeyu Zhang, Hao Tang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.294378+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing",
          "url": "https://arxiv.org/abs/2602.17033"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing",
        "url": "https://arxiv.org/abs/2602.17033"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.3035206437110904,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.885518730173025
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17033",
      "summary": "arXiv:2602.17033v1 Announce Type: new \nAbstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object",
      "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing"
    },
    {
      "arxiv_id": "2602.13850",
      "authors": [
        "Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.311452+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
          "url": "https://arxiv.org/abs/2602.13850"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
        "url": "https://arxiv.org/abs/2602.13850"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.29158638715744,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.873584473619374
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.13850",
      "summary": "arXiv:2602.13850v2 Announce Type: replace \nAbstract: We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \\emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, de",
      "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement"
    },
    {
      "arxiv_id": "2602.16938",
      "authors": [
        "Ofer Meshi, Krisztian Balog, Sally Goldman, Avi Caciularu, Guy Tennenholtz, Jihwan Jeong, Amir Globerson, Craig Boutilier"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.906083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
          "url": "https://arxiv.org/abs/2602.16938"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders",
        "url": "https://arxiv.org/abs/2602.16938"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9408730924129487,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.862871178874883
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16938",
      "summary": "arXiv:2602.16938v1 Announce Type: new \nAbstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical \"realism gap,\" leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both \"good\" and \"bad\" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt",
      "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders"
    },
    {
      "arxiv_id": "2409.12709",
      "authors": [
        "Mine \\\"O\\u{g}retir, Miika Koskinen, Juha Sinisalo, Risto Renkonen, Harri L\\\"ahdesm\\\"aki"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.444710+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SeqRisk: Transformer-augmented latent variable model for robust survival prediction with longitudinal data",
          "url": "https://arxiv.org/abs/2409.12709"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SeqRisk: Transformer-augmented latent variable model for robust survival prediction with longitudinal data",
        "url": "https://arxiv.org/abs/2409.12709"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.2803173065185547,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.86231539298049
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2409.12709",
      "summary": "arXiv:2409.12709v3 Announce Type: replace \nAbstract: In healthcare, risk assessment of patient outcomes has been based on survival analysis for a long time, i.e. modeling time-to-event associations. However, conventional approaches rely on data from a single time-point, making them suboptimal for fully leveraging longitudinal patient history and capturing temporal regularities. Focusing on clinical real-world data and acknowledging its challenges, we utilize latent variable models to effectively handle irregular, noisy, and sparsely observed longitudinal data. We propose SeqRisk, a method that combines variational autoencoder (VAE) or longitudinal VAE (LVAE) with a transformer-based sequence aggregation and Cox proportional hazards module for risk prediction. SeqRisk captures long-range interactions, enhances predictive accuracy and generalizability, as well as provides partial explainability for sample population characteristics in attempts to identify high-risk patients. SeqRisk demo",
      "title": "SeqRisk: Transformer-augmented latent variable model for robust survival prediction with longitudinal data"
    },
    {
      "arxiv_id": "2602.16926",
      "authors": [
        "Yiyuan Jia",
        "Xiaoqin Fu",
        "Liang Zhang"
      ],
      "categories": [
        "cs.CE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:30.517081+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling",
          "url": "https://arxiv.org/abs/2602.16926"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling",
        "url": "https://arxiv.org/abs/2602.16926"
      },
      "published_at": "2026-02-18T22:38:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8035211794865347,
        "semantic_score": 4.54536343216896,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.848884611655496
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16926",
      "summary": "Recent advances in foundation models, including large language models (LLMs), have created new opportunities to automate building energy modeling (BEM). However, systematic evaluation has remained challenging due to the absence of publicly available, task-specific datasets and standardized performance metrics. We present BEMEval, a benchmark framework designed to assess foundation models' performance across BEM tasks. The first benchmark in this suite, BEMEval-Doc2Schema, focuses on structured d",
      "title": "BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling"
    },
    {
      "arxiv_id": "2602.17003",
      "authors": [
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937160+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
          "url": "https://arxiv.org/abs/2602.17003"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History",
        "url": "https://arxiv.org/abs/2602.17003"
      },
      "published_at": "2026-02-19T01:54:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.814550677874679,
        "semantic_score": 3.4157633900642397,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.830314067938918
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17003",
      "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on",
      "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History"
    },
    {
      "arxiv_id": "2510.25015",
      "authors": [
        "Chuyue Sun, Yican Sun, Daneshvar Amrollahi, Ethan Zhang, Shuvendu Lahiri, Shan Lu, David Dill, Clark Barrett"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913264+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus",
          "url": "https://arxiv.org/abs/2510.25015"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus",
        "url": "https://arxiv.org/abs/2510.25015"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.77,
        "llm_relevance_score": 16.94,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.7974667072296144,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.81946479369155
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2510.25015",
      "summary": "arXiv:2510.25015v3 Announce Type: replace \nAbstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.",
      "title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus"
    },
    {
      "arxiv_id": "2602.16918",
      "authors": [
        "Shlok Mishra, Tsung-Yu Lin, Linda Wang, Hongli Xu, Yimin Liu, Michael Hsu, Chaitanya Ahuja, Hao Yuan, Jianpeng Cheng, Hong-You Chen, Haoyuan Xu, Chao Li, Abhijeet Awasthi, Jihye Moon, Don Husa, Michael Ge, Sumedha Singla, Arkabandhu Chowdhury, Phong Dingh, Satya Narayan Shukla, Yonghuan Yang, David Jacobs, Qi Guo, Jun Xiao, Xiangjun Fan, Aashu Singh"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934743+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
          "url": "https://arxiv.org/abs/2602.16918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
        "url": "https://arxiv.org/abs/2602.16918"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.5694930076599123,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.81149109412185
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.16918",
      "summary": "arXiv:2602.16918v1 Announce Type: new \nAbstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, ",
      "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data"
    },
    {
      "arxiv_id": "2511.18696",
      "authors": [
        "Wangjiaxuan Xin"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.914168+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
          "url": "https://arxiv.org/abs/2511.18696"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
        "url": "https://arxiv.org/abs/2511.18696"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.226723301410675,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.80872138787261
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2511.18696",
      "summary": "arXiv:2511.18696v2 Announce Type: replace \nAbstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.",
      "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models"
    },
    {
      "arxiv_id": "2602.16987",
      "authors": [
        "Josef A. Habdank"
      ],
      "categories": [
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.091357+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
          "url": "https://arxiv.org/abs/2602.16987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents",
        "url": "https://arxiv.org/abs/2602.16987"
      },
      "published_at": "2026-02-19T01:21:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8126701463126278,
        "semantic_score": 2.2845508337020872,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.797220980014714
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16987",
      "summary": "As artificial intelligence (AI) capabilities advance rapidly, frontier models increasingly demonstrate systematic deception and scheming, complying with safety protocols during oversight but defecting when unsupervised. This paper examines the ensuing alignment challenge through an analogy from forensic psychology, where internalized belief systems in psychopathic populations reduce antisocial behavior via perceived omnipresent monitoring and inevitable consequences. Adapting this mechanism to s",
      "title": "A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents"
    },
    {
      "arxiv_id": "2602.17176",
      "authors": [
        "Shi Yin, Jinming Mu, Xudong Zhu, Lixin He"
      ],
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.941245+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction",
          "url": "https://arxiv.org/abs/2602.17176"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction",
        "url": "https://arxiv.org/abs/2602.17176"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.1002004384994506,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.782198524961387
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17176",
      "summary": "arXiv:2602.17176v1 Announce Type: cross \nAbstract: Crystal structure prediction (CSP), which aims to predict the three-dimensional atomic arrangement of a crystal from its composition, is central to materials discovery and mechanistic understanding. Existing deep learning models often treat crystallographic symmetry only as a soft heuristic or rely on space group and Wyckoff templates retrieved from known structures, which limits both physical fidelity and the ability to discover genuinely new material structures. In contrast to retrieval-based methods, our approach leverages large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition, effectively circumventing the limitations inherent to database lookups. Crucially, we incorporate domain knowledge into the generative process through an efficient constrained-optimization search that rigorously enforces algebraic consistency between site multiplicities and atomic stoichiometry.",
      "title": "Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction"
    },
    {
      "arxiv_id": "2602.17465",
      "authors": [
        "Hongming Li, Yang Liu, Chao Huang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.909701+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Entropy-Based Data Selection for Language Models",
          "url": "https://arxiv.org/abs/2602.17465"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Entropy-Based Data Selection for Language Models",
        "url": "https://arxiv.org/abs/2602.17465"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.281454598903656,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.76345268536559
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17465",
      "summary": "arXiv:2602.17465v1 Announce Type: new \nAbstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic class",
      "title": "Entropy-Based Data Selection for Language Models"
    },
    {
      "arxiv_id": "2602.16974",
      "authors": [
        "Yongjie Zhou, Shuai Wang, Bevan Koopman, Guido Zuccon"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T02:04:37.270267+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval",
          "url": "https://arxiv.org/abs/2602.16974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval",
        "url": "https://arxiv.org/abs/2602.16974"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.834827327728272,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.756825414190207
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.16974",
      "summary": "arXiv:2602.16974v1 Announce Type: new \nAbstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.\n  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embeddi",
      "title": "Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval"
    },
    {
      "arxiv_id": "2510.21081",
      "authors": [
        "Zhuojin Li, Marco Paolieri, Leana Golubchik"
      ],
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.PF"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.450629+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution",
          "url": "https://arxiv.org/abs/2510.21081"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution",
        "url": "https://arxiv.org/abs/2510.21081"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1706207156181336,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.752618802080068
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2510.21081",
      "summary": "arXiv:2510.21081v2 Announce Type: replace \nAbstract: Deploying deep neural networks on mobile devices is increasingly important but remains challenging due to limited computing resources. On the other hand, their unified memory architecture and narrower gap between CPU and GPU performance provide an opportunity to reduce inference latency by assigning tasks to both CPU and GPU. The main obstacles for such collaborative execution are the significant synchronization overhead required to combine partial results, and the difficulty of predicting execution times of tasks assigned to CPU and GPU (due to the dynamic selection of implementations and parallelism level). To overcome these obstacles, we propose both a lightweight synchronization mechanism based on OpenCL fine-grained shared virtual memory (SVM) and machine learning models to accurately predict execution times. Notably, these models capture the performance characteristics of GPU kernels and account for their dispatch times. A comp",
      "title": "Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution"
    },
    {
      "arxiv_id": "2511.15162",
      "authors": [
        "Ahmed Aboulfotouh, Hatem Abou-Zeid"
      ],
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913896+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Multimodal Wireless Foundation Models",
          "url": "https://arxiv.org/abs/2511.15162"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Multimodal Wireless Foundation Models",
        "url": "https://arxiv.org/abs/2511.15162"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.158600699901581,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.740598786363517
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2511.15162",
      "summary": "arXiv:2511.15162v2 Announce Type: replace-cross \nAbstract: Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluat",
      "title": "Multimodal Wireless Foundation Models"
    },
    {
      "arxiv_id": "2510.14974",
      "authors": [
        "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.912771+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
          "url": "https://arxiv.org/abs/2510.14974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "url": "https://arxiv.org/abs/2510.14974"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.132474786043167,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.7144728725051
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2510.14974",
      "summary": "arXiv:2510.14974v3 Announce Type: replace-cross \nAbstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow ",
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation"
    },
    {
      "arxiv_id": "2602.06530",
      "authors": [
        "Haipeng Li, Rongxuan Peng, Anwei Luo, Shunquan Tan, Changsheng Chen, Anastasia Antsiferova"
      ],
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.282686+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance",
          "url": "https://arxiv.org/abs/2602.06530"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance",
        "url": "https://arxiv.org/abs/2602.06530"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.1176527023315432,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.699650788793477
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.06530",
      "summary": "arXiv:2602.06530v2 Announce Type: replace \nAbstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forg",
      "title": "Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance"
    },
    {
      "arxiv_id": "2508.14292",
      "authors": [
        "M. Ali Bayram, Ali Arda Fincan, Ahmet Semih G\\\"um\\\"u\\c{s}, Sercan Karaka\\c{s}, Banu Diri, Sava\\c{s} Y{\\i}ld{\\i}r{\\i}m, Demircan \\c{C}elik"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.889795+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Tokens with Meaning: A Hybrid Tokenization Approach for Turkish",
          "url": "https://arxiv.org/abs/2508.14292"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Tokens with Meaning: A Hybrid Tokenization Approach for Turkish",
        "url": "https://arxiv.org/abs/2508.14292"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.214208102226257,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.69620618868819
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2508.14292",
      "summary": "arXiv:2508.14292v2 Announce Type: replace \nAbstract: Tokenization shapes how language models perceive morphology and meaning in NLP, yet widely used frequency-driven subword tokenizers (e.g., Byte Pair Encoding and WordPiece) can fragment morphologically rich and agglutinative languages in ways that obscure morpheme boundaries. We introduce a linguistically informed hybrid tokenizer for Turkish that combines (i) dictionary-driven morphological segmentation (roots and affixes), (ii) phonological normalization that maps allomorphic variants to shared identifiers, and (iii) a controlled subword fallback for out-of-vocabulary coverage. Concretely, our released Turkish vocabulary contains 22,231 root tokens mapped to 20,000 canonical root identifiers (with leading spaces to mark word boundaries), 72 affix identifiers that cover 177 allomorphic surface forms, and 12,696 subword units; an orthographic case token preserves capitalization without inflating the vocabulary. We evaluate tokenizati",
      "title": "Tokens with Meaning: A Hybrid Tokenization Approach for Turkish"
    },
    {
      "arxiv_id": "2602.17584",
      "authors": [
        "Sharut Gupta",
        "Sanyam Kansal",
        "Stefanie Jegelka",
        "Phillip Isola",
        "Vikas Garg"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.477279+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Canonicalizing Multimodal Contrastive Representation Learning",
          "url": "https://arxiv.org/abs/2602.17584"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Canonicalizing Multimodal Contrastive Representation Learning",
        "url": "https://arxiv.org/abs/2602.17584"
      },
      "published_at": "2026-02-19T18:09:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8716226183625857,
        "semantic_score": 3.418024110794067,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 27.689646729156653
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17584",
      "summary": "As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\\widetilde{f},\\widetilde{g})$) ",
      "title": "Canonicalizing Multimodal Contrastive Representation Learning"
    },
    {
      "arxiv_id": "2602.17284",
      "authors": [
        "Vitaly Feldman, Moshe Shenfeld"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.472915+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Efficient privacy loss accounting for subsampling and random allocation",
          "url": "https://arxiv.org/abs/2602.17284"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Efficient privacy loss accounting for subsampling and random allocation",
        "url": "https://arxiv.org/abs/2602.17284"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.105296975374222,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.687295061836156
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17284",
      "summary": "arXiv:2602.17284v1 Announce Type: new \nAbstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used",
      "title": "Efficient privacy loss accounting for subsampling and random allocation"
    },
    {
      "arxiv_id": "2602.16990",
      "authors": [
        "Yan Wang, Yi Han, Lingfei Qian, Yueru He, Xueqing Peng, Dongji Feng, Zhuohan Xie, Vincent Jim Zhang, Rosie Guo, Fengran Mo, Jimin Huang, Yankai Chen, Xue Liu, Jian-Yun Nie"
      ],
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.936816+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
          "url": "https://arxiv.org/abs/2602.16990"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
        "url": "https://arxiv.org/abs/2602.16990"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.204667776823044,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.68666586328498
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16990",
      "summary": "arXiv:2602.16990v1 Announce Type: new \nAbstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market ",
      "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation"
    },
    {
      "arxiv_id": "2602.17080",
      "authors": [
        "Minxin Zhang, Yuxuan Liu, Hayden Scheaffer"
      ],
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.469367+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum",
          "url": "https://arxiv.org/abs/2602.17080"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum",
        "url": "https://arxiv.org/abs/2602.17080"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.100546085834503,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.68254417229644
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17080",
      "summary": "arXiv:2602.17080v1 Announce Type: new \nAbstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard ass",
      "title": "Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum"
    },
    {
      "arxiv_id": "2602.16019",
      "authors": [
        "Ahmad Elallaf",
        "Yu Zhang",
        "Yuktha Priya Masupalli",
        "Jeong Yang",
        "Young Lee",
        "Zechun Cao",
        "Gongbo Liang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.120390+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
          "url": "https://arxiv.org/abs/2602.16019"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
        "url": "https://arxiv.org/abs/2602.16019"
      },
      "published_at": "2026-02-17T21:20:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.68,
        "llm_relevance_score": 14.96,
        "recency_score": 0.7231493801640531,
        "semantic_score": 3.795290285348892,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.678439665512947
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16019",
      "summary": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations a",
      "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval"
    },
    {
      "arxiv_id": "2602.16961",
      "authors": [
        "Rahul Thomas, Arka Pal"
      ],
      "categories": [
        "cs.IT",
        "cs.LG",
        "math.IT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.466854+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling",
          "url": "https://arxiv.org/abs/2602.16961"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling",
        "url": "https://arxiv.org/abs/2602.16961"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 15.84,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.745596969127655,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.66759505558959
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16961",
      "summary": "arXiv:2602.16961v1 Announce Type: cross \nAbstract: The goal of $L$-step speculative decoding is to accelerate autoregressive decoding of a target model by using a cheaper draft model to generate a candidate path of $L$ tokens. Based on a verification algorithm involving target and draft model probabilities, a prefix of the candidate sequence is accepted, and an additional correction token is sampled from a residual distribution to ensure that the final output adheres to the target distribution. While standard speculative decoding uses a verification algorithm which is independent at each token on the path, a recent extension called block verification uses a joint condition involving all sampled on-path probabilities. Block verification (BV) was shown to be optimal over all verification algorithms which use only on-path probabilities, improving on standard speculative decoding. In this work, we first show that block verification is optimal even over verification algorithms that use off-",
      "title": "Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling"
    },
    {
      "arxiv_id": "2602.17017",
      "authors": [
        "Deepanjan Bhol"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.937507+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Sales Research Agent and Sales Research Bench",
          "url": "https://arxiv.org/abs/2602.17017"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Sales Research Agent and Sales Research Bench",
        "url": "https://arxiv.org/abs/2602.17017"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 17.16,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.397759032249451,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.639757118711387
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17017",
      "summary": "arXiv:2602.17017v1 Announce Type: new \nAbstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare ",
      "title": "Sales Research Agent and Sales Research Bench"
    },
    {
      "arxiv_id": "2602.17217",
      "authors": [
        "Enrique Crespo-Fernandez, Oliver Ray, Telmo de Menezes e Silva Filho, Peter Flach"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.941967+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Continual learning and refinement of causal models through dynamic predicate invention",
          "url": "https://arxiv.org/abs/2602.17217"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Continual learning and refinement of causal models through dynamic predicate invention",
        "url": "https://arxiv.org/abs/2602.17217"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 17.6,
        "recency_score": 0.8819980864619349,
        "semantic_score": 1.9550105214118958,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.63700860787383
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17217",
      "summary": "arXiv:2602.17217v1 Announce Type: new \nAbstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.",
      "title": "Continual learning and refinement of causal models through dynamic predicate invention"
    },
    {
      "arxiv_id": "2602.17155",
      "authors": [
        "Yicheng Lang, Changsheng Wang, Yihua Zhang, Mingyi Hong, Zheng Zhang, Wotao Yin, Sijia Liu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.471341+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization",
          "url": "https://arxiv.org/abs/2602.17155"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization",
        "url": "https://arxiv.org/abs/2602.17155"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.0443903863430024,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.626388472804937
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17155",
      "summary": "arXiv:2602.17155v1 Announce Type: new \nAbstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as ",
      "title": "Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization"
    },
    {
      "arxiv_id": "2602.17067",
      "authors": [
        "Leixian Shen",
        "Yan Luo",
        "Rui Sheng",
        "Yujia He",
        "Haotian Li",
        "Leni Yang",
        "Huamin Qu"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.090498+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
          "url": "https://arxiv.org/abs/2602.17067"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems",
        "url": "https://arxiv.org/abs/2602.17067"
      },
      "published_at": "2026-02-19T04:16:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8226275477486518,
        "semantic_score": 3.18511426448822,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.60774181223687
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17067",
      "summary": "Personalized feedback plays an important role in self-regulated learning (SRL), helping students track progress and refine their strategies. However, current common solutions, such as text-based reports or learning analytics dashboards, often suffer from poor interpretability, monotonous presentation, and limited explainability. To overcome these challenges, we present StoryLensEdu, a narrative-driven multi-agent system that automatically generates intuitive, engaging, and interactive learning r",
      "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems"
    },
    {
      "arxiv_id": "2602.17045",
      "authors": [
        "Jared Moore",
        "Rasmus Overmark",
        "Ned Cooper",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.906913+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Large Language Models Persuade Without Planning Theory of Mind",
          "url": "https://arxiv.org/abs/2602.17045"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Large Language Models Persuade Without Planning Theory of Mind",
        "url": "https://arxiv.org/abs/2602.17045"
      },
      "published_at": "2026-02-19T03:31:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8200608492171769,
        "semantic_score": 4.264033138751984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.584093987969162
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17045",
      "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategicall",
      "title": "Large Language Models Persuade Without Planning Theory of Mind"
    },
    {
      "arxiv_id": "2510.08886",
      "authors": [
        "Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Yankai Chen, V\\'ictor Guti\\'errez-Basulto, Xiao-Yang Liu, Xue Liu, Jian-Yun Nie"
      ],
      "categories": [
        "cs.CL",
        "cs.CE",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.890130+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs",
          "url": "https://arxiv.org/abs/2510.08886"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs",
        "url": "https://arxiv.org/abs/2510.08886"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.090580123662948,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.57257821012488
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2510.08886",
      "summary": "arXiv:2510.08886v2 Announce Type: replace-cross \nAbstract: Going beyond simple text processing, financial auditing requires detecting semantic, structural, and numerical inconsistencies across large-scale disclosures. As financial reports are filed in XBRL, a structured XML format governed by accounting standards, auditing becomes a structured information extraction and reasoning problem involving concept alignment, taxonomy-defined relations, and cross-document consistency. Although large language models (LLMs) show promise on isolated financial tasks, their capability in professional-grade auditing remains unclear. We introduce FinAuditing, a taxonomy-aligned, structure-aware benchmark built from real XBRL filings. It contains 1,102 annotated instances averaging over 33k tokens and defines three tasks: Financial Semantic Matching (FinSM), Financial Relationship Extraction (FinRE), and Financial Mathematical Reasoning (FinMR). Evaluations of 13 state-of-the-art LLMs reveal substantial",
      "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs"
    },
    {
      "arxiv_id": "2602.17128",
      "authors": [
        "Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang Jr, Cecilia Laschi"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.313317+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy",
          "url": "https://arxiv.org/abs/2602.17128"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy",
        "url": "https://arxiv.org/abs/2602.17128"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.978548288345337,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.560546374807274
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.17128",
      "summary": "arXiv:2602.17128v1 Announce Type: new \nAbstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geom",
      "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy"
    },
    {
      "arxiv_id": "2602.16967",
      "authors": [
        "Yongzhong Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.936368+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Early-Warning Signals of Grokking via Loss-Landscape Geometry",
          "url": "https://arxiv.org/abs/2602.16967"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Early-Warning Signals of Grokking via Loss-Landscape Geometry",
        "url": "https://arxiv.org/abs/2602.16967"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.96996066570282,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.551958752164754
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16967",
      "summary": "arXiv:2602.16967v1 Announce Type: new \nAbstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly ",
      "title": "Early-Warning Signals of Grokking via Loss-Landscape Geometry"
    },
    {
      "arxiv_id": "2602.16742",
      "authors": [
        "Haoxiang Sun",
        "Lizhen Xu",
        "Bing Zhao",
        "Wotao Yin",
        "Wei Wang",
        "Boyu Yang",
        "Rui Wang",
        "Hu Wei"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921615+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
          "url": "https://arxiv.org/abs/2602.16742"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning",
        "url": "https://arxiv.org/abs/2602.16742"
      },
      "published_at": "2026-02-18T01:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7368781347152141,
        "semantic_score": 4.257344871759415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.49422300647463
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16742",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \\textbf{DeepVision-103K}, a comprehensive dataset for RLVR traini",
      "title": "DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning"
    },
    {
      "arxiv_id": "2602.17091",
      "authors": [
        "Kan Watanabe",
        "Tatsuya Shirai",
        "Yutaro Kashiwa",
        "Hajimu Iida"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:31.090040+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
          "url": "https://arxiv.org/abs/2602.17091"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
        "url": "https://arxiv.org/abs/2602.17091"
      },
      "published_at": "2026-02-19T05:29:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8268093577024247,
        "semantic_score": 4.154589235782623,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.481398593485046
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17091",
      "summary": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior w",
      "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation"
    },
    {
      "arxiv_id": "2509.09135",
      "authors": [
        "Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li"
      ],
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.449030+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2509.09135"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2509.09135"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8978666424751283,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.479864728937063
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2509.09135",
      "summary": "arXiv:2509.09135v3 Announce Type: replace-cross \nAbstract: Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-info",
      "title": "Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.16911",
      "authors": [
        "Adrian R\\\"ofer, Nick Heppert, Abhinav Valada"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.312715+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations",
          "url": "https://arxiv.org/abs/2602.16911"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations",
        "url": "https://arxiv.org/abs/2602.16911"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.875248408317566,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.4572464947795
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.16911",
      "summary": "arXiv:2602.16911v1 Announce Type: new \nAbstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experime",
      "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations"
    },
    {
      "arxiv_id": "2602.17124",
      "authors": [
        "Chi-Shiang Gau",
        "Konstantinos D. Polyzos",
        "Athanasios Bacharis",
        "Saketh Madhuvarasu",
        "Tara Javidi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.940352+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "3D Scene Rendering with Multimodal Gaussian Splatting",
          "url": "https://arxiv.org/abs/2602.17124"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "3D Scene Rendering with Multimodal Gaussian Splatting",
        "url": "https://arxiv.org/abs/2602.17124"
      },
      "published_at": "2026-02-19T06:49:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8314357343498857,
        "semantic_score": 1.9188971519470215,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.450332886296906
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17124",
      "summary": "3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typicall",
      "title": "3D Scene Rendering with Multimodal Gaussian Splatting"
    },
    {
      "arxiv_id": "2602.17244",
      "authors": [
        "Oleksii Furman, Patryk Marsza{\\l}ek, Jan Mas{\\l}owski, Piotr Gai\\'nski, Maciej Zi\\k{e}ba, Marek \\'Smieja"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.472211+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations",
          "url": "https://arxiv.org/abs/2602.17244"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations",
        "url": "https://arxiv.org/abs/2602.17244"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9101736068725588,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.392171693334493
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17244",
      "summary": "arXiv:2602.17244v1 Announce Type: new \nAbstract: Counterfactual explanations (CFs) provide human-interpretable insights into model's predictions by identifying minimal changes to input features that would alter the model's output. However, existing methods struggle to generate multiple high-quality explanations that (1) affect only a small portion of the features, (2) can be applied to tabular data with heterogeneous features, and (3) are consistent with the user-defined constraints. We propose CounterFlowNet, a generative approach that formulates CF generation as sequential feature modification using conditional Generative Flow Networks (GFlowNet). CounterFlowNet is trained to sample CFs proportionally to a user-specified reward function that can encode key CF desiderata: validity, sparsity, proximity and plausibility, encouraging high-quality explanations. The sequential formulation yields highly sparse edits, while a unified action space seamlessly supports continuous and categorica",
      "title": "CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations"
    },
    {
      "arxiv_id": "2602.17099",
      "authors": [
        "Liuchang Jing, Mingyu Yang, Lei Li, Jianbin Qin, Wei Wang"
      ],
      "categories": [
        "cs.DB",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T02:04:37.270570+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Multiple Index Merge for Approximate Nearest Neighbor Search",
          "url": "https://arxiv.org/abs/2602.17099"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Multiple Index Merge for Approximate Nearest Neighbor Search",
        "url": "https://arxiv.org/abs/2602.17099"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8055244207382204,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.387522507200156
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17099",
      "summary": "arXiv:2602.17099v1 Announce Type: cross \nAbstract: Approximate $k$ nearest neighbor (AKNN) search in high-dimensional space is a foundational problem in vector databases with widespread applications. Among the numerous AKNN indexes, Proximity Graph-based indexes achieve state-of-the-art search efficiency across various benchmarks. However, their extensive distance computations of high-dimensional vectors lead to slow construction and substantial memory overhead. The limited memory capacity often prevents building the entire index at once when handling large-scale datasets. A common practice is to build multiple sub-indexes separately. However, directly searching on these separated indexes severely compromises search efficiency, as queries cannot leverage cross-graph connections. Therefore, efficient graph index merging is crucial for multi-index searching. In this paper, we focus on efficient two-index merging and the merge order of multiple indexes for AKNN search. To achieve this, we",
      "title": "Multiple Index Merge for Approximate Nearest Neighbor Search"
    },
    {
      "arxiv_id": "2602.17641",
      "authors": [
        "Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.952471+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
          "url": "https://arxiv.org/abs/2602.17641"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
        "url": "https://arxiv.org/abs/2602.17641"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.7839474976062775,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.365945584068214
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17641",
      "summary": "arXiv:2602.17641v1 Announce Type: new \nAbstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for r",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery"
    },
    {
      "arxiv_id": "2602.17270",
      "authors": [],
      "categories": [],
      "entities": [
        "stability-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:28.472553+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Unified Latents (UL): How to train your latents",
          "url": "https://arxiv.org/abs/2602.17270"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Unified Latents (UL): How to train your latents",
        "url": "https://arxiv.org/abs/2602.17270"
      },
      "published_at": "2026-02-19T11:18:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8470731958462046,
        "semantic_score": 2.0107189655303954,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.3577921613766
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17270",
      "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "title": "Unified Latents (UL): How to train your latents"
    },
    {
      "arxiv_id": "2602.17203",
      "authors": [
        "Yuhong Luo, Daniel Schoepflin, Xintong Wang"
      ],
      "categories": [
        "cs.MA",
        "cs.GT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.002735+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation",
          "url": "https://arxiv.org/abs/2602.17203"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation",
        "url": "https://arxiv.org/abs/2602.17203"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.724267226457596,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.306265312919532
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.17203",
      "summary": "arXiv:2602.17203v1 Announce Type: new \nAbstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across ra",
      "title": "Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation"
    },
    {
      "arxiv_id": "2602.17169",
      "authors": [
        "Yuhuan Xia",
        "Tun Li",
        "Hongji Zhou",
        "Xianfa Zhou",
        "Chong Chen",
        "Ruiyu Zhang"
      ],
      "categories": [
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:30.515966+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
          "url": "https://arxiv.org/abs/2602.17169"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
        "url": "https://arxiv.org/abs/2602.17169"
      },
      "published_at": "2026-02-19T08:34:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8374865168096998,
        "semantic_score": 3.9579376459121702,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.295424162721872
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17169",
      "summary": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-ali",
      "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models"
    },
    {
      "arxiv_id": "2602.16747",
      "authors": [
        "Xidong Wang, Shuqi Guo, Yue Shen, Junying Chen, Jian Wang, Jinjie Gu, Ping Zhang, Lei Liu, Benyou Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.922018+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LiveClin: A Live Clinical Benchmark without Leakage",
          "url": "https://arxiv.org/abs/2602.16747"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LiveClin: A Live Clinical Benchmark without Leakage",
        "url": "https://arxiv.org/abs/2602.16747"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.708177387714386,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.290175474176323
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16747",
      "summary": "arXiv:2602.16747v1 Announce Type: new \nAbstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy",
      "title": "LiveClin: A Live Clinical Benchmark without Leakage"
    },
    {
      "arxiv_id": "2602.17467",
      "authors": [
        "Greta Damo, St\\'ephane Petiot, Elena Cabrio, Serena Villata"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.909802+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions",
          "url": "https://arxiv.org/abs/2602.17467"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions",
        "url": "https://arxiv.org/abs/2602.17467"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.6976896166801456,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.27968770314208
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17467",
      "summary": "arXiv:2602.17467v1 Announce Type: new \nAbstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.",
      "title": "PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions"
    },
    {
      "arxiv_id": "2602.17200",
      "authors": [
        "Ye Zhu, Kaleb S. Newman, Johannes F. Lutzeyer, Adriana Romero-Soriano, Michal Drozdzal, Olga Russakovsky"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.296288+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation",
          "url": "https://arxiv.org/abs/2602.17200"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation",
        "url": "https://arxiv.org/abs/2602.17200"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.79498838186264,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.276986468324573
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17200",
      "summary": "arXiv:2602.17200v1 Announce Type: new \nAbstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of gene",
      "title": "GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation"
    },
    {
      "arxiv_id": "2602.17590",
      "authors": [
        "Agnieszka M. Zbrzezny"
      ],
      "categories": [
        "cs.CR",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.003218+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "BMC4TimeSec: Verification Of Timed Security Protocols",
          "url": "https://arxiv.org/abs/2602.17590"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "BMC4TimeSec: Verification Of Timed Security Protocols",
        "url": "https://arxiv.org/abs/2602.17590"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.7735926628112795,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.255590749273214
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.17590",
      "summary": "arXiv:2602.17590v1 Announce Type: cross \nAbstract: We present BMC4TimeSec, an end-to-end tool for verifying Timed Security Protocols (TSP) based on SMT-based bounded model checking and multi-agent modelling in the form of Timed Interpreted Systems (TIS) and Timed Interleaved Interpreted Systems (TIIS). In BMC4TimeSec, TSP executions implement the TIS/TIIS environment (join actions, interleaving, delays, lifetimes), and knowledge automata implement the agents (evolution of participant knowledge, including the intruder). The code is publicly available on \\href{https://github.com/agazbrzezny/BMC4TimeSec}{GitHub}, as is a \\href{https://youtu.be/aNybKz6HwdA}{video} demonstration.",
      "title": "BMC4TimeSec: Verification Of Timed Security Protocols"
    },
    {
      "arxiv_id": "2602.16814",
      "authors": [
        "Eiman Kanjo, Mustafa Aslanov"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.923487+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI",
          "url": "https://arxiv.org/abs/2602.16814"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI",
        "url": "https://arxiv.org/abs/2602.16814"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.65552619099617,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.237524277458107
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16814",
      "summary": "arXiv:2602.16814v1 Announce Type: new \nAbstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual ",
      "title": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI"
    },
    {
      "arxiv_id": "2602.15950",
      "authors": [
        "Yuval Levental"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:32.112283+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
          "url": "https://arxiv.org/abs/2602.15950"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families",
        "url": "https://arxiv.org/abs/2602.15950"
      },
      "published_at": "2026-02-17T19:06:19+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.7164405066501854,
        "semantic_score": 2.9072179198265076,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.223658426476693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15950",
      "summary": "We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text",
      "title": "Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families"
    },
    {
      "arxiv_id": "2602.17287",
      "authors": [
        "Evgeniia Tokarchuk, Maya K. Nachesa, Sergey Troshin, Vlad Niculae"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.473032+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Representation Collapse in Machine Translation Through the Lens of Angular Dispersion",
          "url": "https://arxiv.org/abs/2602.17287"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Representation Collapse in Machine Translation Through the Lens of Angular Dispersion",
        "url": "https://arxiv.org/abs/2602.17287"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.719670671224594,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.201668757686527
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17287",
      "summary": "arXiv:2602.17287v1 Announce Type: new \nAbstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically t",
      "title": "Representation Collapse in Machine Translation Through the Lens of Angular Dispersion"
    },
    {
      "arxiv_id": "2602.16794",
      "authors": [
        "Pengqi Liu, Zijun Yu, Mouloud Belbahri, Arthur Charpentier, Masoud Asgharian, Jesse C. Cresswell"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.457826+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Beyond Procedure: Substantive Fairness in Conformal Prediction",
          "url": "https://arxiv.org/abs/2602.16794"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Beyond Procedure: Substantive Fairness in Conformal Prediction",
        "url": "https://arxiv.org/abs/2602.16794"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.713280528783798,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.195278615245734
      },
      "section": null,
      "source_name": "arXiv stat.ML",
      "story_id": "arxiv:2602.16794",
      "summary": "arXiv:2602.16794v1 Announce Type: new \nAbstract: Conformal prediction (CP) offers distribution-free uncertainty quantification for machine learning models, yet its interplay with fairness in downstream decision-making remains underexplored. Moving beyond CP as a standalone operation (procedural fairness), we analyze the holistic decision-making pipeline to evaluate substantive fairness-the equity of downstream outcomes. Theoretically, we derive an upper bound that decomposes prediction-set size disparity into interpretable components, clarifying how label-clustered CP helps control method-driven contributions to unfairness. To facilitate scalable empirical analysis, we introduce an LLM-in-the-loop evaluator that approximates human assessment of substantive fairness across diverse modalities. Our experiments reveal that label-clustered CP variants consistently deliver superior substantive fairness. Finally, we empirically show that equalized set sizes, rather than coverage, strongly cor",
      "title": "Beyond Procedure: Substantive Fairness in Conformal Prediction"
    },
    {
      "arxiv_id": "2602.17568",
      "authors": [
        "Sofiane Ennadir, Tianze Wang, Oleg Smirnov, Sahar Asadi, Lele Cao"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.947271+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Be Wary of Your Time Series Preprocessing",
          "url": "https://arxiv.org/abs/2602.17568"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Be Wary of Your Time Series Preprocessing",
        "url": "https://arxiv.org/abs/2602.17568"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.831340229511261,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 27.163338315973196
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17568",
      "summary": "arXiv:2602.17568v1 Announce Type: new \nAbstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical",
      "title": "Be Wary of Your Time Series Preprocessing"
    },
    {
      "arxiv_id": "2602.17442",
      "authors": [
        "Marco Avolio, Potito Aghilar, Sabino Roccotelli, Vito Walter Anelli, Chiara Mallamaci, Vincenzo Paparella, Marco Valentini, Alejandro Bellog\\'in, Michelantonio Trizio, Joseph Trotta, Antonio Ferrara, Tommaso Di Noia"
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945087+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
          "url": "https://arxiv.org/abs/2602.17442"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation",
        "url": "https://arxiv.org/abs/2602.17442"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.645743727684021,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.127741814145956
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17442",
      "summary": "arXiv:2602.17442v1 Announce Type: cross \nAbstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ",
      "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation"
    },
    {
      "arxiv_id": "2602.17387",
      "authors": [
        "Changhun Kim, Martin Mayr, Thomas Gorges, Fei Wu, Mathias Seuret, Andreas Maier, Vincent Christlein"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.297829+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition",
          "url": "https://arxiv.org/abs/2602.17387"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition",
        "url": "https://arxiv.org/abs/2602.17387"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.5386538147926334,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.12065190125457
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17387",
      "summary": "arXiv:2602.17387v1 Announce Type: new \nAbstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing soft",
      "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition"
    },
    {
      "arxiv_id": "2602.11893",
      "authors": [
        "Roberto Molinaro, Niall Siegenheim, Henry Martin, Mark Frey, Niels Poulsen, Philipp Seitz, Marvin Vincent Gabler"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.453794+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Universal Diffusion-Based Probabilistic Downscaling",
          "url": "https://arxiv.org/abs/2602.11893"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Universal Diffusion-Based Probabilistic Downscaling",
        "url": "https://arxiv.org/abs/2602.11893"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.472657883167267,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.054655969629202
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.11893",
      "summary": "arXiv:2602.11893v2 Announce Type: replace \nAbstract: We introduce a universal diffusion-based downscaling framework that lifts deterministic low-resolution weather forecasts into probabilistic high-resolution predictions without any model-specific fine-tuning. A single conditional diffusion model is trained on paired coarse-resolution inputs (~25 km resolution) and high-resolution regional reanalysis targets (~5 km resolution), and is applied in a fully zero-shot manner to deterministic forecasts from heterogeneous upstream weather models. Focusing on near-surface variables, we evaluate probabilistic forecasts against independent in situ station observations over lead times up to 90 h. Across a diverse set of AI-based and numerical weather prediction (NWP) systems, the ensemble mean of the downscaled forecasts consistently improves upon each model's own raw deterministic forecast, and substantially larger gains are observed in probabilistic skill as measured by CRPS. These results demo",
      "title": "Universal Diffusion-Based Probabilistic Downscaling"
    },
    {
      "arxiv_id": "2602.16715",
      "authors": [
        "H. Sinan Bank, Daniel R. Herber"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SY",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.920295+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems",
          "url": "https://arxiv.org/abs/2602.16715"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems",
        "url": "https://arxiv.org/abs/2602.16715"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.610293352603913,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.99229143906585
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16715",
      "summary": "arXiv:2602.16715v1 Announce Type: cross \nAbstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.",
      "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems"
    },
    {
      "arxiv_id": "2602.17535",
      "authors": [
        "Behzad Bozorgtabar",
        "Dwarikanath Mahapatra",
        "Sudipta Roy",
        "Muzammal Naseer",
        "Imran Razzak",
        "Zongyuan Ge"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.298835+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
          "url": "https://arxiv.org/abs/2602.17535"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs",
        "url": "https://arxiv.org/abs/2602.17535"
      },
      "published_at": "2026-02-19T16:45:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8665549598976169,
        "semantic_score": 3.6023905992507936,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.968945559148413
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17535",
      "summary": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and",
      "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs"
    },
    {
      "arxiv_id": "2602.17089",
      "authors": [
        "Xinghao Dong, Huchen Yang, Jin-long Wu"
      ],
      "categories": [
        "cs.LG",
        "math.DS",
        "physics.comp-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.469721+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling",
          "url": "https://arxiv.org/abs/2602.17089"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling",
        "url": "https://arxiv.org/abs/2602.17089"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.3804230213165285,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.962421107778464
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.17089",
      "summary": "arXiv:2602.17089v1 Announce Type: new \nAbstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-pres",
      "title": "Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling"
    },
    {
      "arxiv_id": "2602.17108",
      "authors": [
        "Anton Dzega",
        "Aviad Elyashar",
        "Ortal Slobodin",
        "Odeya Cohen",
        "Rami Puzis"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.907413+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
          "url": "https://arxiv.org/abs/2602.17108"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests",
        "url": "https://arxiv.org/abs/2602.17108"
      },
      "published_at": "2026-02-19T06:08:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8290526276887218,
        "semantic_score": 2.7184711396694183,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 26.947523767358142
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17108",
      "summary": "Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the So",
      "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests"
    },
    {
      "arxiv_id": "2602.17385",
      "authors": [
        "Angelo Porrello, Pietro Buzzega, Felix Dangel, Thomas Sommariva, Riccardo Salami, Lorenzo Bonicelli, Simone Calderara"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943986+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
          "url": "https://arxiv.org/abs/2602.17385"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
        "url": "https://arxiv.org/abs/2602.17385"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.451727056503296,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.93372514296523
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17385",
      "summary": "arXiv:2602.17385v1 Announce Type: new \nAbstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for he",
      "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature"
    },
    {
      "arxiv_id": "2602.17251",
      "authors": [
        "Jingying Ma",
        "Feng Wu",
        "Yucheng Xing",
        "Qika Lin",
        "Tianyu Liu",
        "Chenyu Liu",
        "Ziyu Jia",
        "Mengling Feng"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.472328+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Structured Prototype-Guided Adaptation for EEG Foundation Models",
          "url": "https://arxiv.org/abs/2602.17251"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Structured Prototype-Guided Adaptation for EEG Foundation Models",
        "url": "https://arxiv.org/abs/2602.17251"
      },
      "published_at": "2026-02-19T10:54:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8456792223817158,
        "semantic_score": 3.5329410433769226,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.87862026575864
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17251",
      "summary": "Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prot",
      "title": "Structured Prototype-Guided Adaptation for EEG Foundation Models"
    },
    {
      "arxiv_id": "2511.07989",
      "authors": [
        "Taja Kuzman Punger\\v{s}ek, Peter Rupnik, Ivan Porupski, Vuk Dini\\'c, Nikola Ljube\\v{s}i\\'c"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913632+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?",
          "url": "https://arxiv.org/abs/2511.07989"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?",
        "url": "https://arxiv.org/abs/2511.07989"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.47012505531311,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.852123141775046
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2511.07989",
      "summary": "arXiv:2511.07989v2 Announce Type: replace \nAbstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surp",
      "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?"
    },
    {
      "arxiv_id": "2602.17308",
      "authors": [
        "Hui Min Wong",
        "Philip Heesen",
        "Pascal Janetzky",
        "Martin Bendszus",
        "Stefan Feuerriegel"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.943095+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
          "url": "https://arxiv.org/abs/2602.17308"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
        "url": "https://arxiv.org/abs/2602.17308"
      },
      "published_at": "2026-02-19T12:19:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8506691029768593,
        "semantic_score": 3.4760083079338076,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.82667741091067
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17308",
      "summary": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that dem",
      "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions"
    },
    {
      "arxiv_id": "2602.16726",
      "authors": [
        "Hua Yan, Heng Tan, Yu Yang"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.001769+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data",
          "url": "https://arxiv.org/abs/2602.16726"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data",
        "url": "https://arxiv.org/abs/2602.16726"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.416920214891434,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.798918301353368
      },
      "section": null,
      "source_name": "arXiv cs.MA",
      "story_id": "arxiv:2602.16726",
      "summary": "arXiv:2602.16726v1 Announce Type: new \nAbstract: Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objec",
      "title": "Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data"
    },
    {
      "arxiv_id": "2602.17450",
      "authors": [
        "Amirereza Abbasi, Mohsen Hooshmand"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.945195+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research",
          "url": "https://arxiv.org/abs/2602.17450"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research",
        "url": "https://arxiv.org/abs/2602.17450"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.404981923103333,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.78698000956527
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17450",
      "summary": "arXiv:2602.17450v1 Announce Type: new \nAbstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challe",
      "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research"
    },
    {
      "arxiv_id": "2602.17478",
      "authors": [
        "Xuan-Bac Nguyen, Hoang-Quan Nguyen, Sankalp Pandey, Tim Faltermeier, Nicholas Borys, Hugh Churchill, Khoa Luu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.298466+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery",
          "url": "https://arxiv.org/abs/2602.17478"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery",
        "url": "https://arxiv.org/abs/2602.17478"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.182669758796692,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.76466784525863
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17478",
      "summary": "arXiv:2602.17478v1 Announce Type: new \nAbstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to te",
      "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery"
    },
    {
      "arxiv_id": "2510.24983",
      "authors": [
        "Ximan Sun, Xiang Cheng"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.913134+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies",
          "url": "https://arxiv.org/abs/2510.24983"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies",
        "url": "https://arxiv.org/abs/2510.24983"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.68,
        "llm_relevance_score": 14.96,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.708843618631363,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.7508417050933
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2510.24983",
      "summary": "arXiv:2510.24983v2 Announce Type: replace \nAbstract: Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, ",
      "title": "LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies"
    },
    {
      "arxiv_id": "2503.23339",
      "authors": [
        "Neil Mallinar, A. Ali Heydari, Xin Liu, Anthony Z. Faranesh, Brent Winslow, Nova Hammerquist, Benjamin Graef, Cathy Speed, Mark Malhotra, Shwetak Patel, Javier L. Prieto, Daniel McDuff, Ahmed A. Metwally"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.909155+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "A Scalable Framework for Evaluating Health Language Models",
          "url": "https://arxiv.org/abs/2503.23339"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "A Scalable Framework for Evaluating Health Language Models",
        "url": "https://arxiv.org/abs/2503.23339"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.360888171195984,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.74288625765792
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2503.23339",
      "summary": "arXiv:2503.23339v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics: an evaluation fr",
      "title": "A Scalable Framework for Evaluating Health Language Models"
    },
    {
      "arxiv_id": "2602.16843",
      "authors": [
        "Ahmed Rafid, Rumman Adib, Fariya Ahmed, Ajwad Abrar, Mohammed Saidul Islam"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.905771+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
          "url": "https://arxiv.org/abs/2602.16843"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization",
        "url": "https://arxiv.org/abs/2602.16843"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.336542999744415,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.71854108620635
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.16843",
      "summary": "arXiv:2602.16843v1 Announce Type: new \nAbstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison",
      "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization"
    },
    {
      "arxiv_id": "2602.17183",
      "authors": [
        "Kishan Maharaj",
        "Nandakishore Menon",
        "Ashita Saxena",
        "Srikanth Tamilselvam"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.941372+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
          "url": "https://arxiv.org/abs/2602.17183"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
        "url": "https://arxiv.org/abs/2602.17183"
      },
      "published_at": "2026-02-19T09:05:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8392768103067776,
        "semantic_score": 4.4700847327709194,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.709361543077698
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17183",
      "summary": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three setti",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering"
    },
    {
      "arxiv_id": "2602.17260",
      "authors": [
        "Hung Mai, Loi Dinh, Duc Hai Nguyen, Dat Do, Luong Doan, Khanh Nguyen Quoc, Huan Vu, Phong Ho, Naeem Ul Islam, Tuan Do"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.296741+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection",
          "url": "https://arxiv.org/abs/2602.17260"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection",
        "url": "https://arxiv.org/abs/2602.17260"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.1021093368530273,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.684107423314963
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17260",
      "summary": "arXiv:2602.17260v1 Announce Type: new \nAbstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators,",
      "title": "EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection"
    },
    {
      "arxiv_id": "2602.16917",
      "authors": [
        "Sakib Ahammed, Xia Cui, Xinqi Fan, Wenqi Lu, Moi Hoon Yap"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "meta-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:29.284816+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts",
          "url": "https://arxiv.org/abs/2602.16917"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts",
        "url": "https://arxiv.org/abs/2602.16917"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.2924968957901,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.674494982252035
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.16917",
      "summary": "arXiv:2602.16917v1 Announce Type: new \nAbstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity",
      "title": "SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts"
    },
    {
      "arxiv_id": "2602.16157",
      "authors": [
        "Xinyue Gui",
        "Ding Xia",
        "Mark Colley",
        "Yuan Li",
        "Vishal Chauhan",
        "Anubhav Anubhav",
        "Zhongyi Zhou",
        "Ehsan Javanmardi",
        "Stela Hanbyeol Seo",
        "Chia-Ming Chang",
        "Manabu Tsukada",
        "Takeo Igarashi"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:34.119748+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-multimodal",
          "tier": 1,
          "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
          "url": "https://arxiv.org/abs/2602.16157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-multimodal",
        "tier": 1,
        "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
        "url": "https://arxiv.org/abs/2602.16157"
      },
      "published_at": "2026-02-18T03:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7410347537542048,
        "semantic_score": 3.405628204345703,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.64666295809991
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16157",
      "summary": "Field studies are irreplaceable but costly, time-consuming, and error-prone, which need careful preparation. Inspired by rapid-prototyping in manufacturing, we propose a fast, low-cost evaluation method using Vision-Language Model (VLM) personas to simulate outcomes comparable to field results. While LLMs show human-like reasoning and language capabilities, autonomous vehicle (AV)-pedestrian interaction requires spatial awareness, emotional empathy, and behavioral generation. This raises our res",
      "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI"
    },
    {
      "arxiv_id": "2510.25166",
      "authors": [
        "Zhuojin Li, Marco Paolieri, Leana Golubchik"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.PF"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-20T20:28:28.450876+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "A Study on Inference Latency for Vision Transformers on Mobile Devices",
          "url": "https://arxiv.org/abs/2510.25166"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "A Study on Inference Latency for Vision Transformers on Mobile Devices",
        "url": "https://arxiv.org/abs/2510.25166"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.62,
        "llm_relevance_score": 13.64,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.9116324663162234,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.633630552778158
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2510.25166",
      "summary": "arXiv:2510.25166v2 Announce Type: replace \nAbstract: Given the significant advances in machine learning techniques on mobile devices, particularly in the domain of computer vision, in this work we quantitatively study the performance characteristics of 190 real-world vision transformers (ViTs) on mobile devices. Through a comparison with 102 real-world convolutional neural networks (CNNs), we provide insights into the factors that influence the latency of ViT architectures on mobile devices. Based on these insights, we develop a dataset including measured latencies of 1000 synthetic ViTs with representative building blocks and state-of-the-art architectures from two machine learning frameworks and six mobile platforms. Using this dataset, we show that inference latency of new ViTs can be predicted with sufficient accuracy for real-world applications.",
      "title": "A Study on Inference Latency for Vision Transformers on Mobile Devices"
    },
    {
      "arxiv_id": "2602.17393",
      "authors": [
        "Minxing Sun, Yao Mao"
      ],
      "categories": [
        "cs.RO",
        "eess.SP"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.313827+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
          "url": "https://arxiv.org/abs/2602.17393"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
        "url": "https://arxiv.org/abs/2602.17393"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 2.8902847528457642,
        "tier_score": 2.0,
        "topic_score": 3.1500000000000004,
        "total_score": 26.6222828393077
      },
      "section": null,
      "source_name": "arXiv cs.RO",
      "story_id": "arxiv:2602.17393",
      "summary": "arXiv:2602.17393v1 Announce Type: new \nAbstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that di",
      "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots"
    },
    {
      "arxiv_id": "2506.16777",
      "authors": [
        "Heloisa Oss Boll, Antonio Oss Boll, Leticia Puttlitz Boll, Ameen Abu Hanna, Iacer Calixto"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.889465+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "DistillNote: Toward a Functional Evaluation Framework of LLM-Generated Clinical Note Summaries",
          "url": "https://arxiv.org/abs/2506.16777"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "DistillNote: Toward a Functional Evaluation Framework of LLM-Generated Clinical Note Summaries",
        "url": "https://arxiv.org/abs/2506.16777"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.185317522287368,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.567315608749304
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2506.16777",
      "summary": "arXiv:2506.16777v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly used to generate summaries from clinical notes. However, their ability to preserve essential diagnostic information remains underexplored, which could lead to serious risks for patient care. This study introduces DistillNote, an evaluation framework for LLM summaries that targets their functional utility by applying the generated summary downstream in a complex clinical prediction task, explicitly quantifying how much prediction signal is retained. We generated over 192,000 LLM summaries from MIMIC-IV clinical notes with increasing compression rates: standard, section-wise, and distilled section-wise. Heart failure diagnosis was chosen as the prediction task, as it requires integrating a wide range of clinical signals. LLMs were fine-tuned on both the original notes and their summaries, and their diagnostic performance was compared using the AUROC metric. We contrasted DistillNote's resul",
      "title": "DistillNote: Toward a Functional Evaluation Framework of LLM-Generated Clinical Note Summaries"
    },
    {
      "arxiv_id": "2602.11903",
      "authors": [
        "Yu-Chih Chen, Michael Wang, Chieh-Dun Wen, Kai-Siang Ma, Avinab Saha, Li-Heng Chen, Alan Bovik"
      ],
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.MM"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.283125+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals",
          "url": "https://arxiv.org/abs/2602.11903"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals",
        "url": "https://arxiv.org/abs/2602.11903"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.081572169065476,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.56357025552741
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.11903",
      "summary": "arXiv:2602.11903v2 Announce Type: replace-cross \nAbstract: No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining. By jointly optimizing multiple full-reference (FR) objectives with adaptive task weighting, our approach learns shared representations that transfer effectively to NR-VQA. Experiments on gaming video datasets show MTL-VQA achieves performance competitive with state-of-the-art NR-VQA methods across both MOS-supervised and label-efficient/self-supervised settings.",
      "title": "Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals"
    },
    {
      "arxiv_id": "2602.17107",
      "authors": [
        "Xiangyu Zhou, Chenhan Xiao, Yang Weng"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.939903+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)",
          "url": "https://arxiv.org/abs/2602.17107"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)",
        "url": "https://arxiv.org/abs/2602.17107"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 4.168373370170594,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.55037145663253
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.17107",
      "summary": "arXiv:2602.17107v1 Announce Type: new \nAbstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while ",
      "title": "Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)"
    },
    {
      "arxiv_id": "2602.16746",
      "authors": [
        "Yongzhong Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.921852+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking",
          "url": "https://arxiv.org/abs/2602.16746"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking",
        "url": "https://arxiv.org/abs/2602.16746"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 15.399999999999999,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.0007054567337037,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.482703543195637
      },
      "section": null,
      "source_name": "arXiv cs.LG",
      "story_id": "arxiv:2602.16746",
      "summary": "arXiv:2602.16746v1 Announce Type: new \nAbstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experime",
      "title": "Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking"
    },
    {
      "arxiv_id": "2602.16965",
      "authors": [
        "Sourav Chakraborty",
        "Amit Kiran Rege",
        "Claire Monteleoni",
        "Lijun Chen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.466974+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Multi-Agent Lipschitz Bandits",
          "url": "https://arxiv.org/abs/2602.16965"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Multi-Agent Lipschitz Bandits",
        "url": "https://arxiv.org/abs/2602.16965"
      },
      "published_at": "2026-02-18T23:58:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8080247297976323,
        "semantic_score": 3.168580198287964,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.4766049280856
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16965",
      "summary": "We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed se",
      "title": "Multi-Agent Lipschitz Bandits"
    },
    {
      "arxiv_id": "2602.16930",
      "authors": [
        "Farnaz Zamiri Zeraati, Yang Trista Cao, Yuehan Qiao, Hal Daum\\'e III, Hernisa Kacorri"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.934981+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users",
          "url": "https://arxiv.org/abs/2602.16930"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users",
        "url": "https://arxiv.org/abs/2602.16930"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.68,
        "llm_relevance_score": 14.96,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.406607562303543,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.44860564876548
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2602.16930",
      "summary": "arXiv:2602.16930v1 Announce Type: cross \nAbstract: Prompting and steering techniques are well established in general-purpose generative AI, yet assistive visual question answering (VQA) tools for blind users still follow rigid interaction patterns with limited opportunities for customization. User control can be helpful when system responses are misaligned with their goals and contexts, a gap that becomes especially consequential for blind users that may rely on these systems for access. We invite 11 blind users to customize their interactions with a real-world conversational VQA system. Drawing on 418 interactions, reflections, and post-study interviews, we analyze prompting-based techniques participants adopted, including those introduced in the study and those developed independently in real-world settings. VQA interactions were often lengthy: participants averaged 3 turns, sometimes up to 21, with input text typically tenfold shorter than the responses they heard. Built on state-of",
      "title": "Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users"
    },
    {
      "arxiv_id": "2602.16653",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:08.435972+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
          "url": "https://arxiv.org/abs/2602.16653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
        "url": "https://arxiv.org/abs/2602.16653"
      },
      "published_at": "2026-02-18T17:52:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.7877289092159818,
        "semantic_score": 3.160517376661301,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.448246285877282
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16653",
      "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on pu",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments"
    },
    {
      "arxiv_id": "2601.15599",
      "authors": [
        "Cecil Pang, Hiroki Sayama"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.916149+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Autonomous Business System via Neuro-symbolic AI",
          "url": "https://arxiv.org/abs/2601.15599"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Autonomous Business System via Neuro-symbolic AI",
        "url": "https://arxiv.org/abs/2601.15599"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 16.5,
        "recency_score": 0.8819980864619349,
        "semantic_score": 1.8491844654083252,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.431182551870258
      },
      "section": null,
      "source_name": "arXiv cs.AI",
      "story_id": "arxiv:2601.15599",
      "summary": "arXiv:2601.15599v2 Announce Type: replace \nAbstract: Current business environments demand continuous reconfiguration of cross-functional processes, yet enterprise systems remain organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile, large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic and verifiable execution of complex business logic. We introduce Autonomous Business System (AUTOBUS), a system that combines LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic architecture for executing end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre- and post-conditions, required data, evaluation rules, and API-level actions. Enterprise data is represented as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, p",
      "title": "Autonomous Business System via Neuro-symbolic AI"
    },
    {
      "arxiv_id": "2602.17599",
      "authors": [
        "Ivan Rinaldi",
        "Matteo Mendula",
        "Nicola Fanelli",
        "Florence Lev",
        "Matteo Testi",
        "Giovanna Castellano",
        "Gennaro Vessio"
      ],
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:29.299556+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
          "url": "https://arxiv.org/abs/2602.17599"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
        "url": "https://arxiv.org/abs/2602.17599"
      },
      "published_at": "2026-02-19T18:23:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8724926572762978,
        "semantic_score": 3.010554474592209,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.38304713186851
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17599",
      "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifi",
      "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment"
    },
    {
      "arxiv_id": "2602.17221",
      "authors": [
        "Yi-Chih Huang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:27.942096+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
          "url": "https://arxiv.org/abs/2602.17221"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
        "url": "https://arxiv.org/abs/2602.17221"
      },
      "published_at": "2026-02-19T10:12:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 13.2,
        "recency_score": 0.8431957569711974,
        "semantic_score": 2.0431304037570954,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.286326160728294
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17221",
      "summary": "Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index ",
      "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences"
    },
    {
      "arxiv_id": "2602.17189",
      "authors": [
        "Sicheng Mao"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.941634+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Texo: Formula Recognition within 20M Parameters",
          "url": "https://arxiv.org/abs/2602.17189"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Texo: Formula Recognition within 20M Parameters",
        "url": "https://arxiv.org/abs/2602.17189"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.9005395770072937,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.282537663469228
      },
      "section": null,
      "source_name": "arXiv cs.CV",
      "story_id": "arxiv:2602.17189",
      "summary": "arXiv:2602.17189v1 Announce Type: cross \nAbstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.",
      "title": "Texo: Formula Recognition within 20M Parameters"
    },
    {
      "arxiv_id": "2510.04080",
      "authors": [
        "Zixin Song, Bowen Zhang, Qian-Wen Zhang, Di Yin, Xing Sun, Chunping Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.890025+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity",
          "url": "https://arxiv.org/abs/2510.04080"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity",
        "url": "https://arxiv.org/abs/2510.04080"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8955471515655518,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.277545238027486
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2510.04080",
      "summary": "arXiv:2510.04080v2 Announce Type: replace \nAbstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully leverage recent breakthroughs in the NLP community involving Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. Nevertheless, we find that naively applying listwise RL fails to produce meaningful improvements, as the model struggles with complex, coarse-grained reward signals, leading to optimization difficulties. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first t",
      "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity"
    },
    {
      "arxiv_id": "2602.17426",
      "authors": [
        "Marco Autili, Gianluca Filippone, Mashal Afzal Memon, Patrizio Pelliccione"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T21:31:05.596796+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "The Runtime Dimension of Ethics in Self-Adaptive Systems",
          "url": "https://arxiv.org/abs/2602.17426"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "The Runtime Dimension of Ethics in Self-Adaptive Systems",
        "url": "https://arxiv.org/abs/2602.17426"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.894238770008087,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.27623685647002
      },
      "section": null,
      "source_name": "arXiv cs.SE",
      "story_id": "arxiv:2602.17426",
      "summary": "arXiv:2602.17426v1 Announce Type: new \nAbstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirement",
      "title": "The Runtime Dimension of Ethics in Self-Adaptive Systems"
    },
    {
      "arxiv_id": "2602.17366",
      "authors": [
        "Yiming Zhang, Siyue Zhang, Junbo Zhao, Chen Zhao"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:28.908707+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
          "url": "https://arxiv.org/abs/2602.17366"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering",
        "url": "https://arxiv.org/abs/2602.17366"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.880742472410202,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.26274055887214
      },
      "section": null,
      "source_name": "arXiv cs.CL",
      "story_id": "arxiv:2602.17366",
      "summary": "arXiv:2602.17366v1 Announce Type: new \nAbstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 an",
      "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering"
    },
    {
      "arxiv_id": "2602.17264",
      "authors": [
        "Michael M\\\"uller, Amir Reza Mohammadi, Andreas Peintner, Beatriz Barroso Gstrein, G\\\"unther Specht, Eva Zangerle"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-21T02:04:37.270684+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems",
          "url": "https://arxiv.org/abs/2602.17264"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems",
        "url": "https://arxiv.org/abs/2602.17264"
      },
      "published_at": "2026-02-19T21:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 14.3,
        "recency_score": 0.8819980864619349,
        "semantic_score": 3.8074485063552856,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.18944659281722
      },
      "section": null,
      "source_name": "arXiv cs.IR",
      "story_id": "arxiv:2602.17264",
      "summary": "arXiv:2602.17264v1 Announce Type: new \nAbstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisf",
      "title": "On the Reliability of User-Centric Evaluation of Conversational Recommender Systems"
    }
  ],
  "run_date": "2026-02-21",
  "run_id": "8bb0754f-e74e-4e47-bef1-3a38b1b782a0",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-21T03:08:23.126369+00:00",
    "items_total": 120,
    "run_id": "8bb0754f-e74e-4e47-bef1-3a38b1b782a0",
    "started_at": "2026-02-21T03:06:02.579246+00:00",
    "stories_total": 685,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": null,
      "reason_code": "PARSE_HTML_ERROR",
      "reason_text": "Failed to parse HTML content.",
      "remediation_hint": "Source HTML structure may have changed; update selectors.",
      "source_id": "arxiv-api-agents",
      "status": "PARSE_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": null,
      "reason_code": "PARSE_HTML_ERROR",
      "reason_text": "Failed to parse HTML content.",
      "remediation_hint": "Source HTML structure may have changed; update selectors.",
      "source_id": "arxiv-api-alignment",
      "status": "PARSE_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 6,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": "2026-02-19T18:48:08+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-llm",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 2,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": "2026-02-19T18:36:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "arxiv-api-multimodal",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": null,
      "reason_code": "PARSE_HTML_ERROR",
      "reason_text": "Failed to parse HTML content.",
      "remediation_hint": "Source HTML structure may have changed; update selectors.",
      "source_id": "arxiv-api-reasoning",
      "status": "PARSE_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CL",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.CV",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.IR",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ir",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.LG",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.MA",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ma",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.RO",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ro",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv cs.SE",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-se",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "arXiv stat.ML",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": "2026-02-18T20:30:00+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 4,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": "2026-02-19T18:11:28+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": "2026-02-20T15:55:54+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": "2026-02-21T00:14:28+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": "2026-02-19T00:28:31+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": "2026-02-20T05:27:33+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "NO_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.16802",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface",
        "qwen"
      ],
      "first_seen_at": "2026-02-20T20:28:27.922706+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "References Improve LLM Alignment in Non-Verifiable Domains",
          "url": "https://arxiv.org/abs/2602.16802"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "References Improve LLM Alignment in Non-Verifiable Domains",
        "url": "https://arxiv.org/abs/2602.16802"
      },
      "published_at": "2026-02-18T19:03:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 4.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 18.7,
        "recency_score": 0.79163801717324,
        "semantic_score": 3.8740490078926086,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 35.56568702506585
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16802",
      "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains"
    },
    {
      "arxiv_id": "2602.15382",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-20T20:28:38.632390+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
          "url": "https://arxiv.org/abs/2602.15382"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
        "url": "https://arxiv.org/abs/2602.15382"
      },
      "published_at": "2026-02-17T06:31:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.95,
        "llm_relevance_score": 20.9,
        "recency_score": 0.6798716340066229,
        "semantic_score": 3.391104590892792,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 35.17097622489941
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15382",
      "summary": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems"
    },
    {
      "arxiv_id": "2602.16932",
      "authors": [
        "Jinming Nian",
        "Fangchen Li",
        "Dae Hoon Park",
        "Yi Fang"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-20T20:28:27.935210+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
          "url": "https://arxiv.org/abs/2602.16932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
        "url": "https://arxiv.org/abs/2602.16932"
      },
      "published_at": "2026-02-18T22:53:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 19.36,
        "recency_score": 0.8043688575467373,
        "semantic_score": 4.2493581712245945,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 34.61372702877133
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.16932",
      "summary": "Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as execut",
      "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution"
    },
    {
      "arxiv_id": "2602.15725",
      "authors": [
        "Sarim Chaudhry"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:32.112405+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-reasoning",
          "tier": 1,
          "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
          "url": "https://arxiv.org/abs/2602.15725"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-reasoning",
        "tier": 1,
        "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
        "url": "https://arxiv.org/abs/2602.15725"
      },
      "published_at": "2026-02-17T17:01:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.7102672270490344,
        "semantic_score": 4.555808216333389,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.26607544338243
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.15725",
      "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, perfo",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models"
    },
    {
      "arxiv_id": "2602.17547",
      "authors": [
        "Yue Liu",
        "Zhiyuan Hu",
        "Flood Sung",
        "Jiaheng Zhang",
        "Bryan Hooi"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-20T20:28:27.946665+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
          "url": "https://arxiv.org/abs/2602.17547"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
        "url": "https://arxiv.org/abs/2602.17547"
      },
      "published_at": "2026-02-19T17:01:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 19.8,
        "recency_score": 0.867488212209165,
        "semantic_score": 4.372162461280823,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.23965067348999
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.17547",
      "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using ",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks"
    }
  ]
}