{
  "archive_dates": [
    "2026-02-26",
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-26T07:08:21.498420+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-25T09:22:32.580536+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 416361,
          "likes": 537
        },
        "hf_model_id": "mistralai/devstral-small-2-24b-instruct-2512",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Devstral-Small-2-24B-Instruct-2512",
            "url": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Devstral-Small-2-24B-Instruct-2512",
          "url": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        "published_at": "2026-02-25T08:50:48+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9119885511662256,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.711988551166225
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/devstral-small-2-24b-instruct-2512",
        "summary": "Devstral is an agentic LLM for software engineering tasks. **Devstral Small 2** excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. This model is an Instruct model in **FP8**, fine-tuned to follow instructions, making it ideal for chat, agentic and instruction based tasks for SWE use cases. For enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we invite companies to reach out to us. The Devstral Small 2 Instruct model offers the following capabilities: - **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. - **Lightweight**: with its compact size of...",
        "summary_zh": "`Devstral` 是一個用於軟體工程任務的 `agentic LLM`。**Devstral Small 2** 擅長使用工具探索程式碼庫、編輯多個檔案並為軟體工程 `agent` 提供支援。該模型在 `SWE-bench` 上取得了卓越的性能。這個模型是一個 **FP8** 的 `Instruct model`，經過 `fine-tuning` 以遵循指令，使其非常適合用於軟體工程 (`SWE`) 用例中的聊天、`agentic` 和基於指令的任務。對於需要專業能力（例如增加的 `context`、領域特定知識等）的企業，我們歡迎公司與我們聯繫。`Devstral Small 2 Instruct model` 提供以下功能：- **Agentic Coding**：`Devstral` 旨在擅長 `agentic coding` 任務，使其成為軟體工程 `agent` 的絕佳選擇。- **Lightweight**：憑藉其緊湊的尺寸...",
        "title": "mistralai/Devstral-Small-2-24B-Instruct-2512",
        "title_zh": "mistralai/Devstral-Small-2-24B-Instruct-2512"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-25T09:22:32.580850+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 14797,
          "likes": 290
        },
        "hf_model_id": "mistralai/devstral-2-123b-instruct-2512",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Devstral-2-123B-Instruct-2512",
            "url": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Devstral-2-123B-Instruct-2512",
          "url": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
        },
        "published_at": "2026-02-25T08:50:20+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9119589964605953,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.711958996460595
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/devstral-2-123b-instruct-2512",
        "summary": "Devstral is an agentic LLM for software engineering tasks. **Devstral 2** excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. This model is an Instruct model in **FP8**, fine-tuned to follow instructions, making it ideal for chat, agentic and instruction based tasks for SWE use cases. For enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we invite companies to reach out to us. The Devstral 2 Instruct model offers the following capabilities: - **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. - **Improved Performance**: Devstral 2 is a step-up...",
        "summary_zh": "`Devstral` 是一個用於軟體工程任務的 `agentic LLM`。**Devstral 2** 擅長使用工具探索程式碼庫、編輯多個檔案並為軟體工程 `agent` 提供支援。該模型在 `SWE-bench` 上取得了卓越的性能。這個模型是一個 **FP8** 的 `Instruct model`，經過 `fine-tuning` 以遵循指令，使其非常適合用於軟體工程 (`SWE`) 用例中的聊天、`agentic` 和基於指令的任務。對於需要專業能力（例如增加的 `context`、領域特定知識等）的企業，我們歡迎公司與我們聯繫。`Devstral 2 Instruct model` 提供以下功能：- **Agentic Coding**：`Devstral` 旨在擅長 `agentic coding` 任務，使其成為軟體工程 `agent` 的絕佳選擇。- **Improved Performance**：`Devstral 2` 是一個升級版...",
        "title": "mistralai/Devstral-2-123B-Instruct-2512",
        "title_zh": "mistralai/Devstral-2-123B-Instruct-2512"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-24T09:02:59.820823+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 92622,
          "likes": 104,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-397B-A17B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        },
        "published_at": "2026-02-26T03:37:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.986212303535821,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.786212303535821
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
        "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information,...",
        "summary_zh": "此儲存庫包含 Hugging Face Transformers 格式的後訓練模型之 FP8 量化模型權重和配置檔案。這些 artifacts 相容於 Hugging Face Transformers、vLLM、SGLang、KTransformers 等。量化方法是粒度為 128 的細粒度 fp8 量化，其性能指標與原始模型幾乎一致。對於尋求無需基礎設施維護的託管式、可擴展推理的用戶，Alibaba Cloud Model Studio 提供了官方的 Qwen API 服務。特別地，**Qwen3.5-Plus** 是與 Qwen3.5-397B-A17B 相對應的託管版本，具有更多生產級功能，例如預設 1M context length、官方內建工具和自適應工具使用。欲了解更多資訊，請查閱...",
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "title_zh": "Qwen/Qwen3.5-397B-A17B-FP8"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-26T06:30:33.982240+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 23,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-35b-a3b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-35B-A3B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-FP8"
        },
        "published_at": "2026-02-26T03:34:45+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9860228411324601,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.78602284113246
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-35b-a3b-fp8",
        "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Flash** is the hosted version corresponding to Qwen3.5-35B-A3B with more production features, e.g., 1M context length by default and official built-in tools. > For more information, please refer to the...",
        "summary_zh": "此儲存庫包含 Hugging Face Transformers 格式的後訓練模型之 FP8 量化模型權重和配置檔案。這些 artifacts 相容於 Hugging Face Transformers、vLLM、SGLang、KTransformers 等。量化方法是粒度為 128 的細粒度 fp8 量化，其性能指標與原始模型幾乎一致。對於尋求無需基礎設施維護的託管式、可擴展推理的用戶，Alibaba Cloud Model Studio 提供了官方的 Qwen API 服務。特別地，**Qwen3.5-Flash** 是與 Qwen3.5-35B-A3B 相對應的託管版本，具有更多生產級功能，例如預設 1M context length 和官方內建工具。欲了解更多資訊，請參閱...",
        "title": "Qwen/Qwen3.5-35B-A3B-FP8",
        "title_zh": "Qwen/Qwen3.5-35B-A3B-FP8"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-26T06:30:33.982590+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 18,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-122b-a10b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-122B-A10B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-122B-A10B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B-FP8"
        },
        "published_at": "2026-02-26T03:26:52+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9854831870074484,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.785483187007449
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-122b-a10b-fp8",
        "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and...",
        "summary_zh": "此儲存庫包含 Hugging Face Transformers 格式的後訓練模型之 FP8 量化模型權重和配置檔案。這些 artifacts 相容於 Hugging Face Transformers、vLLM、SGLang、KTransformers 等。量化方法是粒度為 128 的細粒度 fp8 量化，其性能指標與原始模型幾乎一致。近幾個月來，我們加大了對開發基礎模型的關注，以提供卓越的實用性和性能。Qwen3.5 代表著一項重大的飛躍，整合了多模態學習、架構效率、強化學習規模和全球可及性方面的突破，賦予開發者和企業前所未有的能力和...",
        "title": "Qwen/Qwen3.5-122B-A10B-FP8",
        "title_zh": "Qwen/Qwen3.5-122B-A10B-FP8"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-26T06:30:33.982811+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 18,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-27b-fp8",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-27B-FP8",
            "url": "https://huggingface.co/Qwen/Qwen3.5-27B-FP8"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-27B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-27B-FP8"
        },
        "published_at": "2026-02-26T03:25:15+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9853725544804094,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.785372554480409
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-27b-fp8",
        "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and...",
        "summary_zh": "此儲存庫包含 Hugging Face Transformers 格式的後訓練模型之 FP8 量化模型權重和配置檔案。這些 artifacts 相容於 Hugging Face Transformers、vLLM、SGLang、KTransformers 等。量化方法是粒度為 128 的細粒度 fp8 量化，其性能指標與原始模型幾乎一致。近幾個月來，我們加大了對開發基礎模型的關注，以提供卓越的實用性和性能。Qwen3.5 代表著一項重大的飛躍，整合了多模態學習、架構效率、強化學習規模和全球可及性方面的突破，賦予開發者和企業前所未有的能力和...",
        "title": "Qwen/Qwen3.5-27B-FP8",
        "title_zh": "Qwen/Qwen3.5-27B-FP8"
      }
    ],
    "stability-ai": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.445951+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 0,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sdxl-turbo-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sdxl-turbo-amdnpu",
            "url": "https://huggingface.co/stabilityai/sdxl-turbo-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sdxl-turbo-amdnpu",
          "url": "https://huggingface.co/stabilityai/sdxl-turbo-amdnpu"
        },
        "published_at": "2026-02-25T22:40:19+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9660665851114087,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 2.7,
          "total_score": 9.46606658511141
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sdxl-turbo-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: other license_name: stabilityai-ai-community license_link: LICENSE \"SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation\" (stabilityai/sdxl-turbo). This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/sdxl-turbo. | Model Details | Description |",
        "summary_zh": "language: en - stable-diffusion pipeline_tag: text-to-image license: other license_name: stabilityai-ai-community license_link: LICENSE 「SDXL-Turbo 是一種快速的生成式 text-to-image 模型，能夠透過單次網路評估從文字提示中合成逼真的圖像」(stabilityai/sdxl-turbo)。此模型版本已針對 AMD NPU 進行優化。欲了解更多資訊，請參考 stabilityai/sdxl-turbo 下的原始模型卡。| 模型詳情 | 描述 |",
        "title": "stabilityai/sdxl-turbo-amdnpu",
        "title_zh": "stabilityai/sdxl-turbo-amdnpu"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.446196+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 1,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sd-turbo-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sd-turbo-amdnpu",
            "url": "https://huggingface.co/stabilityai/sd-turbo-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sd-turbo-amdnpu",
          "url": "https://huggingface.co/stabilityai/sd-turbo-amdnpu"
        },
        "published_at": "2026-02-25T22:22:37+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9648798577594749,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 2.7,
          "total_score": 9.464879857759474
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sd-turbo-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: other license_name: stabilityai-ai-community license_link: LICENSE \"SD-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\" (stabilityai/sd-turbo) This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/sd-turbo. | Model Details | Description |",
        "summary_zh": "language: en - `stable-diffusion` pipeline_tag: `text-to-image` license: other license_name: `stabilityai-ai-community` license_link: LICENSE 「`SD-Turbo` 是一個快速的生成式 `text-to-image` 模型，能夠透過單次網路評估從文字提示合成出逼真的圖像。」(stabilityai/sd-turbo) 此模型版本已針對在 `AMD NPU`s 上運行進行了優化。更多資訊請參考 `stabilityai/sd-turbo` 下的原始 `model card`。| Model Details | Description |",
        "title": "stabilityai/sd-turbo-amdnpu",
        "title_zh": "stabilityai/sd-turbo-amdnpu"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.445697+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 0,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sdxl-base-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sdxl-base-amdnpu",
            "url": "https://huggingface.co/stabilityai/sdxl-base-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sdxl-base-amdnpu",
          "url": "https://huggingface.co/stabilityai/sdxl-base-amdnpu"
        },
        "published_at": "2026-02-25T22:52:04+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9668551903058881,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 1.2000000000000002,
          "total_score": 7.966855190305888
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sdxl-base-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: openrail++ license_link: LICENSE \"SDXL is a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder.\" (stabilityai/stable-diffusion-xl-base-1.0) This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/stable-diffusion-xl-base-1.0. | Model Details | Description | | ----------- | ----------- |",
        "summary_zh": "language: en - `stable-diffusion` pipeline_tag: `text-to-image` license: `openrail++` license_link: LICENSE 「`SDXL` 是一個用於 `text-to-image` 合成的 `latent diffusion model`。與先前版本的 `Stable Diffusion` 相比，`SDXL` 利用了三倍大的 `UNet backbone`，這是透過顯著增加 `attention block` 的數量並包含第二個 `text encoder` 來實現的。」(stabilityai/stable-diffusion-xl-base-1.0) 此模型版本已針對在 `AMD NPU`s 上運行進行了優化。更多資訊請參考 `stabilityai/stable-diffusion-xl-base-1.0` 下的原始 `model card`。| Model Details | Description | | ----------- | ----------- |",
        "title": "stabilityai/sdxl-base-amdnpu",
        "title_zh": "stabilityai/sdxl-base-amdnpu"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.21951",
      "authors": [
        "Bo Xue",
        "Yuan Jin",
        "Luoyi Fu",
        "Jiaxin Ding",
        "Xinbing Wang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844970+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
          "url": "https://arxiv.org/abs/2602.21951"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
        "url": "https://arxiv.org/abs/2602.21951"
      },
      "published_at": "2026-02-25T14:34:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9339875277919537,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.77398752779195
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21951",
      "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We ",
      "summary_zh": "知識圖譜推理 (KGR) 推斷缺失的事實，近期進展日益利用 Large Language Models (LLMs) 的語義先驗和推理能力。然而，主流的生成範式容易記憶表面層次的共現，而非學習真正的關係語義，這限制了 out-of-distribution 泛化。為了解決這個問題，我們提出了 RADAR，它將 KGR 從生成式模式匹配重新定義為判別式關係推理。我們",
      "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
      "title_zh": "RADAR：基於LLM的知識圖譜推理中，將推理視為帶有對齊表示的判別"
    },
    {
      "arxiv_id": "2602.21765",
      "authors": [
        "Kenton Tang",
        "Yuzhu Chen",
        "Fengxiang He"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.865696+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Generalisation of RLHF under Reward Shift and Clipped KL Regularisation",
          "url": "https://arxiv.org/abs/2602.21765"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Generalisation of RLHF under Reward Shift and Clipped KL Regularisation",
        "url": "https://arxiv.org/abs/2602.21765"
      },
      "published_at": "2026-02-25T10:36:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9186936064708846,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.758693606470885
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21765",
      "summary": "Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \\emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF ",
      "summary_zh": "大型語言模型中的對齊和適應嚴重依賴於 reinforcement learning from human feedback (RLHF)；然而，其泛化能力的理論理解仍不成熟，特別是當學習到的獎勵可能發生偏移，且 KL 控制被估計和裁剪時。為了解決這個問題，我們為 RLHF 開發了泛化理論，明確考慮了 (1) `reward shift`：獎勵模型是根據來自早期或混合行為策略的偏好數據進行訓練的，而 RLHF",
      "title": "Generalisation of RLHF under Reward Shift and Clipped KL Regularisation",
      "title_zh": "在獎勵偏移和裁剪 KL 正則化下的 RLHF 泛化"
    },
    {
      "arxiv_id": "2602.21704",
      "authors": [
        "Jianghao Yin",
        "Qin Chen",
        "Kedi Chen",
        "Jie Zhou",
        "Xingjiao Wu",
        "Liang He"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.867426+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.21704"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.21704"
      },
      "published_at": "2026-02-25T09:10:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9132053469180157,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.753205346918016
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21704",
      "summary": "Large Vision-Language Models (LVLMs) exhibit outstanding performance on vision-language tasks but struggle with hallucination problems. Through in-depth analysis of LVLM activation patterns, we reveal two key findings: 1) truthfulness and visual perception capabilities predominantly engage different subsets of attention heads within the model architecture; and 2) truthfulness steering vectors vary significantly across different semantic contexts. Based on these observations, we propose Dynamic M",
      "summary_zh": "Large Vision-Language Models (LVLMs) 在視覺-語言任務上表現出色，但卻飽受幻覺問題的困擾。通過對 LVLM 激活模式的深入分析，我們揭示了兩個關鍵發現：1) 真實性和視覺感知能力主要在模型架構中涉及不同的 attention heads 子集；以及 2) 真實性引導向量在不同的語義上下文中顯著變化。基於這些觀察，我們提出了 Dynamic M",
      "title": "Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models",
      "title_zh": "用於大型視覺-語言模型中幻覺緩解的動態多模態激活引導"
    },
    {
      "arxiv_id": "2602.21670",
      "authors": [
        "Tomoya Kawabe",
        "Rin Takano"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.867647+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
          "url": "https://arxiv.org/abs/2602.21670"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
        "url": "https://arxiv.org/abs/2602.21670"
      },
      "published_at": "2026-02-25T08:08:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9093093059040488,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.74930930590405
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21670",
      "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an ",
      "summary_zh": "多機器人任務規劃需要將自然語言指令分解為異構機器人團隊的可執行動作。傳統的 Planning Domain Definition Language (PDDL) 規劃器提供嚴格的保證，但在處理模糊或長期任務時會遇到困難，而 large language models (LLMs) 可以解釋指令並提出計劃，但可能會產生幻覺或生成不可行的動作。我們提出了一個帶有 prompt optimization 的分層多智能體 LLM-based 規劃器：一個",
      "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
      "title_zh": "帶有 Prompt 優化的分層 LLM 多智能體框架，用於多機器人任務規劃"
    },
    {
      "arxiv_id": "2602.21628",
      "authors": [
        "Yukun Chen",
        "Jiaming Li",
        "Longze Chen",
        "Ze Gong",
        "Jingpeng Li",
        "Zhen Qin",
        "Hengyu Chang",
        "Ancheng Xu",
        "Zhihao Yang",
        "Hamid Alinejad-Rokny",
        "Qiang Qu",
        "Bo Zheng",
        "Min Yang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.854495+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning",
          "url": "https://arxiv.org/abs/2602.21628"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning",
        "url": "https://arxiv.org/abs/2602.21628"
      },
      "published_at": "2026-02-25T06:46:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9041439166550109,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.74414391665501
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21628",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by ",
      "summary_zh": "Reinforcement Learning with Verifiable Rewards (RLVR) 已成為增強 Multimodal Large Language Models (MLLMs) 推理能力的主流範式。然而，僅僅依賴結果監督存在獎勵作弊 (reward hacking) 的風險，即模型學習虛假的推理模式以滿足最終答案檢查。雖然最近基於評分標準的方法提供了細粒度的監督信號，但它們卻因實例級生成的高計算成本和低效的訓練動態而受苦。",
      "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning",
      "title_zh": "RuCL：用於多模態大型語言模型推理的分層基於評分標準的課程學習"
    },
    {
      "arxiv_id": "2602.22175",
      "authors": [
        "Xi Ye",
        "Wuwei Zhang",
        "Fangcong Yin",
        "Howard Yen",
        "Danqi Chen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.842276+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
          "url": "https://arxiv.org/abs/2602.22175"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
        "url": "https://arxiv.org/abs/2602.22175"
      },
      "published_at": "2026-02-25T18:21:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9488637042742559,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.948863704274256
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22175",
      "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads speciali",
      "summary_zh": "理解和推理長上下文是語言模型 (LMs) 的一項關鍵能力。儘管最近的模型支援越來越長的 `context windows`，但其準確性往往隨著輸入長度增加而下降。實際上，模型在整個解碼過程中，通常難以將注意力與最相關的上下文保持一致。在這項工作中，我們提出了 `DySCO`，一種新穎的解碼演算法，用於改進長上下文推理。`DySCO` 利用 `retrieval heads`——`attention heads` 的一個子集，專門",
      "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
      "title_zh": "DySCO：用於長上下文 LMs 的動態注意力縮放解碼"
    },
    {
      "arxiv_id": "2602.22146",
      "authors": [
        "Yining Li",
        "Peizhong Ju",
        "Ness Shroff"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.857659+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
          "url": "https://arxiv.org/abs/2602.22146"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
        "url": "https://arxiv.org/abs/2602.22146"
      },
      "published_at": "2026-02-25T17:54:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9471048866935213,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.947104886693523
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22146",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parame",
      "summary_zh": "`Reinforcement Learning from Human Feedback` (RLHF) 在將 `Large Language Models` (LLMs) 與人類偏好對齊方面扮演著重要角色。儘管帶有預期獎勵約束的 RLHF 可以被 формуlated 為一個原始對偶優化問題，但標準的原始對偶方法僅在 `saddle-point problem` 呈 `convex-concave form` 的分佈式策略下保證收斂。此外，標準的原始對偶方法在 `policy parame` 下，其最終迭代可能會表現出不穩定性或發散。",
      "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
      "title_zh": "透過樂觀原始對偶實現多目標安全 LLM 對齊的可證實最終迭代收斂性"
    },
    {
      "arxiv_id": "2602.22142",
      "authors": [
        "Yulin Zhang",
        "Cheng Shi",
        "Sibei Yang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371331+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
          "url": "https://arxiv.org/abs/2602.22142"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
        "url": "https://arxiv.org/abs/2602.22142"
      },
      "published_at": "2026-02-25T17:45:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9465054626041299,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.94650546260413
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22142",
      "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in stream",
      "summary_zh": "`Multimodal Large Language Models` 的最新進展極大地改進了視覺理解和推理，但其 `quadratic attention` 和 `offline training protocols` 使它們不適用於幀按順序到達且未來觀測不可訪問的 `streaming settings`。我們診斷出當前 `Video-LLMs` 的一個核心局限性，即 `Time-Agnosticism`，其中影片被視為無序的證據袋，而非因果排序的序列，導致在串流中出現兩種故障。",
      "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
      "title_zh": "WeaveTime：將早期幀流式傳輸到 VideoLLMs 中的新興記憶"
    },
    {
      "arxiv_id": "2602.21947",
      "authors": [
        "Sohan Venkatesh",
        "Ashish Mahendran Kurapath",
        "Tejas Melkote"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845442+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Large Language Models are Algorithmically Blind",
          "url": "https://arxiv.org/abs/2602.21947"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Large Language Models are Algorithmically Blind",
        "url": "https://arxiv.org/abs/2602.21947"
      },
      "published_at": "2026-02-25T14:32:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9338718675170448,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.933871867517045
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21947",
      "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider ",
      "summary_zh": "`Large language models` (LLMs) 展現出驚人的知識廣度，但其對計算過程的推理能力仍知之甚少。彌合這一差距對於依賴 LLMs 指導演算法選擇和部署的實踐者而言至關重要。我們利用因果發現作為測試平台來解決這一限制，並評估了八個前沿 LLMs，將其與大規模演算法執行得出的 `ground truth` 進行比較，結果發現系統性、幾乎完全的失敗。模型產生的範圍遠比",
      "title": "Large Language Models are Algorithmically Blind",
      "title_zh": "`Large Language Models` 在演算法上是盲的"
    },
    {
      "arxiv_id": "2602.21887",
      "authors": [
        "Changjiang Gao",
        "Zixian Huang",
        "Kaichen Yang",
        "Jiajun Chen",
        "Jixing Li",
        "Shujian Huang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845911+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
          "url": "https://arxiv.org/abs/2602.21887"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
        "url": "https://arxiv.org/abs/2602.21887"
      },
      "published_at": "2026-02-25T13:10:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.92861531319856,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.928615313198563
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21887",
      "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language sel",
      "summary_zh": "當前的大型推理模型 (LRMs) 在基於 `reinforcement learning` (RL) 的後訓練之後，已在具挑戰性的任務上展現出強大的能力。然而，儘管多語言思維的潛在優勢已被證明，且全球用戶對本地思維軌跡有所需求，但先前的工作主要側重於英語推理，以期獲得最強性能。在本文中，我們提出了 `ExpLang`，一種新穎的 LLM 後訓練管道，可實現 `on-policy thinking language sel`。",
      "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
      "title_zh": "ExpLang：透過在策略思維語言選擇改進 LLM 推理中的探索與利用"
    },
    {
      "arxiv_id": "2602.21798",
      "authors": [
        "Sagi Shaier"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.864992+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Excitation: Momentum For Experts",
          "url": "https://arxiv.org/abs/2602.21798"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Excitation: Momentum For Experts",
        "url": "https://arxiv.org/abs/2602.21798"
      },
      "published_at": "2026-02-25T11:22:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9216650162484484,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.921665016248447
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21798",
      "summary": "We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introduces a competitive update dynamic that amplifies updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization. Notably, we identi",
      "summary_zh": "我們提出 Excitation，一個新穎的 optimization framework，旨在加速稀疏架構（例如 Mixture-of-Experts (MoEs)）中的學習。與傳統均勻處理所有 parameters 的 optimizers 不同，Excitation 透過 batch-level 的 expert utilization 動態調整更新。它引入了一種 competitive update dynamic，可以放大對高利用率 experts 的更新，並選擇性地抑制低利用率的 experts，有效地提升 routing specialization。值得注意的是，我們識別出",
      "title": "Excitation: Momentum For Experts",
      "title_zh": "Excitation：專家的動量"
    },
    {
      "arxiv_id": "2602.21786",
      "authors": [
        "Shunsuke Ubukata"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.852350+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models",
          "url": "https://arxiv.org/abs/2602.21786"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models",
        "url": "https://arxiv.org/abs/2602.21786"
      },
      "published_at": "2026-02-25T11:08:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9207597971855197,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.92075979718552
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21786",
      "summary": "Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the C",
      "summary_zh": "從 Large Language Models (LLMs) 進行 Chain-of-Thought (CoT) distillation 通常會在 Small Language Models (SLMs) 中導致「overthinking」，進而造成 performance degradation 和過度 token consumption。在本研究中，我們提出 Disciplined Chain-of-Thought (D-CoT)，這是一個新穎的 framework，它在 training 期間使用 control tags（例如用於 fact-checking 的 <TEMP_LOW> 和用於 multi-perspective exploration 的 <TEMP_HIGH>）作為 auxiliary scaffolding，以強制執行結構化的 reasoning process。透過優化 C",
      "title": "D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models",
      "title_zh": "D-COT：用於 Small Language Models 高效推理的 Disciplined Chain-of-Thought 學習"
    },
    {
      "arxiv_id": "2602.21743",
      "authors": [
        "Jinghan Li",
        "Junfeng Fang",
        "Jinda Lu",
        "Yuan Wang",
        "Xiaoyan Guo",
        "Tianyu Zhang",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.382558+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization",
          "url": "https://arxiv.org/abs/2602.21743"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization",
        "url": "https://arxiv.org/abs/2602.21743"
      },
      "published_at": "2026-02-25T09:52:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9159257540148591,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.91592575401486
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21743",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) have significantly advanced the reasoning capabilities of large language models. Extending these methods to multimodal settings, however, faces a critical challenge: the instability of std-based normalization, which is easily distorted by extreme samples with nearly positive or negative rewards. Unlike pure-text LLMs, multimodal models are particularly sensitive to such distortions, as both percep",
      "summary_zh": "Reinforcement Learning with Verifiable Rewards (RLVR) 和 Group Relative Policy Optimization (GRPO) 已顯著提升大型語言模型的 reasoning capabilities。然而，將這些方法擴展到 multimodal settings 面臨一個關鍵挑戰：std-based normalization 的不穩定性，它很容易被具有接近正或負 rewards 的 extreme samples 所扭曲。與 pure-text LLMs 不同，multimodal models 對於此類 distortion 特別敏感，因為無論是 percep",
      "title": "Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization",
      "title_zh": "透過 Difficulty-Aware Group Normalization 增強 Multi-Modal LLMs 的推理能力"
    },
    {
      "arxiv_id": "2602.21669",
      "authors": [
        "Duc Trung Vu",
        "Pham Khanh Chi",
        "Dat Phi Van",
        "Linh Ngo Van",
        "Sang Dinh",
        "Trung Le"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.853432+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation",
          "url": "https://arxiv.org/abs/2602.21669"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation",
        "url": "https://arxiv.org/abs/2602.21669"
      },
      "published_at": "2026-02-25T08:04:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.909075693943548,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.90907569394355
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21669",
      "summary": "Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-bas",
      "summary_zh": "Knowledge Distillation (KD) 已成為壓縮 Large Language Models (LLMs) 的關鍵技術。儘管現有的 cross-tokenizer KD 方法已取得顯著進展，但其有效性仍受制於 sequence 和 vocabulary levels 之間次優的 alignment。為了解決這些限制，我們引入 Dual-Space Weighting and Time-Warped Alignment (DWA-KD)，這是一個新穎的 cross-tokenizer distillation framework，透過 dual-space entropy-bas",
      "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation",
      "title_zh": "DWA-KD：用於跨 Tokenizer 知識蒸餾的 Dual-Space Weighting 和 Time-Warped Alignment"
    },
    {
      "arxiv_id": "2602.22208",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.369723+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
          "url": "https://arxiv.org/abs/2602.22208"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
        "url": "https://arxiv.org/abs/2602.22208"
      },
      "published_at": "2026-02-25T18:59:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9513335194137483,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.551333519413753
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22208",
      "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.",
      "summary_zh": "現有的 action-conditioned video generation models (video world models) 僅限於 single-agent perspectives，未能捕捉現實世界環境中的 multi-agent interactions。我們引入 Solaris，這是一個 multiplayer video world model，它能模擬一致的 multi-view observations。為此，我們開發了一個 multiplayer data system，旨在對諸如 Minecraft 等電玩遊戲進行穩健、連續和自動化的 data collection。與為 single-player settings 建構的先前平台不同，我們的系統支持協調的 multi-agent interaction 和同步的 videos + actions capture。透過使用這個系統，我們收集了 12.64 million 的 multiplayer frames，並提出了一個用於 multiplayer movement, memory, grounding, building 和 view consistency 的 evaluation framework。我們使用一個 staged pipeline 訓練 Solaris，該 pipeline 逐步從 single-player 過渡到 multiplayer modeling，並結合了 bidirectional, causal 和 Self Forcing training。在最後階段，我們引入了 Checkpointed Self Forcing，這是一個 memory-efficient 的 Self Forcing variant，可實現更長期的 teacher。結果顯示我們的 architecture 和 training design 優於現有的 baselines。透過 open-sourcing 我們的系統和模型，我們希望為新一代的 multi-agent world models 奠定基礎。",
      "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
      "title_zh": "Solaris：在 Minecraft 中建構一個 Multiplayer Video World Model"
    },
    {
      "arxiv_id": "2602.21818",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.380577+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
          "url": "https://arxiv.org/abs/2602.21818"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
        "url": "https://arxiv.org/abs/2602.21818"
      },
      "published_at": "2026-02-25T11:47:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9232162962920567,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.52321629629206
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21818",
      "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
      "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model"
    },
    {
      "arxiv_id": "2602.22010",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374497+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "World Guidance: World Modeling in Condition Space for Action Generation",
          "url": "https://arxiv.org/abs/2602.22010"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "World Guidance: World Modeling in Condition Space for Action Generation",
        "url": "https://arxiv.org/abs/2602.22010"
      },
      "published_at": "2026-02-25T15:27:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.9374390496320655,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.977439049632068
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22010",
      "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
      "title": "World Guidance: World Modeling in Condition Space for Action Generation"
    },
    {
      "arxiv_id": "2602.22193",
      "authors": [
        "Melody Ma",
        "John Hewitt"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.841548+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
          "url": "https://arxiv.org/abs/2602.22193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
        "url": "https://arxiv.org/abs/2602.22193"
      },
      "published_at": "2026-02-25T18:43:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9502770691954314,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.550277069195435
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22193",
      "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowle",
      "title": "Improving Parametric Knowledge Access in Reasoning Language Models"
    },
    {
      "arxiv_id": "2602.22157",
      "authors": [
        "Leon Pielage",
        "Ole Hätscher",
        "Mitja Back",
        "Bernhard Marschall",
        "Benjamin Risse"
      ],
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.939591+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
          "url": "https://arxiv.org/abs/2602.22157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
        "url": "https://arxiv.org/abs/2602.22157"
      },
      "published_at": "2026-02-25T18:05:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9477836690811684,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.54778366908117
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22157",
      "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring th",
      "title": "Dynamic Personality Adaptation in Large Language Models via State Machines"
    },
    {
      "arxiv_id": "2602.22145",
      "authors": [
        "Satyam Kumar Navneet",
        "Joydeep Chandra",
        "Yong Zhang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.857888+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
          "url": "https://arxiv.org/abs/2602.22145"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
        "url": "https://arxiv.org/abs/2602.22145"
      },
      "published_at": "2026-02-25T17:54:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9470939248948432,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.547093924894845
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22145",
      "summary": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using tw",
      "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models"
    },
    {
      "arxiv_id": "2602.22124",
      "authors": [
        "Patrick Tser Jern Kon",
        "Archana Pradeep",
        "Ang Chen",
        "Alexander P. Ellis",
        "Warren Hunt",
        "Zijian Wang",
        "John Yang",
        "Samuel Thompson"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.858412+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
          "url": "https://arxiv.org/abs/2602.22124"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
        "url": "https://arxiv.org/abs/2602.22124"
      },
      "published_at": "2026-02-25T17:11:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.94427766592111,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.544277665921115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22124",
      "summary": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance fr",
      "summary_zh": "小型語言模型 (SLMs) 在成本、延遲和適應性方面具有引人注目的優勢，但迄今為止，在如 SWE-bench 這類的長週期軟體工程任務上，它們仍落後於大型模型，普遍存在行動循環和解決率低的問題。我們引入了 SWE-Protégé，這是一個後訓練 (post-training) 框架，將軟體修復重新定義為一個 expert-protégé 協作問題。在 SWE-Protégé 中，SLM 仍然是唯一的決策者，同時學習選擇性地尋求指導。",
      "title": "SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
      "title_zh": "SWE-Protégé：學習選擇性地與專家協作，解鎖小型語言模型作為軟體工程代理的能力"
    },
    {
      "arxiv_id": "2602.21939",
      "authors": [
        "Maxim Chupilkin"
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861820+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
          "url": "https://arxiv.org/abs/2602.21939"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
        "url": "https://arxiv.org/abs/2602.21939"
      },
      "published_at": "2026-02-25T14:24:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9333877631791921,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.533387763179192
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21939",
      "summary": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social des",
      "summary_zh": "研究人員如何識別大型語言模型 (LLMs) 所隱藏的信念？隨著 LLMs 變得越來越複雜，以及 alignment faking 的普遍性增加，再加上它們日益整合到高風險決策中，應對這一挑戰變得至關重要。本文提出，一種在社會科學中廣泛使用的簡單方法，即 list experiment，可以應用於研究 LLMs 的隱藏信念。List experiments 最初是為了規避社會期望偏差而開發的。",
      "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
      "title_zh": "隱藏主題：利用 List Experiments 測量敏感的 AI 信念"
    },
    {
      "arxiv_id": "2602.21858",
      "authors": [
        "Dezhi Kong",
        "Zhengzhao Feng",
        "Qiliang Liang",
        "Hao Wang",
        "Haofei Sun",
        "Changpeng Yang",
        "Yang Li",
        "Peng Zhou",
        "Shuai Nie",
        "Hongzhen Wang",
        "Linfeng Zhou",
        "Hao Jia",
        "Jiaming Xu",
        "Runyu Shi",
        "Ying Huang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862801+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
          "url": "https://arxiv.org/abs/2602.21858"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
        "url": "https://arxiv.org/abs/2602.21858"
      },
      "published_at": "2026-02-25T12:32:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9261455202845792,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.52614552028458
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21858",
      "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complex",
      "summary_zh": "多模態大型語言模型 (MLLMs) 在 mobile agent 開發方面取得了顯著進展，但其能力主要局限於 reactive paradigm，即它們僅執行明確的使用者指令。主動智慧 (proactive intelligence) 的新興範式，即代理能夠自主預測需求並採取行動，代表了 mobile agent 的下一個前沿。然而，其發展因缺乏能解決真實世界複雜問題的基準而受到嚴重瓶頸。",
      "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
      "title_zh": "ProactiveMobile：一個用於提升行動裝置上主動智慧的綜合基準"
    },
    {
      "arxiv_id": "2602.21854",
      "authors": [
        "Mustafa Dogan",
        "Ilker Kesen",
        "Iacer Calixto",
        "Aykut Erdem",
        "Erkut Erdem"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.851911+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
          "url": "https://arxiv.org/abs/2602.21854"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
        "url": "https://arxiv.org/abs/2602.21854"
      },
      "published_at": "2026-02-25T12:30:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9259965343210174,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.52599653432102
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21854",
      "summary": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables syst",
      "summary_zh": "儘管多模態大型語言模型 (MLLMs) 在處理交錯的圖像-文本數據方面不斷進步，但評估其 few-shot learning 能力仍是一個開放的挑戰。在本文中，我們引入了 FewMMBench，這是一個綜合基準測試，旨在評估 MLLMs 在 few-shot 條件下的表現，重點關注 In-Context Learning (ICL) 和 Chain-of-Thought (CoT) prompting。FewMMBench 涵蓋了從屬性識別到時間推理等多樣化的多模態理解任務套件，能夠系統地進行評估。",
      "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
      "title_zh": "FewMMBench：一個用於多模態 Few-Shot Learning 的基準測試"
    },
    {
      "arxiv_id": "2602.21788",
      "authors": [
        "Yifan Niu",
        "Han Xiao",
        "Dongyi Liu",
        "Wei Zhou",
        "Jia Li"
      ],
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.948304+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism",
          "url": "https://arxiv.org/abs/2602.21788"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism",
        "url": "https://arxiv.org/abs/2602.21788"
      },
      "published_at": "2026-02-25T11:11:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9209676310089998,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.520967631009
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21788",
      "summary": "Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communic",
      "summary_zh": "擴展 long-context 能力對於 Multimodal Large Language Models (MLLMs) 至關重要。然而，真實世界的多模態數據集極其異質。現有的訓練框架主要依賴於 static parallelism 策略，這在數據異質性下會導致嚴重的負載不平衡、冗餘通訊和次優的硬體利用率。在這項工作中，我們提出了 Dynamic Hybrid Parallelism (DHP)，這是一種高效的並行化策略，能自適應地重新配置通訊。",
      "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism",
      "title_zh": "DHP：利用 Dynamic Hybrid Parallelism 有效率地擴展 MLLM 訓練"
    },
    {
      "arxiv_id": "2602.21736",
      "authors": [
        "Hao Luo",
        "Ye Wang",
        "Wanpeng Zhang",
        "Haoqi Yuan",
        "Yicheng Feng",
        "Haiweng Xu",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.282171+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
          "url": "https://arxiv.org/abs/2602.21736"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
        "url": "https://arxiv.org/abs/2602.21736"
      },
      "published_at": "2026-02-25T09:46:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9155357205585778,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.51553572055858
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21736",
      "summary": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embeddin",
      "summary_zh": "儘管取得了進展，Vision-Language-Action 模型 (VLAs) 仍受限於大規模、多樣化機器人數據的稀缺性。雖然人類操作影片提供了豐富的替代方案，但現有方法被迫在小型、精確標註的數據集和大量野外影片（帶有不可靠的手部追蹤標籤）之間做出選擇。我們提出了 JALA，一個學習 Jointly-Aligned Latent Actions 的預訓練框架。JALA 繞過了完整的視覺動態重建，轉而學習一個預測性動作嵌入",
      "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
      "title_zh": "Joint-Aligned Latent Action：邁向野外可擴展的 VLA 預訓練"
    },
    {
      "arxiv_id": "2602.22072",
      "authors": [
        "Christian Nickel",
        "Laura Schrewe",
        "Florian Mai",
        "Lucie Flek"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859357+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
          "url": "https://arxiv.org/abs/2602.22072"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
        "url": "https://arxiv.org/abs/2602.22072"
      },
      "published_at": "2026-02-25T16:24:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.9411854219395166,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.981185421939514
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22072",
      "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks",
      "summary_zh": "心智理論 (ToM) 指的是代理人建模他人內部狀態的能力。為了探討大型語言模型 (LLMs) 是否展現出真正的 ToM 能力，本研究透過對錯誤信念任務進行擾動來調查其 ToM 穩健性，並檢視 Chain-of-Thought prompting (CoT) 提升性能和解釋 LLM 決策的潛力。我們引入了一個手工製作、豐富註釋的 ToM 數據集，其中包含經典和擾動的錯誤信念任務",
      "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
      "title_zh": "理解人工心智理論：大型語言模型中的擾動任務與推理"
    },
    {
      "arxiv_id": "2602.21652",
      "authors": [
        "Minhao Jiang",
        "Zhikai Li",
        "Xuewen Liu",
        "Jing Zhang",
        "Mengjuan Chen",
        "Qingyi Gu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.868377+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models",
          "url": "https://arxiv.org/abs/2602.21652"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models",
        "url": "https://arxiv.org/abs/2602.21652"
      },
      "published_at": "2026-02-25T07:25:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.9065718251384731,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.946571825138474
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21652",
      "summary": "Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We pr",
      "summary_zh": "大型語言模型已在文本生成方面展現出強大能力，但其不斷增長的參數規模在計算和記憶體效率方面帶來了挑戰。訓練後稀疏性 (PTS) 透過從密集網絡中移除權重來降低模型成本，是一種有效的方法。然而，原生密集矩陣缺乏高稀疏性，導致現有直接移除權重的方法會破壞模型狀態，即使進行後續調優，性能恢復仍不盡理想。我們提出了",
      "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models",
      "title_zh": "稀疏性誘導：用於大型語言模型精確訓練後剪枝"
    },
    {
      "arxiv_id": "2602.22207",
      "authors": [
        "Hanna Yukhymenko",
        "Anton Alexandrov",
        "Martin Vechev"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.856202+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
          "url": "https://arxiv.org/abs/2602.22207"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
        "url": "https://arxiv.org/abs/2602.22207"
      },
      "published_at": "2026-02-25T18:58:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9512938813429049,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.151293881342905
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22207",
      "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies,",
      "summary_zh": "多語言 Large Language Model (LLM) 評估的可靠性目前因翻譯基準的不一致品質而受到損害。現有資源常遭受語義漂移和語境丟失的困擾，這可能導致誤導性的性能指標。在這項工作中，我們提出了一個全自動框架，旨在透過實現可擴展、高品質的數據集和基準翻譯來解決這些挑戰。我們證明，透過調整測試時計算擴展策略，",
      "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
      "title_zh": "譯中復原：用於基準和數據集自動翻譯的高效管線"
    },
    {
      "arxiv_id": "2602.22125",
      "authors": [
        "Thanmay Jayakumar",
        "Mohammed Safi Ur Rahman Khan",
        "Raj Dabre",
        "Ratish Puduppully",
        "Anoop Kunchukuttan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.843129+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
          "url": "https://arxiv.org/abs/2602.22125"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
        "url": "https://arxiv.org/abs/2602.22125"
      },
      "published_at": "2026-02-25T17:12:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9443301272486845,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.144330127248686
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22125",
      "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) ca",
      "summary_zh": "指令遵循基準仍主要以英語為中心，為數億印度語言使用者留下了關鍵的評估空白。我們引入了 IndicIFEval，一個基準，用於評估 LLMs 跨 14 種印度語言的受限生成，使用自動可驗證、基於規則的指令。它包含每種語言約 800 個經過人工驗證的範例，分佈在兩個互補子集：IndicIFEval-Ground，來自 IFEval (Zhou et al., 2023) 的翻譯提示",
      "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
      "title_zh": "IndicIFEval：一個用於 14 種印度語言可驗證指令遵循評估的基準"
    },
    {
      "arxiv_id": "2602.22090",
      "authors": [
        "Bo-Wei Chen",
        "Chung-Chi Chen",
        "An-Zi Yen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.843597+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
          "url": "https://arxiv.org/abs/2602.22090"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
        "url": "https://arxiv.org/abs/2602.22090"
      },
      "published_at": "2026-02-25T16:38:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9420660162978471,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.142066016297846
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22090",
      "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, en",
      "summary_zh": "Large Language Models (LLMs) 在多樣化的自然語言任務中徹底改變了 inference，儘管更大的模型表現更好，但其 computational costs 也更高。我們提出了一種 confidence-driven strategy，根據 confidence estimates 動態選擇最適合的模型。透過評估模型處理任務的信心和回應準確性，那些很可能被正確解決的任務會被保留，而更不確定或複雜的情況則會委託給更大的模型，以確保效能並控制成本。",
      "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
      "title_zh": "信心驅動的多尺度模型選擇，實現成本效益高的 Inference"
    },
    {
      "arxiv_id": "2602.22013",
      "authors": [
        "I-Hsiang Chen",
        "Yu-Wei Liu",
        "Tse-Yu Wu",
        "Yu-Chien Chiang",
        "Jen-Chien Yang",
        "Wei-Ting Chen"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374248+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
          "url": "https://arxiv.org/abs/2602.22013"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
        "url": "https://arxiv.org/abs/2602.22013"
      },
      "published_at": "2026-02-25T15:27:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9374911310259593,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.13749113102596
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22013",
      "summary": "Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address",
      "summary_zh": "Vision-based Retrieval-Augmented Generation (VisRAG) 利用 vision-language models (VLMs) 共同檢索相關的視覺文件並根據 multimodal evidence 生成 grounded answers。然而，當視覺輸入遭受諸如 blur、noise、low light 或 shadow 等失真時，現有的 VisRAG 模型性能會下降，此時 semantic 和 degradation factors 會在 pretrained visual encoders 中糾纏不清，導致 retrieval 和 generation 階段都出現錯誤。為了解決這個問題，我們提出了 RobustVisRAG。",
      "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
      "title_zh": "RobustVisRAG：在視覺退化下的因果意識視覺式 Retrieval-Augmented Generation"
    },
    {
      "arxiv_id": "2602.22001",
      "authors": [
        "Freek Stulp",
        "Samuel Bustamante",
        "João Silvério",
        "Alin Albu-Schäffer",
        "Jeannette Bohg",
        "Shuran Song"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.280283+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
          "url": "https://arxiv.org/abs/2602.22001"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
        "url": "https://arxiv.org/abs/2602.22001"
      },
      "published_at": "2026-02-25T15:19:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.936956349438693,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.13695634943869
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22001",
      "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond",
      "summary_zh": "無論是人類還是機器人，transfer learning 都發生在不同層次的抽象級別上，從高層次的 linguistic transfer 到低層次的 motor skills transfer。在本文中，我們概述了 foundation models 和 transformer networks 對這些不同層次所產生的影響，使機器人比以往任何時候都更接近「full-stack transfer」。從機器人 transfer learning 的角度考慮 LLMs、VLMs 和 VLAs，使我們能夠強調 transfer 中反覆出現的概念，超越單一模態或任務的限制。",
      "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
      "title_zh": "Foundation Models 是實現機器人領域 Full-Stack Transfer 的途徑嗎？"
    },
    {
      "arxiv_id": "2602.21992",
      "authors": [
        "Zekai Lin",
        "Xu Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374735+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.21992"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.21992"
      },
      "published_at": "2026-02-25T15:12:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9364717300331113,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.13647173003311
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21992",
      "summary": "360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) ground",
      "summary_zh": "360 panoramic images 越來越多地應用於 virtual reality、autonomous driving 和 robotics 中，以實現 holistic scene understanding。然而，目前的 Vision-Language Models (VLMs) 在 Equirectangular Projection (ERP) 圖像上的 3D spatial reasoning 方面遇到困難，原因在於 geometric distortion 和有限的 3D supervision。我們推出了 PanoEnv，這是一個從 synthetic 3D environments 構建的大規模 VQA benchmark，包含 14.8K 個問題，涵蓋五個類別（例如 relative position、volume comparison），並提供了 Ground Truth。",
      "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
      "title_zh": "PanoEnv：利用 Reinforcement Learning 探索全景環境中的 3D Spatial Intelligence"
    },
    {
      "arxiv_id": "2602.21956",
      "authors": [
        "Junxin Lu",
        "Tengfei Song",
        "Zhanglin Wu",
        "Pengfei Li",
        "Xiaowei Liang",
        "Hui Yang",
        "Kun Chen",
        "Ning Xie",
        "Yunfei Lu",
        "Jing Zhao",
        "Shiliang Sun",
        "Daimeng Wei"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.375872+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation",
          "url": "https://arxiv.org/abs/2602.21956"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation",
        "url": "https://arxiv.org/abs/2602.21956"
      },
      "published_at": "2026-02-25T14:38:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9342956647740447,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.134295664774044
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21956",
      "summary": "Text Image Machine Translation (TIMT) aims to translate text embedded in images in the source-language into target-language, requiring synergistic integration of visual perception and linguistic understanding. Existing TIMT methods, whether cascaded pipelines or end-to-end multimodal large language models (MLLMs),struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, resulting in text omission, semantic drift, and contextual inconsis",
      "summary_zh": "Text Image Machine Translation (TIMT) 旨在將圖像中 embedded 的源語言文本翻譯成目標語言，這需要 visual perception 和 linguistic understanding 的協同整合。現有的 TIMT 方法，無論是 cascaded pipelines 還是 end-to-end multimodal large language models (MLLMs)，都難以處理高解析度富文本圖像，原因在於 cluttered layouts、diverse fonts 和 non-textual distractions，導致 text omission、semantic drift 和 contextual inconsis。",
      "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation",
      "title_zh": "用於高解析度富文本圖像翻譯中 MLLMs 的 Global-Local Dual Perception"
    },
    {
      "arxiv_id": "2602.21952",
      "authors": [
        "Lingjun Zhang",
        "Yujian Yuan",
        "Changjie Wu",
        "Xinyuan Chang",
        "Xin Cai",
        "Shuang Zeng",
        "Linzhe Shi",
        "Sijin Wang",
        "Hang Zhang",
        "Mu Xu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.376079+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
          "url": "https://arxiv.org/abs/2602.21952"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
        "url": "https://arxiv.org/abs/2602.21952"
      },
      "published_at": "2026-02-25T14:34:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9340394174293075,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.134039417429307
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21952",
      "summary": "Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evoluti",
      "summary_zh": "視覺語言模型 (VLM) 展現出強大的推理能力，為端到端自動駕駛系統帶來了希望。然而，作為 VLM 廣泛使用的推理策略，思維鏈 (CoT) 正面臨嚴峻挑戰。現有的基於文本的 CoT 在文本語義空間和軌跡物理空間之間存在巨大鴻溝。儘管最近的方法利用未來圖像取代文本作為 CoT 過程，但它缺乏清晰的以規劃為導向的目標指導，以生成具有準確場景演變的圖像。",
      "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
      "title_zh": "MindDriver: 引入漸進式多模態推理用於自動駕駛"
    },
    {
      "arxiv_id": "2602.21857",
      "authors": [
        "Jabez Magomere",
        "Elena Kochkina",
        "Samuel Mensah",
        "Simerjot Kaur",
        "Fernando Acero",
        "Arturo Oncevay",
        "Charese H. Smiley",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.863100+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Distill and Align Decomposition for Enhanced Claim Verification",
          "url": "https://arxiv.org/abs/2602.21857"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Distill and Align Decomposition for Enhanced Claim Verification",
        "url": "https://arxiv.org/abs/2602.21857"
      },
      "published_at": "2026-02-25T12:32:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9261101473464864,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.126110147346488
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21857",
      "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward ba",
      "summary_zh": "複雜的聲明驗證需要將句子分解為可驗證的子聲明，然而現有方法難以使分解品質與驗證性能對齊。我們提出了一種強化學習 (RL) 方法，該方法利用 Group Relative Policy Optimization (GRPO) 共同優化分解品質和驗證器對齊。我們的方法整合了：(i) 結構化序列推理；(ii) 對教師蒸餾範例進行的監督式 fine-tuning；以及 (iii) 一個基於多目標獎勵的... ",
      "title": "Distill and Align Decomposition for Enhanced Claim Verification",
      "title_zh": "蒸餾與對齊分解以增強聲明驗證"
    },
    {
      "arxiv_id": "2602.21824",
      "authors": [
        "Marcel Lamott",
        "Saifullah Saifullah",
        "Nauman Riaz",
        "Yves-Noel Weweler",
        "Tobias Alt-Veit",
        "Ahmad Sarmad Ali",
        "Muhammad Armaghan Shakir",
        "Adrian Kalwa",
        "Momina Moetesum",
        "Andreas Dengel",
        "Sheraz Ahmed",
        "Faisal Shafait",
        "Ulrich Schwanecke",
        "Adrian Ulges"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.947600+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion",
          "url": "https://arxiv.org/abs/2602.21824"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion",
        "url": "https://arxiv.org/abs/2602.21824"
      },
      "published_at": "2026-02-25T11:52:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9235508090800542,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.123550809080054
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21824",
      "summary": "Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic docu",
      "summary_zh": "高效的文件智慧模型依賴於大量的帶註釋訓練數據。然而，由於數據獲取過程的勞動密集且成本高昂，獲取足夠且高品質的數據帶來了重大挑戰。此外，利用語言模型註釋真實文件也引發了數據隱私問題。合成文件生成已成為一種有前景且保護隱私的替代方案。我們提出了 DocDjinn，一個用於可控合成文件生成的新穎框架。",
      "title": "DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion",
      "title_zh": "DocDjinn: 利用 VLMs 和手寫擴散進行可控合成文件生成"
    },
    {
      "arxiv_id": "2602.21806",
      "authors": [
        "Xinxue Zhu",
        "Jiacong Wu",
        "Xiaoyu Zhang",
        "Tianlin Li",
        "Yanzhou Mu",
        "Juan Zhai",
        "Chao Shen",
        "Yang Liu"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.231851+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks",
          "url": "https://arxiv.org/abs/2602.21806"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks",
        "url": "https://arxiv.org/abs/2602.21806"
      },
      "published_at": "2026-02-25T11:34:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9224013621590241,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.122401362159025
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21806",
      "summary": "LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable sympto",
      "summary_zh": "LLM agents 已被廣泛應用於實際場景中，它們依賴 agent 框架進行工作流程執行和多 agent 協調。隨著這些系統的規模擴大，理解底層 agent 框架中的錯誤變得至關重要。然而，現有研究主要關注 agent 層級的故障，而忽略了框架層級的錯誤。為彌補這一空白，我們對來自 CrewAI 和 LangChain 的 998 份錯誤報告進行了實證研究，建立了一個包含 15 個根本原因和 7 個可觀察症狀的分類法。",
      "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks",
      "title_zh": "現代 LLM Agent 框架中錯誤的實證研究"
    },
    {
      "arxiv_id": "2602.21779",
      "authors": [
        "Zheyuan Gu",
        "Qingsong Zhao",
        "Yusong Wang",
        "Zhaohong Huang",
        "Xinqi Li",
        "Cheng Yuan",
        "Jiaowei Shao",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.865238+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
          "url": "https://arxiv.org/abs/2602.21779"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
        "url": "https://arxiv.org/abs/2602.21779"
      },
      "published_at": "2026-02-25T10:54:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9198831484414745,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.119883148441474
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21779",
      "summary": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs w",
      "summary_zh": "目前用於 deepfake 偵測的視覺語言模型 (VLMs) 擅長識別空間偽影，但卻忽略了一個關鍵維度：影片偽造中的時間不一致性。使 VLMs 能夠對這些動態線索進行推理仍然是一個明顯的挑戰。為彌補這一空白，我們提出了 Forensic Answer-Questioning (FAQ)，這是一個大規模基準，將時間 deepfake 分析表述為多項選擇任務。FAQ 引入了三層級層次結構，以逐步評估並賦予 VLMs ... ",
      "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
      "title_zh": "超越靜態偽影：視覺語言模型中影片 Deepfake 推理的鑑識基準"
    },
    {
      "arxiv_id": "2602.21728",
      "authors": [
        "Shiqi Yan",
        "Yubo Chen",
        "Ruiqi Zhou",
        "Zhengxi Yao",
        "Shuai Chen",
        "Tianyi Zhang",
        "Shijie Zhang",
        "Wei Qiang Zhang",
        "Yongfeng Huang",
        "Haixin Duan",
        "Yunqi Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.852987+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling",
          "url": "https://arxiv.org/abs/2602.21728"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling",
        "url": "https://arxiv.org/abs/2602.21728"
      },
      "published_at": "2026-02-25T09:35:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9148112082704228,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.114811208270424
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21728",
      "summary": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope o",
      "summary_zh": "大型語言模型（LLMs）的推理過程在問答任務中常受幻覺和事實缺失困擾。一個有前景的解決方案是將 LLMs 的答案奠基於可驗證的知識來源，例如 Knowledge Graphs (KGs)。現有的 KG 增強方法通常透過在生成過程中強制執行規則或模仿固定示範集中的路徑來約束 LLM 推理。然而，這些方法自然地將 LLMs 的推理模式限制在一定範圍內。",
      "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling",
      "title_zh": "Explore-on-Graph: 透過路徑精煉獎勵建模激勵大型語言模型在知識圖譜上的自主探索"
    },
    {
      "arxiv_id": "2602.21716",
      "authors": [
        "Wenbin Wang",
        "Yuge Huang",
        "Jianqing Xu",
        "Yue Yu",
        "Jiangtao Yan",
        "Shouhong Ding",
        "Pan Zhou",
        "Yong Luo"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.383300+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "TranX-Adapter: Bridging Artifacts and Semantics within MLLMs for Robust AI-generated Image Detection",
          "url": "https://arxiv.org/abs/2602.21716"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "TranX-Adapter: Bridging Artifacts and Semantics within MLLMs for Robust AI-generated Image Detection",
        "url": "https://arxiv.org/abs/2602.21716"
      },
      "published_at": "2026-02-25T09:22:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9140153301048911,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.11401533010489
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21716",
      "summary": "Rapid advances in AI-generated image (AIGI) technology enable highly realistic synthesis, threatening public information integrity and security. Recent studies have demonstrated that incorporating texture-level artifact features alongside semantic features into multimodal large language models (MLLMs) can enhance their AIGI detection capability. However, our preliminary analyses reveal that artifact features exhibit high intra-feature similarity, leading to an almost uniform attention map after ",
      "summary_zh": "AI 生成圖像 (AIGI) 技術的快速發展實現了高度逼真的合成，威脅到公共資訊的完整性和安全性。最近的研究表明，將紋理級別的 artifact features 與 semantic features 結合到 multimodal large language models (MLLMs) 中可以增強其 AIGI 檢測能力。然而，我們的初步分析顯示，artifact features 呈現高度的 intra-feature similarity，導致在經過處理後產生幾乎一致的 attention map。",
      "title": "TranX-Adapter: Bridging Artifacts and Semantics within MLLMs for Robust AI-generated Image Detection",
      "title_zh": "TranX-Adapter: 在 MLLMs 內部連結偽影與語義以實現魯棒的 AI 生成圖像檢測"
    },
    {
      "arxiv_id": "2602.21697",
      "authors": [
        "Chenyan Liu",
        "Yun Lin",
        "Jiaxin Chang",
        "Jiawei Liu",
        "Binhang Qi",
        "Bo Jiang",
        "Zhiyong Huang",
        "Jin Song Dong"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.232515+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows",
          "url": "https://arxiv.org/abs/2602.21697"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows",
        "url": "https://arxiv.org/abs/2602.21697"
      },
      "published_at": "2026-02-25T09:02:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9127456891148351,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.112745689114835
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21697",
      "summary": "Large language models (LLMs) for code editing have achieved remarkable progress, yet recent empirical studies reveal a fundamental disconnect between technical accuracy and developer productivity. Despite their strong benchmark performance, developers complete tasks 19% slower when using AI assistance, with over 68.81% of recommendations disrupting their mental flow. This misalignment stems from the use of static commit snapshots that lack temporal information, causing models to optimize for end",
      "summary_zh": "用於程式碼編輯的 Large language models (LLMs) 已取得了顯著進展，但最近的實證研究揭示了技術準確性與開發者生產力之間存在根本性脫節。儘管其基準測試表現強勁，開發者在使用 AI assistance 時完成任務的速度慢了 19%，超過 68.81% 的推薦打斷了他們的 mental flow。這種不一致源於使用缺乏時間信息的 static commit snapshots，導致模型為終端使用者優化。",
      "title": "EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows",
      "title_zh": "EditFlow: 透過重建開發者流程來基準測試和優化程式碼編輯推薦系統"
    },
    {
      "arxiv_id": "2602.21681",
      "authors": [
        "Renshuang Jiang",
        "Yichong Wang",
        "Pan Dong",
        "Xiaoxiang Fang",
        "Zhenling Duan",
        "Tinglue Wang",
        "Yuchen Hu",
        "Jie Yu",
        "Zhe Jiang"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.232727+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch",
          "url": "https://arxiv.org/abs/2602.21681"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch",
        "url": "https://arxiv.org/abs/2602.21681"
      },
      "published_at": "2026-02-25T08:34:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9109536517975981,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.110953651797598
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21681",
      "summary": "Eliminating undefined behaviors (UBs) in Rust programs requires a deep semantic understanding to enable accurate and reliable repair. While existing studies have demonstrated the potential of LLMs to support Rust code analysis and repair, most frameworks remain constrained by inflexible templates or lack grounding in executable semantics, resulting in limited contextual awareness and semantic incorrectness. Here, we present AkiraRust, an LLM-driven repair and verification framework that incorpor",
      "summary_zh": "在 Rust 程式中消除 undefined behaviors (UBs) 需要深刻的 semantic understanding，以實現準確可靠的修復。儘管現有研究已證明 LLMs 在支持 Rust 程式碼分析和修復方面的潛力，但大多數框架仍受限於 inflexible templates 或缺乏在 executable semantics 方面的基礎，導致有限的 contextual awareness 和 semantic incorrectness。在此，我們介紹 AkiraRust，一個 LLM 驅動的修復和驗證框架，其融合了。",
      "title": "AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch",
      "title_zh": "AkiraRust: 透過回饋引導的思考切換重新思考 LLM 輔助的 Rust 修復"
    },
    {
      "arxiv_id": "2602.21680",
      "authors": [
        "David Eckel",
        "Henri Meeß"
      ],
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.952453+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Hierarchical Lead Critic based Multi-Agent Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.21680"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Hierarchical Lead Critic based Multi-Agent Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.21680"
      },
      "published_at": "2026-02-25T08:33:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9109030446671538,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.110903044667154
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21680",
      "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, wh",
      "summary_zh": "Cooperative Multi-Agent Reinforcement Learning (MARL) 解決了需要多個 agents 協調的複雜任務，但通常受限於局部（獨立學習）或全局（集中學習）視角。在本文中，我們提出了一種新穎的 sequential training scheme 和 MARL 架構，它從不同層次結構的多個視角中學習。我們提出了 Hierarchical Lead Critic (HLC) — 其靈感來自團隊結構中自然出現的分佈，該結構。",
      "title": "Hierarchical Lead Critic based Multi-Agent Reinforcement Learning",
      "title_zh": "基於分層式領導評論者 (Hierarchical Lead Critic) 的多智能體強化學習"
    },
    {
      "arxiv_id": "2602.21646",
      "authors": [
        "Yexing Du",
        "Youcheng Pan",
        "Zekun Wang",
        "Zheng Chu",
        "Yichong Huang",
        "Kaiyuan Liu",
        "Bo Yang",
        "Yang Xiang",
        "Ming Liu",
        "Bing Qin"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.854025+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion",
          "url": "https://arxiv.org/abs/2602.21646"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion",
        "url": "https://arxiv.org/abs/2602.21646"
      },
      "published_at": "2026-02-25T07:19:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9062287778062469,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.106228777806248
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21646",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved notable success in enhancing translation performance by integrating multimodal information. However, existing research primarily focuses on image-guided methods, whose applicability is constrained by the scarcity of multilingual image-text pairs. The speech modality overcomes this limitation due to its natural alignment with text and the abundance of existing speech datasets, which enable scalable language coverage. In this paper, we propose",
      "summary_zh": "多模態大型語言模型 (MLLMs) 透過整合多模態資訊，在提升翻譯性能方面取得了顯著成功。然而，現有研究主要集中在圖像引導方法，其適用性受到多語言圖像-文本對稀缺性的限制。語音模態克服了這一限制，因為它與文本自然對齊，並且現有語音資料集豐富，可實現可擴展的語言覆蓋。在本文中，我們提出",
      "title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion",
      "title_zh": "具備語音-文本融合的可擴展多語言多模態機器翻譯"
    },
    {
      "arxiv_id": "2602.21633",
      "authors": [
        "Chenyv Liu",
        "Wentao Tan",
        "Lei Zhu",
        "Fengling Li",
        "Jingjing Li",
        "Guoli Yang",
        "Heng Tao Shen"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.869102+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
          "url": "https://arxiv.org/abs/2602.21633"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
        "url": "https://arxiv.org/abs/2602.21633"
      },
      "published_at": "2026-02-25T06:58:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9048788321062653,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.104878832106266
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21633",
      "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context mode",
      "summary_zh": "標準的視覺-語言-動作 (VLA) 模型依賴於擬合統計數據先驗 (statistical data priors)，限制了它們對底層物理動力學的穩健理解。強化學習 (Reinforcement learning) 透過探索增強了物理基礎 (physical grounding)，但通常依賴於與代理內部狀態隔離的外部獎勵訊號。世界行動模型 (World action models) 已成為一種有前途的範式，它整合了想像與控制，以實現預測性規劃 (predictive planning)。然而，它們依賴於隱式上下文模式",
      "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
      "title_zh": "自我校正 VLA：透過稀疏世界想像進行線上動作精煉"
    },
    {
      "arxiv_id": "2602.22067",
      "authors": [
        "Giuseppe Canonaco",
        "Alberto Pozanco",
        "Daniel Borrajo"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859845+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Semantic Partial Grounding via LLMs",
          "url": "https://arxiv.org/abs/2602.22067"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Semantic Partial Grounding via LLMs",
        "url": "https://arxiv.org/abs/2602.22067"
      },
      "published_at": "2026-02-25T16:13:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.72,
        "llm_relevance_score": 20.16,
        "recency_score": 0.9404569389091265,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 28.30045693890913
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22067",
      "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL desc",
      "summary_zh": "實例化 (Grounding) 是經典規劃中的關鍵步驟，但隨著任務規模的增加，接地動作 (grounded actions) 和原子 (atoms) 的指數級增長，它常成為計算瓶頸。部分實例化 (partial grounding) 的最新進展透過逐步實例化最有潛力的操作符 (operators) 來解決這一挑戰，並由預測模型引導。然而，這些方法主要依賴於關係特徵或學習到的嵌入 (embeddings)，並未利用 PDDL 描述中存在的文本和結構線索",
      "title": "Semantic Partial Grounding via LLMs",
      "title_zh": "透過 LLMs 進行語義部分實例化"
    },
    {
      "arxiv_id": "2602.22120",
      "authors": [
        "Abhipsa Basu",
        "Mohana Singh",
        "Shashank Agnihotri",
        "Margret Keuper",
        "R. Venkatesh Babu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371779+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
          "url": "https://arxiv.org/abs/2602.22120"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
        "url": "https://arxiv.org/abs/2602.22120"
      },
      "published_at": "2026-02-25T17:08:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9440744058030636,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.74407440580306
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22120",
      "summary": "Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical ",
      "summary_zh": "文本到圖像 (T2I) 模型正迅速普及，但其輸出常缺乏地理多樣性，強化刻板印象，並錯誤地呈現地區。鑑於其廣泛的影響力，嚴格評估這些模型如何描繪世界至關重要。現有的多樣性指標要麼依賴於精選資料集，要麼專注於表面層次的視覺相似性，限制了解釋性。我們引入 GeoDiv，一個利用大型語言模型和視覺-語言模型來評估地理",
      "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
      "title_zh": "GeoDiv：衡量文本到圖像模型中地理多樣性的框架"
    },
    {
      "arxiv_id": "2602.22070",
      "authors": [
        "Jessica Y. Bo",
        "Lillio Mok",
        "Ashton Anderson"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859598+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
          "url": "https://arxiv.org/abs/2602.22070"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
        "url": "https://arxiv.org/abs/2602.22070"
      },
      "published_at": "2026-02-25T16:18:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9407857208984074,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.740785720898405
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22070",
      "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdiff",
      "summary_zh": "大型語言模型 (Large language models) 越來越多地用於決策任務，這些任務要求它們處理來自各種來源的資訊，包括人類專家和其他演算法代理。LLMs 如何權衡這些不同來源提供的資訊？我們考慮了廣泛研究的演算法厭惡 (algorithm aversion) 現象，其中人類決策者對來自演算法的預測表現出偏見。借鑒行為經濟學 (behavioural economics) 的實驗範式，我們評估了八種不同",
      "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
      "title_zh": "語言模型對演算法代理和人類專家表現出不一致的偏見"
    },
    {
      "arxiv_id": "2602.21978",
      "authors": [
        "Miyu Oba",
        "Saku Sugawara"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844747+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
          "url": "https://arxiv.org/abs/2602.21978"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
        "url": "https://arxiv.org/abs/2602.21978"
      },
      "published_at": "2026-02-25T14:57:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9355032430674651,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.735503243067463
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21978",
      "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or c",
      "summary_zh": "最近的研究從語言學角度審視了 language models，以更好地理解它們如何習得語言。大多數現有基準專注於判斷語法可接受性，而解釋語法形式所傳達意義的能力卻鮮受關注。我們引入了用於評估語言模型中構式理解的語言學最小對比基準（Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models, CxMP），這是一個基於 Construction Grammar 的基準，將形式-意義配對（form-meaning pairings）或 c 視為...",
      "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
      "title_zh": "CxMP: 一個用於評估語言模型中構式理解的語言學最小對比基準"
    },
    {
      "arxiv_id": "2602.21814",
      "authors": [
        "Heejin Jo"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.864511+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
          "url": "https://arxiv.org/abs/2602.21814"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
        "url": "https://arxiv.org/abs/2602.21814"
      },
      "published_at": "2026-02-25T11:40:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9227836400648952,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.722783640064893
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21814",
      "summary": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises acc",
      "summary_zh": "Large language models 在「洗車問題」（\"car wash problem\"）上屢屢失敗，這是一個需要隱式物理約束推理的熱門推理基準。我們提出了一項變數隔離研究（每條件 n=20，共 6 個條件，總計 120 次試驗），旨在檢視生產系統中哪些 prompt architecture 層能夠實現正確推理。使用帶有受控 hyperparameter（temperature 0.7, top_p 1.0）的 Claude 3.5 Sonnet，我們發現單獨的 STAR（Situation-Task-Action-Result）推理框架就能提高準確性...",
      "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
      "title_zh": "Prompt 架構決定推理品質：一項關於「洗車問題」的變數隔離研究"
    },
    {
      "arxiv_id": "2602.21800",
      "authors": [
        "Madhusudan Ghosh",
        "Rishabh Gupta"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.864731+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
          "url": "https://arxiv.org/abs/2602.21800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
        "url": "https://arxiv.org/abs/2602.21800"
      },
      "published_at": "2026-02-25T11:27:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9219712220328936,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.72197122203289
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21800",
      "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving",
      "summary_zh": "大型語言模型（LLMs）的快速發展導致軟體工程中自動化工具的顯著增加，這些工具能夠執行各種程式碼相關任務，例如 code generation、completion 和 translation。儘管有這些進步，但其有效性受限於固定的 context lengths，限制了其在長而特定領域的程式碼序列中泛化的能力。為了解決這個挑戰，我們研究了旨在改進的 zero-shot、僅推斷（inference-only）方法...",
      "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
      "title_zh": "透過位置嵌入和高效注意力評估長程式碼中的上下文長度外推"
    },
    {
      "arxiv_id": "2602.21772",
      "authors": [
        "Yuxuan Chen",
        "Peize He",
        "Haoyuan Xu",
        "Junzi Zhang"
      ],
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.865460+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
          "url": "https://arxiv.org/abs/2602.21772"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
        "url": "https://arxiv.org/abs/2602.21772"
      },
      "published_at": "2026-02-25T10:47:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9193988467699002,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.7193988467699
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21772",
      "summary": "A universal audio representation should capture fine-grained speech cues and high-level semantics for environmental sounds and music in a single encoder. Existing encoders often excel in one domain but degrade in others. We propose UniWhisper, an efficient continual multi-task training framework that casts heterogeneous audio tasks into a unified instruction and answer format. This enables standard next-token training without task-specific heads and losses. We train it on 38k hours of public aud",
      "summary_zh": "通用的音訊表示應能在單一 encoder 中捕捉細粒度的語音線索以及環境聲音和音樂的高層語義。現有的 encoders 通常在某個領域表現出色，但在其他領域則會下降。我們提出了 UniWhisper，這是一個高效的 continual multi-task training 框架，它將異構的音訊任務轉換為統一的指令和回答格式。這使得標準的 next-token training 能夠在沒有任務特定 heads 和 losses 的情況下進行。我們在 38k 小時的公開音訊資料上訓練了它...",
      "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
      "title_zh": "UniWhisper: 用於穩健通用音訊表示的高效連續多任務訓練"
    },
    {
      "arxiv_id": "2602.21698",
      "authors": [
        "Meiqi Sun",
        "Mingyu Li",
        "Junxiong Zhu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.385006+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought",
          "url": "https://arxiv.org/abs/2602.21698"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought",
        "url": "https://arxiv.org/abs/2602.21698"
      },
      "published_at": "2026-02-25T09:03:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9128048504748937,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.712804850474893
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21698",
      "summary": "Generative AI is widely used to create commercial posters. However, rapid advances in generation have outpaced automated quality assessment. Existing models emphasize generic esthetics or low level distortions and lack the functional criteria required for e-commerce design. It is especially challenging for Chinese content, where complex characters often produce subtle but critical textual artifacts that are overlooked by existing methods. To address this, we introduce E-comIQ-ZH, a framework for",
      "summary_zh": "Generative AI 被廣泛用於創建商業海報。然而，生成方面的快速進步已經超越了自動化品質評估。現有模型側重於通用美學或低級失真，缺乏電商設計所需的功能性標準。對於中文內容來說，這尤其具有挑戰性，因為複雜的字元經常產生細微但關鍵的文本缺陷，而這些缺陷往往被現有方法所忽略。為了解決這個問題，我們引入了 E-comIQ-ZH，這是一個用於...",
      "title": "E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought",
      "title_zh": "E-comIQ-ZH: 一個用於透過 Chain-of-Thought 對電商海報進行細粒度評估的類人對齊資料集和基準"
    },
    {
      "arxiv_id": "2602.22182",
      "authors": [
        "Sourav Saha",
        "Dwaipayan Roy",
        "Mandar Mitra"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.842011+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "LiCQA : A Lightweight Complex Question Answering System",
          "url": "https://arxiv.org/abs/2602.22182"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "LiCQA : A Lightweight Complex Question Answering System",
        "url": "https://arxiv.org/abs/2602.22182"
      },
      "published_at": "2026-02-25T18:28:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9493283658654775,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.349328365865475
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22182",
      "summary": "Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of trainin",
      "summary_zh": "在過去的二十年中，Question Answering (QA) 系統的設計和實施取得了顯著進展。然而，處理複雜問題（其答案分佈在多個文件中）仍然是一個具有挑戰性的問題。最近為處理複雜問題而設計的 QA 系統，要麼基於 knowledge graphs 運作，要麼利用當代的 neural models，這些模型在計算資源和訓練數據量方面，訓練成本皆高昂。",
      "title": "LiCQA : A Lightweight Complex Question Answering System",
      "title_zh": "LiCQA：一個輕量級複雜問答系統"
    },
    {
      "arxiv_id": "2602.22014",
      "authors": [
        "Louis Estève",
        "Christophe Servan",
        "Thomas Lavergne",
        "Agata Savary"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844505+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
          "url": "https://arxiv.org/abs/2602.22014"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
        "url": "https://arxiv.org/abs/2602.22014"
      },
      "published_at": "2026-02-25T15:29:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9375920469607882,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.337592046960786
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22014",
      "summary": "Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven samplin",
      "summary_zh": "近年來，多樣性 (Diversity) 在 NLP 領域受到越來越多的關注。同時，最先進的 Transformer 模型，例如 ModernBERT，使用非常大的 pre-training datasets，這些數據集的驅動因素是規模而非多樣性。這促使我們研究多樣性對 ModernBERT pre-training 的影響。我們在本研究中這樣做，明確旨在減少 pre-training dataset 的大小，同時至少保持可比的性能。我們比較了 diversity-driven sampling。",
      "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
      "title_zh": "為了更健康的模型：法國 ModernBERT 的多樣性飲食案例研究"
    },
    {
      "arxiv_id": "2602.21997",
      "authors": [
        "WeiZhe Xu",
        "Mengyu Liu",
        "Fanxin Kong"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861362+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
          "url": "https://arxiv.org/abs/2602.21997"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
        "url": "https://arxiv.org/abs/2602.21997"
      },
      "published_at": "2026-02-25T15:16:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9367600863173784,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.336760086317376
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21997",
      "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LL",
      "summary_zh": "自動化測試生成對於軟體品質保證至關重要，其中 coverage rate 作為確保徹底測試的關鍵指標。Large Language Models (LLMs) 的最新進展在改進測試生成方面展現了潛力，特別是在實現更高 coverage 方面。然而，儘管現有的基於 LLM 的測試生成解決方案在小型、獨立的 code snippets 上表現良好，但當應用於複雜的 methods under test 時，它們卻力不從心。為了解決這些問題，我們提出了一個可擴展的 LL。",
      "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
      "title_zh": "透過消除已覆蓋程式碼來增強基於 LLM 的測試生成"
    },
    {
      "arxiv_id": "2602.21910",
      "authors": [
        "Alexander Heinlein",
        "Johannes Taraz"
      ],
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.946095+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions",
          "url": "https://arxiv.org/abs/2602.21910"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions",
        "url": "https://arxiv.org/abs/2602.21910"
      },
      "published_at": "2026-02-25T13:38:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9303688683589814,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.33036886835898
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21910",
      "summary": "Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for fur",
      "summary_zh": "Operator learning 有潛力透過學習微分方程的 solution operators，從而強烈影響 scientific computing，將設計優化 (design optimization) 和不確定性量化 (uncertainty quantification) 等 multi-query 任務加速數個數量級。儘管具備已證實的 universal approximation properties，deep operator networks (DeepONets) 在實踐中往往表現出有限的準確性 (accuracy) 和泛化能力 (generalization)，這阻礙了它們的採用。因此，理解這些局限性對於進一步的研究至關重要。",
      "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions",
      "title_zh": "深度算子網路的誤差是其各部分的總和：分支-主幹和模式誤差分解"
    },
    {
      "arxiv_id": "2602.21864",
      "authors": [
        "Yanbin Wei",
        "Jiangyue Yan",
        "Chun Kang",
        "Yang Chen",
        "Hua Liu",
        "James Kwok",
        "Yu Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862565+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
          "url": "https://arxiv.org/abs/2602.21864"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
        "url": "https://arxiv.org/abs/2602.21864"
      },
      "published_at": "2026-02-25T12:45:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9269905846071673,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.326990584607167
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21864",
      "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, re",
      "summary_zh": "Vision-Language Models (VLMs) 已成為跨多個領域進行 zero-shot question answering (QA) 的多功能解決方案。然而，使 VLM 能夠有效理解結構化圖形並執行準確、高效的 QA 仍然具有挑戰性。現有方法通常依賴於單一的 graph topology representation (GTR)，例如固定風格的視覺圖像或統一的文本描述。這種「one-size-fits-all」策略通常忽略了模型特定 (model-specific) 和任務特定 (task-specific) 的偏好。",
      "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
      "title_zh": "DynamicGTR：利用圖拓撲表示偏好提升 VLM 在圖形 QA 上的能力"
    },
    {
      "arxiv_id": "2602.21655",
      "authors": [
        "Zhijiang Tang",
        "Linhua Wang",
        "Jiaxin Qi",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Jianqiang Huang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.868128+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning",
          "url": "https://arxiv.org/abs/2602.21655"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning",
        "url": "https://arxiv.org/abs/2602.21655"
      },
      "published_at": "2026-02-25T07:34:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.907164858234299,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 26.307164858234298
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21655",
      "summary": "Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the",
      "summary_zh": "影像標註仍然是視覺語言理解的一項基本任務，然而，ground-truth 監督仍然主要依賴人工標註的參考。由於人工標註反映了主觀偏好和專業知識，ground-truth 標註通常不完整甚至不正確，這反過來限制了標註模型。我們認為標註品質應通過兩個客觀方面來評估：完整性（標註是否涵蓋了所有顯著的視覺事實？）和正確性（是否是",
      "title": "CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning",
      "title_zh": "CCCaption：用於完整且正確影像標註的雙獎勵強化學習"
    },
    {
      "arxiv_id": "2602.21833",
      "authors": [
        "Norman Peitek",
        "Julia Hess",
        "Sven Apel"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.231634+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models",
          "url": "https://arxiv.org/abs/2602.21833"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models",
        "url": "https://arxiv.org/abs/2602.21833"
      },
      "published_at": "2026-02-25T12:05:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9243977854599104,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 26.074397785459908
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21833",
      "summary": "Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.\n  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across f",
      "summary_zh": "大型語言模型 (LLMs) 越來越多地被用於自動化程式碼重構任務。儘管這些模型可以快速重構程式碼，但其品質可能表現出不一致和不可預測的行為。在本文中，我們系統地研究了 LLMs 在程式碼重構方面的能力，特別專注於改善程式碼可讀性。我們使用 GPT5.1 進行了一項大規模實驗，涉及 230 個 Java snippets，每個都針對程式碼可讀性進行了系統性變更和重構，跨越 f",
      "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models",
      "title_zh": "從重構到穩定化：一項關於使用 Large Language Models 進行迭代程式碼可讀性重構的大規模實驗"
    },
    {
      "arxiv_id": "2602.22115",
      "authors": [
        "Luiz Fernando Paulino Queiroz",
        "Carlos Henrique Leitão Cavalcante",
        "Thiago Alves Rocha"
      ],
      "categories": [
        "cs.LO",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.941032+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing",
          "url": "https://arxiv.org/abs/2602.22115"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing",
        "url": "https://arxiv.org/abs/2602.22115"
      },
      "published_at": "2026-02-25T17:01:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.65,
        "llm_relevance_score": 18.2,
        "recency_score": 0.9436254216511807,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.7,
        "total_score": 25.04362542165118
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22115",
      "summary": "Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by NNs, offering correctness guarantees. However, scalability remains a concern in these methods. This paper proposes an approach leveraging domain slicing to facilitate explanation generation for NNs. By reducing the complexity of logical constraints through slicing, we decrease explanation",
      "summary_zh": "Neural networks (NNs) 廣泛應用於各個領域，但通常缺乏可解釋性。為了解決日益增長的解釋需求，已提出了基於邏輯的方法來解釋 NNs 所做的預測，並提供正確性保證。然而，在這些方法中，可擴展性仍然是一個問題。本文提出了一種利用 domain slicing 來促進 NNs 解釋生成的方法。透過 slicing 減少邏輯約束的複雜性，我們降低了解釋",
      "title": "Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing",
      "title_zh": "切割與解釋：透過 Domain Slicing 為 Neural Networks 提供基於邏輯的解釋"
    },
    {
      "arxiv_id": "2602.22136",
      "authors": [
        "Qunyou Liu",
        "Pengbo Yu",
        "Marina Zapater",
        "David Atienza"
      ],
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.940060+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
          "url": "https://arxiv.org/abs/2602.22136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
        "url": "https://arxiv.org/abs/2602.22136"
      },
      "published_at": "2026-02-25T17:34:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9457487799583457,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.945748779958347
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22136",
      "summary": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bit",
      "summary_zh": "Deep neural networks (DNNs) 對於在 edge 或行動裝置上執行進階任務至關重要，然而，它們的部署經常受到嚴峻的資源限制阻礙，包括有限的記憶體、能源和計算能力。雖然 uniform quantization 提供了一種直接的模型壓縮和硬體需求降低方法，但它未能充分利用跨層的不同穩健性，並常導致準確性下降或次優的資源使用，尤其是在低位元",
      "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
      "title_zh": "SigmaQuant：用於 Edge DNN 推理的硬體感知異質化量化方法"
    },
    {
      "arxiv_id": "2602.22107",
      "authors": [
        "Andrea Apicella",
        "Francesco Isgrò",
        "Andrea Pollastro",
        "Roberto Prevete"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.858632+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
          "url": "https://arxiv.org/abs/2602.22107"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
        "url": "https://arxiv.org/abs/2602.22107"
      },
      "published_at": "2026-02-25T16:56:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9432563440882835,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.943256344088283
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22107",
      "summary": "Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selectio",
      "summary_zh": "儘管關於訓練損失函數的文獻廣泛，但對驗證集上泛化能力的評估仍未得到充分探索。在這項工作中，我們對用於模型選擇的驗證準則如何影響神經分類器中的測試性能進行了系統性的實證和統計研究，並特別關注 early stopping。我們使用在 $k$-fold 評估下的標準基準測試上的 fully connected networks，比較了：(i) 帶有 patience 的 early stopping 和 (ii) 事後選擇",
      "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
      "title_zh": "現在別阻止我：重新思考模型參數選擇的驗證準則"
    },
    {
      "arxiv_id": "2602.21919",
      "authors": [
        "Cuong Anh Pham",
        "Praneeth Vepakomma",
        "Samuel Horváth"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.945847+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Learning in the Null Space: Small Singular Values for Continual Learning",
          "url": "https://arxiv.org/abs/2602.21919"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Learning in the Null Space: Small Singular Values for Continual Learning",
        "url": "https://arxiv.org/abs/2602.21919"
      },
      "published_at": "2026-02-25T13:55:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9314657129101758,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.931465712910175
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21919",
      "summary": "Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on",
      "summary_zh": "緩解災難性遺忘並同時實現進一步學習是 Continual Learning (CL) 中的主要挑戰。基於正交性的訓練方法因其效率和強大的理論性質而受到關注，許多現有方法透過 gradient projection 來強制執行正交性。在本文中，我們重新審視了正交性，並利用小奇異值對應於與先前任務的 input space 幾乎正交方向的事實。基於",
      "title": "Learning in the Null Space: Small Singular Values for Continual Learning",
      "title_zh": "在零空間中學習：用於 Continual Learning 的小奇異值"
    },
    {
      "arxiv_id": "2602.21721",
      "authors": [
        "Delio Jaramillo Velez",
        "Gergely Biczok",
        "Alexandre Graell i Amat",
        "Johan Ostman",
        "Balazs Pejo"
      ],
      "categories": [
        "cs.CR",
        "cs.GT",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.950643+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Private and Robust Contribution Evaluation in Federated Learning",
          "url": "https://arxiv.org/abs/2602.21721"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Private and Robust Contribution Evaluation in Federated Learning",
        "url": "https://arxiv.org/abs/2602.21721"
      },
      "published_at": "2026-02-25T09:27:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9143264021327216,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.914326402132723
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21721",
      "summary": "Cross-silo federated learning allows multiple organizations to collaboratively train machine learning models without sharing raw data, but client updates can still leak sensitive information through inference attacks. Secure aggregation protects privacy by hiding individual updates, yet it complicates contribution evaluation, which is critical for fair rewards and detecting low-quality or malicious participants. Existing marginal-contribution methods, such as the Shapley value, are incompatible ",
      "summary_zh": "跨領域的 federated learning 允許多個組織在不共享原始數據的情況下協同訓練 machine learning 模型，但客戶端更新仍可能透過 inference attacks 洩露敏感資訊。Secure aggregation 透過隱藏個別更新來保護隱私，但這使得 contribution evaluation 變得複雜，而 contribution evaluation 對於公平獎勵和檢測低品質或惡意參與者至關重要。現有的 marginal-contribution 方法，例如 Shapley value，是不相容的",
      "title": "Private and Robust Contribution Evaluation in Federated Learning",
      "title_zh": "Federated Learning 中私密且穩健的貢獻評估"
    },
    {
      "arxiv_id": "2602.21717",
      "authors": [
        "Sijia Xu",
        "Fan Li",
        "Xiaoyang Wang",
        "Zhengyi Yang",
        "Xuemin Lin"
      ],
      "categories": [
        "cs.LG",
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.950887+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation",
          "url": "https://arxiv.org/abs/2602.21717"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation",
        "url": "https://arxiv.org/abs/2602.21717"
      },
      "published_at": "2026-02-25T09:25:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9141824917108181,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 24.914182491710818
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21717",
      "summary": "Tabular data is the primary data format in industrial relational databases, underpinning modern data analytics and decision-making. However, the increasing scale of tabular data poses significant computational and storage challenges to learning-based analytical systems. This highlights the need for data-efficient learning, which enables effective model training and generalization using substantially fewer samples. Dataset condensation (DC) has emerged as a promising data-centric paradigm that sy",
      "summary_zh": "表格數據（Tabular data）是工業關係資料庫中的主要數據格式，支撐著現代數據分析和決策制定。然而，表格數據規模的擴大對基於學習的分析系統帶來了重大的計算和儲存挑戰。這突顯了對 data-efficient learning 的需求，它能夠使用顯著更少的樣本實現有效的模型訓練和泛化。Dataset condensation (DC) 已成為一種有前景的以數據為中心的典範，其",
      "title": "C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation",
      "title_zh": "C$^{2}$TC：用於高效 Tabular Data Condensation 的免訓練框架"
    },
    {
      "arxiv_id": "2602.21677",
      "authors": [
        "Zhenxiang Xu",
        "Jiawei Chen",
        "Sirui Chen",
        "Yong He",
        "Jieyu Yang",
        "Chuan Yuan",
        "Ke Ding",
        "Can Wang"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-26T06:28:42.952702+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Trie-Aware Transformers for Generative Recommendation",
          "url": "https://arxiv.org/abs/2602.21677"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Trie-Aware Transformers for Generative Recommendation",
        "url": "https://arxiv.org/abs/2602.21677"
      },
      "published_at": "2026-02-25T08:25:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.6,
        "llm_relevance_score": 16.8,
        "recency_score": 0.9103728931793862,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 1.7999999999999998,
        "total_score": 24.710372893179386
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21677",
      "summary": "Generative recommendation (GR) aligns with advances in generative AI by casting next-item prediction as token-level generation rather than score-based ranking. Most GR methods adopt a two-stage pipeline: (i) \\textit{item tokenization}, which maps each item to a sequence of discrete, hierarchically organized tokens; and (ii) \\textit{autoregressive generation}, which predicts the next item's tokens conditioned on the tokens of user's interaction history. Although hierarchical tokenization induces ",
      "summary_zh": "生成式推薦 (GR) 透過將下一項目預測（next-item prediction）視為 token-level 生成而非基於分數的排名（score-based ranking），與 generative AI 的進展保持一致。大多數 GR 方法採用兩階段 pipeline：(i) item tokenization，它將每個項目映射到一系列離散的、分層組織的 tokens；以及 (ii) autoregressive generation，它根據用戶互動歷史的 tokens 來預測下一項目的 tokens。儘管分層 tokenization 帶來了",
      "title": "Trie-Aware Transformers for Generative Recommendation",
      "title_zh": "Trie-Aware Transformers 用於生成式推薦"
    },
    {
      "arxiv_id": "2602.21829",
      "authors": [
        "Daniel Oliveira",
        "David Martins de Matos"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.864025+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles",
          "url": "https://arxiv.org/abs/2602.21829"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles",
        "url": "https://arxiv.org/abs/2602.21829"
      },
      "published_at": "2026-02-25T12:01:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.55,
        "llm_relevance_score": 15.400000000000002,
        "recency_score": 0.9241196520512727,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.524119652051276
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21829",
      "summary": "Visual storytelling models that correctly ground entities in images may still hallucinate semantic relationships, generating incorrect dialogue attribution, character interactions, or emotional states. We introduce StoryMovie, a dataset of 1,757 stories aligned with movie scripts and subtitles through LCS matching. Our alignment pipeline synchronizes screenplay dialogue with subtitle timestamps, enabling dialogue attribution by linking character names from scripts to temporal positions from subt",
      "summary_zh": "能夠正確地將圖像中的實體（entities）接地（ground）的視覺敘事模型（Visual storytelling models）仍然可能會產生語義關係的幻覺（hallucinate semantic relationships），導致不正確的對話歸屬（dialogue attribution）、角色互動或情感狀態。我們介紹了 StoryMovie，這是一個包含 1,757 個故事的數據集，這些故事透過 LCS matching 與電影劇本和字幕進行了對齊。我們的對齊 pipeline 將劇本對話與字幕時間戳同步，透過將劇本中的角色名稱連結到字幕中的時間位置來實現對話歸屬",
      "title": "StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles",
      "title_zh": "StoryMovie：用於視覺故事與電影劇本和字幕語義對齊的數據集"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [
        "Deep",
        "Learning,",
        "Machine",
        "AI,",
        "LLM"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:40.615813+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "sebastian-raschka-blog",
          "tier": 0,
          "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026",
          "url": "https://sebastianraschka.com/blog/2026/a-dream-of-spring-for-open-weight.html"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "sebastian-raschka-blog",
        "tier": 0,
        "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026",
        "url": "https://sebastianraschka.com/blog/2026/a-dream-of-spring-for-open-weight.html"
      },
      "published_at": "2026-02-25T08:15:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.55,
        "llm_relevance_score": 15.400000000000002,
        "recency_score": 0.9097240623477243,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 24.559724062347726
      },
      "section": null,
      "source_name": "Sebastian Raschka Blog",
      "story_id": "fallback:9956cdb45be84036",
      "summary": "A Round Up And Comparison of 10 Open-Weight LLM Releases in Spring 2026",
      "summary_zh": "對2026年春季發布的10款 Open-Weight LLM 的匯總與比較",
      "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026",
      "title_zh": "Open-Weight LLMs 的春日之夢：2026年1月至2月發布的10種架構"
    },
    {
      "arxiv_id": "2602.21763",
      "authors": [
        "Heng Wang",
        "Changxing Wu"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.852580+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs",
          "url": "https://arxiv.org/abs/2602.21763"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs",
        "url": "https://arxiv.org/abs/2602.21763"
      },
      "published_at": "2026-02-25T10:28:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.55,
        "llm_relevance_score": 15.400000000000002,
        "recency_score": 0.9182131193058876,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 23.51821311930589
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21763",
      "summary": "Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet e",
      "summary_zh": "由於缺乏顯式語篇標記時需要深度語義理解，隱式語篇關係識別（IDRR）仍然是一項具有挑戰性的任務。另一個局限性是現有方法僅預測關係而沒有提供任何支持性解釋。大型語言模型（LLMs）的最新進展已在深度語言理解和自然語言解釋生成方面展現出強大的推理能力。在這項工作中，我們提出了一種簡單而有效的方法",
      "title": "Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs",
      "title_zh": "利用 LLMs 提供的自然語言解釋改進隱式語篇關係識別"
    },
    {
      "arxiv_id": "2602.21933",
      "authors": [
        "Bitan Majumder",
        "Anirban Sen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
          "url": "https://arxiv.org/abs/2602.21933"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
        "url": "https://arxiv.org/abs/2602.21933"
      },
      "published_at": "2026-02-25T14:12:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.5,
        "llm_relevance_score": 14.0,
        "recency_score": 0.9325768029356026,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 22.132576802935603
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21933",
      "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest ove",
      "summary_zh": "在多語言和 Code-Mixed 環境中進行諷刺語氣偵測，對於 natural language processing 模型來說仍然是一項具有挑戰性的任務，原因包括結構變異、非正式表達以及低資源語言的可取得性。本研究比較了四個大型語言模型，Llama 3.1、Mistral、Gemma 3 和 Phi-4，與一個 fine-tuned 的 DistilBERT 模型，用於在 Code-Mixed Hinglish 文本中進行諷刺語氣偵測。結果表明，較小的、按序 fine-tuned 的 DistilBERT 模型實現了最高的整體表現",
      "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
      "title_zh": "小者為大：比較大型語言模型與領域 Fine-Tuned 模型在 Code-Mixed Hinglish 文本中諷刺語氣偵測的表現"
    },
    {
      "arxiv_id": "2602.22041",
      "authors": [
        "Vassil Guenov",
        "Ashwin George",
        "Arkady Zgonnikov",
        "David A. Abbink",
        "Luciano Cavalcante Siebert"
      ],
      "categories": [
        "cs.MA",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:47.271531+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions",
          "url": "https://arxiv.org/abs/2602.22041"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions",
        "url": "https://arxiv.org/abs/2602.22041"
      },
      "published_at": "2026-02-25T15:48:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.45,
        "llm_relevance_score": 12.6,
        "recency_score": 0.9388538697771021,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 20.738853869777103
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22041",
      "summary": "Heralding the advent of autonomous vehicles and mobile robots that interact with humans, responsibility in spatial interaction is burgeoning as a research topic. Even though metrics of responsibility tailored to spatial interactions have been proposed, they are mostly focused on the responsibility of individual agents. Metrics of causal responsibility focusing on individuals fail in cases of causal overdeterminism -- when many actors simultaneously cause an outcome. To fill the gaps in causal re",
      "summary_zh": "隨著自動駕駛汽車和與人類互動的移動機器人的出現，空間互動中的責任問題正成為一個蓬勃發展的研究課題。儘管已經提出了針對空間互動量身定制的責任指標，但它們大多側重於個別 agents 的責任。專注於個人的因果責任指標在因果過度決定（即多個行為者同時導致一個結果）的情況下會失效。為了填補因果責任的空白",
      "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions",
      "title_zh": "利用群組可行行動空間縮減來填補空間互動中的因果責任鴻溝"
    },
    {
      "arxiv_id": "2602.21889",
      "authors": [
        "Otto Nyberg",
        "Fausto Carcassi",
        "Giovanni Cinà"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862343+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
          "url": "https://arxiv.org/abs/2602.21889"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
        "url": "https://arxiv.org/abs/2602.21889"
      },
      "published_at": "2026-02-25T13:11:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.45,
        "llm_relevance_score": 12.6,
        "recency_score": 0.9286303603278592,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 20.72863036032786
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21889",
      "summary": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how",
      "summary_zh": "在越來越多的領域中，人類決策正由 AI 模型的預測所支援。然而，我們仍然缺乏對這些技術採用後影響的深入理解。在本文中，我們引入了一個通用的計算框架，即 2-Step Agent，它模擬了 AI 輔助決策的影響。我們的框架使用 Bayesian 方法進行 causal inference，以建模 1) 新觀測上的預測如何影響理性 Bayesian agent 的信念，以及 2) 如何",
      "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
      "title_zh": "2-Step Agent：決策者與 AI 決策支援互動的框架"
    },
    {
      "arxiv_id": "2602.22094",
      "authors": [
        "Nguyen Cong Nhat Le",
        "John G. Rogers",
        "Claire N. Bonial",
        "Neil T. Dantam"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859097+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
          "url": "https://arxiv.org/abs/2602.22094"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
        "url": "https://arxiv.org/abs/2602.22094"
      },
      "published_at": "2026-02-25T16:39:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.9421826914202246,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.94218269142022
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22094",
      "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and",
      "summary_zh": "規劃常因情境變化或我們對情境的理解而改變。有時甚至不存在可行的規劃，識別這些不可行性對於判斷何時需要調整需求至關重要。常見的規劃方法側重於在可行情況下進行高效的one-shot規劃，而不是更新領域或檢測不可行性。我們提出了一種 Petri net reachability relaxation，以實現穩健的 invariant synthesis、高效的 goal-unreachability detection，以及",
      "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
      "title_zh": "Petri 網弛豫用於不可行性解釋和序列任務規劃"
    },
    {
      "arxiv_id": "2602.21965",
      "authors": [
        "Joseph Margaryan",
        "Thomas Hamelryck"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944174+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Compact Circulant Layers with Spectral Priors",
          "url": "https://arxiv.org/abs/2602.21965"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Compact Circulant Layers with Spectral Priors",
        "url": "https://arxiv.org/abs/2602.21965"
      },
      "published_at": "2026-02-25T14:48:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.9349209003842114,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.93492090038421
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21965",
      "summary": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simp",
      "summary_zh": "在醫療、機器人學和自主系統等領域的關鍵應用需要緊湊（即記憶體高效）、具不確定性感知能力的 neural networks，以適用於邊緣和其他資源受限的部署。我們研究了緊湊型 spectral circulant 和 block-circulant-with-circulant-blocks (BCCB) layers：它們是 FFT-diagonalizable circular convolutions，其權重直接存在於 real FFT (RFFT) 的一半 (1D) 或半平面 (2D) 中。在頻域中參數化濾波器讓我們能夠施加簡單的",
      "title": "Compact Circulant Layers with Spectral Priors",
      "title_zh": "具有頻譜先驗的緊湊型循環層"
    },
    {
      "arxiv_id": "2602.21961",
      "authors": [
        "Bendegúz Sulyok",
        "Gergely Palla",
        "Filippo Radicchi",
        "Santo Fortunato"
      ],
      "categories": [
        "cs.LG",
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944417+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
          "url": "https://arxiv.org/abs/2602.21961"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
        "url": "https://arxiv.org/abs/2602.21961"
      },
      "published_at": "2026-02-25T14:44:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.9346504184244654,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.934650418424464
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21961",
      "summary": "We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of t",
      "summary_zh": "我們研究了採用 adaptive topology 訓練的 sparse artificial neural networks 的 robustness。我們專注於一個簡單而有效的架構，該架構由三個具有 99% sparsity 的 sparse layers 接著一個 dense layer 組成，並應用於 MNIST 和 Fashion MNIST 等圖像分類任務。通過在每個 epoch 之間更新 sparse layers 的 topology，儘管權重數量顯著減少，我們仍能達到具有競爭力的 accuracy。我們的主要貢獻是對",
      "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
      "title_zh": "在採用自適應拓撲訓練的稀疏人工神經網路中的魯棒性"
    },
    {
      "arxiv_id": "2602.21862",
      "authors": [
        "Chia Cheng Chang",
        "An-Zi Yen",
        "Hen-Hsen Huang",
        "Hsin-Hsi Chen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.851484+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
          "url": "https://arxiv.org/abs/2602.21862"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
        "url": "https://arxiv.org/abs/2602.21862"
      },
      "published_at": "2026-02-25T12:43:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.926840389968352,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.92684038996835
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21862",
      "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large la",
      "summary_zh": "由於個人可能難以回憶所有生活細節並經常混淆事件，因此建立一個協助用戶回憶遺忘經驗的系統至關重要。儘管許多研究提出了 memory recall systems，但這些系統主要依賴於 deep learning 技術，這些技術需要大量的訓練，並且由於個人 lifelogs 的有限可用性而常面臨 data scarcity。隨著 lifelogs 隨時間增長，系統也必須快速適應新累積的數據。最近，large la",
      "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
      "title_zh": "用於主動資訊存取的個性化圖賦能大型語言模型"
    },
    {
      "arxiv_id": "2602.21844",
      "authors": [
        "Ruichen Xu",
        "Ying-Jun Angela Zhang",
        "Jianwei Huang"
      ],
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.GT"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.947379+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning",
          "url": "https://arxiv.org/abs/2602.21844"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning",
        "url": "https://arxiv.org/abs/2602.21844"
      },
      "published_at": "2026-02-25T12:22:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.9255143700339079,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.92551437003391
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21844",
      "summary": "Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients (\"privacy stragglers\"), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSA",
      "summary_zh": "Differentially private federated learning 面臨一個根本性的張力：保護客戶端數據的 privacy protection mechanisms 同時也產生可量化的 privacy costs，這些成本會阻礙參與，從而損害協作訓練過程。現有的 incentive mechanisms 依賴於 unbiased client selection，迫使伺服器即使對那些對隱私最敏感的客戶端（\"privacy stragglers\"）也要進行補償，這導致系統性效率低下和 suboptimal resource allocation。我們引入了 JSA",
      "title": "JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning",
      "title_zh": "JSAM：差分隱私聯邦學習中具隱私後進者韌性的聯合客戶端選擇與激勵機制設計"
    },
    {
      "arxiv_id": "2602.21650",
      "authors": [
        "Zichen Song",
        "Weijia Li"
      ],
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.868637+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "PPCR-IM: A System for Multi-layer DAG-based Public Policy Consequence Reasoning and Social Indicator Mapping",
          "url": "https://arxiv.org/abs/2602.21650"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "PPCR-IM: A System for Multi-layer DAG-based Public Policy Consequence Reasoning and Social Indicator Mapping",
        "url": "https://arxiv.org/abs/2602.21650"
      },
      "published_at": "2026-02-25T07:23:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.906465854764942,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 17.90646585476494
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21650",
      "summary": "Public policy decisions are typically justified using a narrow set of headline indicators, leaving many downstream social impacts unstructured and difficult to compare across policies. We propose PPCR-IM, a system for multi-layer DAG-based consequence reasoning and social indicator mapping that addresses this gap. Given a policy description and its context, PPCR-IM uses an LLM-driven, layer-wise generator to construct a directed acyclic graph of intermediate consequences, allowing child nodes to",
      "summary_zh": "公共政策決策通常僅憑藉一小組主要指標來證明其合理性，這使得許多下游的社會影響缺乏結構化，難以在不同政策之間進行比較。我們提出了 PPCR-IM，這是一個用於多層 DAG-based 後果推理和社會指標映射的系統，旨在解決此一問題。PPCR-IM 接收政策描述及其背景，並使用 LLM-driven 的層次生成器來建構中間後果的 directed acyclic graph，允許子節點...",
      "title": "PPCR-IM: A System for Multi-layer DAG-based Public Policy Consequence Reasoning and Social Indicator Mapping",
      "title_zh": "PPCR-IM：一個用於多層 DAG-based 公共政策後果推理和社會指標映射的系統"
    },
    {
      "arxiv_id": "2602.21797",
      "authors": [
        "Paolo Andreini",
        "Alessandra Bernardi",
        "Monica Bianchini",
        "Barbara Toniella Corradini",
        "Sara Marziali",
        "Giacomo Nunziati",
        "Franco Scarselli"
      ],
      "categories": [
        "math.AG",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.948044+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Neural Learning of Fast Matrix Multiplication Algorithms: A StrassenNet Approach",
          "url": "https://arxiv.org/abs/2602.21797"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Neural Learning of Fast Matrix Multiplication Algorithms: A StrassenNet Approach",
        "url": "https://arxiv.org/abs/2602.21797"
      },
      "published_at": "2026-02-25T11:22:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.921647948535812,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 17.67164794853581
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21797",
      "summary": "Fast matrix multiplication can be described as searching for low-rank decompositions of the matrix--multiplication tensor. We design a neural architecture, \\textsc{StrassenNet}, which reproduces the Strassen algorithm for $2\\times 2$ multiplication. Across many independent runs the network always converges to a rank-$7$ tensor, thus numerically recovering Strassen's optimal algorithm. We then train the same architecture on $3\\times 3$ multiplication with rank $r\\in\\{19,\\dots,23\\}$. Our experimen",
      "summary_zh": "快速矩陣乘法可以描述為尋找矩陣乘法 tensor 的 low-rank 分解。我們設計了一種名為 \textsc{StrassenNet} 的神經架構，它能重現 $2\times 2$ 乘法的 Strassen 演算法。在多次獨立運行中，該網路總是收斂到一個 rank-7 的 tensor，從而在數值上恢復了 Strassen 的最佳演算法。然後，我們在 rank $r\\in\\{19,\\dots,23\\}$ 的 $3\times 3$ 乘法上訓練相同的架構。我們的實驗...",
      "title": "Neural Learning of Fast Matrix Multiplication Algorithms: A StrassenNet Approach",
      "title_zh": "快速矩陣乘法演算法的神經學習：一種 StrassenNet 方法"
    },
    {
      "arxiv_id": "2602.22015",
      "authors": [
        "Pengcheng Hao",
        "Ercan Engin Kuruoglu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.943292+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
          "url": "https://arxiv.org/abs/2602.22015"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
        "url": "https://arxiv.org/abs/2602.22015"
      },
      "published_at": "2026-02-25T15:29:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.4,
        "llm_relevance_score": 11.200000000000001,
        "recency_score": 0.9376072395475998,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 17.5876072395476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22015",
      "summary": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-taile",
      "summary_zh": "Bayesian deep learning (BDL) 已成為一種原則性方法，透過將 deep neural networks 與 Bayesian inference 相結合，產生可靠的 uncertainty estimates，而選擇 informative prior distributions 仍然是一個重大挑戰。已經提出了各種 function-space variational inference (FSVI) regularisation 方法，為模型預測分配有意義的先驗。然而，這些方法通常依賴於 Gaussian prior，這未能捕捉到 heavy-taile...",
      "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
      "title_zh": "具有 Student's t 先驗的函數空間經驗貝葉斯正則化"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dani Mitchell"
      ],
      "categories": [
        "Amazon Bedrock",
        "Amazon Bedrock AgentCore",
        "Amazon Bedrock Knowledge Bases",
        "Amazon Machine Learning",
        "Artificial Intelligence"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-26T06:28:40.333906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
          "url": "https://aws.amazon.com/blogs/machine-learning/building-intelligent-event-agents-using-amazon-bedrock-agentcore-and-amazon-bedrock-knowledge-bases"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
        "url": "https://aws.amazon.com/blogs/machine-learning/building-intelligent-event-agents-using-amazon-bedrock-agentcore-and-amazon-bedrock-knowledge-bases"
      },
      "published_at": "2026-02-25T19:51:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 5.6000000000000005,
        "recency_score": 0.9547828361601223,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 17.054782836160122
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:6b7719d61556b08b",
      "summary": "This post demonstrates how to quickly deploy a production-ready event assistant using the components of Amazon Bedrock AgentCore. We'll build an intelligent companion that remembers attendee preferences and builds personalized experiences over time, while Amazon Bedrock AgentCore handles the heavy lifting of production deployment:&nbsp;Amazon Bedrock AgentCore Memory for maintaining both conversation context and long-term preferences without custom storage solutions,&nbsp;Amazon Bedrock AgentCore Identity&nbsp;for secure multi-IDP authentication, and&nbsp;Amazon Bedrock AgentCore Runtime&nbsp;for serverless scaling and session isolation. We will also use Amazon Bedrock Knowledge Bases for managed RAG and event data retrieval.",
      "summary_zh": "這篇文章展示了如何使用 Amazon Bedrock AgentCore 的組件快速部署一個生產就緒的活動助手。我們將建構一個智慧型伴侶，它能記住參與者的偏好並隨著時間建立個性化體驗，而 Amazon Bedrock AgentCore 則負責處理生產部署的繁重工作：Amazon Bedrock AgentCore Memory 用於在無需自訂儲存解決方案的情況下維護 conversation context 和 long-term preferences，Amazon Bedrock AgentCore Identity 用於安全的 multi-IDP authentication，以及 Amazon Bedrock AgentCore Runtime 用於 serverless scaling 和 session isolation。我們還將使用 Amazon Bedrock Knowledge Bases 進行託管 RAG 和活動資料檢索。",
      "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
      "title_zh": "使用 Amazon Bedrock AgentCore 和 Amazon Bedrock Knowledge Bases 建構智慧活動代理"
    },
    {
      "arxiv_id": "2602.21734",
      "authors": [
        "Selin Coban",
        "Miguel Perez",
        "Horst Lichter"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.232291+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Proto-ML: An IDE for ML Solution Prototyping",
          "url": "https://arxiv.org/abs/2602.21734"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Proto-ML: An IDE for ML Solution Prototyping",
        "url": "https://arxiv.org/abs/2602.21734"
      },
      "published_at": "2026-02-25T09:43:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.45,
        "llm_relevance_score": 12.6,
        "recency_score": 0.9153598359165408,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 0.0,
        "total_score": 16.71535983591654
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21734",
      "summary": "Prototyping plays a critical role in the development of machine learning (ML) solutions, yet existing tools often provide limited support for effective collaboration and knowledge reuse among stakeholders. This paper introduces Proto-ML, an IDE designed to strengthen ML prototyping workflows. By addressing key deficiencies such as insufficient stakeholder involvement, limited cross-project knowledge reuse, and fragmented tool support, Proto-ML offers a unified framework that enables structured d",
      "summary_zh": "原型設計在 machine learning (ML) 解決方案的開發中扮演著關鍵角色，然而現有工具通常對利害關係人之間的有效協作和知識重用提供的支援有限。本文介紹了 Proto-ML，這是一個旨在強化 ML 原型設計工作流程的 IDE。透過解決主要缺陷，例如利害關係人參與不足、跨專案知識重用有限以及工具支援碎片化等問題，Proto-ML 提供了一個統一的框架，可實現結構化...",
      "title": "Proto-ML: An IDE for ML Solution Prototyping",
      "title_zh": "Proto-ML：一個用於 ML 解決方案原型設計的 IDE"
    },
    {
      "arxiv_id": "2602.21845",
      "authors": [
        "Lin Zhu",
        "Lei You"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.863589+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "xai-cola: A Python library for sparsifying counterfactual explanations",
          "url": "https://arxiv.org/abs/2602.21845"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "xai-cola: A Python library for sparsifying counterfactual explanations",
        "url": "https://arxiv.org/abs/2602.21845"
      },
      "published_at": "2026-02-25T12:25:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.3,
        "llm_relevance_score": 8.4,
        "recency_score": 0.9256868488506775,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 16.525686848850675
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21845",
      "summary": "Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced by arbitrary generators, reducing superfluous feature changes while preserving their validity. It offers a documented API that takes as input raw tabular data in pandas DataFrame form, a preprocessin",
      "summary_zh": "反事實解釋 (Counterfactual explanation, CE) 是後驗可解釋性 (post-hoc explainability) 領域中的一個重要方向。然而，大多數 CE generator 所產生的解釋往往高度冗餘。本研究介紹了一個開源的 Python 函式庫 xai-cola，它提供了一個端到端 (end-to-end) 的 pipeline，用於稀疏化由任意 generator 產生的 CE，在保持其有效性的同時，減少了多餘的特徵變動 (feature changes)。它提供了一個有完整文件說明的 API，可以接收 pandas DataFrame 形式的原始表格數據 (raw tabular data)，並進行前處理 (preprocessin)...",
      "title": "xai-cola: A Python library for sparsifying counterfactual explanations",
      "title_zh": "xai-cola: 一個用於稀疏化反事實解釋的 Python 函式庫"
    },
    {
      "arxiv_id": "2602.22003",
      "authors": [
        "Hailiang Liu",
        "Yan-Han Chen"
      ],
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.943538+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Neural solver for Wasserstein Geodesics and optimal transport dynamics",
          "url": "https://arxiv.org/abs/2602.22003"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Neural solver for Wasserstein Geodesics and optimal transport dynamics",
        "url": "https://arxiv.org/abs/2602.22003"
      },
      "published_at": "2026-02-25T15:21:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.35,
        "llm_relevance_score": 9.799999999999999,
        "recency_score": 0.9370647997365565,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 16.187064799736554
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22003",
      "summary": "In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to",
      "summary_zh": "近年來，機器學習領域越來越多地採用最佳傳輸 (optimal transport, OT) 框架來建模分佈關係 (distributional relationships)。在這項工作中，我們介紹了一種基於樣本的 neural solver，用於計算源分佈 (source distribution) 和目標分佈 (target distribution) 之間的 Wasserstein geodesic，以及相關的 velocity field。基於 optimal transport (OT) 問題的動態表述 (dynamical formulation)，我們將約束優化 (constrained optimization) 重構為一個 minimax problem，並使用 deep neural networks 來...",
      "title": "Neural solver for Wasserstein Geodesics and optimal transport dynamics",
      "title_zh": "用於 Wasserstein Geodesics 和最佳傳輸動力學的神經求解器"
    },
    {
      "arxiv_id": "2602.21693",
      "authors": [
        "Jiafeng Lin",
        "Yuxuan Wang",
        "Huakun Luo",
        "Zhongyi Pei",
        "Jianmin Wang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-26T06:28:42.951969+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
          "url": "https://arxiv.org/abs/2602.21693"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
        "url": "https://arxiv.org/abs/2602.21693"
      },
      "published_at": "2026-02-25T08:51:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 5.6000000000000005,
        "recency_score": 0.9120043844382356,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.712004384438238
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21693",
      "summary": "Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and",
      "summary_zh": "多模態時間序列預測 (Multimodal time series forecasting) 因其潛力而受到廣泛關注，它能透過利用其他模態中固有的豐富資訊，提供比傳統單模態模型更準確的預測。然而，由於模態對齊 (modality alignment) 的基本挑戰，現有方法通常難以有效將多模態數據納入預測中，特別是那些對時間序列波動具有因果影響的文本資訊，例如緊急報告 (emergency reports) 和...",
      "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
      "title_zh": "TiMi: 透過多模態專家混合賦能時間序列 Transformer"
    },
    {
      "arxiv_id": "2602.21638",
      "authors": [
        "Anqi Li",
        "Ruihan Wang",
        "Zhaoming Chen",
        "Yuqian Chen",
        "Yu Lu",
        "Yi Zhu",
        "Yuan Xie",
        "Zhenzhong Lan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:43.854284+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
          "url": "https://arxiv.org/abs/2602.21638"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
        "url": "https://arxiv.org/abs/2602.21638"
      },
      "published_at": "2026-02-25T07:05:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 5.6000000000000005,
        "recency_score": 0.9053177628692456,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.705317762869246
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21638",
      "summary": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human co",
      "summary_zh": "有效處理客戶抗拒 (client resistance) 是心理諮詢中一項複雜的臨床技能，然而執業者 (practitioners) 往往缺乏及時且可擴展的監督反饋 (supervisory feedback) 來改進他們的方法。儘管當前的 NLP 研究已經審查了整體諮詢品質和一般治療技能，但它未能提供客戶表現出抗拒行為等高風險時刻的 granular evaluation。在這項工作中，我們提出了一個全面的 pipeline，用於對人類協同 (human co...) 進行多維評估...",
      "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
      "title_zh": "在基於文本的 LLM 諮詢中對諮詢師回應客戶抗拒行為進行多維評估和可解釋反饋"
    },
    {
      "arxiv_id": "2602.21667",
      "authors": [
        "Sheng Xu",
        "Enshu Wang",
        "Hongfei Xue",
        "Jian Teng",
        "Bingyi Liu",
        "Yi Zhu",
        "Pu Wang",
        "Libing Wu",
        "Chunming Qiao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.385478+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
          "url": "https://arxiv.org/abs/2602.21667"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
        "url": "https://arxiv.org/abs/2602.21667"
      },
      "published_at": "2026-02-25T08:00:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.2,
        "llm_relevance_score": 5.6000000000000005,
        "recency_score": 0.9088274155111615,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.45,
        "total_score": 15.158827415511162
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21667",
      "summary": "Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoi",
      "summary_zh": "協同感知 (Collaborative perception) 允許聯網車輛 (connected vehicles) 透過共享感測資訊來克服遮擋和有限的視角。然而，現有方法難以在嚴格的頻寬限制下實現高準確性，並且對隨機傳輸 packet loss 仍然高度脆弱。我們引入了 QPoint2Comm，這是一個量化點雲通訊框架，它在顯著降低頻寬的同時，保留了高擬真度 (high-fidelity) 的 3D 資訊。QPoint2Comm 並非傳輸 intermediate features，而是...",
      "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
      "title_zh": "發送更少，感知更多：用於容忍丟失的協同感知之遮罩量化點雲通訊"
    }
  ],
  "run_date": "2026-02-26",
  "run_id": "f17e98a2-f42b-4e4b-9ff7-42294930bee4",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-26T07:08:21.477585+00:00",
    "items_total": 315,
    "run_id": "f17e98a2-f42b-4e4b-9ff7-42294930bee4",
    "started_at": "2026-02-26T06:28:37.493683+00:00",
    "stories_total": 230,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-agents",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-alignment",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Efficiency",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-efficiency",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Evaluation",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-evaluation",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Interpretability",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-interpretability",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-llm",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-multimodal",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-reasoning",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Retrieval",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-retrieval",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reinforcement Learning",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-rl",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Safety",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-safety",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 53,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.AI",
      "newest_item_date": "2026-02-25T18:58:25+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 23,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CL",
      "newest_item_date": "2026-02-25T18:58:25+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CR",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-cs-cr",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 62,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CV",
      "newest_item_date": "2026-02-25T18:59:53+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.HC",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-cs-hc",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.IR",
      "newest_item_date": "2026-02-25T18:28:38+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ir",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 49,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.LG",
      "newest_item_date": "2026-02-25T18:58:25+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.MA",
      "newest_item_date": "2026-02-25T15:48:52+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ma",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 20,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.RO",
      "newest_item_date": "2026-02-25T18:01:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ro",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 8,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.SE",
      "newest_item_date": "2026-02-25T17:11:49+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-se",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv stat.ML",
      "newest_item_date": "2026-02-25T17:10:59+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": "2026-02-25T20:56:13+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": "2026-02-25T18:00:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 6,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": "2026-02-25T18:59:01+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": "2026-02-25T08:50:48+00:00",
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 3,
      "items_updated": 1,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": "2026-02-26T03:37:31+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 3,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": "2026-02-25T22:52:04+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": "2026-02-25T08:15:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.22144",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-26T06:28:41.858128+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
          "url": "https://arxiv.org/abs/2602.22144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
        "url": "https://arxiv.org/abs/2602.22144"
      },
      "published_at": "2026-02-25T17:50:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.946829783916334,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 34.946829783916336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22144",
      "summary": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.",
      "summary_zh": "物體幻覺 (Object hallucination) 是大型視覺語言模型 (LVLMs) 中的一個關鍵問題，其輸出包含輸入圖像中未出現的物體。由此現象自然引出一個問題：LVLM 流程中的哪個組件主要導致物體幻覺？是感知視覺資訊的 vision encoder，還是生成文本回應的 language decoder？在這項工作中，我們透過設計系統性實驗來分析 vision encoder 和 language decoder 在幻覺生成中的作用，以期回答這個問題。我們的觀察顯示，物體幻覺主要與來自 language decoder 的強大先驗 (strong priors) 相關。基於這一發現，我們提出了一個簡單且免訓練 (training-free) 的框架：No-Language-Hallucination Decoding (NoLan)，它透過動態抑制語言先驗來優化輸出分佈，並根據多模態 (multimodal) 和純文本 (text-only) 輸入之間的輸出分佈差異進行調變。實驗結果表明，NoLan 能有效減少各種 LVLMs 在不同任務上的物體幻覺。例如，NoLan 在 POPE 上取得了顯著改進，將 LLaVA-1.5 7B 和 Qwen-VL 7B 的準確性分別提高了高達 6.45 和 7.21。程式碼已公開於：https://github.com/lingfengren/NoLan。",
      "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
      "title_zh": "NoLan: 透過動態抑制語言先驗來緩解大型視覺語言模型中的物體幻覺"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Danielle Robinson"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Bedrock",
        "Amazon Bedrock AgentCore",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Amazon SageMaker Lakehouse",
        "Announcements",
        "Artificial Intelligence"
      ],
      "entities": [
        "aws",
        "vllm"
      ],
      "first_seen_at": "2026-02-26T06:28:40.333502+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
          "url": "https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
        "url": "https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock"
      },
      "published_at": "2026-02-25T20:56:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9591079117667239,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 34.45910791176672
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:92a11eed3231b3dd",
      "summary": "In this post, we explain how we implemented multi-LoRA inference for Mixture of Experts (MoE) models in vLLM, describe the kernel-level optimizations we performed, and show you how you can benefit from this work. We use GPT-OSS 20B as our primary example throughout this post.",
      "summary_zh": "在這篇文章中，我們解釋了如何在 vLLM 中為 Mixture of Experts (MoE) 模型實作 multi-LoRA 推理 (inference)，描述了我們執行的 kernel-level 優化，並向您展示如何從這項工作中受益。我們在整篇文章中以 GPT-OSS 20B 作為主要範例。",
      "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
      "title_zh": "在 Amazon SageMaker AI 和 Amazon Bedrock 上使用 vLLM 高效率地服務數十個 fine-tuned 模型"
    },
    {
      "arxiv_id": "2602.22190",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.856821+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
          "url": "https://arxiv.org/abs/2602.22190"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
        "url": "https://arxiv.org/abs/2602.22190"
      },
      "published_at": "2026-02-25T18:34:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 25.2,
        "recency_score": 0.9497448871336173,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 34.34974488713362
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22190",
      "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.",
      "summary_zh": "開源原生 GUI agents 在長程導航任務上仍然落後於閉源系統。這一差距源於兩個限制：高品質、與動作對齊的推理數據稀缺，以及直接採用通用後訓練流程卻忽視了 GUI agents 的獨特挑戰。我們在這些流程中發現了兩個基本問題：(i) 標準的 SFT 與 CoT 推理通常會損害 grounding，以及 (ii) 逐步 RLVR 型訓練面臨部分可驗證性，即多個動作可能是正確的，但只有單一示範動作用於驗證。這使得離線逐步指標對線上任務成功率的預測能力較弱。在這項工作中，我們提出了 GUI-Libra，這是一個量身定制的訓練方案，旨在應對這些挑戰。首先，為了緩解動作對齊推理數據的稀缺性，我們引入了一個數據構建和過濾流程，並發布了一個精心策劃的 81K GUI 推理數據集。其次，為了調和推理與 grounding，我們提出了動作感知型 SFT，它混合了推理後動作 (reasoning-then-action) 和直接動作 (direct-action) 數據，並重新加權 tokens 以強調動作和 grounding。第三，為了在部分可驗證性下穩定 RL，我們指出了 KL regularization 在 RLVR 中被忽視的重要性，並表明 KL trust region 對於提高離線到線上可預測性至關重要；我們進一步引入了 success-adaptive scaling 以降低不可靠負梯度的權重。在各種網路和行動基準測試中，GUI-Libra 持續提高了逐步準確性 (step-wise accuracy) 和端到端任務完成率 (end-to-end task completion)。我們的結果表明，精心設計的後訓練和數據策劃可以解鎖顯著更強的任務解決能力，而無需昂貴的線上數據採集。我們發布了我們的數據集、程式碼和模型，以促進對具有推理能力的 GUI agents 數據高效後訓練的進一步研究。",
      "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
      "title_zh": "GUI-Libra：透過動作感知監督和部分可驗證 RL 訓練原生 GUI Agent 進行推理和行動"
    },
    {
      "arxiv_id": "2602.21835",
      "authors": [],
      "categories": [],
      "entities": [
        "meta-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.379748+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
          "url": "https://arxiv.org/abs/2602.21835"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
        "url": "https://arxiv.org/abs/2602.21835"
      },
      "published_at": "2026-02-25T12:08:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9246203524569181,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.52462035245692
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21835",
      "summary": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.",
      "summary_zh": "視訊基礎模型 (Video foundation models) 旨在將視訊理解、生成、編輯和指令遵循整合到單一框架內，使其成為下一代多模態系統的核心方向。然而，現有的評估基準仍然零散且範圍有限，因為它們各自針對單一任務，依賴任務專用指標，並且通常使用短或簡單的視訊片段。因此，它們未能捕捉這些模型旨在提供的統一能力。為了解決這一差距，我們引入了 UniVBench，這是一個專為評估視訊基礎模型而構建的基準，涵蓋四項核心能力：視訊理解、視訊生成、視訊編輯，以及一個新提出的任務——視訊重建 (video reconstruction)，該任務評估模型能多忠實地重現其所遇到的視訊內容。我們的基準透過納入 200 個高品質、多樣化且多鏡頭 (multi-shot) 的視訊，大幅擴展了評估的複雜性，每個視訊都搭配詳細字幕、多格式編輯指令和參考圖像。所有視訊均為人工創建並仔細驗證，提供比先前基準更豐富的電影資訊。此外，我們開發了一個統一的代理評估系統 (UniV-Eval)，它標準化了所有任務的提示、指令解析和評分，實現了統一視訊模型的公平、可擴展和可重現的比較。透過將評估奠基於基於指令的多鏡頭視訊任務，UniVBench 提供了衡量視訊基礎模型旨在實現的整合能力的第一個框架。大量人工註釋確保我們的評估與人類判斷一致，從而實現嚴格評估並加速邁向強大視訊智慧的進程。",
      "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
      "title_zh": "UniVBench：邁向視訊基礎模型的統一評估"
    },
    {
      "arxiv_id": "2602.21977",
      "authors": [
        "Liangwei Lyu",
        "Jiaqi Xu",
        "Jianwei Ding",
        "Qiyao Deng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.375190+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters",
          "url": "https://arxiv.org/abs/2602.21977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters",
        "url": "https://arxiv.org/abs/2602.21977"
      },
      "published_at": "2026-02-25T14:56:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.93546859544083,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.77546859544083
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21977",
      "summary": "Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoR",
      "summary_zh": "低秩適應 (Low-Rank Adaptation, LoRA) 已成為有效 fine-tuning Text-to-Image diffusion models 的主導技術，其在開源平台上的廣泛採用培養了模型共享和客製化的活躍文化。然而，使 LoRA 具有吸引力的模組化和隨插即用 (plug-and-play) 的靈活性，也引入了更廣泛的攻擊面。為突顯此風險，我們提出了 Masquerade-LoRA (MasqLoRA)，這是第一個系統性攻擊框架，它利用一個獨立的 LoR",
      "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters",
      "title_zh": "當 LoRA 背叛時：透過偽裝成良性 Adapter 對 Text-to-Image 模型進行後門攻擊"
    }
  ]
}