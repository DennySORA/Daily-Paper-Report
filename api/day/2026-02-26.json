{
  "archive_dates": [
    "2026-02-28",
    "2026-02-27",
    "2026-02-26",
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-28T06:15:37.246994+00:00",
  "model_releases_by_entity": {},
  "papers": [
    {
      "arxiv_id": "2602.23164",
      "authors": [
        "Aviral Chawla",
        "Galen Hall",
        "Juniper Lovato"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-27T06:24:49.912408+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
          "url": "https://arxiv.org/abs/2602.23164"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
        "url": "https://arxiv.org/abs/2602.23164"
      },
      "published_at": "2026-02-26T16:28:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8543090613238071,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.054309061323806
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23164",
      "summary": "Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting \"world models\". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, ",
      "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers"
    },
    {
      "arxiv_id": "2602.23114",
      "authors": [
        "Xudong Yan",
        "Songhe Feng",
        "Jiaxin Wang",
        "Xin Su",
        "Yi Jin"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:52.358123+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning",
          "url": "https://arxiv.org/abs/2602.23114"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning",
        "url": "https://arxiv.org/abs/2602.23114"
      },
      "published_at": "2026-02-26T15:27:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8507056444056023,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.050705644405603
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23114",
      "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervise",
      "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning"
    },
    {
      "arxiv_id": "2602.22903",
      "authors": [
        "Yunpeng Hong",
        "Chenyang Bu",
        "Jie Zhang",
        "Yi He",
        "Di Wu",
        "Xindong Wu"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:49.919512+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA",
          "url": "https://arxiv.org/abs/2602.22903"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA",
        "url": "https://arxiv.org/abs/2602.22903"
      },
      "published_at": "2026-02-26T11:47:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8378220761476568,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.037822076147657
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22903",
      "summary": "Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporatio",
      "title": "PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA"
    },
    {
      "arxiv_id": "2602.22882",
      "authors": [
        "Umberto Biccari",
        "Alain Ibáñez de Opakua",
        "José María Mato",
        "Óscar Millet",
        "Roberto Morales",
        "Enrique Zuazua"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "meta-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:49.920562+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Fair feature attribution for multi-output prediction: a Shapley-based perspective",
          "url": "https://arxiv.org/abs/2602.22882"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Fair feature attribution for multi-output prediction: a Shapley-based perspective",
        "url": "https://arxiv.org/abs/2602.22882"
      },
      "published_at": "2026-02-26T11:22:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8363455536902784,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.036345553690278
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22882",
      "summary": "In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity m",
      "title": "Fair feature attribution for multi-output prediction: a Shapley-based perspective"
    },
    {
      "arxiv_id": "2602.22850",
      "authors": [
        "Yi He",
        "Yina Cao",
        "Jixiu Zhai",
        "Di Wang",
        "Junxiao Kong",
        "Tianchi Lu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:48.638280+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction",
          "url": "https://arxiv.org/abs/2602.22850"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction",
        "url": "https://arxiv.org/abs/2602.22850"
      },
      "published_at": "2026-02-26T10:38:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8338258005554327,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.033825800555434
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22850",
      "summary": "Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its \"black-box\" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse s",
      "title": "MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction"
    },
    {
      "arxiv_id": "2602.22790",
      "authors": [
        "Hyunwoo Kim",
        "Hanau Yi",
        "Jaehee Bae",
        "Yumin Kim"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:48.641084+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
          "url": "https://arxiv.org/abs/2602.22790"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
        "url": "https://arxiv.org/abs/2602.22790"
      },
      "published_at": "2026-02-26T09:23:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8294635262811922,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.029463526281193
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22790",
      "summary": "The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, inter",
      "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift"
    },
    {
      "arxiv_id": "2602.22913",
      "authors": [
        "Yang Yu",
        "Lei Kou",
        "Huaikuan Yi",
        "Bin Chen",
        "Yayu Cao",
        "Lei Shen",
        "Chao Zhang",
        "Bing Wang",
        "Xiaoyi Zeng"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:49.919092+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress",
          "url": "https://arxiv.org/abs/2602.22913"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress",
        "url": "https://arxiv.org/abs/2602.22913"
      },
      "published_at": "2026-02-26T12:00:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8385923730104776,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 9.788592373010477
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22913",
      "summary": "With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender a",
      "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress"
    },
    {
      "arxiv_id": "2602.23203",
      "authors": [
        "Junhu Fu",
        "Shuyu Liang",
        "Wutong Li",
        "Chen Ma",
        "Peng Huang",
        "Kehao Wang",
        "Ke Chen",
        "Shengli Lin",
        "Pinghong Zhou",
        "Zeju Li",
        "Yuanyuan Wang",
        "Yi Guo"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:48.627713+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation",
          "url": "https://arxiv.org/abs/2602.23203"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation",
        "url": "https://arxiv.org/abs/2602.23203"
      },
      "published_at": "2026-02-26T16:51:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556895286356004,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.1500000000000004,
        "total_score": 9.2056895286356
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23203",
      "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colo",
      "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation"
    },
    {
      "arxiv_id": "2602.23259",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625222+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
          "url": "https://arxiv.org/abs/2602.23259"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
        "url": "https://arxiv.org/abs/2602.23259"
      },
      "published_at": "2026-02-26T17:32:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8581352978213661,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.058135297821366
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23259",
      "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
      "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving"
    },
    {
      "arxiv_id": "2602.23165",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.356391+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
          "url": "https://arxiv.org/abs/2602.23165"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
        "url": "https://arxiv.org/abs/2602.23165"
      },
      "published_at": "2026-02-26T16:30:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8544257457606835,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.054425745760684
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23165",
      "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.",
      "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation"
    },
    {
      "arxiv_id": "2602.23152",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629495+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Trinity of Consistency as a Defining Principle for General World Models",
          "url": "https://arxiv.org/abs/2602.23152"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Trinity of Consistency as a Defining Principle for General World Models",
        "url": "https://arxiv.org/abs/2602.23152"
      },
      "published_at": "2026-02-26T16:15:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8535836023309089,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.053583602330908
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23152",
      "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
      "title": "The Trinity of Consistency as a Defining Principle for General World Models"
    },
    {
      "arxiv_id": "2602.23058",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.359274+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "GeoWorld: Geometric World Models",
          "url": "https://arxiv.org/abs/2602.23058"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "GeoWorld: Geometric World Models",
        "url": "https://arxiv.org/abs/2602.23058"
      },
      "published_at": "2026-02-26T14:42:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8480866749880698,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.04808667498807
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23058",
      "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
      "title": "GeoWorld: Geometric World Models"
    },
    {
      "arxiv_id": "2602.23008",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.633646+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
          "url": "https://arxiv.org/abs/2602.23008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
        "url": "https://arxiv.org/abs/2602.23008"
      },
      "published_at": "2026-02-26T13:50:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.845033574883759,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.045033574883758
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23008",
      "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.",
      "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization"
    },
    {
      "arxiv_id": "2602.22897",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.637327+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
          "url": "https://arxiv.org/abs/2602.22897"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
        "url": "https://arxiv.org/abs/2602.22897"
      },
      "published_at": "2026-02-26T11:35:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.837097053329024,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.037097053329024
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22897",
      "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
      "title": "OmniGAIA: Towards Native Omni-Modal AI Agents"
    },
    {
      "arxiv_id": "2602.22766",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.225070+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
          "url": "https://arxiv.org/abs/2602.22766"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
        "url": "https://arxiv.org/abs/2602.22766"
      },
      "published_at": "2026-02-26T08:56:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8279231546127186,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.027923154612719
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22766",
      "summary": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
      "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space"
    },
    {
      "arxiv_id": "2602.23363",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.349411+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.23363"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.23363"
      },
      "published_at": "2026-02-26T18:59:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8633515463920113,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.063351546392012
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23363",
      "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases",
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.23361",
      "authors": [
        "Sven Elflein",
        "Ruilong Li",
        "Sérgio Agostinho",
        "Zan Gojcic",
        "Laura Leal-Taixé",
        "Qunjie Zhou",
        "Aljosa Osep"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.349701+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
          "url": "https://arxiv.org/abs/2602.23361"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
        "url": "https://arxiv.org/abs/2602.23361"
      },
      "published_at": "2026-02-26T18:59:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8633385562465634,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.063338556246563
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23361",
      "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Train",
      "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale"
    },
    {
      "arxiv_id": "2602.23359",
      "authors": [
        "Vaibhav Agrawal",
        "Rishubh Parihar",
        "Pradhaan Bhat",
        "Ravi Kiran Sarvadevabhatla",
        "R. Venkatesh Babu"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.621313+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
          "url": "https://arxiv.org/abs/2602.23359"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
        "url": "https://arxiv.org/abs/2602.23359"
      },
      "published_at": "2026-02-26T18:59:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8633105781355921,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.063310578135592
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23359",
      "summary": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene repre",
      "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation"
    },
    {
      "arxiv_id": "2602.23358",
      "authors": [
        "Elad Kimchi Shoshani",
        "Leeyam Gabay",
        "Yedid Hoshen"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.900869+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Dataset is Worth 1 MB",
          "url": "https://arxiv.org/abs/2602.23358"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Dataset is Worth 1 MB",
        "url": "https://arxiv.org/abs/2602.23358"
      },
      "published_at": "2026-02-26T18:59:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.863308579733789,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.06330857973379
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23358",
      "summary": "A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files.",
      "title": "A Dataset is Worth 1 MB"
    },
    {
      "arxiv_id": "2602.23357",
      "authors": [
        "Aheli Saha",
        "René Schuster",
        "Didier Stricker"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.350394+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training",
          "url": "https://arxiv.org/abs/2602.23357"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training",
        "url": "https://arxiv.org/abs/2602.23357"
      },
      "published_at": "2026-02-26T18:57:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8632376394667206,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.06323763946672
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23357",
      "summary": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding ",
      "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training"
    },
    {
      "arxiv_id": "2602.23353",
      "authors": [
        "Simon Roschmann",
        "Paul Krzakala",
        "Sonia Mazelet",
        "Quentin Bouniot",
        "Zeynep Akata"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.621549+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
          "url": "https://arxiv.org/abs/2602.23353"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
        "url": "https://arxiv.org/abs/2602.23353"
      },
      "published_at": "2026-02-26T18:55:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8630718018704356,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.063071801870436
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23353",
      "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setti",
      "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport"
    },
    {
      "arxiv_id": "2602.23351",
      "authors": [
        "Amita Kamath",
        "Jack Hessel",
        "Khyathi Chandu",
        "Jena D. Hwang",
        "Kai-Wei Chang",
        "Ranjay Krishna"
      ],
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.217729+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
          "url": "https://arxiv.org/abs/2602.23351"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
        "url": "https://arxiv.org/abs/2602.23351"
      },
      "published_at": "2026-02-26T18:54:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8630118684096861,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.063011868409687
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23351",
      "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP,",
      "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning"
    },
    {
      "arxiv_id": "2602.23349",
      "authors": [
        "Jose Javier Gonzalez Ortiz",
        "Abhay Gupta",
        "Chris Renard",
        "Davis Blalock"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.621815+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "FlashOptim: Optimizers for Memory Efficient Training",
          "url": "https://arxiv.org/abs/2602.23349"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "FlashOptim: Optimizers for Memory Efficient Training",
        "url": "https://arxiv.org/abs/2602.23349"
      },
      "published_at": "2026-02-26T18:52:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8629079936033015,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062907993603302
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23349",
      "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory b",
      "title": "FlashOptim: Optimizers for Memory Efficient Training"
    },
    {
      "arxiv_id": "2602.23342",
      "authors": [
        "Weijian Chen",
        "Haotian Liu",
        "Yangshen Deng",
        "Long Xiang",
        "Liang Huang",
        "Gezi Li",
        "Bo Tang"
      ],
      "categories": [
        "cs.DB",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:57.012750+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
          "url": "https://arxiv.org/abs/2602.23342"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
        "url": "https://arxiv.org/abs/2602.23342"
      },
      "published_at": "2026-02-26T18:48:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8626753194658545,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062675319465855
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23342",
      "summary": "On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-bas",
      "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search"
    },
    {
      "arxiv_id": "2602.23339",
      "authors": [
        "Tilemachos Aravanis",
        "Vladan Stojnić",
        "Bill Psomas",
        "Nikos Komodakis",
        "Giorgos Tolias"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.350791+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
          "url": "https://arxiv.org/abs/2602.23339"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
        "url": "https://arxiv.org/abs/2602.23339"
      },
      "published_at": "2026-02-26T18:45:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8624996072054122,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062499607205412
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23339",
      "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompt",
      "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?"
    },
    {
      "arxiv_id": "2602.23336",
      "authors": [
        "Camilo Gomez",
        "Pengyang Wang",
        "Liansheng Tang"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.901850+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Differentiable Zero-One Loss via Hypersimplex Projections",
          "url": "https://arxiv.org/abs/2602.23336"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Differentiable Zero-One Loss via Hypersimplex Projections",
        "url": "https://arxiv.org/abs/2602.23336"
      },
      "published_at": "2026-02-26T18:41:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8622580612835109,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062258061283512
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23336",
      "summary": "Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, or",
      "title": "Differentiable Zero-One Loss via Hypersimplex Projections"
    },
    {
      "arxiv_id": "2602.23335",
      "authors": [
        "Dany Haddad",
        "Dan Bareket",
        "Joseph Chee Chang",
        "Jay DeYoung",
        "Jena D. Hwang",
        "Uri Katz",
        "Mark Polak",
        "Sangho Suh",
        "Harshit Surana",
        "Aryeh Tiktinsky",
        "Shriya Atmakuri",
        "Jonathan Bragg",
        "Mike D'Arcy",
        "Sergey Feldman",
        "Amal Hassan-Ali",
        "Rubén Lozano",
        "Bodhisattwa Prasad Majumder",
        "Charles McGrady",
        "Amanpreet Singh",
        "Brooke Vlahos",
        "Yoav Goldberg",
        "Doug Downey"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622041+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
          "url": "https://arxiv.org/abs/2602.23335"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
        "url": "https://arxiv.org/abs/2602.23335"
      },
      "published_at": "2026-02-26T18:40:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8621951905920641,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062195190592064
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23335",
      "summary": "AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this ",
      "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset"
    },
    {
      "arxiv_id": "2602.23334",
      "authors": [
        "Yuhao Liu",
        "Salim Ullah",
        "Akash Kumar"
      ],
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622323+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators",
          "url": "https://arxiv.org/abs/2602.23334"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators",
        "url": "https://arxiv.org/abs/2602.23334"
      },
      "published_at": "2026-02-26T18:40:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8621692452938409,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062169245293841
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23334",
      "summary": "Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption a",
      "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators"
    },
    {
      "arxiv_id": "2602.23331",
      "authors": [
        "Salim Fares"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622555+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Utilizing LLMs for Industrial Process Automation",
          "url": "https://arxiv.org/abs/2602.23331"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Utilizing LLMs for Industrial Process Automation",
        "url": "https://arxiv.org/abs/2602.23331"
      },
      "published_at": "2026-02-26T18:38:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8620475123979562,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062047512397957
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23331",
      "summary": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to ut",
      "title": "Utilizing LLMs for Industrial Process Automation"
    },
    {
      "arxiv_id": "2602.23330",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622766+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
          "url": "https://arxiv.org/abs/2602.23330"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
        "url": "https://arxiv.org/abs/2602.23330"
      },
      "published_at": "2026-02-26T18:37:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8620235669662999,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0620235669663
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23330",
      "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis",
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks"
    },
    {
      "arxiv_id": "2602.23329",
      "authors": [
        "Chen Bo Calvin Zhang",
        "Christina Q. Knight",
        "Nicholas Kruus",
        "Jason Hausenloy",
        "Pedro Medeiros",
        "Nathaniel Li",
        "Aiden Kim",
        "Yury Orlovskiy",
        "Coleman Breen",
        "Bryce Cai",
        "Jasper Götting",
        "Andrew Bo Liu",
        "Samira Nedungadi",
        "Paula Rodriguez",
        "Yannis Yiming He",
        "Mohamed Shaaban",
        "Zifan Wang",
        "Seth Donoughe",
        "Julian Michael"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CY",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622978+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks",
          "url": "https://arxiv.org/abs/2602.23329"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks",
        "url": "https://arxiv.org/abs/2602.23329"
      },
      "published_at": "2026-02-26T18:37:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8620105968018736,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.062010596801874
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23329",
      "summary": "Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on comp",
      "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks"
    },
    {
      "arxiv_id": "2602.23320",
      "authors": [
        "Tianjun Yao",
        "Yongqiang Chen",
        "Yujia Zheng",
        "Pan Li",
        "Zhiqiang Shen",
        "Kun Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.902393+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
          "url": "https://arxiv.org/abs/2602.23320"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
        "url": "https://arxiv.org/abs/2602.23320"
      },
      "published_at": "2026-02-26T18:28:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8614530643059521,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.061453064305953
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23320",
      "summary": "Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric mem",
      "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory"
    },
    {
      "arxiv_id": "2602.23318",
      "authors": [
        "Aloïs Rautureau",
        "Tristan Cazenave",
        "Éric Piette"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.623198+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments",
          "url": "https://arxiv.org/abs/2602.23318"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments",
        "url": "https://arxiv.org/abs/2602.23318"
      },
      "published_at": "2026-02-26T18:25:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8613284418013873,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.061328441801388
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23318",
      "summary": "Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recyclin",
      "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments"
    },
    {
      "arxiv_id": "2602.23312",
      "authors": [
        "Rafael R. Baptista",
        "André de Lima Salgado",
        "Ricardo V. Godoy",
        "Marcelo Becker",
        "Thiago Boaventura",
        "Gustavo J. G. Lahr"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "eess.SY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.623622+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction",
          "url": "https://arxiv.org/abs/2602.23312"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction",
        "url": "https://arxiv.org/abs/2602.23312"
      },
      "published_at": "2026-02-26T18:20:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8609965354296755,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060996535429675
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23312",
      "summary": "Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we p",
      "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction"
    },
    {
      "arxiv_id": "2602.23306",
      "authors": [
        "Yiran Guan",
        "Sifan Tu",
        "Dingkang Liang",
        "Linghao Zhu",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.351002+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding",
          "url": "https://arxiv.org/abs/2602.23306"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding",
        "url": "https://arxiv.org/abs/2602.23306"
      },
      "published_at": "2026-02-26T18:10:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8604137663393612,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060413766339362
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23306",
      "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computati",
      "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding"
    },
    {
      "arxiv_id": "2602.23305",
      "authors": [
        "Samuel Tonks",
        "Steve Hood",
        "Ryan Musso",
        "Ceridwen Hopely",
        "Steve Titus",
        "Minh Doan",
        "Iain Styles",
        "Alexander Krull"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.902869+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Proper Scoring Rule for Virtual Staining",
          "url": "https://arxiv.org/abs/2602.23305"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Proper Scoring Rule for Virtual Staining",
        "url": "https://arxiv.org/abs/2602.23305"
      },
      "published_at": "2026-02-26T18:09:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8603619837357865,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060361983735786
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23305",
      "summary": "Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment o",
      "title": "A Proper Scoring Rule for Virtual Staining"
    },
    {
      "arxiv_id": "2602.23300",
      "authors": [
        "Soumya Dutta",
        "Smruthi Balaji",
        "Sriram Ganapathy"
      ],
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.218333+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations",
          "url": "https://arxiv.org/abs/2602.23300"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations",
        "url": "https://arxiv.org/abs/2602.23300"
      },
      "published_at": "2026-02-26T18:08:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8602932770153418,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060293277015342
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23300",
      "summary": "Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-",
      "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations"
    },
    {
      "arxiv_id": "2602.23297",
      "authors": [
        "Yiqing Wang",
        "Chunming He",
        "Ming-Chen Lu",
        "Mercy Pawar",
        "Leslie Niziol",
        "Maria Woodward",
        "Sina Farsiu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.351250+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM",
          "url": "https://arxiv.org/abs/2602.23297"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM",
        "url": "https://arxiv.org/abs/2602.23297"
      },
      "published_at": "2026-02-26T18:07:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8602454842719849,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060245484271984
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23297",
      "summary": "Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Aug",
      "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM"
    },
    {
      "arxiv_id": "2602.23295",
      "authors": [
        "Ayush Roy",
        "Wei-Yang Alex Lee",
        "Rudrasis Chakraborty",
        "Vishnu Suresh Lokhande"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.909245+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation",
          "url": "https://arxiv.org/abs/2602.23295"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation",
        "url": "https://arxiv.org/abs/2602.23295"
      },
      "published_at": "2026-02-26T18:07:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8602036677995469,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060203667799547
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23295",
      "summary": "In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided d",
      "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation"
    },
    {
      "arxiv_id": "2602.23290",
      "authors": [
        "Zhengyang Wei",
        "Renzhi Jing",
        "Yiyi He",
        "Jenny Suckale"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.352113+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction",
          "url": "https://arxiv.org/abs/2602.23290"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction",
        "url": "https://arxiv.org/abs/2602.23290"
      },
      "published_at": "2026-02-26T18:02:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8599388773404074,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059938877340407
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23290",
      "summary": "The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges",
      "title": "LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction"
    },
    {
      "arxiv_id": "2602.23286",
      "authors": [
        "Sungho Park",
        "Jueun Kim",
        "Wook-Shin Han"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.624324+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
          "url": "https://arxiv.org/abs/2602.23286"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
        "url": "https://arxiv.org/abs/2602.23286"
      },
      "published_at": "2026-02-26T17:59:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8597667077424761,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059766707742476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23286",
      "summary": "Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end cons",
      "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables"
    },
    {
      "arxiv_id": "2602.23283",
      "authors": [
        "Mike Y. Michelis",
        "Nana Obayashi",
        "Josie Hughes",
        "Robert K. Katzschmann"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:55.005399+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots",
          "url": "https://arxiv.org/abs/2602.23283"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots",
        "url": "https://arxiv.org/abs/2602.23283"
      },
      "published_at": "2026-02-26T17:55:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8594990674127418,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059499067412743
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23283",
      "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment usi",
      "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots"
    },
    {
      "arxiv_id": "2602.23276",
      "authors": [
        "Hyungyung Lee",
        "Hangyul Yoon",
        "Edward Choi"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.624770+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays",
          "url": "https://arxiv.org/abs/2602.23276"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays",
        "url": "https://arxiv.org/abs/2602.23276"
      },
      "published_at": "2026-02-26T17:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8592593563146905,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05925935631469
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23276",
      "summary": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, w",
      "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays"
    },
    {
      "arxiv_id": "2602.23271",
      "authors": [
        "Haotian Zhai",
        "Elias Stengel-Eskin",
        "Pratik Patil",
        "Liu Leqi"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.624990+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Evaluating Stochasticity in Deep Research Agents",
          "url": "https://arxiv.org/abs/2602.23271"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Evaluating Stochasticity in Deep Research Agents",
        "url": "https://arxiv.org/abs/2602.23271"
      },
      "published_at": "2026-02-26T17:46:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8589819319424347,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058981931942435
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23271",
      "summary": "Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability i",
      "title": "Evaluating Stochasticity in Deep Research Agents"
    },
    {
      "arxiv_id": "2602.23266",
      "authors": [
        "Siyuan Liu",
        "Jiahui Xu",
        "Feng Jiang",
        "Kuang Wang",
        "Zefeng Zhao",
        "Chu-Ren Huang",
        "Jinghang Gu",
        "Changqing Yin",
        "Haizhou Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.218768+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems",
          "url": "https://arxiv.org/abs/2602.23266"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems",
        "url": "https://arxiv.org/abs/2602.23266"
      },
      "published_at": "2026-02-26T17:39:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8585783847921091,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05857838479211
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23266",
      "summary": "Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon th",
      "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems"
    },
    {
      "arxiv_id": "2602.23262",
      "authors": [
        "Jasmine Bayrooti",
        "Weiwei Kong",
        "Natalia Ponomareva",
        "Carlos Esteves",
        "Ameesh Makadia",
        "Amanda Prorok"
      ],
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.352349+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling",
          "url": "https://arxiv.org/abs/2602.23262"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling",
        "url": "https://arxiv.org/abs/2602.23262"
      },
      "published_at": "2026-02-26T17:36:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8583915848193004,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0583915848193
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23262",
      "summary": "Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP frame",
      "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling"
    },
    {
      "arxiv_id": "2602.23258",
      "authors": [
        "Yutong Wang",
        "Siyuan Xiong",
        "Xuebo Liu",
        "Wenkang Zhou",
        "Liang Ding",
        "Miao Zhang",
        "Min Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
          "url": "https://arxiv.org/abs/2602.23258"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
        "url": "https://arxiv.org/abs/2602.23258"
      },
      "published_at": "2026-02-26T17:31:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8580886181199603,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05808861811996
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23258",
      "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting ",
      "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning"
    },
    {
      "arxiv_id": "2602.23253",
      "authors": [
        "Yijie Guo",
        "Iretiayo Akinola",
        "Lars Johannsmeier",
        "Hugo Hadfield",
        "Abhishek Gupta",
        "Yashraj Narang"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:55.006041+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly",
          "url": "https://arxiv.org/abs/2602.23253"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly",
        "url": "https://arxiv.org/abs/2602.23253"
      },
      "published_at": "2026-02-26T17:26:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.857760938521113,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057760938521113
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23253",
      "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we",
      "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly"
    },
    {
      "arxiv_id": "2602.23248",
      "authors": [
        "Yegon Kim",
        "Juho Lee"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625731+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
          "url": "https://arxiv.org/abs/2602.23248"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
        "url": "https://arxiv.org/abs/2602.23248"
      },
      "published_at": "2026-02-26T17:25:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8577103082933571,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057710308293357
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23248",
      "summary": "As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a \"translator\" model that turns a fixed solver m",
      "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games"
    },
    {
      "arxiv_id": "2602.23242",
      "authors": [
        "Yegon Kim",
        "Juho Lee"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625961+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Model-Free Universal AI",
          "url": "https://arxiv.org/abs/2602.23242"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Model-Free Universal AI",
        "url": "https://arxiv.org/abs/2602.23242"
      },
      "published_at": "2026-02-26T17:21:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8574661338709486,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05746613387095
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23242",
      "summary": "In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is stro",
      "title": "A Model-Free Universal AI"
    },
    {
      "arxiv_id": "2602.23239",
      "authors": [
        "Radha Sarma"
      ],
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626219+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
          "url": "https://arxiv.org/abs/2602.23239"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
        "url": "https://arxiv.org/abs/2602.23239"
      },
      "published_at": "2026-02-26T17:16:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8571694463516373,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057169446351637
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23239",
      "summary": "AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain",
      "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive"
    },
    {
      "arxiv_id": "2602.23235",
      "authors": [
        "Zhou Xu",
        "Bowen Zhou",
        "Qi Wang",
        "Shuwen Feng",
        "Jingyu Xiao"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626467+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
          "url": "https://arxiv.org/abs/2602.23235"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
        "url": "https://arxiv.org/abs/2602.23235"
      },
      "published_at": "2026-02-26T17:12:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8569541889286624,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056954188928662
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23235",
      "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid in",
      "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents"
    },
    {
      "arxiv_id": "2602.23234",
      "authors": [
        "Evangelia Christakopoulou",
        "Vivekkumar Patel",
        "Hemanth Velaga",
        "Sandip Gaikwad"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626728+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments",
          "url": "https://arxiv.org/abs/2602.23234"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments",
        "url": "https://arxiv.org/abs/2602.23234"
      },
      "published_at": "2026-02-26T17:11:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.856880795532382,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056880795532383
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23234",
      "summary": "Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically",
      "title": "Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments"
    },
    {
      "arxiv_id": "2602.23232",
      "authors": [
        "Aishik Sanyal"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626965+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
          "url": "https://arxiv.org/abs/2602.23232"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
        "url": "https://arxiv.org/abs/2602.23232"
      },
      "published_at": "2026-02-26T17:11:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8568629440350954,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056862944035096
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23232",
      "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we op",
      "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays"
    },
    {
      "arxiv_id": "2602.23229",
      "authors": [
        "Marco Garosi",
        "Matteo Farina",
        "Alessandro Conti",
        "Massimiliano Mancini",
        "Elisa Ricci"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.353202+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Large Multimodal Models as General In-Context Classifiers",
          "url": "https://arxiv.org/abs/2602.23229"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Large Multimodal Models as General In-Context Classifiers",
        "url": "https://arxiv.org/abs/2602.23229"
      },
      "published_at": "2026-02-26T17:08:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8566943649022369,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056694364902237
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23229",
      "summary": "Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and f",
      "title": "Large Multimodal Models as General In-Context Classifiers"
    },
    {
      "arxiv_id": "2602.23228",
      "authors": [
        "Yizhi Li",
        "Xiaohan Chen",
        "Miao Jiang",
        "Wentao Tang",
        "Gaoang Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.627241+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
          "url": "https://arxiv.org/abs/2602.23228"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
        "url": "https://arxiv.org/abs/2602.23228"
      },
      "published_at": "2026-02-26T17:08:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8566844495155795,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05668444951558
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23228",
      "summary": "With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts",
      "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction"
    },
    {
      "arxiv_id": "2602.23225",
      "authors": [
        "Pengxiang Li",
        "Dilxat Muhtar",
        "Lu Yin",
        "Tianlong Chen",
        "Shiwei Liu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.627481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
          "url": "https://arxiv.org/abs/2602.23225"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
        "url": "https://arxiv.org/abs/2602.23225"
      },
      "published_at": "2026-02-26T17:04:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8564950876577858,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056495087657787
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23225",
      "summary": "Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch be",
      "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?"
    },
    {
      "arxiv_id": "2602.23224",
      "authors": [
        "Mohammad Mahdavian",
        "Gordon Tan",
        "Binbin Xu",
        "Yuan Ren",
        "Dongfeng Bai",
        "Bingbing Liu"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.353611+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception",
          "url": "https://arxiv.org/abs/2602.23224"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception",
        "url": "https://arxiv.org/abs/2602.23224"
      },
      "published_at": "2026-02-26T17:04:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8564742703218392,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05647427032184
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23224",
      "summary": "We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and poi",
      "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception"
    },
    {
      "arxiv_id": "2602.23214",
      "authors": [
        "Chenhe Du",
        "Xuanyu Tian",
        "Qing Wu",
        "Muyu Liu",
        "Jingyi Yu",
        "Hongjiang Wei",
        "Yuyao Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.910370+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction",
          "url": "https://arxiv.org/abs/2602.23214"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction",
        "url": "https://arxiv.org/abs/2602.23214"
      },
      "published_at": "2026-02-26T16:58:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.85612441654403,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05612441654403
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23214",
      "summary": "Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fa",
      "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction"
    },
    {
      "arxiv_id": "2602.23205",
      "authors": [
        "Wenjia Wang",
        "Liang Pan",
        "Huaijin Pi",
        "Yuke Lou",
        "Xuqian Ren",
        "Yifan Wu",
        "Zhouyingcheng Liao",
        "Lei Yang",
        "Rishabh Dabral",
        "Christian Theobalt",
        "Taku Komura"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.354460+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
          "url": "https://arxiv.org/abs/2602.23205"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
        "url": "https://arxiv.org/abs/2602.23205"
      },
      "published_at": "2026-02-26T16:53:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8558252216450316,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055825221645032
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23205",
      "summary": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to j",
      "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents"
    },
    {
      "arxiv_id": "2602.23204",
      "authors": [
        "Roberto Pellerito",
        "Nico Messikommer",
        "Giovanni Cioffi",
        "Marco Cannici",
        "Davide Scaramuzza"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.354666+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Motion-aware Event Suppression for Event Cameras",
          "url": "https://arxiv.org/abs/2602.23204"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Motion-aware Event Suppression for Event Cameras",
        "url": "https://arxiv.org/abs/2602.23204"
      },
      "published_at": "2026-02-26T16:53:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8558202689671075,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055820268967107
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23204",
      "summary": "In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the c",
      "title": "Motion-aware Event Suppression for Event Cameras"
    },
    {
      "arxiv_id": "2602.23201",
      "authors": [
        "Max S. Bennett",
        "Thomas P. Zollo",
        "Richard Zemel"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.910578+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language",
          "url": "https://arxiv.org/abs/2602.23201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language",
        "url": "https://arxiv.org/abs/2602.23201"
      },
      "published_at": "2026-02-26T16:50:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556578370177216,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055657837017721
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23201",
      "summary": "Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores ove",
      "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language"
    },
    {
      "arxiv_id": "2602.23200",
      "authors": [
        "Sayed Mohammadreza Tayaranian Hosseini",
        "Amir Ardakani",
        "Warren J. Gross"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.910799+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models",
          "url": "https://arxiv.org/abs/2602.23200"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models",
        "url": "https://arxiv.org/abs/2602.23200"
      },
      "published_at": "2026-02-26T16:50:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556419916489381,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055641991648939
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23200",
      "summary": "Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without s",
      "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models"
    },
    {
      "arxiv_id": "2602.23199",
      "authors": [
        "Jiahao Zhao",
        "Feng Jiang",
        "Shaowei Qin",
        "Zhonghui Zhang",
        "Junhao Liu",
        "Guibing Guo",
        "Hamid Alinejad-Rokny",
        "Min Yang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.627989+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation",
          "url": "https://arxiv.org/abs/2602.23199"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation",
        "url": "https://arxiv.org/abs/2602.23199"
      },
      "published_at": "2026-02-26T16:50:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.855634069074583,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055634069074584
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23199",
      "summary": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural langua",
      "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation"
    },
    {
      "arxiv_id": "2602.23197",
      "authors": [
        "Chungpa Lee",
        "Jy-yong Sohn",
        "Kangwook Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.911026+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models",
          "url": "https://arxiv.org/abs/2602.23197"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models",
        "url": "https://arxiv.org/abs/2602.23197"
      },
      "published_at": "2026-02-26T16:49:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8555617789722489,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05556177897225
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23197",
      "summary": "Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention mo",
      "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models"
    },
    {
      "arxiv_id": "2602.23193",
      "authors": [
        "Elzo Brito dos Santos Filho"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628235+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
          "url": "https://arxiv.org/abs/2602.23193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
        "url": "https://arxiv.org/abs/2602.23193"
      },
      "published_at": "2026-02-26T16:45:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8553677152110095,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055367715211009
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23193",
      "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which sepa",
      "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering"
    },
    {
      "arxiv_id": "2602.23192",
      "authors": [
        "Thomas Woergaard",
        "Raghavendra Selvan"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.911259+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification",
          "url": "https://arxiv.org/abs/2602.23192"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification",
        "url": "https://arxiv.org/abs/2602.23192"
      },
      "published_at": "2026-02-26T16:44:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8552964375380195,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05529643753802
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23192",
      "summary": "Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under ",
      "title": "FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification"
    },
    {
      "arxiv_id": "2602.23188",
      "authors": [
        "Ismaël Zighed",
        "Andrea Nóvoa",
        "Luca Magri",
        "Taraneh Sayadi"
      ],
      "categories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.911502+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation",
          "url": "https://arxiv.org/abs/2602.23188"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation",
        "url": "https://arxiv.org/abs/2602.23188"
      },
      "published_at": "2026-02-26T16:43:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8552182369250633,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055218236925063
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23188",
      "summary": "We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics. The ROM is parameterized by an",
      "title": "Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation"
    },
    {
      "arxiv_id": "2602.23184",
      "authors": [
        "Sara Rosenthal",
        "Yannis Katsis",
        "Vraj Shah",
        "Lihong He",
        "Lucian Popa",
        "Marina Danilevsky"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.219820+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations",
          "url": "https://arxiv.org/abs/2602.23184"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations",
        "url": "https://arxiv.org/abs/2602.23184"
      },
      "published_at": "2026-02-26T16:41:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8550885782489298,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05508857824893
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23184",
      "summary": "We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com",
      "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations"
    },
    {
      "arxiv_id": "2602.23179",
      "authors": [
        "Gal Kesten-Pomeranz",
        "Yaniv Nikankin",
        "Anja Reusch",
        "Tomer Tsaban",
        "Ora Schueler-Furman",
        "Yonatan Belinkov"
      ],
      "categories": [
        "cs.LG",
        "q-bio.BM"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.911919+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models",
          "url": "https://arxiv.org/abs/2602.23179"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models",
        "url": "https://arxiv.org/abs/2602.23179"
      },
      "published_at": "2026-02-26T16:39:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8549569601608746,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054956960160874
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23179",
      "summary": "Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that ",
      "title": "Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Bharathan Balaji"
      ],
      "categories": [
        "Amazon Bedrock",
        "Amazon Nova",
        "Artificial Intelligence"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:47.089971+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Reinforcement fine-tuning for Amazon Nova: Teaching AI through feedback",
          "url": "https://aws.amazon.com/blogs/machine-learning/reinforcement-fine-tuning-for-amazon-nova-teaching-ai-through-feedback"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Reinforcement fine-tuning for Amazon Nova: Teaching AI through feedback",
        "url": "https://aws.amazon.com/blogs/machine-learning/reinforcement-fine-tuning-for-amazon-nova-teaching-ai-through-feedback"
      },
      "published_at": "2026-02-26T17:48:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8590962716375273,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 9.359096271637528
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:1c5548dd1e4fb9ad",
      "summary": "In this post, we explore reinforcement fine-tuning (RFT) for Amazon Nova models, which can be a powerful customization technique that learns through evaluation rather than imitation. We'll cover how RFT works, when to use it versus supervised fine-tuning, real-world applications from code generation to customer service, and implementation options ranging from fully managed Amazon Bedrock to multi-turn agentic workflows with Nova Forge. You'll also learn practical guidance on data preparation, reward function design, and best practices for achieving optimal results.",
      "title": "Reinforcement fine-tuning for Amazon Nova: Teaching AI through feedback"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dmitry Soldatkin"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon SageMaker",
        "Artificial Intelligence"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:47.090263+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Large model inference container – latest capabilities and performance enhancements",
          "url": "https://aws.amazon.com/blogs/machine-learning/large-model-inference-container-latest-capabilities-and-performance-enhancements"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Large model inference container – latest capabilities and performance enhancements",
        "url": "https://aws.amazon.com/blogs/machine-learning/large-model-inference-container-latest-capabilities-and-performance-enhancements"
      },
      "published_at": "2026-02-26T17:45:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8589391827480386,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 9.358939182748038
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:3a7d023d8edc4a5f",
      "summary": "AWS recently released significant updates to the Large Model Inference (LMI) container, delivering comprehensive performance improvements, expanded model support, and streamlined deployment capabilities for customers hosting LLMs on AWS. These releases focus on reducing operational complexity while delivering measurable performance gains across popular model architectures.",
      "title": "Large model inference container – latest capabilities and performance enhancements"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Abubakarr Jaye, Nigel Boachie Kumankumah, Chidera Biringa, Sulaiman Vesal, Anjel Patel, Dayquan Julienne"
      ],
      "categories": [
        "Research Blog"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:46.577911+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "microsoft-research-blog",
          "tier": 0,
          "title": "CORPGEN advances AI agents for real work",
          "url": "https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "microsoft-research-blog",
        "tier": 0,
        "title": "CORPGEN advances AI agents for real work",
        "url": "https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work"
      },
      "published_at": "2026-02-26T17:06:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8565912504903239,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 9.356591250490323
      },
      "section": null,
      "source_name": "Microsoft Research Blog",
      "story_id": "fallback:f571deaa6ee27974",
      "summary": "<p>By mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today’s best models are evaluated one [&#8230;]</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/\">CORPGEN advances AI agents for real work</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>",
      "title": "CORPGEN advances AI agents for real work"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dr. Asa Kalavade"
      ],
      "categories": [
        "Artificial Intelligence",
        "Customer Solutions",
        "Generative AI",
        "Thought Leadership"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:47.089655+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Learnings from COBOL modernization in the real world",
          "url": "https://aws.amazon.com/blogs/machine-learning/learnings-from-cobol-modernization-in-the-real-world"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Learnings from COBOL modernization in the real world",
        "url": "https://aws.amazon.com/blogs/machine-learning/learnings-from-cobol-modernization-in-the-real-world"
      },
      "published_at": "2026-02-26T18:16:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8607743393052921,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.0,
        "total_score": 8.360774339305292
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:1e022b2e2a847e0d",
      "summary": "Delivering successful COBOL modernization requires a solution that can reverse engineer deterministically, produce validated and traceable specs, and help those specs flow into any AI-powered coding assistant for the forward engineering. A successful modernization requires both reverse engineering and forward engineering. Learn more about COBOL in this post.",
      "title": "Learnings from COBOL modernization in the real world"
    },
    {
      "arxiv_id": "2602.23172",
      "authors": [
        "Maximilian Luz",
        "Rohit Mohan",
        "Thomas Nürnberg",
        "Yakov Miron",
        "Daniele Cattaneo",
        "Abhinav Valada"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628472+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking",
          "url": "https://arxiv.org/abs/2602.23172"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking",
        "url": "https://arxiv.org/abs/2602.23172"
      },
      "published_at": "2026-02-26T16:34:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8547046663462944,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054704666346295
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23172",
      "summary": "Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a h",
      "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking"
    },
    {
      "arxiv_id": "2602.23167",
      "authors": [
        "Shuang Liang",
        "Yang Hua",
        "Linshan Jiang",
        "Peishen Yan",
        "Tao Song",
        "Bin Yao",
        "Haibing Guan"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.912138+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)",
          "url": "https://arxiv.org/abs/2602.23167"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)",
        "url": "https://arxiv.org/abs/2602.23167"
      },
      "published_at": "2026-02-26T16:31:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8545009068865617,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054500906886561
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23167",
      "summary": "In open Federated Learning (FL) environments where no central authority exists, ensuring collaboration fairness relies on decentralized reward settlement, yet the prohibitive cost of permissionless blockchains directly clashes with the high-frequency, iterative nature of model training. Existing solutions either compromise decentralization or suffer from scalability bottlenecks due to linear on-chain costs. To address this, we present SettleFL, a trustless and scalable reward settlement protocol",
      "title": "SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)"
    },
    {
      "arxiv_id": "2602.23163",
      "authors": [
        "Usman Anwar",
        "Julianna Piskorz",
        "David D. Baek",
        "David Africa",
        "Jim Weatherall",
        "Max Tegmark",
        "Christian Schroeder de Witt",
        "Mihaela van der Schaar",
        "David Krueger"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.IT",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628741+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
          "url": "https://arxiv.org/abs/2602.23163"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
        "url": "https://arxiv.org/abs/2602.23163"
      },
      "published_at": "2026-02-26T16:27:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8542645672189072,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054264567218908
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23163",
      "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these ap",
      "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring"
    },
    {
      "arxiv_id": "2602.23161",
      "authors": [
        "Junkai Lu",
        "Peng Chen",
        "Xingjian Wu",
        "Yang Shu",
        "Chenjuan Guo",
        "Christian S. Jensen",
        "Bin Yang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628975+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering",
          "url": "https://arxiv.org/abs/2602.23161"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering",
        "url": "https://arxiv.org/abs/2602.23161"
      },
      "published_at": "2026-02-26T16:20:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8538286476058378,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053828647605838
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23161",
      "summary": "Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitat",
      "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering"
    },
    {
      "arxiv_id": "2602.23153",
      "authors": [
        "Guofeng Mei",
        "Wei Lin",
        "Luigi Riz",
        "Yujiao Wu",
        "Yiming Wang",
        "Fabio Poiesi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629246+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
          "url": "https://arxiv.org/abs/2602.23153"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
        "url": "https://arxiv.org/abs/2602.23153"
      },
      "published_at": "2026-02-26T16:16:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8535905179668128,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053590517966812
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23153",
      "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We",
      "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model"
    },
    {
      "arxiv_id": "2602.23148",
      "authors": [
        "Nitin Gupta",
        "Vishal Pallagani",
        "John A. Aydin",
        "Biplav Srivastava"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629774+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
          "url": "https://arxiv.org/abs/2602.23148"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
        "url": "https://arxiv.org/abs/2602.23148"
      },
      "published_at": "2026-02-26T16:13:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8534561670705827,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053456167070582
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23148",
      "summary": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing",
      "title": "On Sample-Efficient Generalized Planning via Learned Transition Models"
    },
    {
      "arxiv_id": "2602.23141",
      "authors": [
        "Tao Liu",
        "Gang Wan",
        "Kan Ren",
        "Shibo Wen"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.356992+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors",
          "url": "https://arxiv.org/abs/2602.23141"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors",
        "url": "https://arxiv.org/abs/2602.23141"
      },
      "published_at": "2026-02-26T16:04:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8529130518861302,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052913051886131
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23141",
      "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on ",
      "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors"
    },
    {
      "arxiv_id": "2602.23136",
      "authors": [
        "Jayadev Billa"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630028+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
          "url": "https://arxiv.org/abs/2602.23136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
        "url": "https://arxiv.org/abs/2602.23136"
      },
      "published_at": "2026-02-26T15:52:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8522144233069153,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052214423306916
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23136",
      "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder ",
      "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs"
    },
    {
      "arxiv_id": "2602.23135",
      "authors": [
        "Tyler Bonnet",
        "Marek Rei"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630325+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding",
          "url": "https://arxiv.org/abs/2602.23135"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding",
        "url": "https://arxiv.org/abs/2602.23135"
      },
      "published_at": "2026-02-26T15:51:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8521582026821202,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05215820268212
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23135",
      "summary": "Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By ",
      "title": "DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding"
    },
    {
      "arxiv_id": "2602.23133",
      "authors": [
        "Xin Yuan",
        "Zhiyong Zhang",
        "Xin Xu",
        "Zheng Wang",
        "Chia-Wen Lin"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.357234+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification",
          "url": "https://arxiv.org/abs/2602.23133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification",
        "url": "https://arxiv.org/abs/2602.23133"
      },
      "published_at": "2026-02-26T15:50:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8520635236974187,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052063523697418
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23133",
      "summary": "With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Convent",
      "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification"
    },
    {
      "arxiv_id": "2602.23123",
      "authors": [
        "Keito Inoshita"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630598+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
          "url": "https://arxiv.org/abs/2602.23123"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
        "url": "https://arxiv.org/abs/2602.23123"
      },
      "published_at": "2026-02-26T15:37:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8512828233430255,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051282823343026
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23123",
      "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts ",
      "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection"
    },
    {
      "arxiv_id": "2602.23121",
      "authors": [
        "C. Seas",
        "G. Fitzpatrick",
        "J. A. Hamilton",
        "M. C. Carlisle"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630839+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning",
          "url": "https://arxiv.org/abs/2602.23121"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning",
        "url": "https://arxiv.org/abs/2602.23121"
      },
      "published_at": "2026-02-26T15:35:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8511783899585443,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051178389958544
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23121",
      "summary": "Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset created by Draper Labs using three static analyzers and the NIST SATE Juliet human-labeled dataset designed for testing static analyzers. In contrast with the work of Russell et al. on these datasets, we ",
      "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning"
    },
    {
      "arxiv_id": "2602.23120",
      "authors": [
        "Arian Sabaghi",
        "José Oramas"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.357458+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement",
          "url": "https://arxiv.org/abs/2602.23120"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement",
        "url": "https://arxiv.org/abs/2602.23120"
      },
      "published_at": "2026-02-26T15:33:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.851065104078032,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051065104078033
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23120",
      "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised ma",
      "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement"
    },
    {
      "arxiv_id": "2602.23117",
      "authors": [
        "Xiaosen Wang",
        "Zhijin Ge",
        "Bohan Liu",
        "Zheng Fang",
        "Fengfan Zhou",
        "Ruixuan Zhang",
        "Shaokang Wang",
        "Yuyang Luo"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.631110+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation",
          "url": "https://arxiv.org/abs/2602.23117"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation",
        "url": "https://arxiv.org/abs/2602.23117"
      },
      "published_at": "2026-02-26T15:30:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8509016049618101,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05090160496181
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23117",
      "summary": "Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to pot",
      "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation"
    },
    {
      "arxiv_id": "2602.23115",
      "authors": [
        "David Dirnfeld",
        "Fabien Delattre",
        "Pedro Miraldo",
        "Erik Learned-Miller"
      ],
      "categories": [
        "cs.CV",
        "cs.CG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.357905+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time",
          "url": "https://arxiv.org/abs/2602.23115"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time",
        "url": "https://arxiv.org/abs/2602.23115"
      },
      "published_at": "2026-02-26T15:27:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8507371526055433,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050737152605544
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23115",
      "summary": "Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a nove",
      "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time"
    },
    {
      "arxiv_id": "2602.23111",
      "authors": [
        "Yanyi Li",
        "Yimu Zhang",
        "Cong Fang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.914665+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
          "url": "https://arxiv.org/abs/2602.23111"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
        "url": "https://arxiv.org/abs/2602.23111"
      },
      "published_at": "2026-02-26T15:23:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.850486104036507,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050486104036507
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23111",
      "summary": "Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal",
      "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training"
    }
  ],
  "run_date": "2026-02-26",
  "run_id": "80fa95bc-bc61-4f30-a2ed-78ede4447947",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-26T23:59:59+00:00",
    "items_total": 310,
    "run_id": "80fa95bc-bc61-4f30-a2ed-78ede4447947-2026-02-26",
    "started_at": "2026-02-25T23:59:59+00:00",
    "stories_total": 309,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-24T09:02:59.820823+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 92622,
        "likes": 104,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
      },
      "published_at": "2026-02-26T03:37:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8097915129541714,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.609791512954171
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
      "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information,...",
      "title": "Qwen/Qwen3.5-397B-A17B-FP8"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-26T06:30:33.982240+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 0,
        "likes": 23,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-35b-a3b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-35B-A3B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-FP8"
      },
      "published_at": "2026-02-26T03:34:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.809635942955992,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.609635942955991
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-35b-a3b-fp8",
      "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Flash** is the hosted version corresponding to Qwen3.5-35B-A3B with more production features, e.g., 1M context length by default and official built-in tools. > For more information, please refer to the...",
      "title": "Qwen/Qwen3.5-35B-A3B-FP8"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-26T06:30:33.982590+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 0,
        "likes": 18,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-122b-a10b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-122B-A10B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-122B-A10B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B-FP8"
      },
      "published_at": "2026-02-26T03:26:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8091928260644278,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.609192826064428
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-122b-a10b-fp8",
      "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and...",
      "title": "Qwen/Qwen3.5-122B-A10B-FP8"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-26T06:30:33.982811+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 0,
        "likes": 18,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-27b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-27B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-27B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-27B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-27B-FP8"
      },
      "published_at": "2026-02-26T03:25:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8091019842840812,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.60910198428408
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-27b-fp8",
      "summary": "> This repository contains FP8-quantized model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > The quantization method is fine-grained fp8 quantization with block size of 128, and its performance metrics are nearly identical to those of the original model. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and...",
      "title": "Qwen/Qwen3.5-27B-FP8"
    },
    {
      "arxiv_id": "2602.23166",
      "authors": [
        "Zhaochen Su",
        "Jincheng Gao",
        "Hangyu Guo",
        "Zhenhua Liu",
        "Lueyang Zhang",
        "Xinyu Geng",
        "Shijue Huang",
        "Peng Xia",
        "Guanyu Jiang",
        "Cheng Wang",
        "Yue Zhang",
        "Yi R. Fung",
        "Junxian He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:52.356178+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
          "url": "https://arxiv.org/abs/2602.23166"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
        "url": "https://arxiv.org/abs/2602.23166"
      },
      "published_at": "2026-02-26T16:30:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8544643144599529,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.054464314459953
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23166",
      "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-ho",
      "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios"
    }
  ]
}