{
  "archive_dates": [
    "2026-02-27",
    "2026-02-26",
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-27T07:11:03.439680+00:00",
  "model_releases_by_entity": {},
  "papers": [
    {
      "arxiv_id": "2602.23008",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.633646+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
          "url": "https://arxiv.org/abs/2602.23008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
        "url": "https://arxiv.org/abs/2602.23008"
      },
      "published_at": "2026-02-26T13:50:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.93099381499659,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.93099381499659
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23008",
      "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.",
      "summary_zh": "探索仍然是使用 reinforcement learning 訓練的 large language model agent 的主要瓶頸。儘管先前的方法利用了 pretrained 知識，但它們在需要發現新穎狀態的環境中表現不佳。我們提出 Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2)，這是一個混合式 RL 框架，它利用記憶進行探索，並結合 on-policy 和 off-policy 更新，使 LLMs 在有記憶的情況下表現良好，同時確保在沒有記憶時也具備魯棒性。在 ScienceWorld 和 WebShop 上，EMPO^2 分別比 GRPO 提高了 128.6% 和 11.3%。此外，在 out-of-distribution 測試中，EMPO^2 展示了對新任務的卓越適應性，僅需少量有記憶的試驗且無需參數更新。這些結果突顯了 EMPO^2 作為構建更具探索性和泛化能力的 LLM-based agents 的有前景框架。",
      "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
      "title_zh": "透過混合式 On- and Off-Policy 優化實現探索性記憶增強型 LLM Agent"
    },
    {
      "arxiv_id": "2602.22766",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.225070+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
          "url": "https://arxiv.org/abs/2602.22766"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
        "url": "https://arxiv.org/abs/2602.22766"
      },
      "published_at": "2026-02-26T08:56:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9121428534279657,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.912142853427966
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22766",
      "summary": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
      "summary_zh": "Latent visual reasoning 旨在透過 Multimodal Large Language Models 的隱藏狀態進行沉思，以模仿人類的想像過程。儘管被認為是一種有前景的視覺推理範式，但其有效性的潛在機制仍不明確。為了揭示其功效的真正來源，我們利用 Causal Mediation Analysis 探究 latent reasoning 的有效性。我們將該過程建模為一個因果鏈：輸入作為 treatment，latent tokens 作為 mediator，最終答案作為 outcome。我們的發現揭示了兩個關鍵的脫節：(a) 輸入-潛在脫節 (Input-Latent Disconnect)：對輸入進行劇烈擾動，對 latent tokens 造成的改變卻微乎其微，這表明 latent tokens 未能有效關注輸入序列。(b) 潛在-答案脫節 (Latent-Answer Disconnect)：對 latent tokens 進行擾動，對最終答案的影響甚微，這表明 latent tokens 對 outcome 的因果效應有限。此外，廣泛的 probing analysis 揭示 latent tokens 編碼的視覺資訊有限且相似度高。因此，我們質疑 latent reasoning 的必要性，並提出一個直接的替代方案 CapImagine，它教導模型使用文本進行明確的想像。在以視覺為中心的 benchmarks 上的實驗表明，CapImagine 顯著優於複雜的 latent-space baselines，突顯了透過明確想像進行視覺推理的卓越潛力。",
      "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
      "title_zh": "想像力有助於視覺推理，但在 Latent Space 中尚未實現"
    },
    {
      "arxiv_id": "2602.23320",
      "authors": [
        "Tianjun Yao",
        "Yongqiang Chen",
        "Yujia Zheng",
        "Pan Li",
        "Zhiqiang Shen",
        "Kun Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.902393+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
          "url": "https://arxiv.org/abs/2602.23320"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
        "url": "https://arxiv.org/abs/2602.23320"
      },
      "published_at": "2026-02-26T18:28:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9490835614301165,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.78908356143012
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23320",
      "summary": "Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric mem",
      "summary_zh": "Self-reflection 使 language agents 能夠迭代地完善解決方案，但卻經常產生重複的輸出，從而限制了推理性能。最近的研究試圖透過各種方法解決這一限制，其中增加 reflective diversity 已顯示出前景。我們的實證分析揭示了 reflective diversity 與任務成功之間存在強烈的正相關，這進一步激發了對多樣化 reflection signals 的需求。我們介紹 ParamMem，這是一個參數化記憶",
      "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory",
      "title_zh": "ParamMem: 以參數化反射記憶增強語言 Agent"
    },
    {
      "arxiv_id": "2602.23306",
      "authors": [
        "Yiran Guan",
        "Sifan Tu",
        "Dingkang Liang",
        "Linghao Zhu",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.351002+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding",
          "url": "https://arxiv.org/abs/2602.23306"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding",
        "url": "https://arxiv.org/abs/2602.23306"
      },
      "published_at": "2026-02-26T18:10:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9479385418621452,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.78793854186215
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23306",
      "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computati",
      "summary_zh": "Omni-modal reasoning 對於智慧系統理解並從多樣化數據源中推斷資訊至關重要。儘管現有的 omni-modal large language models (OLLM) 在感知多種模態方面表現出色，但它們缺乏近期 large reasoning models (LRM) 所具備的複雜推理能力。然而，透過額外訓練來增強 OLLMs 的推理能力面臨著重大挑戰，包括對高品質數據的需求、任務特定的適應性以及龐大的計算",
      "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding",
      "title_zh": "ThinkOmni: 透過引導式解碼將文本推理提升至全模態情境"
    },
    {
      "arxiv_id": "2602.23225",
      "authors": [
        "Pengxiang Li",
        "Dilxat Muhtar",
        "Lu Yin",
        "Tianlong Chen",
        "Shiwei Liu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.627481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
          "url": "https://arxiv.org/abs/2602.23225"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
        "url": "https://arxiv.org/abs/2602.23225"
      },
      "published_at": "2026-02-26T17:04:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.943621239302886,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.78362123930289
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23225",
      "summary": "Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch be",
      "summary_zh": "Diffusion Language Models (DLMs) 通常被宣傳為能夠實現並行 token 生成，然而實用的快速 DLMs 卻經常收斂到從左到右的 autoregressive (AR)-like 解碼動態。相比之下，真正的 non-AR 生成是有前景的，因為它消除了 AR 的順序瓶頸，更好地利用並行硬體來減少同步/通信開銷並改善延遲隨輸出長度而變化的情況。我們認為 AR-like 解碼的一個主要驅動因素是",
      "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
      "title_zh": "為何 Diffusion Language Models 在真正的並行 (Non-Autoregressive) 解碼方面表現掙扎？"
    },
    {
      "arxiv_id": "2602.23201",
      "authors": [
        "Max S. Bennett",
        "Thomas P. Zollo",
        "Richard Zemel"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.910578+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language",
          "url": "https://arxiv.org/abs/2602.23201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language",
        "url": "https://arxiv.org/abs/2602.23201"
      },
      "published_at": "2026-02-26T16:50:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9426988201343826,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.782698820134385
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23201",
      "summary": "Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores ove",
      "summary_zh": "現代 machine learning 模型部署於多樣化、非穩態的環境中，必須持續適應新任務和不斷演進的知識。Continual fine-tuning 和 in-context learning 成本高昂且脆弱，而 neural memory 方法則承諾以最小的遺忘實現輕量級更新。然而，現有的 neural memory 模型通常假設單一固定目標和同質資訊流，導致使用者無法控制模型記住或忽略的內容。",
      "title": "Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language",
      "title_zh": "告訴我該學什麼：將神經記憶推廣至自然語言可控"
    },
    {
      "arxiv_id": "2602.23163",
      "authors": [
        "Usman Anwar",
        "Julianna Piskorz",
        "David D. Baek",
        "David Africa",
        "Jim Weatherall",
        "Max Tegmark",
        "Christian Schroeder de Witt",
        "Mihaela van der Schaar",
        "David Krueger"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.IT",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628741+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
          "url": "https://arxiv.org/abs/2602.23163"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
        "url": "https://arxiv.org/abs/2602.23163"
      },
      "published_at": "2026-02-26T16:27:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9411638212847852,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.781163821284785
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23163",
      "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these ap",
      "summary_zh": "Large language models 正開始展現 steganographic capabilities。此類能力可能允許 misaligned 模型規避監督機制。然而，目前缺乏檢測和量化此類行為的原則性方法。傳統的 steganography 定義及其相關檢測方法需要已知非 steganographic 訊號的參考分佈。對於 LLMs 中的 steganographic reasoning 情況，了解此類參考分佈是不可行的；這使得這些方法...",
      "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
      "title_zh": "一個基於決策理論的隱寫術形式化及其在 LLM 監測中的應用"
    },
    {
      "arxiv_id": "2602.23136",
      "authors": [
        "Jayadev Billa"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630028+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
          "url": "https://arxiv.org/abs/2602.23136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
        "url": "https://arxiv.org/abs/2602.23136"
      },
      "published_at": "2026-02-26T15:52:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9389051284249424,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.778905128424945
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23136",
      "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder ",
      "summary_zh": "Multimodal LLMs 可以處理語音和圖像，但它們無法聽到說話者的聲音或看到物體的紋理。我們證明這並非編碼失敗：說話者身份、情感和視覺屬性在每個 LLM 層中都能存活（在 linear probes 中高於機率 3-55 倍），然而移除 64-71% 的模態特定變異性卻能改善 decoder loss。Decoder 對這些方向沒有習得的用途；它們的存在是雜訊。我們將此形式化為一個 mismatched decoder 問題：一個 decoder...",
      "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
      "title_zh": "模態崩潰作為不匹配解碼：多模態 LLMs 的資訊理論極限"
    },
    {
      "arxiv_id": "2602.23093",
      "authors": [
        "Dhwanil M. Mori",
        "Neil F. Johnson"
      ],
      "categories": [
        "cs.AI",
        "cs.SI",
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.631404+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents",
          "url": "https://arxiv.org/abs/2602.23093"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents",
        "url": "https://arxiv.org/abs/2602.23093"
      },
      "published_at": "2026-02-26T15:12:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9362768369053118,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.77627683690531
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23093",
      "summary": "Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agent",
      "summary_zh": "近未來的基礎設施系統可能由自主 AI agents 控制，它們會重複請求存取有限的資源，例如能源、頻寬或計算能力。我們使用一個框架研究這種情境的簡化版本，其中 N 個 AI-agents 在每一輪獨立決定是否從固定容量 C 的系統中請求一個單位。一種 AI 版本的「Lord of the Flies」出現了，其中控制部落以其自身的集體特性和身份出現。該 LLM agent...",
      "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents",
      "title_zh": "三個 AI-agents 走進一家酒吧 . . . . 聰明 AI-Agents 中出現了《蒼蠅王》式的部落主義"
    },
    {
      "arxiv_id": "2602.22988",
      "authors": [
        "Bum Jun Kim",
        "Shohei Taniguchi",
        "Makoto Kawano",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.634132+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability",
          "url": "https://arxiv.org/abs/2602.22988"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability",
        "url": "https://arxiv.org/abs/2602.22988"
      },
      "published_at": "2026-02-26T13:33:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9298609336576249,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.769860933657625
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22988",
      "summary": "Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the ",
      "summary_zh": "Transformer 中的訓練分歧浪費了算力，然而從業者往往在昂貴的運行開始後才發現不穩定性。因此，他們需要在訓練開始前為 Transformer 預估失敗的可能性。我們對 Residual Koopman Spectral Profiling (RKSP) 的研究提供了這樣的估計。從初始化時的單次前向傳播中，RKSP 透過將 whitened dynamic mode decomposition 應用於層級殘差快照來提取 Koopman spectral features。我們核心的診斷工具，即...",
      "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability",
      "title_zh": "用於預測和預防 Transformer 訓練不穩定性的殘差 Koopman 頻譜分析"
    },
    {
      "arxiv_id": "2602.22727",
      "authors": [
        "Yangguang Lin",
        "Quan Fang",
        "Yufei Li",
        "Jiachen Sun",
        "Junyu Gao",
        "Jitao Sang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.373084+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.22727"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.22727"
      },
      "published_at": "2026-02-26T08:08:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9091095435021509,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.74910954350215
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22727",
      "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the",
      "summary_zh": "大型視覺語言模型 (LVLMs) 中的物件幻覺嚴重阻礙了其可靠部署。現有方法難以平衡效率和準確性：它們通常需要昂貴的 reference models 和多次 forward passes，或者應用靜態編輯，這可能抑制真實的視覺證據。為了解決這個問題，我們引入了 HulluEdit，這是一個單次通過、無需 reference 的干預框架。我們的核心創新是 orthogonal subspace editing：我們分解了 hidden states 的",
      "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models",
      "title_zh": "HulluEdit: 用於緩解大型視覺語言模型中幻覺的單次通過證據一致子空間編輯"
    },
    {
      "arxiv_id": "2602.22719",
      "authors": [
        "Vamshi Sunku Mohan",
        "Kaustubh Gupta",
        "Aneesha Das",
        "Chandan Singh"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.924052+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks",
          "url": "https://arxiv.org/abs/2602.22719"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks",
        "url": "https://arxiv.org/abs/2602.22719"
      },
      "published_at": "2026-02-26T07:46:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9077395470228296,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.74773954702283
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22719",
      "summary": "State-space models (SSMs) have emerged as an efficient strategy for building powerful language models, avoiding the quadratic complexity of computing attention in transformers. Despite their promise, the interpretability and steerability of modern SSMs remain relatively underexplored. We take a major step in this direction by identifying activation subspace bottlenecks in the Mamba family of SSM models using tools from mechanistic interpretability. We then introduce a test-time steering interven",
      "summary_zh": "State-space models (SSMs) 已成為構建強大語言模型的一種有效策略，避免了 Transformer 中計算 attention 的二次複雜性。儘管它們前景光明，但現代 SSMs 的可解釋性 (interpretability) 和可引導性 (steerability) 仍相對未被充分探索。我們透過使用 mechanistic interpretability 的工具，識別 Mamba 系列 SSM models 中的 activation subspace bottlenecks，朝此方向邁出了重要一步。然後，我們引入了一種 test-time steering interven",
      "title": "Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks",
      "title_zh": "透過 Activation Subspace Bottlenecks 解釋與引導 State-Space Models"
    },
    {
      "arxiv_id": "2602.22675",
      "authors": [
        "Qianben Chen",
        "Tianrui Qin",
        "King Zhu",
        "Qiexiang Wang",
        "Chengjun Yu",
        "Shu Xu",
        "Jiaqi Wu",
        "Jiayu Zhang",
        "Xinpeng Liu",
        "Xin Gui",
        "Jingyi Cao",
        "Piaohong Wang",
        "Dingfeng Shi",
        "He Zhu",
        "Tiannan Wang",
        "Yuqing Wang",
        "Maojia Song",
        "Tianyu Zheng",
        "Ge Zhang",
        "Jian Yang",
        "Jiaheng Liu",
        "Minghao Liu",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.227330+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization",
          "url": "https://arxiv.org/abs/2602.22675"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization",
        "url": "https://arxiv.org/abs/2602.22675"
      },
      "published_at": "2026-02-26T06:46:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9039641214058122,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.74396412140581
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22675",
      "summary": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \\emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient c",
      "summary_zh": "最近的深度研究 agents 主要透過擴展推理深度來提高性能，但在搜尋密集型場景中，這會導致高昂的 inference 成本和延遲。此外，跨異質研究環境的泛化能力仍然具有挑戰性。在這項工作中，我們提出了 \\emph{Search More, Think Less} (SMTL)，這是一個針對長程 agentic 搜尋的框架，旨在提高效率和泛化能力。SMTL 用並行證據獲取取代了序列推理，實現了高效的 c",
      "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization",
      "title_zh": "搜尋更多，思考更少：重新思考長程 Agentic 搜尋以提高效率和泛化能力"
    },
    {
      "arxiv_id": "2602.22790",
      "authors": [
        "Hyunwoo Kim",
        "Hanau Yi",
        "Jaehee Bae",
        "Yumin Kim"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:48.641084+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
          "url": "https://arxiv.org/abs/2602.22790"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
        "url": "https://arxiv.org/abs/2602.22790"
      },
      "published_at": "2026-02-26T09:23:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9138399179456002,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.5138399179456
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22790",
      "summary": "The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, inter",
      "summary_zh": "大型語言模型 (LLMs) 的快速發展已將 prompt engineering 從一種局部技藝轉變為系統級的治理挑戰。隨著模型跨代擴展和更新，prompt 行為對 instruction-following policies、alignment regimes 和 decoding strategies 的轉變變得敏感，我們將這種現象稱為 GPT-scale model drift。在這種情況下，表面層次的格式約定和 ad hoc 優化不足以確保穩定的、inter",
      "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
      "title_zh": "自然語言宣告式提示 (NLD-P)：一種在模型漂移下用於提示設計的模組化治理方法"
    },
    {
      "arxiv_id": "2602.23153",
      "authors": [
        "Guofeng Mei",
        "Wei Lin",
        "Luigi Riz",
        "Yujiao Wu",
        "Yiming Wang",
        "Fabio Poiesi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629246+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
          "url": "https://arxiv.org/abs/2602.23153"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
        "url": "https://arxiv.org/abs/2602.23153"
      },
      "published_at": "2026-02-26T16:16:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.87,
        "llm_relevance_score": 24.36,
        "recency_score": 0.9404212050108824,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 32.50042120501088
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23153",
      "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We",
      "summary_zh": "處理 3D 資料的 Large Multimodal Models (LMMs) 通常依賴於笨重、預訓練的 visual encoders 來提取幾何特徵。儘管最近的 2D LMMs 已開始為提高效率和可擴展性而淘汰此類 encoders，但由於點雲的無序和大規模性質，將此範式擴展到 3D 仍然具有挑戰性。這留下了一個關鍵的未解之謎：我們如何在沒有繁瑣 encoder 的情況下，設計一個能夠有效且高效地對無序 3D 資料進行 tokenization 的 LMM？我們",
      "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
      "title_zh": "高效的無編碼器基於傅立葉的 3D 大型多模態模型"
    },
    {
      "arxiv_id": "2602.23258",
      "authors": [
        "Yutong Wang",
        "Siyuan Xiong",
        "Xuebo Liu",
        "Wenkang Zhou",
        "Liang Ding",
        "Miao Zhang",
        "Min Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
          "url": "https://arxiv.org/abs/2602.23258"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
        "url": "https://arxiv.org/abs/2602.23258"
      },
      "published_at": "2026-02-26T17:31:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9453768701421661,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.94537687014217
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23258",
      "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting ",
      "summary_zh": "儘管 Multi-Agent Systems (MAS) 在複雜推理方面表現出色，但它們卻受到個體參與者產生錯誤資訊的連鎖影響所苦。現有解決方案通常訴諸於僵化的結構工程或昂貴的 fine-tuning，限制了它們的部署性和適應性。我們提出了 AgentDropoutV2，這是一個測試時的 rectify-or-reject pruning 框架，旨在無需重新訓練即可動態優化 MAS 的資訊流。我們的方法作用如同一個主動式防火牆，攔截",
      "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
      "title_zh": "AgentDropoutV2: 透過測試時的糾正或拒絕剪枝優化 Multi-Agent Systems (MAS) 中的資訊流"
    },
    {
      "arxiv_id": "2602.23239",
      "authors": [
        "Radha Sarma"
      ],
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626219+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
          "url": "https://arxiv.org/abs/2602.23239"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
        "url": "https://arxiv.org/abs/2602.23239"
      },
      "published_at": "2026-02-26T17:16:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9443641964962155,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.944364196496217
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23239",
      "summary": "AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain",
      "summary_zh": "AI 系統正越來越多地部署在高風險情境中——醫療診斷、法律研究、金融分析——假設它們可以受規範約束。本文證明，對於基於優化的系統，特別是透過 Reinforcement Learning from Human Feedback (RLHF) 訓練的 Large Language Models 而言，此假設在形式上是無效的。我們確立了真正的能動性需要兩個必要且共同充分的架構條件：維持特定狀態的能力",
      "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
      "title_zh": "能動性與架構限制：為何基於優化的系統無法對規範做出回應"
    },
    {
      "arxiv_id": "2602.23200",
      "authors": [
        "Sayed Mohammadreza Tayaranian Hosseini",
        "Amir Ardakani",
        "Warren J. Gross"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.910799+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models",
          "url": "https://arxiv.org/abs/2602.23200"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models",
        "url": "https://arxiv.org/abs/2602.23200"
      },
      "published_at": "2026-02-26T16:50:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.942681362910466,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.942681362910466
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23200",
      "summary": "Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without s",
      "summary_zh": "在解碼過程中降低 Large Language Models (LLMs) 的硬體足跡對於高效的長序列生成至關重要。一個關鍵瓶頸是 key-value (KV) cache，其大小隨序列長度而擴展，並且容易主導模型的記憶體足跡。先前的研究提出了量化方法，這些方法專注於壓縮 KV cache 同時保持其資訊。我們引入了 InnerQ，這是一種硬體感知 KV-cache 量化方案，可在不犧牲效能的情況下降低解碼延遲",
      "title": "InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models",
      "title_zh": "InnerQ: 針對 Large Language Models 的硬體感知免調校 KV Cache 量化"
    },
    {
      "arxiv_id": "2602.22983",
      "authors": [
        "Xun Huang",
        "Simeng Qin",
        "Xiaoshuang Jia",
        "Ranjie Duan",
        "Huanqian Yan",
        "Zhitao Zeng",
        "Fei Yang",
        "Yang Liu",
        "Xiaojun Jia"
      ],
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.634386+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
          "url": "https://arxiv.org/abs/2602.22983"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
        "url": "https://arxiv.org/abs/2602.22983"
      },
      "published_at": "2026-02-26T13:25:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9293552440847138,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.929355244084714
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22983",
      "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this",
      "summary_zh": "隨著 Large Language Models (LLMs) 越來越廣泛地使用，其安全風險日益受到關注。現有研究表明，LLMs 極易受到 jailbreak 攻擊，且其有效性因語言語境而異。本文探討了古典中文在 jailbreak 攻擊中的作用。由於其簡潔性和晦澀性，古典中文可以部分繞過現有的安全限制，暴露 LLMs 中顯著的漏洞。基於這一觀察，本文",
      "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
      "title_zh": "晦澀但有效：透過仿生搜尋進行古典中文 Jailbreak Prompt 優化"
    },
    {
      "arxiv_id": "2602.22963",
      "authors": [
        "Zehao Li",
        "Hongwei Yu",
        "Hao Jiang",
        "Qiang Sheng",
        "Yilong Xu",
        "Baolong Bi",
        "Yang Li",
        "Zhenlong Yuan",
        "Yujun Cai",
        "Zhaoqi Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.635852+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.22963"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.22963"
      },
      "published_at": "2026-02-26T13:00:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9277388847881088,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.92773888478811
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22963",
      "summary": "Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an ite",
      "summary_zh": "Multimodal Large Language Models (MLLMs) 透過統一的多模態推理，顯著推進了影片不實資訊偵測，但它們常依賴固定深度的推論，並過度信任內部產生的假設，尤其是在關鍵證據稀疏、碎片化或需要外部驗證的情境中。為了解決這些限制，我們提出了 FactGuard，這是一個用於影片不實資訊偵測的 agentic 框架，它將驗證公式化為一個迭代的",
      "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
      "title_zh": "FactGuard: 透過 Reinforcement Learning 進行 Agentic 影片不實資訊偵測"
    },
    {
      "arxiv_id": "2602.22953",
      "authors": [
        "Elron Bandel",
        "Asaf Yehudai",
        "Lilach Eden",
        "Yehoshua Sagron",
        "Yotam Perlitz",
        "Elad Venezian",
        "Natalia Razinkov",
        "Natan Ergas",
        "Shlomit Shachor Ifergan",
        "Segev Shlomov",
        "Michal Jacovi",
        "Leshem Choshen",
        "Liat Ein-Dor",
        "Yoav Katz",
        "Michal Shmueli-Scheuer"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.636361+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "General Agent Evaluation",
          "url": "https://arxiv.org/abs/2602.22953"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "General Agent Evaluation",
        "url": "https://arxiv.org/abs/2602.22953"
      },
      "published_at": "2026-02-26T12:48:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9269349781698458,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.926934978169847
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22953",
      "summary": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair ev",
      "summary_zh": "通用型代理 (general-purpose agents)——即能在陌生環境中執行任務且無需特定領域工程的系統——的潛力仍未實現。現有代理主要為專用型，儘管諸如 OpenAI SDK Agent 和 Claude Code 等新興實作暗示了更廣泛的能力，但尚未對其通用性能進行系統性評估。當前的代理基準 (agentic benchmarks) 假設了特定領域的整合，以排除公平評估的方式編碼任務資訊",
      "title": "General Agent Evaluation",
      "title_zh": "通用代理評估"
    },
    {
      "arxiv_id": "2602.22918",
      "authors": [
        "Jonathan Steinberg",
        "Oren Gal"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.222353+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.22918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.22918"
      },
      "published_at": "2026-02-26T12:06:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9242353566786523,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.924235356678654
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22918",
      "summary": "Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integrat",
      "summary_zh": "視覺語言模型 (VLMs) 能從圖像中讀取文本，但這些光學字元識別 (OCR) 資訊是如何進入語言處理流程的呢？我們透過因果干預 (causal interventions) 探討了 OCR 的路由機制，跨越三種架構系列 (Qwen3-VL, Phi-4, InternVL3.5)。藉由計算原始圖像與文本繪入版本之間的啟用差異 (activation differences)，我們識別出架構特定的 OCR 瓶頸，其主要位置取決於視覺語言整合",
      "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models",
      "title_zh": "視覺轉化為文本之處：定位視覺語言模型中的 OCR 路由瓶頸"
    },
    {
      "arxiv_id": "2602.22911",
      "authors": [
        "Hung-Hsuan Chen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.637057+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion",
          "url": "https://arxiv.org/abs/2602.22911"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion",
        "url": "https://arxiv.org/abs/2602.22911"
      },
      "published_at": "2026-02-26T11:55:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9235541981756861,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.923554198175687
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22911",
      "summary": "Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) ",
      "summary_zh": "低秩適應 (LoRA) 在參數高效微調 (PEFT) 領域佔主導地位。然而，它在複雜推理任務中面臨著關鍵的「線性天花板」：僅僅增加秩 (rank) 會因內在的線性約束而導致報酬遞減。我們引入了 NoRA (Non-linear Rank Adaptation)，這是一種權重級別的平行適配器，它注入了 SiLU 門控 (gating) 和結構性 dropout，以引導流形擴張 (manifold expansion)。在 SlimOrca 基準測試中，NoRA 打破了這一線性障礙：NoRA 在秩為 64 時表現卓越 (PPL 3.89)",
      "title": "NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion",
      "title_zh": "NoRA：透過流形擴張打破低秩適應的線性天花板"
    },
    {
      "arxiv_id": "2602.22868",
      "authors": [
        "Yushi Ye",
        "Feng Hong",
        "Huangjie Zheng",
        "Xu Chen",
        "Zhiyong Chen",
        "Yanfeng Wang",
        "Jiangchao Yao"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.223182+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference",
          "url": "https://arxiv.org/abs/2602.22868"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference",
        "url": "https://arxiv.org/abs/2602.22868"
      },
      "published_at": "2026-02-26T11:08:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9205298177269562,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.92052981772696
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22868",
      "summary": "Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Conti",
      "summary_zh": "擴散式大型語言模型 (DLLMs) 承諾實現快速的非自回歸推理，但在平行解碼中面臨嚴重的品質-速度權衡。這源於「組合矛盾」(combinatorial contradiction) 現象，即平行詞元形成語義不一致的組合。我們透過將連續表示 (continuous representations) 整合到離散解碼過程中來解決這個問題，因為它們保留了豐富的詞元間依賴性 (inter-position dependency)。我們提出了 ReMix (Rejection Mixing)，這是一個引入了一種新穎的 Conti",
      "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference",
      "title_zh": "Rejection Mixing：用於高效 DLLM 推理的遮罩詞元快速語義傳播"
    },
    {
      "arxiv_id": "2602.22859",
      "authors": [
        "Hongrui Jia",
        "Chaoya Jiang",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.369314+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
          "url": "https://arxiv.org/abs/2602.22859"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
        "url": "https://arxiv.org/abs/2602.22859"
      },
      "published_at": "2026-02-26T10:53:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9196203917136929,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.919620391713693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22859",
      "summary": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a ",
      "summary_zh": "隨著大型多模態模型 (LMMs) 的規模擴大和強化學習 (RL) 方法的成熟，LMMs 在複雜推理和決策方面取得了顯著進展。然而，訓練仍依賴靜態數據和固定範式 (fixed recipes)，這使得診斷能力盲點或提供動態、有針對性的強化變得困難。受到測試驅動的錯誤暴露 (error exposure) 和基於回饋的糾正優於重複練習的發現所啟發，我們提出了診斷驅動的漸進式演化 (Diagnostic-driven Progressive Evolution, DPE)，這是一種",
      "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
      "title_zh": "從盲點到增益：大型多模態模型的診斷驅動迭代訓練"
    },
    {
      "arxiv_id": "2602.22839",
      "authors": [
        "Hao Zheng",
        "Guozhao Mo",
        "Xinru Yan",
        "Qianhao Yuan",
        "Wenkai Zhang",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.639013+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation",
          "url": "https://arxiv.org/abs/2602.22839"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation",
        "url": "https://arxiv.org/abs/2602.22839"
      },
      "published_at": "2026-02-26T10:26:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.917888157604824,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.917888157604825
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22839",
      "summary": "Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide ",
      "summary_zh": "簡報生成需要深入的內容研究、連貫的視覺設計以及基於觀察的疊代精煉。然而，現有的 presentation agents 通常依賴預定義的 workflows 和固定 templates。為了解決這個問題，我們提出了 DeepPresenter，這是一個 agentic framework，它能夠適應多樣化的 user intents，實現有效的 feedback-driven refinement，並且能夠超越既定 scripted pipeline 的限制。具體來說，DeepPresenter 能夠自主規劃、渲染和修改中間 slide",
      "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation",
      "title_zh": "DeepPresenter：用於代理式簡報生成的環境基礎反思"
    },
    {
      "arxiv_id": "2602.22817",
      "authors": [
        "Shuo He",
        "Lang Feng",
        "Qi Wei",
        "Xin Cheng",
        "Lei Feng",
        "Bo An"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.640038+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks",
          "url": "https://arxiv.org/abs/2602.22817"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks",
        "url": "https://arxiv.org/abs/2602.22817"
      },
      "published_at": "2026-02-26T09:58:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9160648183833976,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.916064818383397
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22817",
      "summary": "Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, whe",
      "summary_zh": "基於群組的 reinforcement learning (RL)，例如 GRPO，已提升 large language models 在 long-horizon agentic tasks 上的能力。為了實現更 fine-grained 的 policy updates，近期研究越來越傾向於 stepwise group-based policy optimization，這種方法獨立處理 rollout trajectory 中的每個步驟，同時使用一個 memory module 來保留歷史 context。然而，我們發現在估計 stepwise relative advantages 時存在一個關鍵問題，即 context inconsistency，當",
      "title": "Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks",
      "title_zh": "Hierarchy-of-Groups Policy Optimization 用於長週期代理式任務"
    },
    {
      "arxiv_id": "2602.22808",
      "authors": [
        "Shiqian Su",
        "Sen Xing",
        "Xuan Dong",
        "Muyan Zhong",
        "Bin Wang",
        "Xizhou Zhu",
        "Yuntao Chen",
        "Wenhai Wang",
        "Yue Deng",
        "Pengxiang Zhu",
        "Ziyuan Liu",
        "Tiantong Li",
        "Jiaheng Yu",
        "Zhe Chen",
        "Lidong Bing",
        "Jifeng Dai"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.640600+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
          "url": "https://arxiv.org/abs/2602.22808"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
        "url": "https://arxiv.org/abs/2602.22808"
      },
      "published_at": "2026-02-26T09:45:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.915231832811309,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.91523183281131
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22808",
      "summary": "Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commerci",
      "summary_zh": "儘管 large language models (LLMs) 已取得顯著進展，但獨立 LLMs 在處理需要與 external tools 和 dynamic environments 互動的真實世界複雜任務時，其能力已開始趨於平穩。儘管近期的 agent frameworks 旨在透過 tool integration 和 external interaction 增強模型 autonomy，但它們仍然存在 naive workflows、unstable performance、對各種 benchmarks 和 tasks 支援有限以及過度依賴昂貴 commerci",
      "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
      "title_zh": "MiroFlow：邁向用於通用深度研究任務的高性能且穩健的開源 Agent Framework"
    },
    {
      "arxiv_id": "2602.22769",
      "authors": [
        "Yujie Zhao",
        "Boqin Yuan",
        "Junbo Huang",
        "Haocheng Yuan",
        "Zhongming Yu",
        "Haozhou Xu",
        "Lanxiang Hu",
        "Abhilash Shankarampeta",
        "Zimeng Huang",
        "Wentao Ni",
        "Yuandong Tian",
        "Jishen Zhao"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.642308+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications",
          "url": "https://arxiv.org/abs/2602.22769"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications",
        "url": "https://arxiv.org/abs/2602.22769"
      },
      "published_at": "2026-02-26T08:59:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9123413505512267,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.912341350551227
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22769",
      "summary": "Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed ",
      "summary_zh": "Large Language Models (LLMs) 被部署為越來越複雜應用中的 autonomous agents，其中啟用 long-horizon memory 對於實現強大性能至關重要。然而，在實際應用與 current evaluation standards for agent memory 之間存在顯著差距：現有 benchmarks 主要關注以對話為中心的 human-agent interactions。實際上，agent memory 由連續的 agent-environment interactions stream 組成，這些 interactions 主要由",
      "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications",
      "title_zh": "AMA-Bench：評估代理式應用程式的 Long-Horizon 記憶"
    },
    {
      "arxiv_id": "2602.22755",
      "authors": [
        "Abhay Sheshadri",
        "Aidan Ewart",
        "Kai Fronsdal",
        "Isha Gupta",
        "Samuel R. Bowman",
        "Sara Price",
        "Samuel Marks",
        "Rowan Wang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.225551+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors",
          "url": "https://arxiv.org/abs/2602.22755"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors",
        "url": "https://arxiv.org/abs/2602.22755"
      },
      "published_at": "2026-02-26T08:43:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9113028865839934,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.911302886583996
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22755",
      "summary": "We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to c",
      "summary_zh": "我們引入了 AuditBench，這是一個 alignment auditing benchmark。AuditBench 包含 56 個植入了 hidden behaviors 的 language models。每個模型都具有 14 種令人擔憂的行為之一——例如 sycophantic deference、opposition to AI regulation 或 secret geopolitical loyalties——當直接詢問時，模型並不會承認這些行為。AuditBench 模型具有高度多樣性——有些很 subtle，有些則很 overt，我們使用不同的 training techniques，既用於植入 behaviors，也用於訓練模型不",
      "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors",
      "title_zh": "AuditBench：評估具有隱藏行為模型的對齊審計技術"
    },
    {
      "arxiv_id": "2602.22718",
      "authors": [
        "Rui Wei",
        "Hanfei Yu",
        "Shubham Jain",
        "Yogarajan Sivakumar",
        "Devesh Tiwari",
        "Jian Li",
        "Seung-Jong Park",
        "Hao Wang"
      ],
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.644739+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "RLHFless: Serverless Computing for Efficient RLHF",
          "url": "https://arxiv.org/abs/2602.22718"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "RLHFless: Serverless Computing for Efficient RLHF",
        "url": "https://arxiv.org/abs/2602.22718"
      },
      "published_at": "2026-02-26T07:45:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9076712590006444,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.907671259000644
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22718",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumptio",
      "summary_zh": "來自人類回饋的強化學習 (RLHF) 已廣泛應用於大型語言模型 (LLM) 的訓練後階段，以使模型輸出與人類偏好保持一致。最近的模型，例如 DeepSeek-R1，也已顯示 RLHF 在改進 LLM 於複雜任務上的推理能力方面的潛力。在 RL 中，推論 (inference) 與訓練 (training) 並存，在整個工作流程中產生動態的資源需求。與傳統 RL 相比，RLHF 由於模型規模擴大和資源消耗，進一步挑戰了訓練效率。",
      "title": "RLHFless: Serverless Computing for Efficient RLHF",
      "title_zh": "RLHFless：用於高效 RLHF 的無伺服器運算"
    },
    {
      "arxiv_id": "2602.22700",
      "authors": [
        "Yanpei Guo",
        "Wenjie Qu",
        "Linyu Wu",
        "Shengfang Zhai",
        "Lionel Z. Wang",
        "Ming Xu",
        "Yue Liu",
        "Binhang Yuan",
        "Dawn Song",
        "Jiaheng Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.645782+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
          "url": "https://arxiv.org/abs/2602.22700"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
        "url": "https://arxiv.org/abs/2602.22700"
      },
      "published_at": "2026-02-26T07:21:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9061230264073495,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.90612302640735
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22700",
      "summary": "Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model substitution, quantization abuse, and token overbilling-without trusted hardware or access to model internals. IMMACULATE selectively audits a small fraction of requests using verifiable computation, ach",
      "summary_zh": "商用大型語言模型通常以 black-box API 服務的形式部署，要求使用者信任提供者能正確執行推論並誠實報告 token 使用量。我們提出了 IMMACULATE，這是一個實用的審計框架，它無需信任硬體或存取模型內部，即可檢測出經濟驅動的偏差，例如模型替換 (model substitution)、量化濫用 (quantization abuse) 和 token 超額計費 (token overbilling)。IMMACULATE 利用可驗證計算選擇性地審計一小部分請求，達到...",
      "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
      "title_zh": "IMMACULATE：一種透過可驗證計算實現的實用 LLM 審計框架"
    },
    {
      "arxiv_id": "2602.22681",
      "authors": [
        "Shuchen Zhu",
        "Rizhen Hu",
        "Mingze Wang",
        "Mou Sun",
        "Xue Wang",
        "Kun Yuan",
        "Zaiwen Wen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.924918+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
          "url": "https://arxiv.org/abs/2602.22681"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
        "url": "https://arxiv.org/abs/2602.22681"
      },
      "published_at": "2026-02-26T06:54:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9044832127559915,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.90448321275599
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22681",
      "summary": "Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address t",
      "summary_zh": "預訓練大型語言模型 (LLM) 需要龐大的計算資源，因此優化器 (optimizer) 的效率至關重要。優化格局 (optimization landscape) 呈現高度異向性 (anisotropic)，損失減少主要由沿著平坦方向的進展所驅動。儘管基於矩陣的優化器，例如 Muon 和 SOAP，利用精細的曲率資訊來超越 AdamW，但它們的更新傾向於等向性 (isotropy)——沿著平坦方向相對保守，但在尖銳方向上卻可能過於激進。為了解決這個問題...",
      "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
      "title_zh": "透過平坦方向動態增強加速 LLM 預訓練"
    },
    {
      "arxiv_id": "2602.22680",
      "authors": [
        "Yue Xu",
        "Qian Chen",
        "Zizhan Ma",
        "Dongrui Liu",
        "Wenxuan Wang",
        "Xiting Wang",
        "Li Xiong",
        "Wenjie Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.646780+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
          "url": "https://arxiv.org/abs/2602.22680"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
        "url": "https://arxiv.org/abs/2602.22680"
      },
      "published_at": "2026-02-26T06:52:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.904347131769681,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.904347131769683
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22680",
      "summary": "Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surfac",
      "summary_zh": "大型語言模型 (LLM) 已催生出能夠推理、規劃並與工具及環境互動以完成複雜任務的代理 (agents)。隨著這些代理在更長的互動時間範圍內運作，它們的有效性越來越依賴於使行為適應個別使用者並在時間上保持連續性，從而產生了個人化的 LLM 驅動代理。在這種長期、依賴使用者的設定中，個人化滲透到整個決策流程，而非僅限於表面...",
      "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
      "title_zh": "邁向個人化 LLM 驅動的代理：基礎、評估與未來方向"
    },
    {
      "arxiv_id": "2602.22661",
      "authors": [
        "Zhanhui Zhou",
        "Lingjie Chen",
        "Hanghang Tong",
        "Dawn Song"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.647371+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "dLLM: Simple Diffusion Language Modeling",
          "url": "https://arxiv.org/abs/2602.22661"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "dLLM: Simple Diffusion Language Modeling",
        "url": "https://arxiv.org/abs/2602.22661"
      },
      "published_at": "2026-02-26T06:26:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9026687407734955,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 31.652668740773496
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22661",
      "summary": "Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\n  To address this gap, we intr",
      "summary_zh": "儘管擴散語言模型 (DLMs) 正在快速發展，但許多近期模型都趨向於一組共享的組件。然而，這些組件分散於臨時的研究代碼庫中，或缺乏透明的實作，使其難以重現或擴展。隨著該領域的加速發展，對於一個能夠標準化這些通用組件，同時又足夠靈活以支持新方法和架構的統一框架，存在著明確的需求。為了解決這一空白，我們引...",
      "title": "dLLM: Simple Diffusion Language Modeling",
      "title_zh": "dLLM：簡單擴散語言模型"
    },
    {
      "arxiv_id": "2602.23164",
      "authors": [
        "Aviral Chawla",
        "Galen Hall",
        "Juniper Lovato"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-27T06:24:49.912408+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
          "url": "https://arxiv.org/abs/2602.23164"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
        "url": "https://arxiv.org/abs/2602.23164"
      },
      "published_at": "2026-02-26T16:28:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9412128415103677,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 31.141212841510367
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23164",
      "summary": "Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting \"world models\". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, ",
      "summary_zh": "基礎模型（Foundation models）必須處理多個生成過程（generative processes），然而 mechanistic interpretability 大多是孤立地研究其能力；目前尚不清楚單一 Transformer 如何組織多個可能相互衝突的「世界模型」（world models）。先前對 Othello 棋類神經網路的實驗測試了「世界模型」的學習能力，但僅限於單一遊戲和單一套規則。我們引入了 MetaOthello，這是一個受控的 Othello 變體套件，它們共享語法但規則或 tokenizations 不同，",
      "title": "MetaOthello: A Controlled Study of Multiple World Models in Transformers",
      "title_zh": "MetaOthello：Transformer 中多個「世界模型」的受控研究"
    },
    {
      "arxiv_id": "2602.23116",
      "authors": [
        "Junghyun Lee",
        "Minju Hong",
        "Kwang-Sung Jun",
        "Chulhee Yun",
        "Se-Young Yun"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.914183+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Regularized Online RLHF with Generalized Bilinear Preferences",
          "url": "https://arxiv.org/abs/2602.23116"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Regularized Online RLHF with Generalized Bilinear Preferences",
        "url": "https://arxiv.org/abs/2602.23116"
      },
      "published_at": "2026-02-26T15:27:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9372819230186457,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.0,
        "total_score": 30.937281923018645
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23116",
      "summary": "We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is p",
      "summary_zh": "我們考慮了具有一般偏好的上下文線上 RLHF 問題，其目標是識別 Nash Equilibrium。我們採用了廣義雙線性偏好模型 (GBPM) 來透過低秩、斜對稱矩陣捕捉潛在的非遞移偏好。我們研究了使用任何強凸正規化器（其中 $η^{-1}$ 是正規化強度）的一般偏好學習，這超越了先前僅限於逆向 KL-regularization 的工作。我們分析的核心是 p",
      "title": "Regularized Online RLHF with Generalized Bilinear Preferences",
      "title_zh": "帶廣義雙線性偏好之正規化線上 RLHF"
    },
    {
      "arxiv_id": "2602.22913",
      "authors": [
        "Yang Yu",
        "Lei Kou",
        "Huaikuan Yi",
        "Bin Chen",
        "Yayu Cao",
        "Lei Shen",
        "Chao Zhang",
        "Bing Wang",
        "Xiaoyi Zeng"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:49.919092+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress",
          "url": "https://arxiv.org/abs/2602.22913"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress",
        "url": "https://arxiv.org/abs/2602.22913"
      },
      "published_at": "2026-02-26T12:00:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9238973879629134,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 30.873897387962913
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22913",
      "summary": "With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender a",
      "summary_zh": "隨著大型語言模型（Large Language Models）的快速發展，生成式推薦（generative recommendation）正逐漸重塑推薦系統（recommender systems）的範式。然而，大多數現有方法仍局限於互動驅動的下一項預測範式（interaction-driven next-item prediction paradigm），未能快速適應不斷變化的趨勢，或解決真實世界場景中多樣化的推薦任務以及業務特定要求。為此，我們提出了 SIGMA，一個語義基礎指令驅動生成式多任務推薦系統",
      "title": "SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress",
      "title_zh": "SIGMA：在 AliExpress 上的語義基礎指令驅動生成式多任務推薦系統"
    },
    {
      "arxiv_id": "2602.23351",
      "authors": [
        "Amita Kamath",
        "Jack Hessel",
        "Khyathi Chandu",
        "Jena D. Hwang",
        "Kai-Wei Chang",
        "Ranjay Krishna"
      ],
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.217729+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
          "url": "https://arxiv.org/abs/2602.23351"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
        "url": "https://arxiv.org/abs/2602.23351"
      },
      "published_at": "2026-02-26T18:54:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9508009334050317,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.550800933405036
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23351",
      "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP,",
      "summary_zh": "視覺語言模型（Vision-Language Models, VLMs）中缺乏推理能力一直是研究討論的焦點。我們認為這種現象源於其訓練數據中的「報告偏誤」（reporting bias）。也就是說，人們在預設情況下，對視覺內容的溝通會省略某些類型推理所需的隱性資訊；例如，「at the game today!」比「a photo of 37 people standing behind a field」更可能是常見的圖片說明。我們調查了流行 VLMs OpenCLIP 背後的數據，",
      "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
      "title_zh": "規模無法克服語用學：報告偏誤對視覺語言推理的影響"
    },
    {
      "arxiv_id": "2602.23248",
      "authors": [
        "Yegon Kim",
        "Juho Lee"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.625731+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
          "url": "https://arxiv.org/abs/2602.23248"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
        "url": "https://arxiv.org/abs/2602.23248"
      },
      "published_at": "2026-02-26T17:25:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9449600771067316,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.544960077106733
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23248",
      "summary": "As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a \"translator\" model that turns a fixed solver m",
      "summary_zh": "隨著大型語言模型（large language models）的能力日益增強，其輸出能被能力較弱的系統輕鬆檢查變得至關重要。證明者-驗證者博弈（Prover-verifier games）可用於提高模型輸出的可檢查性，但與僅以最大化正確性為目標的基準模型相比，其準確性會下降——這種現象被稱為「可讀性稅」（legibility tax）。我們提出了一種解決方案，透過將正確性與可檢查性條件解耦，並轉而訓練一個「翻譯器」（translator）模型，該模型能將固定的求解器 m",
      "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
      "title_zh": "透過解耦的證明者-驗證者博弈來緩解可讀性稅"
    },
    {
      "arxiv_id": "2602.23232",
      "authors": [
        "Aishik Sanyal"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626965+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
          "url": "https://arxiv.org/abs/2602.23232"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
        "url": "https://arxiv.org/abs/2602.23232"
      },
      "published_at": "2026-02-26T17:11:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9440265155217977,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.5440265155218
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23232",
      "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we op",
      "summary_zh": "基於指標的機器意識方法建議，應透過架構檢查和因果干預來支持，並在不同任務中對與機制連結的證據進行三角測量。受 Humphrey 的 ipsundrum 假說啟發，我們實現了 ReCoN-Ipsundrum，這是一個可檢視的代理，它將 ReCoN 狀態機擴展為一個在感官顯著性 Ns 上運行的循環持久性迴路，並帶有一個可選的 affect proxy 來報告 valence/arousal。在固定參數消融實驗 (ReCoN, Ipsundrum, Ipsundrum+affect) 中，我們...",
      "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
      "title_zh": "ReCoN-Ipsundrum：一種帶有情感耦合控制和與機制連結的意識指標檢測的可檢視循環持久性迴路代理"
    },
    {
      "arxiv_id": "2602.23197",
      "authors": [
        "Chungpa Lee",
        "Jy-yong Sohn",
        "Kangwook Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.911026+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models",
          "url": "https://arxiv.org/abs/2602.23197"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models",
        "url": "https://arxiv.org/abs/2602.23197"
      },
      "published_at": "2026-02-26T16:49:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9425929906752064,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.54259299067521
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23197",
      "summary": "Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention mo",
      "summary_zh": "基於 Transformer 的大型語言模型展現出 in-context learning 能力，使其能夠透過帶有演示的 few-shot prompting 適應下游任務。在實踐中，這類模型通常會進行 fine-tuning 以提高在下游任務上的 zero-shot 性能，使其無需範例即可解決任務，從而降低 inference 成本。然而，fine-tuning 可能會降低 in-context learning 能力，限制了 fine-tuned 模型在 fine-tuning 期間未曾見過的任務上的性能。使用 linear attention 模...",
      "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models",
      "title_zh": "在不忘記 In-Context Learning 的情況下進行 Fine-Tuning：對 Linear Attention Models 的理論分析"
    },
    {
      "arxiv_id": "2602.23184",
      "authors": [
        "Sara Rosenthal",
        "Yannis Katsis",
        "Vraj Shah",
        "Lihong He",
        "Lucian Popa",
        "Marina Danilevsky"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.219820+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations",
          "url": "https://arxiv.org/abs/2602.23184"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations",
        "url": "https://arxiv.org/abs/2602.23184"
      },
      "published_at": "2026-02-26T16:41:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9420716540564541,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.542071654056457
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23184",
      "summary": "We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com",
      "summary_zh": "我們提出了 MTRAG-UN，這是一個用於探索多輪 retrieval augmented generation（大型語言模型的流行應用）中開放挑戰的基準。我們發布了一個包含 666 個任務的基準，這些任務涵蓋 6 個領域的 2,800 多個對話輪次，並附帶相應的語料庫。我們的實驗表明，retrieval 和 generation 模型在處理包含 UNanswerable、UNderspecified 和 NONstandalone 問題以及 UNclear 回應的對話時仍然面臨困難。我們的基準可在 https://github.com 獲取。",
      "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations",
      "title_zh": "MTRAG-UN：一個用於多輪 RAG 對話開放挑戰的基準"
    },
    {
      "arxiv_id": "2602.23057",
      "authors": [
        "Jeongin Bae",
        "Baeseong Park",
        "Gunho Park",
        "Minsub Kim",
        "Joonhyung Lee",
        "Junhee Yoo",
        "Sunghyeon Woo",
        "Jiwon Ryu",
        "Se Jung Kwon",
        "Dongsoo Lee"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.632916+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention",
          "url": "https://arxiv.org/abs/2602.23057"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention",
        "url": "https://arxiv.org/abs/2602.23057"
      },
      "published_at": "2026-02-26T14:42:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9343174766799965,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.53431747668
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23057",
      "summary": "Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention rew",
      "summary_zh": "Transformer attention 通常使用 softmax normalization 實現，這強制 attention 權重具有單位和歸一化。儘管在許多情況下有效，但此約束可能會限制控制 attention 幅度的靈活性，並可能導致訓練期間 attention 模式過於集中或不穩定。先前的研究探索了 attention sinks 或 gating mechanisms 等修改，但這些方法對 attention rew... 的控制僅限於有限或間接。",
      "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention",
      "title_zh": "Affine-Scaled Attention：邁向靈活穩定的 Transformer Attention"
    },
    {
      "arxiv_id": "2602.23036",
      "authors": [
        "Jaehong Cho",
        "Hyunmin Choi",
        "Guseul Heo",
        "Jongse Park"
      ],
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.633420+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure",
          "url": "https://arxiv.org/abs/2602.23036"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure",
        "url": "https://arxiv.org/abs/2602.23036"
      },
      "published_at": "2026-02-26T14:22:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9330217941415689,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.533021794141572
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23036",
      "summary": "Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or so",
      "summary_zh": "大型語言模型 (LLM) 的服務基礎設施正在經歷向異構和解耦的轉變。現代部署越來越多地整合了多樣化的 accelerators 和 near-memory processing 技術，引入了顯著的硬體異構性，而系統軟體則越來越多地將計算、記憶體和模型組件分佈在分散式資源上，以提高可擴展性和效率。因此，LLM serving 的性能不再由硬體或軟體... 決定。",
      "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure",
      "title_zh": "LLMServingSim 2.0：一個用於異構和解耦 LLM Serving 基礎設施的統一模擬器"
    },
    {
      "arxiv_id": "2602.22932",
      "authors": [
        "Wenhui Tan",
        "Xiaoyi Yu",
        "Jiaze Li",
        "Yijing Chen",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Ruihua Song",
        "Jian Luan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.367509+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
          "url": "https://arxiv.org/abs/2602.22932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
        "url": "https://arxiv.org/abs/2602.22932"
      },
      "published_at": "2026-02-26T12:24:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9254074391962379,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.52540743919624
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22932",
      "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queri",
      "summary_zh": "有效理解長影片仍然是 multimodal large language models (MLLMs) 面臨的一個基本挑戰。在本文中，我們提出了 MLLM-Sampler Joint Evolution (MSJoE)，這是一個新穎的框架，它聯合演化 MLLM 和一個輕量級的關鍵幀 Sampler，以實現高效的長影片理解。MSJoE 基於一個關鍵假設：對於回答影片的每個問題，只有一小部分關鍵幀真正具有資訊量。具體來說，MSJoE 首先推導出幾個 queri",
      "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding",
      "title_zh": "MSJoE: 聯合演化 MLLM 和 Sampler 以實現高效長影片理解"
    },
    {
      "arxiv_id": "2602.22871",
      "authors": [
        "Roy Miles",
        "Aysim Toker",
        "Andreea-Maria Oncescu",
        "Songcen Xu",
        "Jiankang Deng",
        "Ismail Elezi"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.638008+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching",
          "url": "https://arxiv.org/abs/2602.22871"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching",
        "url": "https://arxiv.org/abs/2602.22871"
      },
      "published_at": "2026-02-26T11:08:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9205596501951838,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.520559650195185
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22871",
      "summary": "Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sa",
      "summary_zh": "使用 large language models 進行推理通常受益於生成多個 chains-of-thought，但現有的聚合策略通常是 trajectory-level 的（例如，選擇最佳的 trace 或對最終答案進行投票），這會丟棄來自部分或「幾乎正確」嘗試的有用中間工作。我們提出了 Stitching Noisy Diffusion Thoughts，這是一個 self-consistency 框架，它將廉價的 diffusion-sampled 推理轉化為一個可重複使用的 step-level 候選池。給定一個問題，我們 (i) sa",
      "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching",
      "title_zh": "透過 Reward-Guided Stitching 對 Diffusion Language Models 進行測試時擴展"
    },
    {
      "arxiv_id": "2602.22831",
      "authors": [
        "Phil Blandfort",
        "Tushar Karayil",
        "Urja Pawar",
        "Robert Graham",
        "Alex McKenzie",
        "Dmitrii Krasheninnikov"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.639295+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Moral Preferences of LLMs Under Directed Contextual Influence",
          "url": "https://arxiv.org/abs/2602.22831"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Moral Preferences of LLMs Under Directed Contextual Influence",
        "url": "https://arxiv.org/abs/2602.22831"
      },
      "published_at": "2026-02-26T10:17:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9173242121547841,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.517324212154787
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22831",
      "summary": "Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, w",
      "summary_zh": "LLMs 的道德基準通常使用 context-free prompts，隱式地假設偏好是穩定的。然而，在部署中，prompts 通常包含語境信號，例如用戶請求、社會規範線索等，這些可能會引導決策。我們研究定向語境影響如何在 trolley-problem-style 道德分類情境中重塑決策。我們引入了一個用於 trolley-problem-style 道德分類中定向語境影響的初步評估 harness：對於每個 demographic factor，w",
      "title": "Moral Preferences of LLMs Under Directed Contextual Influence",
      "title_zh": "在定向語境影響下的 LLMs 道德偏好"
    },
    {
      "arxiv_id": "2602.22812",
      "authors": [
        "Hiroki Matsutani",
        "Naoki Matsuda",
        "Naoto Sugiura"
      ],
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.922144+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching",
          "url": "https://arxiv.org/abs/2602.22812"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching",
        "url": "https://arxiv.org/abs/2602.22812"
      },
      "published_at": "2026-02-26T09:53:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9157542148118799,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.51575421481188
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22812",
      "summary": "Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a",
      "summary_zh": "由於資源受限的 edge devices 上的 local LLM 推理會造成嚴重的性能瓶頸，本文提出分散式 prompt caching，透過在多個低階 edge devices 之間協同共享中間處理狀態來提升推理性能。為了充分利用 prompt 相似性，我們的分散式 caching 機制也支援 partial matching。由於這種方法會引入與無線網路上的狀態共享相關的 communication overhead，我們引入了一個",
      "title": "Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching",
      "title_zh": "透過分散式 Prompt 快取加速資源受限邊緣設備上的 Local LLMs"
    },
    {
      "arxiv_id": "2602.22775",
      "authors": [
        "Joydeep Chandra",
        "Satyam Kumar Navneet",
        "Yong Zhang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.641822+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation",
          "url": "https://arxiv.org/abs/2602.22775"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation",
        "url": "https://arxiv.org/abs/2602.22775"
      },
      "published_at": "2026-02-26T09:11:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9131051223816176,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.51310512238162
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22775",
      "summary": "As mental health chatbots proliferate to address the global treatment gap, a critical question emerges: How do we design for relational safety the quality of interaction patterns that unfold across conversations rather than the correctness of individual responses? Current safety evaluations assess single-turn crisis responses, missing the therapeutic dynamics that determine whether chatbots help or harm over time. We introduce TherapyProbe, a design probe methodology that generates actionable de",
      "summary_zh": "隨著心理健康 chatbots 普及以解決全球治療差距，一個關鍵問題浮現：我們如何為 relational safety（在對話中展開的互動模式品質，而非單個回應的正確性）進行設計？當前的安全評估側重於單輪危機回應，卻忽略了決定 chatbots 隨時間推移是幫助還是傷害的 therapeutic dynamics。我們引入了 TherapyProbe，這是一個 design probe 方法論，它生成可操作的 de",
      "title": "TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation",
      "title_zh": "TherapyProbe：透過對抗性模擬生成心理健康聊天機器人關係安全設計知識"
    },
    {
      "arxiv_id": "2602.22771",
      "authors": [
        "Yusuke Watanabe",
        "Yohei Kobashi",
        "Takeshi Kojima",
        "Yusuke Iwasawa",
        "Yasushi Okuno",
        "Yutaka Matsuo"
      ],
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.642049+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making",
          "url": "https://arxiv.org/abs/2602.22771"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making",
        "url": "https://arxiv.org/abs/2602.22771"
      },
      "published_at": "2026-02-26T09:03:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9126053764068727,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.512605376406874
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22771",
      "summary": "Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinabi",
      "summary_zh": "臨床決策常在資訊不完整的情況下進行。臨床專家必須判斷現有資訊是否足以做出判斷，因為過早下結論和不必要的棄權都可能危及患者安全。為了評估大型語言模型 (LLMs) 的這項能力，我們開發了 ClinDet-Bench，這是一個基於臨床評分系統的基準測試，它將資訊不完整的情境分解為可確定和不可確定的條件。確定判斷的可行性",
      "title": "ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making",
      "title_zh": "ClinDet-Bench：超越棄權，評估 LLMs 在臨床決策中判斷的可確定性"
    },
    {
      "arxiv_id": "2602.22765",
      "authors": [
        "Zhe Yang",
        "Yudong Wang",
        "Rang Li",
        "Zhifang Sui"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.225333+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Towards Better RL Training Data Utilization via Second-Order Rollout",
          "url": "https://arxiv.org/abs/2602.22765"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Towards Better RL Training Data Utilization via Second-Order Rollout",
        "url": "https://arxiv.org/abs/2602.22765"
      },
      "published_at": "2026-02-26T08:55:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9121164607874264,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.51211646078743
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22765",
      "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating m",
      "summary_zh": "強化學習 (RL) 賦予了大型語言模型 (LLMs) 強大的推理能力，但 vanilla RL 主要透過僅使用 first-order rollout（為一個問題生成多個回應）進行訓練來提高生成能力，我們認為這種方法由於忽略了批評能力訓練而未能充分利用訓練資料的潛力。為了解決這個問題，我們進一步引入了 second-order rollout 的概念（生成多個",
      "title": "Towards Better RL Training Data Utilization via Second-Order Rollout",
      "title_zh": "透過 Second-Order Rollout 實現更好的 RL 訓練資料利用"
    },
    {
      "arxiv_id": "2602.22751",
      "authors": [
        "Qiannian Zhao",
        "Chen Yang",
        "Jinhao Jing",
        "Yunke Zhang",
        "Xuhui Ren",
        "Lu Yu",
        "Shijie Zhang",
        "Hongzhi Yin"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.643285+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning",
          "url": "https://arxiv.org/abs/2602.22751"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning",
        "url": "https://arxiv.org/abs/2602.22751"
      },
      "published_at": "2026-02-26T08:40:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9111119970627355,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.511111997062738
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22751",
      "summary": "Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently",
      "summary_zh": "大型推理模型 (LRMs) 已成為解決複雜現實世界任務的強大範例。實際上，這些模型主要透過帶有可驗證獎勵的強化學習 (RLVR) 進行訓練，然而大多數現有的僅基於結果的 RLVR 流程幾乎完全依賴二元正確性訊號，並在很大程度上忽略了模型的內在不確定性。我們將這種差異稱為不確定性獎勵失配 (uncertainty-reward mismatch)，在此情況下，高不確定性解決方案和低不確定性解決方案被同等對待",
      "title": "Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning",
      "title_zh": "知其所知：用於可驗證 RL 推理的後設認知熵校準"
    },
    {
      "arxiv_id": "2602.22716",
      "authors": [
        "Guanting Ye",
        "Qiyan Zhao",
        "Wenhao Yu",
        "Liangyu Yuan",
        "Mingkai Li",
        "Xiaofeng Zhang",
        "Jianmin Ji",
        "Yanyong Zhang",
        "Qing Jiang",
        "Ka-Veng Yuen"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.645012+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
          "url": "https://arxiv.org/abs/2602.22716"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
        "url": "https://arxiv.org/abs/2602.22716"
      },
      "published_at": "2026-02-26T07:42:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9074590736270792,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.507459073627082
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22716",
      "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering t",
      "summary_zh": "建立在大型語言模型 (LLMs) 之上的 3D 大型視覺語言模型 (3D LVLMs) 在各種多模態任務中取得了顯著進展。然而，其繼承的依賴於位置的建模機制，即 Rotary Position Embedding (RoPE)，對於 3D 多模態理解仍然不夠理想。vanilla RoPE 公式在編碼 3D tokens 時未能保留基本的三維空間結構，其相對距離計算忽略了角度依賴性，阻礙了",
      "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
      "title_zh": "SoPE：基於球座標的位置嵌入，用於增強 3D LVLMs 的空間感知"
    },
    {
      "arxiv_id": "2602.22697",
      "authors": [
        "Ning Gao",
        "Wei Zhang",
        "Yuqin Dai",
        "Ling Shi",
        "Ziyin Wang",
        "Yujie Wang",
        "Wei He",
        "Jinpeng Wang",
        "Chaozheng Wang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.646300+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue",
          "url": "https://arxiv.org/abs/2602.22697"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue",
        "url": "https://arxiv.org/abs/2602.22697"
      },
      "published_at": "2026-02-26T07:19:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.8,
        "llm_relevance_score": 22.400000000000002,
        "recency_score": 0.9060548599938307,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 30.50605485999383
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22697",
      "summary": "The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric",
      "summary_zh": "大型語言模型 (LLMs) 的快速發展加速了從對話式聊天機器人到通用代理的轉變。然而，有效平衡同理心溝通與預算感知決策仍然是一個開放的挑戰。由於現有方法未能捕捉這些複雜的策略權衡，我們提出了 InteractCS-RL，這是一個將任務導向對話重新構建為多粒度強化學習過程的框架。具體而言，我們首先建立了一個以使用者為中心的",
      "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue",
      "title_zh": "強化現實世界服務代理：在任務導向對話中平衡效用與成本"
    },
    {
      "arxiv_id": "2602.23266",
      "authors": [
        "Siyuan Liu",
        "Jiahui Xu",
        "Feng Jiang",
        "Kuang Wang",
        "Zefeng Zhao",
        "Chu-Ren Huang",
        "Jinghang Gu",
        "Changqing Yin",
        "Haizhou Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.218768+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems",
          "url": "https://arxiv.org/abs/2602.23266"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems",
        "url": "https://arxiv.org/abs/2602.23266"
      },
      "published_at": "2026-02-26T17:39:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.945916457865204,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.985916457865205
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23266",
      "summary": "Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon th",
      "summary_zh": "對於級聯式語音對話系統而言，實現類人響應速度是一個關鍵且具挑戰性的目標。傳統的 ASR-LLM-TTS 管道遵循嚴格的序列範式，需要在語音合成開始之前完成全部轉錄和完整推理，這導致了高響應延遲。我們提出了 Discourse-Aware Dual-Track Streaming Response (DDTSR) 框架，這是一種低延遲架構，能夠實現「邊聽邊思考」和「邊說邊思考」。DDTSR 是基於...",
      "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems",
      "title_zh": "語篇感知雙軌串流回應：用於低延遲語音對話系統"
    },
    {
      "arxiv_id": "2602.23148",
      "authors": [
        "Nitin Gupta",
        "Vishal Pallagani",
        "John A. Aydin",
        "Biplav Srivastava"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629774+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
          "url": "https://arxiv.org/abs/2602.23148"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
        "url": "https://arxiv.org/abs/2602.23148"
      },
      "published_at": "2026-02-26T16:13:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.9402731873969707,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.98027318739697
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23148",
      "summary": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing",
      "summary_zh": "廣義規劃研究的是如何構建解決策略，使其能夠泛化到共享共同領域模型的規劃問題家族，該模型由轉移函數 $γ: S \\times A \\rightarrow S$ 形式化定義。經典方法透過符號抽象和對 $γ$ 的顯式推理來實現這種泛化。相比之下，近期基於 Transformer 的規劃器，例如 PlanGPT 和 Plansformer，則主要將廣義規劃視為直接的 action-sequence 預測，繞過了...",
      "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
      "title_zh": "論基於學習轉移模型的樣本高效廣義規劃"
    },
    {
      "arxiv_id": "2602.22663",
      "authors": [
        "Wenxuan Song",
        "Jiayi Chen",
        "Xiaoquan Sun",
        "Huashuo Lei",
        "Yikai Qin",
        "Wei Zhao",
        "Pengxiang Ding",
        "Han Zhao",
        "Tongxin Wang",
        "Pengxu Hou",
        "Zhide Zhong",
        "Haodong Yan",
        "Donglin Wang",
        "Jun Ma",
        "Haoang Li"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:55.012002+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
          "url": "https://arxiv.org/abs/2602.22663"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
        "url": "https://arxiv.org/abs/2602.22663"
      },
      "published_at": "2026-02-26T06:27:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.78,
        "llm_relevance_score": 21.84,
        "recency_score": 0.902767998001511,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.94276799800151
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22663",
      "summary": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect",
      "summary_zh": "Vision-Language-Action (VLA) 模型已成為一種通用型機器人代理。然而，現有的 VLA 受到過度參數規模、高昂的 pre-training 要求以及對多樣化實體適用性有限的阻礙。為了提高 VLA 的實用性，我們提出了一個綜合基準和一個改進的基準模型。首先，我們提出了 CEBench，這是一個新的基準，涵蓋了模擬和真實世界中考慮 domain randomization 的多樣化實體。我們收集了...",
      "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
      "title_zh": "重新思考 Vision-language-action 模型的實用性：一個綜合基準和改進的基準模型"
    },
    {
      "arxiv_id": "2602.23353",
      "authors": [
        "Simon Roschmann",
        "Paul Krzakala",
        "Sonia Mazelet",
        "Quentin Bouniot",
        "Zeynep Akata"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.621549+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
          "url": "https://arxiv.org/abs/2602.23353"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
        "url": "https://arxiv.org/abs/2602.23353"
      },
      "published_at": "2026-02-26T18:55:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9508669635403157,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.150866963540317
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23353",
      "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setti",
      "summary_zh": "柏拉圖式表徵假設 (Platonic Representation Hypothesis) 認為，在不同模態上訓練的神經網絡會趨向於一個共享的統計世界模型。最近的工作透過使用輕量級對齊層來對齊 frozen pretrained vision 和 language 模型，以利用這種收斂性，但通常依賴於 contrastive losses 和數百萬個配對樣本。在這項工作中，我們探討是否可以在顯著減少監督的情況下實現有意義的對齊。我們引入了一種半監督設定...",
      "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
      "title_zh": "SOTAlign：透過最佳傳輸實現單模態視覺和語言模型的半監督對齊"
    },
    {
      "arxiv_id": "2602.23342",
      "authors": [
        "Weijian Chen",
        "Haotian Liu",
        "Yangshen Deng",
        "Long Xiang",
        "Liang Huang",
        "Gezi Li",
        "Bo Tang"
      ],
      "categories": [
        "cs.DB",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:57.012750+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
          "url": "https://arxiv.org/abs/2602.23342"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
        "url": "https://arxiv.org/abs/2602.23342"
      },
      "published_at": "2026-02-26T18:48:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9504301493386187,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.150430149338618
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23342",
      "summary": "On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-bas",
      "summary_zh": "磁碟上的基於圖的近似最近鄰搜索 (ANNS) 對於大規模、高維向量檢索至關重要，但其性能普遍被認為受到高昂 I/O 成本的限制。有趣的是，我們觀察到隨著向量數據維度（例如，數百或數千維）的增加，磁碟上的基於圖的索引系統的性能是 compute-bound，而非 I/O-bound。這一見解揭示了一個重要的優化機會：現有的磁碟上基於圖的...",
      "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
      "title_zh": "AlayaLaser：用於大規模高維向量相似性搜索的有效索引佈局和搜索策略"
    },
    {
      "arxiv_id": "2602.23330",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "categories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622766+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
          "url": "https://arxiv.org/abs/2602.23330"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
        "url": "https://arxiv.org/abs/2602.23330"
      },
      "published_at": "2026-02-26T18:37:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9497120979332915,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.14971209793329
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23330",
      "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis",
      "summary_zh": "大型語言模型 (LLMs) 的進步加速了自主金融交易系統的發展。雖然主流方法部署了模仿分析師和經理角色的 multi-agent systems，但它們往往依賴於抽象指令，忽略了真實世界工作流程的複雜性，這可能導致推理性能下降和決策透明度降低。因此，我們提出了一個 multi-agent LLM 交易框架，該框架明確地分解了投資分析",
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "title_zh": "邁向專家投資團隊：具備細粒度交易任務的多代理 LLM 系統"
    },
    {
      "arxiv_id": "2602.23286",
      "authors": [
        "Sungho Park",
        "Jueun Kim",
        "Wook-Shin Han"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.624324+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
          "url": "https://arxiv.org/abs/2602.23286"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
        "url": "https://arxiv.org/abs/2602.23286"
      },
      "published_at": "2026-02-26T17:59:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9472256618422913,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.147225661842292
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23286",
      "summary": "Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end cons",
      "summary_zh": "真實世界的 Table-Text question answering (QA) 任務需要模型能夠跨長文本和源表格進行推理，執行多跳並執行聚合等複雜操作。然而，現有基準測試規模小，手動整理——因此容易出錯——並且包含淺層問題，很少需要兩跳以上或涉及聚合、分組或其他可在自然語言查詢中表達的進階分析操作。我們提出了 SPARTA，一個端到端的綜合",
      "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
      "title_zh": "SPARTA：基於文本和表格的樹狀多跳問答的可擴展且有原則的基準測試"
    },
    {
      "arxiv_id": "2602.23271",
      "authors": [
        "Haotian Zhai",
        "Elias Stengel-Eskin",
        "Pratik Patil",
        "Liu Leqi"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.624990+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Evaluating Stochasticity in Deep Research Agents",
          "url": "https://arxiv.org/abs/2602.23271"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Evaluating Stochasticity in Deep Research Agents",
        "url": "https://arxiv.org/abs/2602.23271"
      },
      "published_at": "2026-02-26T17:46:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9463610554672156,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.146361055467217
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23271",
      "summary": "Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability i",
      "summary_zh": "Deep Research Agents (DRAs) 是有前景的代理系統，它們收集和綜合資訊以支援金融決策、醫療分析和科學發現等領域的研究。儘管研究品質近期有所改善（例如，當真實情況可用時的結果準確性），DRA 系統設計卻經常忽略了真實世界部署的關鍵障礙：stochasticity。在相同查詢下，DRAs 的重複執行可能表現出顯著的變異性",
      "title": "Evaluating Stochasticity in Deep Research Agents",
      "title_zh": "評估深度研究代理中的隨機性"
    },
    {
      "arxiv_id": "2602.23235",
      "authors": [
        "Zhou Xu",
        "Bowen Zhou",
        "Qi Wang",
        "Shuwen Feng",
        "Jingyu Xiao"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.626467+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
          "url": "https://arxiv.org/abs/2602.23235"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
        "url": "https://arxiv.org/abs/2602.23235"
      },
      "published_at": "2026-02-26T17:12:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.944127042215749,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.14412704221575
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23235",
      "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid in",
      "summary_zh": "純視覺 GUI agents 提供了通用的互動能力，但由於高解析度螢幕截圖和歷史軌跡中固有的巨大時空冗餘，它們面臨嚴重的效率瓶頸。我們發現現有壓縮範式存在兩個關鍵的錯位：時間不匹配，其中統一的歷史編碼偏離了 agent 的「fading memory」注意力模式；以及空間拓撲衝突，其中非結構化剪枝損害了網格結構",
      "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
      "title_zh": "用於高效高解析度 GUI Agents 的時空 Token 剪枝"
    },
    {
      "arxiv_id": "2602.23228",
      "authors": [
        "Yizhi Li",
        "Xiaohan Chen",
        "Miao Jiang",
        "Wentao Tang",
        "Gaoang Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.627241+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
          "url": "https://arxiv.org/abs/2602.23228"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
        "url": "https://arxiv.org/abs/2602.23228"
      },
      "published_at": "2026-02-26T17:08:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9438298638163279,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.14382986381633
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23228",
      "summary": "With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts",
      "summary_zh": "隨著數位娛樂的爆炸式增長，自動影片摘要對於內容索引、個人化推薦和高效媒體歸檔等應用變得不可或缺。對於長篇影片（例如電影和電視劇）的自動劇情摘要生成，對現有的 Vision-Language Models (VLMs) 提出了重大挑戰。儘管這些通用模型擅長單圖圖像描述，但在長時間情境下卻經常出現關鍵性失敗",
      "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
      "title_zh": "MovieTeller：具備 ID Consistent Progressive Abstraction 的工具增強電影劇情摘要"
    },
    {
      "arxiv_id": "2602.23193",
      "authors": [
        "Elzo Brito dos Santos Filho"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.628235+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
          "url": "https://arxiv.org/abs/2602.23193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
        "url": "https://arxiv.org/abs/2602.23193"
      },
      "published_at": "2026-02-26T16:45:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9423791859616436,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.142379185961644
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23193",
      "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which sepa",
      "summary_zh": "基於 Large Language Models (LLMs) 的自主代理已從反應式助理發展為能夠規劃、透過工具執行動作並迭代環境觀察的系統。然而，它們仍然容易受到結構性限制的影響：缺乏原生狀態、長期上下文退化，以及機率性生成與確定性執行要求之間的差距。本文提出了 ESAA (Event Sourcing for Autonomous Agents) 架構，該架構分離了",
      "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
      "title_zh": "ESAA: 基於 LLM 的軟體工程中用於自主代理的 Event Sourcing"
    },
    {
      "arxiv_id": "2602.23123",
      "authors": [
        "Keito Inoshita"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.630598+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
          "url": "https://arxiv.org/abs/2602.23123"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
        "url": "https://arxiv.org/abs/2602.23123"
      },
      "published_at": "2026-02-26T15:37:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9378787623370013,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.137878762337003
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23123",
      "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts ",
      "summary_zh": "在注意力經濟中，聳人聽聞的內容讓消費者接觸過度的情感刺激，阻礙了冷靜的決策。本研究提出了一種基於 Multi-Agent LLM 的 Emotional deToxification (MALLET) 系統，這是一個多代理資訊淨化系統，由四個代理組成：Emotion Analysis、Emotion Adjustment、Balance Monitoring 和 Personal Guide。Emotion Analysis Agent 使用一個 6-emotion BERT classifier 來量化刺激強度，而 Emotion Adjustment Agent 則重寫文本",
      "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
      "title_zh": "透過個性化強度控制實現基於 Multi-Agent Large Language Model 的情感排毒以保護消費者"
    },
    {
      "arxiv_id": "2602.23111",
      "authors": [
        "Yanyi Li",
        "Yimu Zhang",
        "Cong Fang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.914665+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
          "url": "https://arxiv.org/abs/2602.23111"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
        "url": "https://arxiv.org/abs/2602.23111"
      },
      "published_at": "2026-02-26T15:23:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9370009975135631,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.137000997513564
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23111",
      "summary": "Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal",
      "summary_zh": "在大型批次 LLM 訓練中，activations 已成為主要的記憶體瓶頸。然而，現有的壓縮方法未能利用 activations 的光譜結構，導致收斂速度慢或壓縮效果有限。為了解決這個問題，我們連結了演算法的快速收斂與 subspace projection 的要求之間的關係，並表明有效的壓縮應能產生對原始 activation 的低變異度無偏估計。我們提出了 Principal",
      "title": "PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training",
      "title_zh": "PRAC: 用於 LLM 活化壓縮和記憶體高效訓練的主隨機子空間"
    },
    {
      "arxiv_id": "2602.23088",
      "authors": [
        "Matthew Sutton",
        "Katrin Amunts",
        "Timo Dickscheid",
        "Christian Schiffer"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.358809+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy",
          "url": "https://arxiv.org/abs/2602.23088"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy",
        "url": "https://arxiv.org/abs/2602.23088"
      },
      "published_at": "2026-02-26T15:10:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9361608931339819,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.136160893133983
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23088",
      "summary": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which",
      "summary_zh": "Foundation models 越來越有潛力支持互動式、代理式工作流程，這些工作流程可協助研究人員分析和解釋影像資料。此類工作流程通常需要將 vision 與 language 耦合，以提供自然語言介面。然而，在許多研究和臨床環境中，學習這種耦合所需的配對影像-文本資料稀缺且難以取得。其中一種情況是對細胞體染色的人體腦組織切片的顯微分析，這",
      "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy",
      "title_zh": "詞彙中的細胞結構：用於人腦顯微鏡學的弱監督視覺語言建模"
    },
    {
      "arxiv_id": "2602.23079",
      "authors": [
        "Boyang Zhang",
        "Yang Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.915372+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent",
          "url": "https://arxiv.org/abs/2602.23079"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent",
        "url": "https://arxiv.org/abs/2602.23079"
      },
      "published_at": "2026-02-26T15:05:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9358077323905323,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.135807732390532
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23079",
      "summary": "The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with ",
      "summary_zh": "大型語言模型 (LLMs) 的快速發展賦予了強大的 authorship inference 能力，這引發了人們對新聞文章等文本資料中意外 deanonymization 風險日益增長的擔憂。在這項工作中，我們引入了一個 LLM agent，旨在透過結構化、可解釋的 pipeline 來評估和緩解此類風險。我們框架的核心是所提出的 $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) 方法，該方法將定量的 stylometric features 與",
      "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent",
      "title_zh": "使用文體學輔助的 LLM 代理評估去匿名化風險"
    }
  ],
  "radar": [
    {
      "arxiv_id": "2602.23050",
      "authors": [
        "Alexej Klushyn",
        "Richard Kurle",
        "Maximilian Soelch",
        "Botond Cseke",
        "Patrick van der Smagt"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.915985+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Latent Matters: Learning Deep State-Space Models",
          "url": "https://arxiv.org/abs/2602.23050"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Latent Matters: Learning Deep State-Space Models",
        "url": "https://arxiv.org/abs/2602.23050"
      },
      "published_at": "2026-02-26T14:35:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.933894750425175,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.133894750425178
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23050",
      "summary": "Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by maximising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with cla",
      "summary_zh": "深度狀態空間模型 (DSSMs) 透過學習觀測序列資料的潛在動力學，實現時間序列預測。它們通常透過最大化 evidence lower bound 進行訓練。然而，正如我們所示，這並不能確保模型實際學習到潛在動力學。因此，我們提出一個約束優化框架，作為訓練 DSSMs 的通用方法。在此基礎上，我們引入了 extended Kalman VAE (EKVAE)，它結合了 amortised variational inference 和 cla",
      "title": "Latent Matters: Learning Deep State-Space Models",
      "title_zh": "潛在變數的重要性：學習深度狀態空間模型"
    },
    {
      "arxiv_id": "2602.23005",
      "authors": [
        "Man Zhang",
        "Tao Yue",
        "Yihua He"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:56.345312+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Managing Uncertainty in LLM-based Multi-Agent System Operation",
          "url": "https://arxiv.org/abs/2602.23005"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Managing Uncertainty in LLM-based Multi-Agent System Operation",
        "url": "https://arxiv.org/abs/2602.23005"
      },
      "published_at": "2026-02-26T13:49:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9308849899045293,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.13088498990453
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23005",
      "summary": "Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering conce",
      "summary_zh": "將基於 LLM 的多代理軟體系統應用於安全關鍵領域，例如生命週期心臟超音波檢查，會引入無法單靠提高模型準確性來解決的系統級風險。在系統操作期間，除了個別 LLM 行為之外，不確定性會透過代理協調、資料管道、human-in-the-loop 互動和 runtime 控制邏輯傳播。然而，現有研究大多將不確定性視為模型層級的問題，而非一等公民的軟體工程概念",
      "title": "Managing Uncertainty in LLM-based Multi-Agent System Operation",
      "title_zh": "在基於 LLM 的多代理系統操作中管理不確定性"
    },
    {
      "arxiv_id": "2602.22942",
      "authors": [
        "Hongchao Du",
        "Shangyu Wu",
        "Qiao Li",
        "Riwei Pan",
        "Jinheng Li",
        "Youcheng Sun",
        "Chun Jason Xue"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:54.067367+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "ClawMobile: Rethinking Smartphone-Native Agentic Systems",
          "url": "https://arxiv.org/abs/2602.22942"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "ClawMobile: Rethinking Smartphone-Native Agentic Systems",
        "url": "https://arxiv.org/abs/2602.22942"
      },
      "published_at": "2026-02-26T12:34:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9260931801350053,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.126093180135005
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22942",
      "summary": "Smartphones represent a uniquely challenging environment for agentic systems. Unlike cloud or desktop settings, mobile devices combine constrained execution contexts, fragmented control interfaces, and rapidly changing application states. As large language models (LLMs) evolve from conversational assistants to action-oriented agents, achieving reliable smartphone-native autonomy requires rethinking how reasoning and control are composed.\n  We introduce ClawMobile as a concrete exploration of thi",
      "summary_zh": "智慧型手機對 agentic systems 來說是一個獨特且充滿挑戰的環境。與 cloud 或桌面環境不同，行動裝置結合了受限的 execution contexts、分散的 control interfaces 和快速變化的 application states。隨著 large language models (LLMs) 從會話式助理演變為面向行動的 agents，實現可靠的智慧型手機原生自主性需要重新思考 reasoning 和 control 如何組成。我們引入 ClawMobile 作為對此的具體探索",
      "title": "ClawMobile: Rethinking Smartphone-Native Agentic Systems",
      "title_zh": "ClawMobile：重新思考智慧型手機原生代理系統"
    },
    {
      "arxiv_id": "2602.22810",
      "authors": [
        "Luca Viano",
        "Till Freihaut",
        "Emanuele Nevali",
        "Volkan Cevher",
        "Matthieu Geist",
        "Giorgia Ramponi"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.922382+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Multi-agent imitation learning with function approximation: Linear Markov games and beyond",
          "url": "https://arxiv.org/abs/2602.22810"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Multi-agent imitation learning with function approximation: Linear Markov games and beyond",
        "url": "https://arxiv.org/abs/2602.22810"
      },
      "published_at": "2026-02-26T09:50:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9155613331981616,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.115561333198162
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22810",
      "summary": "In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level \"all policy deviation concentrability coefficient\" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller t",
      "summary_zh": "在這項工作中，我們首次對線性 Markov games 中的 multi-agent imitation learning (MAIL) 進行了理論分析，其中 transition dynamics 和每個 agent 的 reward function 在某些給定 features 中都是線性的。我們證明，透過利用這種結構，可以將 state-action 層級的 \"all policy deviation concentrability coefficient\" (Freihaut et al., arXiv:2510.09325) 替換為在 feature 層級定義的 concentrability coefficient，後者可能小得多",
      "title": "Multi-agent imitation learning with function approximation: Linear Markov games and beyond",
      "title_zh": "帶有函數近似的多代理模仿學習：線性 Markov games 及其他"
    },
    {
      "arxiv_id": "2602.22787",
      "authors": [
        "Ivo Brink",
        "Alexander Boer",
        "Dennis Ulmer"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.641333+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Probing for Knowledge Attribution in Large Language Models",
          "url": "https://arxiv.org/abs/2602.22787"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Probing for Knowledge Attribution in Large Language Models",
        "url": "https://arxiv.org/abs/2602.22787"
      },
      "published_at": "2026-02-26T09:21:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9137161771685159,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.113716177168516
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22787",
      "summary": "Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simp",
      "summary_zh": "大型語言模型 (LLMs) 經常產生流暢但沒有根據的主張，即 hallucinations，這分為兩種：(i) faithfulness violations——誤用使用者 context，以及 (ii) factuality violations——源於內部知識的錯誤。適當的緩解措施取決於了解模型的答案是基於 prompt 還是其內部 weights。這項工作專注於 contributive attribution 問題：識別每個輸出的主要知識來源。我們展示了一個 probe，一個簡單",
      "title": "Probing for Knowledge Attribution in Large Language Models",
      "title_zh": "探測大型語言模型中的知識歸因"
    },
    {
      "arxiv_id": "2602.22764",
      "authors": [
        "Jiahong Xiang",
        "Wenxiao He",
        "Xihua Wang",
        "Hongliang Tian",
        "Yuqun Zhang"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:56.345774+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents",
          "url": "https://arxiv.org/abs/2602.22764"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents",
        "url": "https://arxiv.org/abs/2602.22764"
      },
      "published_at": "2026-02-26T08:54:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9120013977976567,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.112001397797655
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22764",
      "summary": "The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level",
      "summary_zh": "Rust 程式語言學習曲線陡峭且存在顯著的編碼挑戰，使得自動化問題解決對於其更廣泛的採用至關重要。最近，由 LLM 驅動的 code agents 在解決複雜軟體工程任務方面取得了顯著成功，但由於缺乏大規模的 repository-level benchmark，它們在 Rust 上的應用受到限制。為彌補這一空白，我們引入了 Rust-SWE-bench，一個包含 500 個真實世界 repository-level 問題的 benchmark。",
      "title": "Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents",
      "title_zh": "使用基於 LLM 的 Agent 評估和改進自動化 Repository-Level Rust 問題解決方案"
    },
    {
      "arxiv_id": "2602.22740",
      "authors": [
        "Tongfei Chen",
        "Shuo Yang",
        "Yuguang Yang",
        "Linlin Yang",
        "Runtang Guo",
        "Changbai Li",
        "He Long",
        "Chunyu Xie",
        "Dawei Leng",
        "Baochang Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.643838+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation",
          "url": "https://arxiv.org/abs/2602.22740"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation",
        "url": "https://arxiv.org/abs/2602.22740"
      },
      "published_at": "2026-02-26T08:29:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9104141670502066,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.11041416705021
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22740",
      "summary": "Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and ",
      "summary_zh": "Referring Image Segmentation (RIS) 旨在分割影像中由自然語言表達所識別的物體。本文介紹了 Alignment-Aware Masked Learning (AML)，這是一種透過顯式估計 pixel-level vision-language alignment、在優化期間過濾掉對齊不良的區域，並專注於可靠線索來增強 RIS 的訓練策略。這種方法在 RefCOCO datasets 上實現了 state-of-the-art 的性能，並增強了對多樣化描述的魯棒性。",
      "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation",
      "title_zh": "AMLRIS：用於指稱影像分割的 Alignment-aware Masked Learning"
    },
    {
      "arxiv_id": "2602.22721",
      "authors": [
        "Fengyu Li",
        "Junhao Zhu",
        "Kaishi Song",
        "Lu Chen",
        "Zhongming Yao",
        "Tianyi Li",
        "Christian S. Jensen"
      ],
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.226466+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA",
          "url": "https://arxiv.org/abs/2602.22721"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA",
        "url": "https://arxiv.org/abs/2602.22721"
      },
      "published_at": "2026-02-26T07:49:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9079370859149855,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.107937085914983
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22721",
      "summary": "Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.\n  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g",
      "summary_zh": "Table Question Answering (TQA) 旨在回答關於結構化表格的自然語言問題。Large Language Models (LLMs) 為此問題提供了有前景的解決方案，其中以 operator-centric 的方式生成表格操作 pipelines 的 multi-step 解決方案提供了 state-of-the-art 的性能。然而，這些解決方案依賴於多次 LLM 呼叫，導致過高的延遲和計算成本。我們提出了 Operation-R1，這是第一個訓練輕量級 LLMs (例如) 的框架。",
      "title": "Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA",
      "title_zh": "以一步式 LLM Pipeline 生成取代多步驟資料準備 Pipeline 組合，實現 Table QA"
    },
    {
      "arxiv_id": "2602.22710",
      "authors": [
        "Aaron Broukhim",
        "Nadir Weibel",
        "Eshin Jolly"
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.645304+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Same Words, Different Judgments: Modality Effects on Preference Alignment",
          "url": "https://arxiv.org/abs/2602.22710"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Same Words, Different Judgments: Modality Effects on Preference Alignment",
        "url": "https://arxiv.org/abs/2602.22710"
      },
      "published_at": "2026-02-26T07:34:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9069550697113371,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.106955069711336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22710",
      "summary": "Preference-based reinforcement learning (PbRL) is the dominant framework for aligning AI systems to human preferences, but its application to speech remains underexplored. We present a controlled cross-modal study of human and synthetic preference annotations, comparing text and audio evaluations of identical semantic content across 100 prompts. Audio preferences prove as reliable as text, with inter-rater agreement reaching good levels (ICC(2,k) $\\approx$ .80) at $\\sim$9 raters -- the first ICC",
      "summary_zh": "Preference-based reinforcement learning (PbRL) 是將 AI systems 與人類偏好對齊的主導框架，但其在語音方面的應用仍未充分探索。我們提出了一項受控的 cross-modal 研究，針對人類和合成偏好註釋，比較了 100 個 prompts 中相同語義內容的文本和音頻評估。音頻偏好證明與文本一樣可靠，評分者間的一致性（ICC(2,k) $\\approx$ .80）在約 9 個評分者時達到了良好水平——這是首次的 ICC 評估。",
      "title": "Same Words, Different Judgments: Modality Effects on Preference Alignment",
      "title_zh": "相同的詞語，不同的判斷：Modality 對 Preference Alignment 的影響"
    },
    {
      "arxiv_id": "2602.22703",
      "authors": [
        "Hao Yu",
        "Shuning Jia",
        "Guanghao Li",
        "Wenhao Jiang",
        "Chun Yuan"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:49.924286+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.22703"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.22703"
      },
      "published_at": "2026-02-26T07:28:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9065657084854268,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.106565708485427
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22703",
      "summary": "Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhanc",
      "summary_zh": "Vision-language models (VLMs) 由於對基本圖形元素的感知有限，往往在幾何推理方面表現不佳。為解決這一挑戰，我們引入了 GeoPerceive，這是一個包含與 domain-specific language (DSL) 表示配對的圖形實例的 benchmark，以及一個高效的自動 data generation pipeline。這種設計使得幾何感知能夠獨立於推理進行隔離評估。為利用 GeoPerceive 提供的數據來增強...",
      "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning",
      "title_zh": "透過 Translator-Guided Reinforcement Learning 增強 VLM 中的幾何感知"
    },
    {
      "arxiv_id": "2602.22698",
      "authors": [
        "Siyue Su",
        "Jian Yang",
        "Bo Li",
        "Guanglin Niu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.646029+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs",
          "url": "https://arxiv.org/abs/2602.22698"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs",
        "url": "https://arxiv.org/abs/2602.22698"
      },
      "published_at": "2026-02-26T07:20:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9060999541240339,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.106099954124034
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22698",
      "summary": "Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both",
      "summary_zh": "利用大型語言模型（LLMs）進行知識圖譜補全（KGC）前景廣闊，但受制於根本的粒度不匹配問題。LLMs 在零散的 token 序列上運作，而實體則是知識圖譜（KGs）情境中的基本單元。現有方法通常將預測限制在有限的候選集，或透過匯集多個 token 或將實體分解為固定長度 token 序列來使實體與 LLM 的詞彙對齊，這些方法未能同時捕捉到",
      "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs",
      "title_zh": "Tokenization、Fusion 和 Decoupling：彌合大型語言模型與知識圖譜之間的粒度不匹配"
    },
    {
      "arxiv_id": "2602.22683",
      "authors": [
        "Zhuohang Jiang",
        "Xu Yuan",
        "Haohao Qu",
        "Shanru Lin",
        "Kanglong Liu",
        "Wenqi Fan",
        "Qing Li"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.646558+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses",
          "url": "https://arxiv.org/abs/2602.22683"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses",
        "url": "https://arxiv.org/abs/2602.22683"
      },
      "published_at": "2026-02-26T06:55:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.75,
        "llm_relevance_score": 21.0,
        "recency_score": 0.9045366039658446,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 29.104536603965844
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22683",
      "summary": "The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from th",
      "summary_zh": "AI 智慧眼鏡作為最熱門的穿戴式裝置之一，其快速發展為多模態互動開闢了新領域，其中基於外部知識來源的視覺問答（VQA）正成為一項核心應用。現有適用於智慧眼鏡的 Vision Language Models（VLMs）通常在傳統多模態資料集上進行訓練和評估；然而，這些資料集缺乏反映智慧眼鏡使用情境所需的豐富性和真實性，並且與",
      "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses",
      "title_zh": "SUPERGLASSES：將視覺語言模型作為 AI 智慧眼鏡的智能代理進行基準測試"
    },
    {
      "arxiv_id": "2602.23363",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.349411+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.23363"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.23363"
      },
      "published_at": "2026-02-26T18:59:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9511751648084156,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.751175164808416
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23363",
      "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases",
      "summary_zh": "我們介紹 MediX-R1，這是一個針對醫療多模態大型語言模型（MLLMs）的開放式 Reinforcement Learning（RL）框架，它能夠提供基於臨床、超越選擇題格式的自由形式答案。MediX-R1 透過 Group Based RL 和專為醫療推理量身定制的複合獎勵來 fine-tune 一個基準 vision-language backbone：一個基於 LLM 的準確性獎勵，用嚴格的 YES/NO 決策判斷語義正確性，以及一個基於醫療 embedding 的語義獎勵來捕捉釋義",
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
      "title_zh": "MediX-R1：開放式醫療強化學習"
    },
    {
      "arxiv_id": "2602.23349",
      "authors": [
        "Jose Javier Gonzalez Ortiz",
        "Abhay Gupta",
        "Chris Renard",
        "Davis Blalock"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.621815+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "FlashOptim: Optimizers for Memory Efficient Training",
          "url": "https://arxiv.org/abs/2602.23349"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "FlashOptim: Optimizers for Memory Efficient Training",
        "url": "https://arxiv.org/abs/2602.23349"
      },
      "published_at": "2026-02-26T18:52:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9506864920323426,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.75068649203234
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23349",
      "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory b",
      "summary_zh": "神經網路的標準混合精度訓練為每個模型參數需要大量的加速器記憶體位元組。這些位元組不僅反映參數本身，還包括其梯度和一個或多個優化器狀態變數。由於這些值通常每個需要 4 位元組，對於擁有少於 100GB 加速器記憶體的研究人員來說，訓練一個 7 億參數的模型也可能不切實際。我們介紹 FlashOptim，這是一套優化方案，可減少每個參數的記憶體",
      "title": "FlashOptim: Optimizers for Memory Efficient Training",
      "title_zh": "FlashOptim：用於記憶體高效訓練的優化器"
    },
    {
      "arxiv_id": "2602.23339",
      "authors": [
        "Tilemachos Aravanis",
        "Vladan Stojnić",
        "Bill Psomas",
        "Nikos Komodakis",
        "Giorgos Tolias"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.350791+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
          "url": "https://arxiv.org/abs/2602.23339"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
        "url": "https://arxiv.org/abs/2602.23339"
      },
      "published_at": "2026-02-26T18:45:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9502365629148919,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.75023656291489
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23339",
      "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompt",
      "summary_zh": "開放詞彙分割（OVS）將視覺語言模型（VLMs）的 zero-shot 識別能力擴展到像素級預測，從而實現透過文字提示指定的任意類別的分割。儘管最近有所進展，但 OVS 仍落後於完全監督的方法，原因有二：用於訓練 VLMs 的粗糙影像級監督，以及自然語言的語義歧義。我們透過引入一種 few-shot 設定來解決這些限制，該設定增強了文字提示",
      "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
      "title_zh": "檢索與分割：少量範例足以彌補開放詞彙分割中的監督差距嗎？"
    },
    {
      "arxiv_id": "2602.23335",
      "authors": [
        "Dany Haddad",
        "Dan Bareket",
        "Joseph Chee Chang",
        "Jay DeYoung",
        "Jena D. Hwang",
        "Uri Katz",
        "Mark Polak",
        "Sangho Suh",
        "Harshit Surana",
        "Aryeh Tiktinsky",
        "Shriya Atmakuri",
        "Jonathan Bragg",
        "Mike D'Arcy",
        "Sergey Feldman",
        "Amal Hassan-Ali",
        "Rubén Lozano",
        "Bodhisattwa Prasad Majumder",
        "Charles McGrady",
        "Amanpreet Singh",
        "Brooke Vlahos",
        "Yoav Goldberg",
        "Doug Downey"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.622041+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
          "url": "https://arxiv.org/abs/2602.23335"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
        "url": "https://arxiv.org/abs/2602.23335"
      },
      "published_at": "2026-02-26T18:40:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9499011798098498,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.749901179809846
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23335",
      "summary": "AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this ",
      "summary_zh": "AI 驅動的科學研究工具正迅速整合到研究工作流程中，然而該領域缺乏清晰的視角來了解研究人員如何在真實世界情境中使用這些系統。我們提出並分析了 Asta Interaction Dataset，這是一個大規模的資源，包含來自一個 LLM 驅動的 retrieval-augmented generation 平台中兩個已部署工具（一個文獻探索介面和一個科學問答介面）的超過 200,000 條用戶查詢和互動日誌。利用此資料集...",
      "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
      "title_zh": "理解 AI 驅動的科學研究工具中的使用和參與度：Asta Interaction Dataset"
    },
    {
      "arxiv_id": "2602.23229",
      "authors": [
        "Marco Garosi",
        "Matteo Farina",
        "Alessandro Conti",
        "Massimiliano Mancini",
        "Elisa Ricci"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:52.353202+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Large Multimodal Models as General In-Context Classifiers",
          "url": "https://arxiv.org/abs/2602.23229"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Large Multimodal Models as General In-Context Classifiers",
        "url": "https://arxiv.org/abs/2602.23229"
      },
      "published_at": "2026-02-26T17:08:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9438407878363027,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.7438407878363
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23229",
      "summary": "Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and f",
      "summary_zh": "我們應該使用哪種多模態模型進行分類？先前的研究表明，答案在於類似 CLIP 的對比式 Vision-Language Models (VLMs)，因為它們在 zero-shot classification 中表現卓越。相比之下，Large Multimodal Models (LMM) 更適合複雜任務。在這項工作中，我們認為這個答案忽略了 LMM 一個重要的能力：in-context learning。我們針對多樣化的資料集，對最先進的 LMM 在 closed-world classification 和 f...",
      "title": "Large Multimodal Models as General In-Context Classifiers",
      "title_zh": "大型多模態模型作為通用上下文分類器"
    },
    {
      "arxiv_id": "2602.23075",
      "authors": [
        "Mengze Hong",
        "Di Jiang",
        "Chen Jason Zhang",
        "Zichang Guo",
        "Yawen Li",
        "Jun Chen",
        "Shaobo Cui",
        "Zhiyang Su"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:51.220768+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery",
          "url": "https://arxiv.org/abs/2602.23075"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery",
        "url": "https://arxiv.org/abs/2602.23075"
      },
      "published_at": "2026-02-26T15:02:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9356225387705532,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.73562253877055
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23075",
      "summary": "Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted clai",
      "summary_zh": "大型語言模型 (LLMs) 為提升學術活動效率創造了新的機會；然而，AI 輔助的倫理部署仍面臨挑戰，包括 (1) AI 生成內容的可信度，(2) 學術誠信和智慧財產權的保護，以及 (3) 資訊隱私的保護。在這項工作中，我們提出了 CiteLLM，這是一個專門的代理平台，旨在實現可信的參考文獻發現，以驗證作者起草的主張。",
      "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery",
      "title_zh": "CiteLLM：一個用於可信科學參考文獻發現的代理平台"
    },
    {
      "arxiv_id": "2602.23047",
      "authors": [
        "Haichuan Hu",
        "Ye Shang",
        "Guoqing Xie",
        "Congqing He",
        "Quanjun Zhang"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:56.345057+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "CL4SE: A Context Learning Benchmark For Software Engineering Tasks",
          "url": "https://arxiv.org/abs/2602.23047"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "CL4SE: A Context Learning Benchmark For Software Engineering Tasks",
        "url": "https://arxiv.org/abs/2602.23047"
      },
      "published_at": "2026-02-26T14:28:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9334538486808015,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.7334538486808
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23047",
      "summary": "Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engine",
      "summary_zh": "上下文工程已成為一個關鍵範式，用於釋放大型語言模型 (LLMs) 在軟體工程 (SE) 任務中的潛力，在測試時無需模型 fine-tuning 即可實現性能提升。儘管其取得了成功，現有研究缺乏對 SE 特定上下文類型的系統性分類法，也缺乏專用基準來量化不同上下文在核心 SE 工作流程中的異質影響。為了解決這個差距，我們提出了 CL4SE (Context Learning for Software Engineering)...",
      "title": "CL4SE: A Context Learning Benchmark For Software Engineering Tasks",
      "title_zh": "CL4SE：一個用於軟體工程任務的上下文學習基準"
    },
    {
      "arxiv_id": "2602.22828",
      "authors": [
        "Jianmin Li",
        "Ying Chang",
        "Su-Kit Tang",
        "Yujia Liu",
        "Yanwen Wang",
        "Shuyuan Lin",
        "Binkai Ou"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.639546+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought",
          "url": "https://arxiv.org/abs/2602.22828"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought",
        "url": "https://arxiv.org/abs/2602.22828"
      },
      "published_at": "2026-02-26T10:11:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.7,
        "llm_relevance_score": 19.599999999999998,
        "recency_score": 0.9168975008611902,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 27.716897500861187
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22828",
      "summary": "Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM a",
      "summary_zh": "背景：Retrieval augmented generation (RAG) 技術能夠賦予大型語言模型 (LLMs) 在無需 fine tuning 的情況下生成更準確、專業和及時的回應。然而，由於中醫 (TCM) 臨床診斷與治療中涉及複雜的推理過程和顯著的個體差異，傳統 RAG 方法在該領域往往表現不佳。目標：為了解決傳統 RAG 方法在中醫領域的局限性...",
      "title": "TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought",
      "title_zh": "TCM-DiffRAG：基於知識圖譜和 Chain of Thought 的中醫個性化證候辨識推理方法"
    }
  ],
  "run_date": "2026-02-27",
  "run_id": "c3bc0fab-b27e-4e28-9f18-2723d3631f85",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-27T07:11:03.416577+00:00",
    "items_total": 423,
    "run_id": "c3bc0fab-b27e-4e28-9f18-2723d3631f85",
    "started_at": "2026-02-27T06:24:44.081355+00:00",
    "stories_total": 305,
    "success": true
  },
  "sources_status": [
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Agents",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-agents",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Alignment",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-alignment",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Efficiency",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-efficiency",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Evaluation",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-evaluation",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Interpretability",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-interpretability",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API LLM",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-llm",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Multimodal",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-multimodal",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reasoning",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-reasoning",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Retrieval",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-retrieval",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Reinforcement Learning",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-rl",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv API Safety",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-api-safety",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 106,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.AI",
      "newest_item_date": "2026-02-26T18:59:32+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ai",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 19,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CL",
      "newest_item_date": "2026-02-26T18:54:06+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cl",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CR",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-cs-cr",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 74,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.CV",
      "newest_item_date": "2026-02-26T18:59:46+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-cv",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.HC",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "arxiv-cs-hc",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.IR",
      "newest_item_date": "2026-02-26T18:48:29+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ir",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 64,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.LG",
      "newest_item_date": "2026-02-26T18:59:32+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-lg",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.MA",
      "newest_item_date": "2026-02-26T18:28:04+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ma",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 21,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.RO",
      "newest_item_date": "2026-02-26T18:20:26+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-ro",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 7,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv cs.SE",
      "newest_item_date": "2026-02-26T18:38:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-cs-se",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 2,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "arxiv_api",
      "name": "arXiv stat.ML",
      "newest_item_date": "2026-02-26T18:47:06+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "arxiv-stat-ml",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 3,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "AWS Machine Learning Blog",
      "newest_item_date": "2026-02-26T18:16:43+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "aws-ml-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "DeepMind Blog",
      "newest_item_date": "2026-02-26T16:01:50+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "deepmind-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 4,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Google AI Blog",
      "newest_item_date": "2026-02-26T18:55:00+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "google-ai-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face 01.AI (Yi)",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-01-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Cohere",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-cohere",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 7,
      "last_fetch_status_code": null,
      "method": "hf_daily_papers",
      "name": "Hugging Face Daily Papers",
      "newest_item_date": "2026-02-26T17:32:30+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_UPDATED",
      "reason_text": "Fetch and parse succeeded; items updated.",
      "remediation_hint": null,
      "source_id": "hf-daily-papers",
      "status": "HAS_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face DeepSeek AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-deepseek-ai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Google",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-google",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Meta Llama",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-meta-llama",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Microsoft",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-microsoft",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Mistral AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-mistralai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face OpenAI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-openai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Qwen",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-qwen",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "hf_org",
      "name": "Hugging Face Stability AI",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "hf-stabilityai",
      "status": "NO_UPDATE",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Meta AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "meta-ai-blog",
      "status": "FETCH_FAILED",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 1,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Microsoft Research Blog",
      "newest_item_date": "2026-02-26T17:06:34+00:00",
      "reason_code": "FETCH_PARSE_OK_HAS_NEW",
      "reason_text": "Fetch and parse succeeded; new items found.",
      "remediation_hint": null,
      "source_id": "microsoft-research-blog",
      "status": "HAS_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "NVIDIA AI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "nvidia-ai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "OpenAI Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "openai-blog",
      "status": "NO_UPDATE",
      "tier": 0
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "papers_with_code",
      "name": "Papers With Code",
      "newest_item_date": null,
      "reason_code": "FETCH_NETWORK_ERROR",
      "reason_text": "Network error during fetch.",
      "remediation_hint": "Check network connectivity and DNS resolution.",
      "source_id": "papers-with-code",
      "status": "FETCH_FAILED",
      "tier": 1
    },
    {
      "category": "other",
      "items_new": 0,
      "items_updated": 0,
      "last_fetch_status_code": null,
      "method": "rss_atom",
      "name": "Sebastian Raschka Blog",
      "newest_item_date": null,
      "reason_code": "FETCH_PARSE_OK_NO_DELTA",
      "reason_text": "Fetch and parse succeeded; no changes since last run.",
      "remediation_hint": null,
      "source_id": "sebastian-raschka-blog",
      "status": "NO_UPDATE",
      "tier": 0
    }
  ],
  "top5": [
    {
      "arxiv_id": "2602.22897",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.637327+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
          "url": "https://arxiv.org/abs/2602.22897"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
        "url": "https://arxiv.org/abs/2602.22897"
      },
      "published_at": "2026-02-26T11:35:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 25.2,
        "recency_score": 0.922249958303012,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 34.32224995830301
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22897",
      "summary": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
      "summary_zh": "人類智慧自然地將全模態感知（涵蓋視覺、音訊和語言）與複雜推理和工具使用交織在一起，以與世界互動。然而，當前的 multi-modal LLMs 主要局限於雙模態互動（例如 vision-language），缺乏通用 AI 助理所需的統一認知能力。為彌補這一差距，我們引入了 OmniGAIA，這是一個綜合基準，旨在評估全模態代理在需要跨影片、音訊和圖像模態進行深度推理和多輪工具執行的任務上的表現。OmniGAIA 透過一種新穎的全模態事件圖方法構建，合成源自真實世界的複雜、多跳查詢，這些查詢需要跨模態推理和外部工具整合。此外，我們提出了 OmniAtlas，一個在工具整合推理範式下具備主動全模態感知能力的原生全模態基礎代理。OmniAtlas 透過 hindsight-guided tree exploration 策略和 OmniDPO 進行 fine-grained error correction 所合成的 trajectories 進行訓練，有效地增強了現有開源模型的工具使用能力。這項工作標誌著朝向適用於真實世界場景的下一代原生全模態 AI 助理邁出了一步。",
      "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
      "title_zh": "OmniGAIA：邁向原生全模態 AI 代理"
    },
    {
      "arxiv_id": "2602.23166",
      "authors": [
        "Zhaochen Su",
        "Jincheng Gao",
        "Hangyu Guo",
        "Zhenhua Liu",
        "Lueyang Zhang",
        "Xinyu Geng",
        "Shijue Huang",
        "Peng Xia",
        "Guanyu Jiang",
        "Cheng Wang",
        "Yue Zhang",
        "Yi R. Fung",
        "Junxian He"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-27T06:24:52.356178+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
          "url": "https://arxiv.org/abs/2602.23166"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
        "url": "https://arxiv.org/abs/2602.23166"
      },
      "published_at": "2026-02-26T16:30:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.85,
        "llm_relevance_score": 23.8,
        "recency_score": 0.9413838876247548,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.94138388762475
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23166",
      "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-ho",
      "summary_zh": "真實世界的 multimodal agents 透過視覺證據解決多步驟工作流程。例如，代理可以透過將接線照片與電路圖連結，並透過線上文件驗證修復來排除設備故障，或者透過解釋交通地圖並在路線限制下檢查時刻表來規劃行程。然而，現有的 multimodal benchmarks 主要評估單輪視覺推理或特定的工具技能，它們未能完全捕捉現實性、視覺細微之處以及長路徑推理的複雜性。（摘要未完）",
      "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios",
      "title_zh": "AgentVista：在極具挑戰性的真實視覺場景中評估多模態代理"
    },
    {
      "arxiv_id": "2602.23152",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.629495+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Trinity of Consistency as a Defining Principle for General World Models",
          "url": "https://arxiv.org/abs/2602.23152"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Trinity of Consistency as a Defining Principle for General World Models",
        "url": "https://arxiv.org/abs/2602.23152"
      },
      "published_at": "2026-02-26T16:15:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.88,
        "llm_relevance_score": 24.64,
        "recency_score": 0.9404135858884657,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.78041358588847
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.23152",
      "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
      "summary_zh": "構建能夠學習、模擬和推斷客觀物理定律的 World Models 是追求 Artificial General Intelligence 的一個基礎性挑戰。以 Sora 等影片生成模型為代表的最新進展已證明 data-driven scaling laws 在近似物理動態方面的潛力，而新興的 Unified Multimodal Model (UMM) 則為整合感知、語言和推理提供了一個有前景的架構範式。儘管有這些進步，該領域仍缺乏一個有原則的理論框架來定義通用 World Model 所需的基本屬性。在本文中，我們提出 World Model 必須根植於「一致性三位一體」：Modal Consistency 作為語義介面，Spatial Consistency 作為幾何基礎，以及 Temporal Consistency 作為因果引擎。透過這三部分視角，我們系統地審視了 multimodal learning 的演變，揭示了一條從鬆散耦合的專業模組走向統一架構的軌跡，這些架構能夠實現內部世界模擬器的協同湧現。為了補充這個概念框架，我們引入了 CoW-Bench，這是一個以 multi-frame reasoning 和 generation 場景為中心的基準。CoW-Bench 在統一的評估協議下評估影片生成模型和 UMMs。我們的工作建立了一條通向通用世界模型的有原則途徑，闡明了當前系統的局限性以及未來進展的架構要求。",
      "title": "The Trinity of Consistency as a Defining Principle for General World Models",
      "title_zh": "一致性三位一體：通用世界模型的定義原則"
    },
    {
      "arxiv_id": "2602.22968",
      "authors": [
        "Alaa Anani",
        "Tobias Lorenz",
        "Bernt Schiele",
        "Mario Fritz",
        "Jonas Fischer"
      ],
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.635350+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits",
          "url": "https://arxiv.org/abs/2602.22968"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits",
        "url": "https://arxiv.org/abs/2602.22968"
      },
      "published_at": "2026-02-26T13:07:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 25.2,
        "recency_score": 0.9281899786001883,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.328189978600186
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22968",
      "summary": "Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certifi",
      "summary_zh": "了解神經網路如何得出其預測對於除錯、審計和部署至關重要。Mechanistic interpretability 透過識別電路 (circuits) 來實現這一目標——這些電路是負責特定行為的最小子網路。然而，現有的電路發現方法很脆弱：電路 сильно依賴於所選的概念資料集，並且經常無法 out-of-distribution 傳輸，這引發了人們對它們是否捕捉到概念或資料集特定 artifact 的疑慮。我們引入了 Certifi（摘要未完）",
      "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits",
      "title_zh": "Certified Circuits：機制電路的穩定性保證"
    },
    {
      "arxiv_id": "2602.22724",
      "authors": [
        "Tian Zhang",
        "Yiwei Xu",
        "Juan Wang",
        "Keyan Guo",
        "Xiaoyang Xu",
        "Bowen Xiao",
        "Quanlong Guan",
        "Jinlin Fan",
        "Jiawei Liu",
        "Zhiquan Liu",
        "Hongxin Hu"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-27T06:24:48.644430+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification",
          "url": "https://arxiv.org/abs/2602.22724"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification",
        "url": "https://arxiv.org/abs/2602.22724"
      },
      "published_at": "2026-02-26T07:59:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.9,
        "llm_relevance_score": 25.2,
        "recency_score": 0.9085257544074448,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 33.30852575440744
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22724",
      "summary": "Large language model (LLM) agents increasingly rely on external tools and retrieval systems to autonomously complete complex tasks. However, this design exposes agents to indirect prompt injection (IPI), where attacker-controlled context embedded in tool outputs or retrieved content silently steers agent actions away from user intent. Unlike prompt-based attacks, IPI unfolds over multi-turn trajectories, making malicious control difficult to disentangle from legitimate task execution. Existing i",
      "summary_zh": "大型語言模型 (LLM) 代理越來越依賴外部工具和檢索系統來自主完成複雜任務。然而，這種設計使代理面臨 indirect prompt injection (IPI)，即攻擊者控制的上下文嵌入在工具輸出或檢索內容中，默默地將代理的行為從用戶意圖上引開。與基於提示的攻擊不同，IPI 在 multi-turn 軌跡中展開，使得惡意控制難以與合法任務執行區分開來。現有方法（摘要未完）",
      "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification",
      "title_zh": "AgentSentry：透過時間因果診斷和上下文淨化緩解 LLM 代理中的間接提示注入"
    }
  ]
}