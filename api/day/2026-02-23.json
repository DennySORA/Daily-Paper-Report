{
  "archive_dates": [
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-25T07:14:52.509186+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:59.268831+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 8536,
          "likes": 285
        },
        "hf_model_id": "mistralai/magistral-small-2509",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Magistral-Small-2509",
            "url": "https://huggingface.co/mistralai/Magistral-Small-2509"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Magistral-Small-2509",
          "url": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        "published_at": "2026-02-23T18:04:17+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8564996053878591,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.65649960538786
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/magistral-small-2509",
        "summary": "Building upon Mistral Small 3.2 (2506), **with added reasoning capabilities**, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized. Learn more about Magistral in our blog post. The model was presented in the paper Magistral. - **Multimodality**: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision. - **Performance upgrade**: Magistral Small 1.2 should give you significantly better performance than Magistral Small 1.1 as seen in the benchmark results. - **Better tone and persona**: You should experience better LaTeX and Markdown formatting, and shorter...",
        "title": "mistralai/Magistral-Small-2509"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.20114",
      "authors": [
        "Kairan Zhao",
        "Iurie Luca",
        "Peter Triantafillou"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:36.860069+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Benchmarking Unlearning for Vision Transformers",
          "url": "https://arxiv.org/abs/2602.20114"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Benchmarking Unlearning for Vision Transformers",
        "url": "https://arxiv.org/abs/2602.20114"
      },
      "published_at": "2026-02-23T18:33:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.858225245149677,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.058225245149677
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20114",
      "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for V",
      "title": "Benchmarking Unlearning for Vision Transformers"
    },
    {
      "arxiv_id": "2602.20008",
      "authors": [
        "Louis Fabrice Tshimanga",
        "Andrea Zanola",
        "Federico Del Pup",
        "Manfredo Atzori"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:39.757339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
          "url": "https://arxiv.org/abs/2602.20008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
        "url": "https://arxiv.org/abs/2602.20008"
      },
      "published_at": "2026-02-23T16:15:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.850061515590649,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.050061515590649
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20008",
      "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism",
      "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation"
    },
    {
      "arxiv_id": "2602.19961",
      "authors": [
        "Yibo Yan",
        "Jiahao Huo",
        "Guanbo Feng",
        "Mingdong Ou",
        "Yi Cao",
        "Xin Zou",
        "Shuliang Liu",
        "Yuanhuiyi Lyu",
        "Yu Huang",
        "Jungang Li",
        "Kening Zheng",
        "Xu Zheng",
        "Philip S. Yu",
        "James Kwok",
        "Xuming Hu"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:38.837000+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
          "url": "https://arxiv.org/abs/2602.19961"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
        "url": "https://arxiv.org/abs/2602.19961"
      },
      "published_at": "2026-02-23T15:27:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8472356363116955,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.047235636311695
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19961",
      "summary": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through t",
      "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval"
    },
    {
      "arxiv_id": "2602.19837",
      "authors": [
        "Bj√∂rn Hoppmann",
        "Christoph Scholz"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-24T09:02:36.867406+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
          "url": "https://arxiv.org/abs/2602.19837"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
        "url": "https://arxiv.org/abs/2602.19837"
      },
      "published_at": "2026-02-23T13:39:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8409216838318857,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.040921683831886
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19837",
      "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that",
      "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent"
    },
    {
      "arxiv_id": "2602.19816",
      "authors": [
        "Yungang Yi"
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:36.868039+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
          "url": "https://arxiv.org/abs/2602.19816"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
        "url": "https://arxiv.org/abs/2602.19816"
      },
      "published_at": "2026-02-23T13:13:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8393882070019161,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.039388207001917
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19816",
      "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music ",
      "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling"
    },
    {
      "arxiv_id": "2602.19714",
      "authors": [
        "Joel Bucher",
        "Lahari Goswami",
        "Sverrir Thorgeirsson",
        "April Yi Wang"
      ],
      "categories": [
        "cs.HC",
        "cs.SE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:43.726306+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git",
          "url": "https://arxiv.org/abs/2602.19714"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git",
        "url": "https://arxiv.org/abs/2602.19714"
      },
      "published_at": "2026-02-23T11:05:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8319744853500137,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.031974485350014
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19714",
      "summary": "Git is widely used for collaborative software development, but it can be challenging for newcomers. While most learning tools focus on individual workflows, Git is inherently collaborative. We present GitAcademy, a browser-based learning platform that embeds a full Git environment with a split-view collaborative mode: learners work on their own local repositories connected to a shared remote repository, while simultaneously seeing their partner's actions mirrored in real time. This design is not",
      "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git"
    },
    {
      "arxiv_id": "2602.20161",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753157+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
          "url": "https://arxiv.org/abs/2602.20161"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
        "url": "https://arxiv.org/abs/2602.20161"
      },
      "published_at": "2026-02-23T18:59:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8598180139648283,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.05981801396483
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20161",
      "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
      "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device"
    },
    {
      "arxiv_id": "2602.20160",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753482+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
          "url": "https://arxiv.org/abs/2602.20160"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
        "url": "https://arxiv.org/abs/2602.20160"
      },
      "published_at": "2026-02-23T18:59:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8598050769855563,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.059805076985556
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20160",
      "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
      "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction"
    },
    {
      "arxiv_id": "2602.20159",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857474+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "A Very Big Video Reasoning Suite",
          "url": "https://arxiv.org/abs/2602.20159"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "A Very Big Video Reasoning Suite",
        "url": "https://arxiv.org/abs/2602.20159"
      },
      "published_at": "2026-02-23T18:59:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8598010964157105,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.059801096415711
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20159",
      "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "title": "A Very Big Video Reasoning Suite"
    },
    {
      "arxiv_id": "2602.20140",
      "authors": [
        "Akshay Subramanian",
        "Elton Pan",
        "Juno Nam",
        "Maurice Weiler",
        "Shuhui Qu",
        "Cheol Woo Park",
        "Tommi S. Jaakkola",
        "Elsa Olivetti",
        "Rafael Gomez-Bombarelli"
      ],
      "categories": [
        "physics.chem-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:50.719997+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
          "url": "https://arxiv.org/abs/2602.20140"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
        "url": "https://arxiv.org/abs/2602.20140"
      },
      "published_at": "2026-02-23T18:52:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.859355389188688,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.059355389188688
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20140",
      "summary": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameter",
      "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment"
    },
    {
      "arxiv_id": "2602.20093",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.744691+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
          "url": "https://arxiv.org/abs/2602.20093"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
        "url": "https://arxiv.org/abs/2602.20093"
      },
      "published_at": "2026-02-23T18:02:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8564133649779656,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.056413364977965
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20093",
      "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
      "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation"
    },
    {
      "arxiv_id": "2602.20021",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.864097+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Agents of Chaos",
          "url": "https://arxiv.org/abs/2602.20021"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Agents of Chaos",
        "url": "https://arxiv.org/abs/2602.20021"
      },
      "published_at": "2026-02-23T16:28:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508391263623658,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.050839126362366
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20021",
      "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
      "title": "Agents of Chaos"
    },
    {
      "arxiv_id": "2602.19929",
      "authors": [
        "Chenran Kou",
        "Changsheng You",
        "Mingjiang Wu",
        "Dingzhu Wen",
        "Zezhong Zhang",
        "Chengwen Xing"
      ],
      "categories": [
        "cs.NI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.842832+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
          "url": "https://arxiv.org/abs/2602.19929"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
        "url": "https://arxiv.org/abs/2602.19929"
      },
      "published_at": "2026-02-23T15:06:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.845992172363236,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.045992172363237
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19929",
      "summary": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing gene",
      "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models"
    },
    {
      "arxiv_id": "2602.19895",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.769682+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.19895"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.19895"
      },
      "published_at": "2026-02-23T14:37:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.844259860267345,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.044259860267346
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19895",
      "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
      "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning"
    },
    {
      "arxiv_id": "2602.19811",
      "authors": [
        "Laurent Bindschaedler"
      ],
      "categories": [
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.843687+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
          "url": "https://arxiv.org/abs/2602.19811"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
        "url": "https://arxiv.org/abs/2602.19811"
      },
      "published_at": "2026-02-23T13:12:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8392949468267993,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.0392949468268
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19811",
      "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema",
      "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
    },
    {
      "arxiv_id": "2602.20044",
      "authors": [
        "H. C. W. Price",
        "H. AlMuhanna",
        "P. M. Bassani",
        "M. Ho",
        "T. S. Evans"
      ],
      "categories": [
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:48.734281+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook",
          "url": "https://arxiv.org/abs/2602.20044"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook",
        "url": "https://arxiv.org/abs/2602.20044"
      },
      "published_at": "2026-02-23T16:57:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8525138924772098,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 8.80251389247721
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20044",
      "summary": "Within twelve days of launch, an AI-native social platform exhibits extreme attention concentration, hierarchical role separation, and one-way attention flow, consistent with the hypothesis that stratification in agent ecosystems can emerge rapidly rather than gradually. We analyse publicly observable traces from a 12-day window of Moltbook (28 January -- 8 February 2026), comprising 20,040 posts and 192,410 comments from 15,083 accounts across 759 submolts. We construct co-participation and dir",
      "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook"
    },
    {
      "arxiv_id": "2602.19694",
      "authors": [
        "Bo Liu",
        "Tong Li",
        "Zhu Xiao",
        "Ruihui Li",
        "Geyong Min",
        "Zhuo Tang",
        "Kenli Li"
      ],
      "categories": [
        "cs.ET"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.844054+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs",
          "url": "https://arxiv.org/abs/2602.19694"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs",
        "url": "https://arxiv.org/abs/2602.19694"
      },
      "published_at": "2026-02-23T10:42:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8306168951205809,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 8.78061689512058
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19694",
      "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose",
      "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs"
    },
    {
      "arxiv_id": "2602.20157",
      "authors": [
        "Zhongxiao Cong",
        "Qitao Zhao",
        "Minsik Jeon",
        "Shubham Tulsiani"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753878+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
          "url": "https://arxiv.org/abs/2602.20157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
        "url": "https://arxiv.org/abs/2602.20157"
      },
      "published_at": "2026-02-23T18:59:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8597901499436562,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059790149943657
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20157",
      "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from ",
      "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning"
    },
    {
      "arxiv_id": "2602.20156",
      "authors": [
        "David Schmotz",
        "Luca Beurer-Kellner",
        "Sahar Abdelnabi",
        "Maksym Andriushchenko"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.761579+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
          "url": "https://arxiv.org/abs/2602.20156"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
        "url": "https://arxiv.org/abs/2602.20156"
      },
      "published_at": "2026-02-23T18:59:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8597871645663742,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059787164566375
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20156",
      "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark ev",
      "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
    },
    {
      "arxiv_id": "2602.20152",
      "authors": [
        "Zhenyao Ma",
        "Yue Liang",
        "Dongxu Li"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857733+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
          "url": "https://arxiv.org/abs/2602.20152"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
        "url": "https://arxiv.org/abs/2602.20152"
      },
      "published_at": "2026-02-23T18:59:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8597642770182525,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059764277018253
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20152",
      "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically in",
      "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data"
    },
    {
      "arxiv_id": "2602.20150",
      "authors": [
        "Wei-Cheng Huang",
        "Jiaheng Han",
        "Xiaohan Ye",
        "Zherong Pan",
        "Kris Hauser"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.754140+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
          "url": "https://arxiv.org/abs/2602.20150"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
        "url": "https://arxiv.org/abs/2602.20150"
      },
      "published_at": "2026-02-23T18:58:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8597244740379111,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05972447403791
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20150",
      "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical c",
      "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization"
    },
    {
      "arxiv_id": "2602.20144",
      "authors": [
        "Zehao Wang",
        "Mingzhe Han",
        "Wei Cheng",
        "Yue-Kai Huang",
        "Philip Ji",
        "Denton Wu",
        "Mahdi Safari",
        "Flemming Holtorf",
        "Kenaish AlQubaisi",
        "Norbert M. Linke",
        "Danyang Zhuo",
        "Yiran Chen",
        "Ting Wang",
        "Dirk Englund",
        "Tingjun Chen"
      ],
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857998+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Agentic AI for Scalable and Robust Optical Systems Control",
          "url": "https://arxiv.org/abs/2602.20144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Agentic AI for Scalable and Robust Optical Systems Control",
        "url": "https://arxiv.org/abs/2602.20144"
      },
      "published_at": "2026-02-23T18:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8594936530870452,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059493653087046
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20144",
      "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordinati",
      "title": "Agentic AI for Scalable and Robust Optical Systems Control"
    },
    {
      "arxiv_id": "2602.20141",
      "authors": [
        "Clarisse Wibault",
        "Johannes Forkel",
        "Sebastian Towers",
        "Tiphaine Wibault",
        "Juan Duque",
        "George Whittle",
        "Andreas Schaab",
        "Yucheng Yang",
        "Chiyuan Wang",
        "Michael Osborne",
        "Benjamin Moll",
        "Jakob Foerster"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.858213+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
          "url": "https://arxiv.org/abs/2602.20141"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
        "url": "https://arxiv.org/abs/2602.20141"
      },
      "published_at": "2026-02-23T18:53:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8594110899541963,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059411089954196
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20141",
      "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected ",
      "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games"
    },
    {
      "arxiv_id": "2602.20137",
      "authors": [
        "Martin Sinnona",
        "Valentin Bonas",
        "Emmanuel Iarussi",
        "Viviana Siless"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.754344+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Do Large Language Models Understand Data Visualization Rules?",
          "url": "https://arxiv.org/abs/2602.20137"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Do Large Language Models Understand Data Visualization Rules?",
        "url": "https://arxiv.org/abs/2602.20137"
      },
      "published_at": "2026-02-23T18:47:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8590948371309071,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059094837130907
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20137",
      "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivat",
      "title": "Do Large Language Models Understand Data Visualization Rules?"
    },
    {
      "arxiv_id": "2602.20135",
      "authors": [
        "Mohammad Amanlou",
        "Erfan Shafiee Moghaddam",
        "Yasaman Amou Jafari",
        "Mahdi Noori",
        "Farhan Farsi",
        "Behnam Bahrak"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.858466+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
          "url": "https://arxiv.org/abs/2602.20135"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
        "url": "https://arxiv.org/abs/2602.20135"
      },
      "published_at": "2026-02-23T18:46:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8590113180817611,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059011318081762
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20135",
      "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entiti",
      "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration"
    },
    {
      "arxiv_id": "2602.20133",
      "authors": [
        "Mert Cemri",
        "Shubham Agrawal",
        "Akshat Gupta",
        "Shu Liu",
        "Audrey Cheng",
        "Qiuyang Mang",
        "Ashwin Naren",
        "Lutfi Eren Erdogan",
        "Koushik Sen",
        "Matei Zaharia",
        "Alex Dimakis",
        "Ion Stoica"
      ],
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859012+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
          "url": "https://arxiv.org/abs/2602.20133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
        "url": "https://arxiv.org/abs/2602.20133"
      },
      "published_at": "2026-02-23T18:45:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8589556432265529,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058955643226554
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20133",
      "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promis",
      "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization"
    },
    {
      "arxiv_id": "2602.20132",
      "authors": [
        "Wendi Li",
        "Sharon Li"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.762454+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LAD: Learning Advantage Distribution for Reasoning",
          "url": "https://arxiv.org/abs/2602.20132"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LAD: Learning Advantage Distribution for Reasoning",
        "url": "https://arxiv.org/abs/2602.20132"
      },
      "published_at": "2026-02-23T18:44:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.85887511990959,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05887511990959
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20132",
      "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishin",
      "title": "LAD: Learning Advantage Distribution for Reasoning"
    },
    {
      "arxiv_id": "2602.20130",
      "authors": [
        "Zaifu Zhan",
        "Min Zeng",
        "Shuang Zhou",
        "Yiran Song",
        "Xiaoyi Chen",
        "Yu Hou",
        "Yifan Wu",
        "Yang Ruan",
        "Rui Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859218+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
          "url": "https://arxiv.org/abs/2602.20130"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
        "url": "https://arxiv.org/abs/2602.20130"
      },
      "published_at": "2026-02-23T18:42:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8587955981171456,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058795598117145
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20130",
      "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and ",
      "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering"
    },
    {
      "arxiv_id": "2602.20122",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859433+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NanoKnow: How to Know What Your Language Model Knows",
          "url": "https://arxiv.org/abs/2602.20122"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NanoKnow: How to Know What Your Language Model Knows",
        "url": "https://arxiv.org/abs/2602.20122"
      },
      "published_at": "2026-02-23T18:37:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8584964633340039,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058496463334004
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20122",
      "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions q",
      "title": "NanoKnow: How to Know What Your Language Model Knows"
    },
    {
      "arxiv_id": "2602.20119",
      "authors": [
        "Jiahui Fu",
        "Junyu Nan",
        "Lingfeng Sun",
        "Hongyu Li",
        "Jianing Qian",
        "Jennifer L. Barry",
        "Kris Kitani",
        "George Konidaris"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859637+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
          "url": "https://arxiv.org/abs/2602.20119"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
        "url": "https://arxiv.org/abs/2602.20119"
      },
      "published_at": "2026-02-23T18:35:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8583464382891629,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058346438289163
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20119",
      "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high lev",
      "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning"
    },
    {
      "arxiv_id": "2602.20117",
      "authors": [
        "Andre He",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Allen Nie",
        "Darion Cassel",
        "Sam Bayless",
        "Huzefa Rangwala"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859841+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
          "url": "https://arxiv.org/abs/2602.20117"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
        "url": "https://arxiv.org/abs/2602.20117"
      },
      "published_at": "2026-02-23T18:34:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8582977602997484,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058297760299748
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20117",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diver",
      "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models"
    },
    {
      "arxiv_id": "2602.20102",
      "authors": [
        "Thanh Q. Tran",
        "Arun Verma",
        "Kiwan Wong",
        "Bryan Kian Hsiang Low",
        "Daniela Rus",
        "Wei Xiao"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.860691+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
          "url": "https://arxiv.org/abs/2602.20102"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
        "url": "https://arxiv.org/abs/2602.20102"
      },
      "published_at": "2026-02-23T18:19:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8574210360142834,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057421036014283
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20102",
      "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints di",
      "title": "BarrierSteer: LLM Safety via Learning Barrier Steering"
    },
    {
      "arxiv_id": "2602.20097",
      "authors": [
        "Pu Jiao",
        "Sheng Di",
        "Jiannan Tian",
        "Mingze Xia",
        "Xuan Wu",
        "Yang Zhang",
        "Xin Liang",
        "Franck Cappello"
      ],
      "categories": [
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:54.700582+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation",
          "url": "https://arxiv.org/abs/2602.20097"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation",
        "url": "https://arxiv.org/abs/2602.20097"
      },
      "published_at": "2026-02-23T18:09:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8567911027624747,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056791102762475
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20097",
      "summary": "Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or lar",
      "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation"
    },
    {
      "arxiv_id": "2602.20094",
      "authors": [
        "Yuzhe Wang",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861133+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
          "url": "https://arxiv.org/abs/2602.20094"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
        "url": "https://arxiv.org/abs/2602.20094"
      },
      "published_at": "2026-02-23T18:06:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8566165890166932,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056616589016693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20094",
      "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a ",
      "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching"
    },
    {
      "arxiv_id": "2602.20092",
      "authors": [
        "Leshem Choshen",
        "Ryan Cotterell",
        "Mustafa Omer Gul",
        "Jaap Jumelet",
        "Tal Linzen",
        "Aaron Mueller",
        "Suchir Salhan",
        "Raj Sanjay Shah",
        "Alex Warstadt",
        "Ethan Gotlieb Wilcox"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.829640+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
          "url": "https://arxiv.org/abs/2602.20092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
        "url": "https://arxiv.org/abs/2602.20092"
      },
      "published_at": "2026-02-23T18:02:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563866024784763,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056386602478476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20092",
      "summary": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.\n  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evalua",
      "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop"
    },
    {
      "arxiv_id": "2602.20091",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.829885+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "How Retrieved Context Shapes Internal Representations in RAG",
          "url": "https://arxiv.org/abs/2602.20091"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "How Retrieved Context Shapes Internal Representations in RAG",
        "url": "https://arxiv.org/abs/2602.20091"
      },
      "published_at": "2026-02-23T18:02:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563677701097969,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056367770109798
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20091",
      "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediat",
      "title": "How Retrieved Context Shapes Internal Representations in RAG"
    },
    {
      "arxiv_id": "2602.20089",
      "authors": [
        "Zanxi Ruan",
        "Qiuyu Kong",
        "Songqun Gao",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861348+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
          "url": "https://arxiv.org/abs/2602.20089"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
        "url": "https://arxiv.org/abs/2602.20089"
      },
      "published_at": "2026-02-23T17:57:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.856103169567404,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056103169567404
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20089",
      "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them ",
      "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues"
    },
    {
      "arxiv_id": "2602.20084",
      "authors": [
        "Martin Sinnona",
        "Valentin Bonas",
        "Viviana Siless",
        "Emmanuel Iarussi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.755339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Do Large Language Models Understand Data Visualization Principles?",
          "url": "https://arxiv.org/abs/2602.20084"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Do Large Language Models Understand Data Visualization Principles?",
        "url": "https://arxiv.org/abs/2602.20084"
      },
      "published_at": "2026-02-23T17:51:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8557158308997189,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05571583089972
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20084",
      "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into f",
      "title": "Do Large Language Models Understand Data Visualization Principles?"
    },
    {
      "arxiv_id": "2602.20083",
      "authors": [
        "Xinzhao Li",
        "Alptekin Vardar",
        "Franz M√ºller",
        "Navya Goli",
        "Umamaheswara Tida",
        "Kai Ni",
        "X. Sharon Hu",
        "Thomas K√§mpfe",
        "Ruiyang Qin"
      ],
      "categories": [
        "cs.ET",
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:55.702309+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval",
          "url": "https://arxiv.org/abs/2602.20083"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval",
        "url": "https://arxiv.org/abs/2602.20083"
      },
      "published_at": "2026-02-23T17:49:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556009508381545,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055600950838155
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20083",
      "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimensio",
      "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval"
    },
    {
      "arxiv_id": "2602.20070",
      "authors": [
        "Florentin Coeurdoux",
        "Etienne Lempereur",
        "Nathana√´l Cuvelle-Magar",
        "Thomas Eboli",
        "St√©phane Mallat",
        "Anastasia Borovykh",
        "Eric Vanden-Eijnden"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.763944+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
          "url": "https://arxiv.org/abs/2602.20070"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
        "url": "https://arxiv.org/abs/2602.20070"
      },
      "published_at": "2026-02-23T17:26:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8542344680772738,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054234468077274
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20070",
      "summary": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaœÜ(x)^\\topŒ∑_t$, where $Œ∑_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no diffi",
      "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants"
    },
    {
      "arxiv_id": "2602.20062",
      "authors": [
        "Nicolas Anguita",
        "Francesco Locatello",
        "Andrew M. Saxe",
        "Marco Mondelli",
        "Flavia Mancini",
        "Samuel Lippl",
        "Clementine Domine"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.764402+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
          "url": "https://arxiv.org/abs/2602.20062"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
        "url": "https://arxiv.org/abs/2602.20062"
      },
      "published_at": "2026-02-23T17:19:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8538430336566455,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053843033656646
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20062",
      "summary": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of",
      "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning"
    },
    {
      "arxiv_id": "2602.20061",
      "authors": [
        "Zoha Hayat Bhatti",
        "Bakhtawar Ahtisham",
        "Seemal Tausif",
        "Niklas George",
        "Nida ul Habib Bajwa",
        "Mobin Javed"
      ],
      "categories": [
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:46.724594+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cr",
          "tier": 1,
          "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios",
          "url": "https://arxiv.org/abs/2602.20061"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cr",
        "tier": 1,
        "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios",
        "url": "https://arxiv.org/abs/2602.20061"
      },
      "published_at": "2026-02-23T17:17:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8537442149502392,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053744214950239
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20061",
      "summary": "Large Language Models and commercial speech synthesis systems now enable highly realistic AI-generated voice scams (vishing), raising urgent concerns about deception at scale. Yet it remains unclear whether individuals can reliably distinguish AI-generated speech from human-recorded voices in realistic scam contexts and what perceptual strategies underlie their judgments. We conducted a controlled online study in which 22 participants evaluated 16 vishing-style audio clips (8 AI-generated, 8 hum",
      "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios"
    },
    {
      "arxiv_id": "2602.20060",
      "authors": [
        "Junli Wang",
        "Xueyi Liu",
        "Yinan Zheng",
        "Zebing Xing",
        "Pengfei Li",
        "Guang Li",
        "Kun Ma",
        "Guang Chen",
        "Hangjun Ye",
        "Zhongpu Xia",
        "Long Chen",
        "Qichao Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.756142+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
          "url": "https://arxiv.org/abs/2602.20060"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
        "url": "https://arxiv.org/abs/2602.20060"
      },
      "published_at": "2026-02-23T17:17:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.853717535860385,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053717535860386
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20060",
      "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propo",
      "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving"
    },
    {
      "arxiv_id": "2602.20053",
      "authors": [
        "Jiahui Chen",
        "Zehang Deng",
        "Zeyu Zhang",
        "Chaoyang Li",
        "Lianchen Jia",
        "Lifeng Sun"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.756541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Decoupling Defense Strategies for Robust Image Watermarking",
          "url": "https://arxiv.org/abs/2602.20053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Decoupling Defense Strategies for Robust Image Watermarking",
        "url": "https://arxiv.org/abs/2602.20053"
      },
      "published_at": "2026-02-23T17:02:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8528573352892697,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05285733528927
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20053",
      "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage ",
      "title": "Decoupling Defense Strategies for Robust Image Watermarking"
    },
    {
      "arxiv_id": "2602.20052",
      "authors": [
        "Marco Scharringhausen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.830328+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Entropy in Large Language Models",
          "url": "https://arxiv.org/abs/2602.20052"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Entropy in Large Language Models",
        "url": "https://arxiv.org/abs/2602.20052"
      },
      "published_at": "2026-02-23T17:02:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8528474643124202,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052847464312421
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20052",
      "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Ou",
      "title": "Entropy in Large Language Models"
    },
    {
      "arxiv_id": "2602.20046",
      "authors": [
        "Eleonora Grassucci",
        "Giordano Cicchetti",
        "Danilo Comminiello"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.764602+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Closing the gap in multimodal medical representation alignment",
          "url": "https://arxiv.org/abs/2602.20046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Closing the gap in multimodal medical representation alignment",
        "url": "https://arxiv.org/abs/2602.20046"
      },
      "published_at": "2026-02-23T16:57:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8525454676505418,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052545467650543
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20046",
      "summary": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remain",
      "title": "Closing the gap in multimodal medical representation alignment"
    },
    {
      "arxiv_id": "2602.20042",
      "authors": [
        "Han Bao",
        "Yue Huang",
        "Xiaoda Wang",
        "Zheyuan Zhang",
        "Yujun Zhou",
        "Carl Yang",
        "Xiangliang Zhang",
        "Yanfang Ye"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.830545+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
          "url": "https://arxiv.org/abs/2602.20042"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
        "url": "https://arxiv.org/abs/2602.20042"
      },
      "published_at": "2026-02-23T16:51:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8521942597024218,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052194259702421
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20042",
      "summary": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flatt",
      "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously"
    },
    {
      "arxiv_id": "2602.20041",
      "authors": [
        "Ghadah Alosaimi",
        "Maha Alsayyari",
        "Yixin Sun",
        "Stamos Katsigiannis",
        "Amir Atapour-Abarghouei",
        "Toby P. Breckon"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.757143+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
          "url": "https://arxiv.org/abs/2602.20041"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
        "url": "https://arxiv.org/abs/2602.20041"
      },
      "published_at": "2026-02-23T16:50:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8521133839925387,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05211338399254
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20041",
      "summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) s",
      "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover"
    },
    {
      "arxiv_id": "2602.20017",
      "authors": [
        "Gaurav Najpande",
        "Tampu Ravi Kumar",
        "Manan Roy Choudhury",
        "Neha Valeti",
        "Yanjie Fu",
        "Vivek Gupta"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.831223+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
          "url": "https://arxiv.org/abs/2602.20017"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
        "url": "https://arxiv.org/abs/2602.20017"
      },
      "published_at": "2026-02-23T16:23:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8505447318203985,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050544731820398
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20017",
      "summary": "Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation be",
      "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning"
    },
    {
      "arxiv_id": "2602.19991",
      "authors": [
        "Yaya Sy",
        "Dioula Doucour√©",
        "Christophe Cerisara",
        "Irina Illina"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.836410+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
          "url": "https://arxiv.org/abs/2602.19991"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
        "url": "https://arxiv.org/abs/2602.19991"
      },
      "published_at": "2026-02-23T15:57:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8489779847598862,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048977984759887
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19991",
      "summary": "Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation",
      "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text"
    },
    {
      "arxiv_id": "2602.19990",
      "authors": [
        "Monica Marconi Sciarroni",
        "Emanuele Storti"
      ],
      "categories": [
        "cs.DB",
        "cs.DC",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.745094+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT",
          "url": "https://arxiv.org/abs/2602.19990"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT",
        "url": "https://arxiv.org/abs/2602.19990"
      },
      "published_at": "2026-02-23T15:55:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8488757991156096,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04887579911561
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19990",
      "summary": "Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scena",
      "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT"
    },
    {
      "arxiv_id": "2602.19987",
      "authors": [
        "Ha-Anh Hoang Nguyen",
        "Tri-Duc Phan Le",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Duc-Trong Le",
        "Hoang-Quynh Le"
      ],
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.765526+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
          "url": "https://arxiv.org/abs/2602.19987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
        "url": "https://arxiv.org/abs/2602.19987"
      },
      "published_at": "2026-02-23T15:53:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8487510314031745,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048751031403174
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19987",
      "summary": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics ",
      "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction"
    },
    {
      "arxiv_id": "2602.19982",
      "authors": [
        "Alaa El Ichi",
        "Khalide Jbilou"
      ],
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.765906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Computationally Efficient Multidimensional Vision Transformer",
          "url": "https://arxiv.org/abs/2602.19982"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Computationally Efficient Multidimensional Vision Transformer",
        "url": "https://arxiv.org/abs/2602.19982"
      },
      "published_at": "2026-02-23T15:49:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.848535923856177,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048535923856177
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19982",
      "summary": "Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and ",
      "title": "A Computationally Efficient Multidimensional Vision Transformer"
    },
    {
      "arxiv_id": "2602.19980",
      "authors": [
        "Itamar Trainin",
        "Shauli Ravfogel",
        "Omri Abend",
        "Amir Feder"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.766138+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
          "url": "https://arxiv.org/abs/2602.19980"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
        "url": "https://arxiv.org/abs/2602.19980"
      },
      "published_at": "2026-02-23T15:47:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8483994226914425,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048399422691443
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19980",
      "summary": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct concl",
      "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks"
    },
    {
      "arxiv_id": "2602.19974",
      "authors": [
        "Tianyu Wang",
        "Zhiyuan Ma",
        "Qian Wang",
        "Xinyi Zhang",
        "Xinwei Long",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.757744+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
          "url": "https://arxiv.org/abs/2602.19974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
        "url": "https://arxiv.org/abs/2602.19974"
      },
      "published_at": "2026-02-23T15:39:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8479537373228511,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047953737322851
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19974",
      "summary": "Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture compri",
      "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection"
    },
    {
      "arxiv_id": "2602.19967",
      "authors": [
        "Yongsheng Chen",
        "Yong Chen",
        "Wei Guo",
        "Xinghui Zhong"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.766519+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
          "url": "https://arxiv.org/abs/2602.19967"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
        "url": "https://arxiv.org/abs/2602.19967"
      },
      "published_at": "2026-02-23T15:29:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.847362142742883,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047362142742884
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19967",
      "summary": "Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by ",
      "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems"
    },
    {
      "arxiv_id": "2602.19938",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.767349+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
          "url": "https://arxiv.org/abs/2602.19938"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
        "url": "https://arxiv.org/abs/2602.19938"
      },
      "published_at": "2026-02-23T15:11:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.846270298831692,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046270298831692
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19938",
      "summary": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deplo",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs"
    },
    {
      "arxiv_id": "2602.19931",
      "authors": [
        "Pin-Han Huang",
        "Shang-Tse Chen",
        "Hsuan-Tien Lin"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.767551+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
          "url": "https://arxiv.org/abs/2602.19931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
        "url": "https://arxiv.org/abs/2602.19931"
      },
      "published_at": "2026-02-23T15:06:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8460117557420326,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046011755742033
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19931",
      "summary": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations ",
      "title": "Expanding the Role of Diffusion Models for Robust Classifier Training"
    },
    {
      "arxiv_id": "2602.19919",
      "authors": [
        "Xiang Li",
        "Zikai Wei",
        "Yiyan Qi",
        "Wanyun Zhou",
        "Xiang Liu",
        "Penglei Sun",
        "Yongqi Zhang",
        "Xiaowen Chu"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768144+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
          "url": "https://arxiv.org/abs/2602.19919"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
        "url": "https://arxiv.org/abs/2602.19919"
      },
      "published_at": "2026-02-23T14:58:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8455409011083921,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045540901108392
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19919",
      "summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and stati",
      "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling"
    },
    {
      "arxiv_id": "2602.19918",
      "authors": [
        "Jiaqi Xue",
        "Mengxin Zheng",
        "Qian Lou"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768380+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "RobPI: Robust Private Inference against Malicious Client",
          "url": "https://arxiv.org/abs/2602.19918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "RobPI: Robust Private Inference against Malicious Client",
        "url": "https://arxiv.org/abs/2602.19918"
      },
      "published_at": "2026-02-23T14:58:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8454988208375421,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045498820837542
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19918",
      "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredicta",
      "title": "RobPI: Robust Private Inference against Malicious Client"
    },
    {
      "arxiv_id": "2602.19917",
      "authors": [
        "Thanh Nguyen",
        "Tung Luu",
        "Tri Ton",
        "Sungwoong Kim",
        "Chang D. Yoo"
      ],
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768601+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.19917"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.19917"
      },
      "published_at": "2026-02-23T14:57:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.845483163596946,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045483163596947
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19917",
      "summary": "Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations s",
      "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.19910",
      "authors": [
        "Wei He",
        "Xianghan Meng",
        "Zhiyuan Huang",
        "Xianbiao Qi",
        "Rong Xiao",
        "Chun-Guang Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.758935+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
          "url": "https://arxiv.org/abs/2602.19910"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
        "url": "https://arxiv.org/abs/2602.19910"
      },
      "published_at": "2026-02-23T14:51:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8450888924590765,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045088892459077
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19910",
      "summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this",
      "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery"
    },
    {
      "arxiv_id": "2602.19907",
      "authors": [
        "Kiran Kokilepersaud",
        "Mohit Prabhushankar",
        "Ghassan AlRegib",
        "Stephanie Trejo Corona",
        "Charles Wykoff"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.769265+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
          "url": "https://arxiv.org/abs/2602.19907"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
        "url": "https://arxiv.org/abs/2602.19907"
      },
      "published_at": "2026-02-23T14:46:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8447945319812917,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044794531981292
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19907",
      "summary": "In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these sam",
      "title": "Gradient based Severity Labeling for Biomarker Classification in OCT"
    },
    {
      "arxiv_id": "2602.19898",
      "authors": [
        "Stefan Fabian",
        "Aljoscha Schmidt",
        "Jonas S√º√ü",
        " Dishant",
        "Aum Oza",
        "Oskar von Stryk"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.769069+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
          "url": "https://arxiv.org/abs/2602.19898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
        "url": "https://arxiv.org/abs/2602.19898"
      },
      "published_at": "2026-02-23T14:38:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8443399905842813,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044339990584282
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19898",
      "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manip",
      "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform"
    },
    {
      "arxiv_id": "2602.19891",
      "authors": [
        "Wen-Liang Lin",
        "Yun-Chien Cheng"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.759758+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
          "url": "https://arxiv.org/abs/2602.19891"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
        "url": "https://arxiv.org/abs/2602.19891"
      },
      "published_at": "2026-02-23T14:33:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8440478447754073,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044047844775408
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19891",
      "summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on ",
      "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images"
    },
    {
      "arxiv_id": "2602.19878",
      "authors": [
        "Daham Mustafa",
        "Diego Collarana",
        "Yixin Peng",
        "Rafiqul Haque",
        "Christoph Lange-Bever",
        "Christoph Quix",
        "Stephan Decker"
      ],
      "categories": [
        "cs.CL",
        "cs.LO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.837951+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics",
          "url": "https://arxiv.org/abs/2602.19878"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics",
        "url": "https://arxiv.org/abs/2602.19878"
      },
      "published_at": "2026-02-23T14:24:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.843541958495367,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043541958495368
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19878",
      "summary": "Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic.\n  We classify ODRL's left operands by value-domain structure ",
      "title": "Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics"
    },
    {
      "arxiv_id": "2602.19870",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Haofei Wang",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.760581+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
          "url": "https://arxiv.org/abs/2602.19870"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
        "url": "https://arxiv.org/abs/2602.19870"
      },
      "published_at": "2026-02-23T14:15:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8430061281317068,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043006128131706
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19870",
      "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such ",
      "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs"
    },
    {
      "arxiv_id": "2602.19863",
      "authors": [
        "Filip Wolf",
        "Bla≈æ Rolih",
        "Luka ƒåehovin Zajc"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.760775+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
          "url": "https://arxiv.org/abs/2602.19863"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
        "url": "https://arxiv.org/abs/2602.19863"
      },
      "published_at": "2026-02-23T14:09:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8426198388543682,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042619838854367
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19863",
      "summary": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastiv",
      "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation"
    },
    {
      "arxiv_id": "2602.19850",
      "authors": [
        "Junhui Lee",
        "Hyosung Kim",
        "Saekwang Nam"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.769462+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data",
          "url": "https://arxiv.org/abs/2602.19850"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data",
        "url": "https://arxiv.org/abs/2602.19850"
      },
      "published_at": "2026-02-23T13:53:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8416967788559385,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041696778855938
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19850",
      "summary": "Real-time prediction of deformation in highly compliant soft materials remains a significant challenge in soft robotics. While vision-based soft tactile sensors can track internal marker displacements, learning-based models for 3D contact estimation heavily depend on their training datasets, inherently limiting their ability to generalize to complex scenarios such as multi-point sensing. To address this limitation, we introduce TactiVerse, a U-Net-based framework that formulates contact geometry",
      "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data"
    },
    {
      "arxiv_id": "2602.19848",
      "authors": [
        "Francisco Filho",
        "Kelvin Cunha",
        "F√°bio Papais",
        "Emanoel dos Santos",
        "Rodrigo Mota",
        "Thales Bezerra",
        "Erico Medeiros",
        "Paulo Borba",
        "Tsang Ing Ren"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.761232+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
          "url": "https://arxiv.org/abs/2602.19848"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
        "url": "https://arxiv.org/abs/2602.19848"
      },
      "published_at": "2026-02-23T13:52:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.841651967488859,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041651967488859
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19848",
      "summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight mod",
      "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Matthew Lee"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Customer Solutions",
        "Healthcare"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.297996+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials",
          "url": "https://aws.amazon.com/blogs/machine-learning/how-sonrai-uses-amazon-sagemaker-ai-to-accelerate-precision-medicine-trials"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials",
        "url": "https://aws.amazon.com/blogs/machine-learning/how-sonrai-uses-amazon-sagemaker-ai-to-accelerate-precision-medicine-trials"
      },
      "published_at": "2026-02-23T17:31:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8545667349736795,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 2.7,
        "total_score": 10.05456673497368
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9b135fce3a12d59c",
      "summary": "In this post, we explore how Sonrai, a life sciences AI company, partnered with AWS to build a robust MLOps framework using Amazon SageMaker AI that addresses these challenges while maintaining the traceability and reproducibility required in regulated environments.",
      "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Laura Kulowski"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.297555+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Scaling data annotation using vision-language models to power physical AI systems",
          "url": "https://aws.amazon.com/blogs/machine-learning/scaling-data-annotation-using-vision-language-models-to-power-physical-ai-systems"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Scaling data annotation using vision-language models to power physical AI systems",
        "url": "https://aws.amazon.com/blogs/machine-learning/scaling-data-annotation-using-vision-language-models-to-power-physical-ai-systems"
      },
      "published_at": "2026-02-23T23:20:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8755230241180252,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 9.375523024118024
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:a33585692c1d4c8b",
      "summary": "In this post, we examine how Bedrock Robotics tackles this challenge. By joining the AWS Physical AI Fellowship, the startup partnered with the AWS Generative AI Innovation Center to apply vision-language models that analyze construction video footage, extract operational details, and generate labeled training datasets at scale, to improve data preparation for autonomous construction equipment.",
      "title": "Scaling data annotation using vision-language models to power physical AI systems"
    },
    {
      "arxiv_id": "2602.19840",
      "authors": [
        "Jingzhuo Wu",
        "Jiajun Zhang",
        "Keyan Jin",
        "Dehua Ma",
        "Junbo Wang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.838384+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
          "url": "https://arxiv.org/abs/2602.19840"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
        "url": "https://arxiv.org/abs/2602.19840"
      },
      "published_at": "2026-02-23T13:40:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8409664563170874,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.040966456317088
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19840",
      "summary": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal p",
      "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation"
    },
    {
      "arxiv_id": "2602.19832",
      "authors": [
        "Penghui Niu",
        "Taotao Cai",
        "Suqi Zhang",
        "Junhua Gu",
        "Ping Zhang",
        "Qiqi Liu",
        "Jianxin Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.761433+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting",
          "url": "https://arxiv.org/abs/2602.19832"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting",
        "url": "https://arxiv.org/abs/2602.19832"
      },
      "published_at": "2026-02-23T13:30:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8403972446692286,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04039724466923
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19832",
      "summary": "The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling bet",
      "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting"
    },
    {
      "arxiv_id": "2602.19764",
      "authors": [
        "Yirui Sun",
        "Guangyu Zhuge",
        "Keliang Liu",
        "Jie Gu",
        "Zhihao xia",
        "Qionglin Ren",
        "Chunxu tian",
        "Zhongxue Ga"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.769641+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling",
          "url": "https://arxiv.org/abs/2602.19764"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling",
        "url": "https://arxiv.org/abs/2602.19764"
      },
      "published_at": "2026-02-23T12:12:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8358496600569154,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.035849660056915
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19764",
      "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed t",
      "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling"
    },
    {
      "arxiv_id": "2602.19699",
      "authors": [
        "Elisa Alboni",
        "Pietro Noah Crestaz",
        "Elias Fontanari",
        "Andrea Del Prete"
      ],
      "categories": [
        "cs.RO",
        "math.OC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.770009+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization",
          "url": "https://arxiv.org/abs/2602.19699"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization",
        "url": "https://arxiv.org/abs/2602.19699"
      },
      "published_at": "2026-02-23T10:45:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8308005355881578,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.030800535588158
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19699",
      "summary": "Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost traje",
      "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization"
    },
    {
      "arxiv_id": "2602.19653",
      "authors": [
        "Bailey Dacre",
        "Rodrigo Moreno",
        "J√∏rn Lambertsen",
        "Kasper Stoy",
        "Andr√©s Fa√≠√±a"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.770364+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array",
          "url": "https://arxiv.org/abs/2602.19653"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array",
        "url": "https://arxiv.org/abs/2602.19653"
      },
      "published_at": "2026-02-23T09:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8278594887199184,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.027859488719919
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19653",
      "summary": "Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust contro",
      "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array"
    },
    {
      "arxiv_id": "2602.19643",
      "authors": [
        "Alex Robertson",
        "Huizhi Liang",
        "Mahbub Gani",
        "Rohit Kumar",
        "Srijith Rajamohan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.838999+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
          "url": "https://arxiv.org/abs/2602.19643"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
        "url": "https://arxiv.org/abs/2602.19643"
      },
      "published_at": "2026-02-23T09:41:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.827125855218518,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.027125855218518
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19643",
      "summary": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and mo",
      "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge"
    },
    {
      "arxiv_id": "2602.19628",
      "authors": [
        "Pasan Peiris",
        "Matthias Galster",
        "Antonija Mitrovic",
        "Sanna Malinen",
        "Raul Vincent Lumapas",
        "Jay Holland"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:43.726516+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training",
          "url": "https://arxiv.org/abs/2602.19628"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training",
        "url": "https://arxiv.org/abs/2602.19628"
      },
      "published_at": "2026-02-23T09:16:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8256872959872077,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.025687295987208
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19628",
      "summary": "Watching training videos passively leads to superficial learning. Adding gamification can increase engagement. We study how software engineering students and industry practitioners view gamifying video-based training. We conducted a survey with students and professionals. Students and professionals share similar perceptions toward video-based training in general and support combining gamification and video-based training. Our findings can inform the design of gamified training solutions for soft",
      "title": "Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training"
    },
    {
      "arxiv_id": "2602.19626",
      "authors": [
        "Roberto Tacconelli"
      ],
      "categories": [
        "cs.IT",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.839206+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
          "url": "https://arxiv.org/abs/2602.19626"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
        "url": "https://arxiv.org/abs/2602.19626"
      },
      "published_at": "2026-02-23T09:14:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8255372715332798,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.02553727153328
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19626",
      "summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local pre",
      "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding"
    },
    {
      "arxiv_id": "2602.19855",
      "authors": [
        "Francois Vandenhende",
        "Anna Georgiou",
        "Theodoros Psaras",
        "Ellie Karekla"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.838189+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals",
          "url": "https://arxiv.org/abs/2602.19855"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals",
        "url": "https://arxiv.org/abs/2602.19855"
      },
      "published_at": "2026-02-23T13:55:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8418351246486234,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.9000000000000004,
        "total_score": 7.941835124648624
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19855",
      "summary": "We present SHIELD, a novel methodology for automated and integrated safety signal detection in clinical trials. SHIELD combines disproportionality analysis with semantic clustering of adverse event (AE) terms applied to MedDRA term embeddings. For each AE, the pipeline computes an information-theoretic disproportionality measure (Information Component) with effect size derived via empirical Bayesian shrinkage. A utility matrix is constructed by weighting semantic term-term similarities by signal",
      "title": "SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals"
    },
    {
      "arxiv_id": "2602.20020",
      "authors": [
        "Wanyong Feng",
        "Andrew Lan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.830993+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "gencat: Generative computerized adaptive testing",
          "url": "https://arxiv.org/abs/2602.20020"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "gencat: Generative computerized adaptive testing",
        "url": "https://arxiv.org/abs/2602.20020"
      },
      "published_at": "2026-02-23T16:28:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508371568296307,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 7.800837156829631
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20020",
      "summary": "Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response T",
      "title": "gencat: Generative computerized adaptive testing"
    },
    {
      "arxiv_id": "2602.19639",
      "authors": [
        "Made Krisnanda",
        "Raymond Chiong",
        "Yang Yang",
        "Kirill Glavatskiy"
      ],
      "categories": [
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:41.736040+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Effects of Property Recovery Incentives and Social Interaction on Self-Evacuation Decisions in Natural Disasters: An Agent-Based Modelling Approach",
          "url": "https://arxiv.org/abs/2602.19639"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Effects of Property Recovery Incentives and Social Interaction on Self-Evacuation Decisions in Natural Disasters: An Agent-Based Modelling Approach",
        "url": "https://arxiv.org/abs/2602.19639"
      },
      "published_at": "2026-02-23T09:34:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8266856044931146,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 7.776685604493115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19639",
      "summary": "Understanding evacuation decision-making behaviour is one of the key components for designing disaster mitigation policies. This study investigates how communications between household agents in a community influence self-evacuation decisions. We develop an agent-based model that simulates household agents' decisions to evacuate or stay. These agents interact within the framework of evolutionary game theory, effectively competing for limited shared resources, which include property recovery fund",
      "title": "Effects of Property Recovery Incentives and Social Interaction on Self-Evacuation Decisions in Natural Disasters: An Agent-Based Modelling Approach"
    },
    {
      "arxiv_id": "2602.20054",
      "authors": [
        "A. Giordano",
        "G. De Meurichy",
        "V. Telazzi",
        "C. Mucignat",
        "I. Lunati",
        "D. A. L. M. Louchard",
        "M. Iovieno",
        "S. F. Armanini",
        "M. Kovac"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.768177+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement",
          "url": "https://arxiv.org/abs/2602.20054"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement",
        "url": "https://arxiv.org/abs/2602.20054"
      },
      "published_at": "2026-02-23T17:04:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8529422304064738,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.6000000000000005,
        "total_score": 7.652942230406475
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20054",
      "summary": "This work assesses the hydrodynamic efficiency of Underwater Unmanned Vehicles (UUVs) equipped with soft morphing wings compared to conventional rigid wings. Unlike rigid wings, deformable counterparts can alter their aerodynamic properties on demand. Improvements in hydrodynamic efficiency extend a UUV's operational range and may determine mission feasibility. Structural and Computational Fluid Dynamics (CFD) simulations were conducted for both a soft morphing wing and a UUV incorporating it. T",
      "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement"
    },
    {
      "arxiv_id": "2602.19711",
      "authors": [
        "Krzysztof Kutt",
        "El≈ºbieta Sroka",
        "Oleksandra Ishchuk",
        "Luiz do Valle Miranda"
      ],
      "categories": [
        "cs.IR",
        "cs.DL",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.746050+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs",
          "url": "https://arxiv.org/abs/2602.19711"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs",
        "url": "https://arxiv.org/abs/2602.19711"
      },
      "published_at": "2026-02-23T11:02:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.831759778903909,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.45,
        "total_score": 7.48175977890391
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19711",
      "summary": "The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowled",
      "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs"
    },
    {
      "arxiv_id": "2602.19943",
      "authors": [
        "Abulikemu Abuduweili",
        "Yuyang Pang",
        "Feihan Li",
        "Changliu Liu"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.768696+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Scaling Law of Neural Koopman Operators",
          "url": "https://arxiv.org/abs/2602.19943"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Scaling Law of Neural Koopman Operators",
        "url": "https://arxiv.org/abs/2602.19943"
      },
      "published_at": "2026-02-23T15:13:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.846414294569325,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.1500000000000004,
        "total_score": 7.196414294569325
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19943",
      "summary": "Data-driven neural Koopman operator theory has emerged as a powerful tool for linearizing and controlling nonlinear robotic systems. However, the performance of these data-driven models fundamentally depends on the trade-off between sample size and model dimensions, a relationship for which the scaling laws have remained unclear. This paper establishes a rigorous framework to address this challenge by deriving and empirically validating scaling laws that connect sample size, latent space dimensi",
      "title": "Scaling Law of Neural Koopman Operators"
    },
    {
      "arxiv_id": "2602.19862",
      "authors": [
        "Lars Fischer",
        "Daniel Fl√∂gel",
        "S√∂ren Hohmann"
      ],
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.769279+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems",
          "url": "https://arxiv.org/abs/2602.19862"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems",
        "url": "https://arxiv.org/abs/2602.19862"
      },
      "published_at": "2026-02-23T14:01:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8421869371228453,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.1500000000000004,
        "total_score": 7.1921869371228455
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19862",
      "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in un",
      "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems"
    },
    {
      "arxiv_id": "2602.19728",
      "authors": [
        "Adamya Shyam",
        "Venkateswara Rao Kagita",
        "Bharti Rana",
        "Vikas Kumar"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.745826+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "GrIT: Group Informed Transformer for Sequential Recommendation",
          "url": "https://arxiv.org/abs/2602.19728"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "GrIT: Group Informed Transformer for Sequential Recommendation",
        "url": "https://arxiv.org/abs/2602.19728"
      },
      "published_at": "2026-02-23T11:26:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8331452724868029,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.0,
        "total_score": 7.033145272486803
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19728",
      "summary": "Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling",
      "title": "GrIT: Group Informed Transformer for Sequential Recommendation"
    },
    {
      "arxiv_id": "2602.19883",
      "authors": [
        "Daham Mustafa",
        "Diego Collarana",
        "Yixin Peng",
        "Rafiqul Haque",
        "Christoph Lange-Bever",
        "Christoph Quix",
        "Stephan Decker"
      ],
      "categories": [
        "cs.CL",
        "cs.LO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.837751+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection",
          "url": "https://arxiv.org/abs/2602.19883"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection",
        "url": "https://arxiv.org/abs/2602.19883"
      },
      "published_at": "2026-02-23T14:28:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8437440813012508,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 6.293744081301251
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19883",
      "summary": "ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics that maps each ODRL constraint to the set of knowledge-base concepts satisfying it. Conflict detection reduces to denotation intersection under a three-valued verdict -- Conflict, Compatible, or Unknown -- that is sound ",
      "title": "Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection"
    },
    {
      "arxiv_id": "2602.19815",
      "authors": [
        "Akhilesh Kakolu Ramarao"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.838576+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Keyboards for the Endangered Idu Mishmi Language",
          "url": "https://arxiv.org/abs/2602.19815"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Keyboards for the Endangered Idu Mishmi Language",
        "url": "https://arxiv.org/abs/2602.19815"
      },
      "published_at": "2026-02-23T13:13:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8393872354883498,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 2.25,
        "total_score": 6.28938723548835
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19815",
      "summary": "We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training pr",
      "title": "Keyboards for the Endangered Idu Mishmi Language"
    }
  ],
  "run_date": "2026-02-23",
  "run_id": "c69faafa-b71c-42aa-a997-f1ef500a209c",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-23T23:59:59+00:00",
    "items_total": 231,
    "run_id": "c69faafa-b71c-42aa-a997-f1ef500a209c-2026-02-23",
    "started_at": "2026-02-22T23:59:59+00:00",
    "stories_total": 231,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": null,
      "authors": [
        "Sanhita Sarkar"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Bedrock",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Healthcare",
        "Partner solutions",
        "Technical How-to",
        "AI/ML",
        "Amazon OpenSearch Service",
        "Generative AI",
        "machine-learning"
      ],
      "entities": [
        "aws",
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:36.298481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS",
          "url": "https://aws.amazon.com/blogs/machine-learning/agentic-ai-with-multi-model-framework-using-hugging-face-smolagents-on-aws"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS",
        "url": "https://aws.amazon.com/blogs/machine-learning/agentic-ai-with-multi-model-framework-using-hugging-face-smolagents-on-aws"
      },
      "published_at": "2026-02-23T15:47:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8483788021227392,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.348378802122738
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:b23c2e47f2f1c478",
      "summary": "Hugging Face smolagents is an open source Python library designed to make it straightforward to build and run agents using a few lines of code. We will show you how to build an agentic AI solution by integrating Hugging Face smolagents with Amazon Web Services (AWS) managed services. You'll learn how to deploy a healthcare AI agent that demonstrates multi-model deployment options, vector-enhanced knowledge retrieval, and clinical decision support capabilities.",
      "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS"
    },
    {
      "arxiv_id": "2602.19672",
      "authors": [],
      "categories": [],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.870588+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
          "url": "https://arxiv.org/abs/2602.19672"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
        "url": "https://arxiv.org/abs/2602.19672"
      },
      "published_at": "2026-02-23T10:17:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8291761029494228,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.029176102949423
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19672",
      "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
      "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Johannes Maunz, Tobias B√∂sch Borgards, Bartlomiej Gralewicz"
      ],
      "categories": [
        "Amazon SageMaker",
        "Amazon SageMaker HyperPod",
        "Artificial Intelligence",
        "Intermediate (200)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.298239+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod",
          "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-ai-model-production-at-hexagon-with-amazon-sagemaker-hyperpod"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod",
        "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-ai-model-production-at-hexagon-with-amazon-sagemaker-hyperpod"
      },
      "published_at": "2026-02-23T17:29:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8544144299397097,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.45,
        "total_score": 10.80441442993971
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:ec9604dedcf28634",
      "summary": "In this blog post, we demonstrate how Hexagon collaborated with Amazon Web Services to scale their AI model production by pretraining state-of-the-art segmentation models, using the model training infrastructure of Amazon SageMaker HyperPod.",
      "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-24T09:02:59.820598+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 390092,
        "likes": 964,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-397b-a17b",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-397B-A17B",
        "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
      },
      "published_at": "2026-02-23T13:56:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8418682530715604,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.64186825307156
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-397b-a17b",
      "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
      "title": "Qwen/Qwen3.5-397B-A17B"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-24T09:02:59.820823+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 68820,
        "likes": 83,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
      },
      "published_at": "2026-02-23T13:55:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8418205095823544,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.641820509582354
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
      "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a...",
      "title": "Qwen/Qwen3.5-397B-A17B-FP8"
    }
  ]
}