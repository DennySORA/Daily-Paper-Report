{
  "archive_dates": [
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-25T08:12:22.082677+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:59.268831+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 8536,
          "likes": 285
        },
        "hf_model_id": "mistralai/magistral-small-2509",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Magistral-Small-2509",
            "url": "https://huggingface.co/mistralai/Magistral-Small-2509"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Magistral-Small-2509",
          "url": "https://huggingface.co/mistralai/Magistral-Small-2509"
        },
        "published_at": "2026-02-23T18:04:17+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8530868282982215,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.653086828298221
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/magistral-small-2509",
        "summary": "Building upon Mistral Small 3.2 (2506), **with added reasoning capabilities**, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized. Learn more about Magistral in our blog post. The model was presented in the paper Magistral. - **Multimodality**: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision. - **Performance upgrade**: Magistral Small 1.2 should give you significantly better performance than Magistral Small 1.1 as seen in the benchmark results. - **Better tone and persona**: You should experience better LaTeX and Markdown formatting, and shorter...",
        "title": "mistralai/Magistral-Small-2509"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.20114",
      "authors": [
        "Kairan Zhao",
        "Iurie Luca",
        "Peter Triantafillou"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:36.860069+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Benchmarking Unlearning for Vision Transformers",
          "url": "https://arxiv.org/abs/2602.20114"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Benchmarking Unlearning for Vision Transformers",
        "url": "https://arxiv.org/abs/2602.20114"
      },
      "published_at": "2026-02-23T18:33:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8548055921387815,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.054805592138782
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20114",
      "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for V",
      "title": "Benchmarking Unlearning for Vision Transformers"
    },
    {
      "arxiv_id": "2602.20008",
      "authors": [
        "Louis Fabrice Tshimanga",
        "Andrea Zanola",
        "Federico Del Pup",
        "Manfredo Atzori"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:39.757339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
          "url": "https://arxiv.org/abs/2602.20008"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
        "url": "https://arxiv.org/abs/2602.20008"
      },
      "published_at": "2026-02-23T16:15:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8466743914788094,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.04667439147881
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20008",
      "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism",
      "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation"
    },
    {
      "arxiv_id": "2602.19961",
      "authors": [
        "Yibo Yan",
        "Jiahao Huo",
        "Guanbo Feng",
        "Mingdong Ou",
        "Yi Cao",
        "Xin Zou",
        "Shuliang Liu",
        "Yuanhuiyi Lyu",
        "Yu Huang",
        "Jungang Li",
        "Kening Zheng",
        "Xu Zheng",
        "Philip S. Yu",
        "James Kwok",
        "Xuming Hu"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:38.837000+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
          "url": "https://arxiv.org/abs/2602.19961"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
        "url": "https://arxiv.org/abs/2602.19961"
      },
      "published_at": "2026-02-23T15:27:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8438597720953661,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.043859772095367
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19961",
      "summary": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through t",
      "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval"
    },
    {
      "arxiv_id": "2602.19837",
      "authors": [
        "Bj√∂rn Hoppmann",
        "Christoph Scholz"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-24T09:02:36.867406+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
          "url": "https://arxiv.org/abs/2602.19837"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
        "url": "https://arxiv.org/abs/2602.19837"
      },
      "published_at": "2026-02-23T13:39:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8375709779603269,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.037570977960327
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19837",
      "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that",
      "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent"
    },
    {
      "arxiv_id": "2602.19816",
      "authors": [
        "Yungang Yi"
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:36.868039+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
          "url": "https://arxiv.org/abs/2602.19816"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
        "url": "https://arxiv.org/abs/2602.19816"
      },
      "published_at": "2026-02-23T13:13:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8360436113662054,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.036043611366205
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19816",
      "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music ",
      "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling"
    },
    {
      "arxiv_id": "2602.19714",
      "authors": [
        "Joel Bucher",
        "Lahari Goswami",
        "Sverrir Thorgeirsson",
        "April Yi Wang"
      ],
      "categories": [
        "cs.HC",
        "cs.SE"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-24T09:02:43.726306+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git",
          "url": "https://arxiv.org/abs/2602.19714"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git",
        "url": "https://arxiv.org/abs/2602.19714"
      },
      "published_at": "2026-02-23T11:05:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8286594301592063,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.028659430159207
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19714",
      "summary": "Git is widely used for collaborative software development, but it can be challenging for newcomers. While most learning tools focus on individual workflows, Git is inherently collaborative. We present GitAcademy, a browser-based learning platform that embeds a full Git environment with a split-view collaborative mode: learners work on their own local repositories connected to a shared remote repository, while simultaneously seeing their partner's actions mirrored in real time. This design is not",
      "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git"
    },
    {
      "arxiv_id": "2602.20161",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753157+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
          "url": "https://arxiv.org/abs/2602.20161"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
        "url": "https://arxiv.org/abs/2602.20161"
      },
      "published_at": "2026-02-23T18:59:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563920144654028,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.056392014465404
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20161",
      "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
      "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device"
    },
    {
      "arxiv_id": "2602.20160",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753482+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
          "url": "https://arxiv.org/abs/2602.20160"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
        "url": "https://arxiv.org/abs/2602.20160"
      },
      "published_at": "2026-02-23T18:59:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563791290343467,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.056379129034347
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20160",
      "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
      "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction"
    },
    {
      "arxiv_id": "2602.20159",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857474+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "A Very Big Video Reasoning Suite",
          "url": "https://arxiv.org/abs/2602.20159"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "A Very Big Video Reasoning Suite",
        "url": "https://arxiv.org/abs/2602.20159"
      },
      "published_at": "2026-02-23T18:59:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563751643253342,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.056375164325335
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20159",
      "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "title": "A Very Big Video Reasoning Suite"
    },
    {
      "arxiv_id": "2602.20140",
      "authors": [
        "Akshay Subramanian",
        "Elton Pan",
        "Juno Nam",
        "Maurice Weiler",
        "Shuhui Qu",
        "Cheol Woo Park",
        "Tommi S. Jaakkola",
        "Elsa Olivetti",
        "Rafael Gomez-Bombarelli"
      ],
      "categories": [
        "physics.chem-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:50.719997+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-alignment",
          "tier": 1,
          "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
          "url": "https://arxiv.org/abs/2602.20140"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-alignment",
        "tier": 1,
        "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
        "url": "https://arxiv.org/abs/2602.20140"
      },
      "published_at": "2026-02-23T18:52:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8559312330470729,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.055931233047072
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20140",
      "summary": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameter",
      "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment"
    },
    {
      "arxiv_id": "2602.20093",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.744691+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
          "url": "https://arxiv.org/abs/2602.20093"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
        "url": "https://arxiv.org/abs/2602.20093"
      },
      "published_at": "2026-02-23T18:02:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8530009315187199,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.05300093151872
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20093",
      "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
      "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation"
    },
    {
      "arxiv_id": "2602.20021",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.864097+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Agents of Chaos",
          "url": "https://arxiv.org/abs/2602.20021"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Agents of Chaos",
        "url": "https://arxiv.org/abs/2602.20021"
      },
      "published_at": "2026-02-23T16:28:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8474489038110059,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.047448903811006
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20021",
      "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
      "title": "Agents of Chaos"
    },
    {
      "arxiv_id": "2602.19929",
      "authors": [
        "Chenran Kou",
        "Changsheng You",
        "Mingjiang Wu",
        "Dingzhu Wen",
        "Zezhong Zhang",
        "Chengwen Xing"
      ],
      "categories": [
        "cs.NI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.842832+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
          "url": "https://arxiv.org/abs/2602.19929"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models",
        "url": "https://arxiv.org/abs/2602.19929"
      },
      "published_at": "2026-02-23T15:06:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8426212628080044,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.042621262808005
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19929",
      "summary": "For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing gene",
      "title": "BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models"
    },
    {
      "arxiv_id": "2602.19895",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.769682+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.19895"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.19895"
      },
      "published_at": "2026-02-23T14:37:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8408958532197104,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.04089585321971
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19895",
      "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
      "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning"
    },
    {
      "arxiv_id": "2602.19811",
      "authors": [
        "Laurent Bindschaedler"
      ],
      "categories": [
        "cs.DB"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.843687+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
          "url": "https://arxiv.org/abs/2602.19811"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)",
        "url": "https://arxiv.org/abs/2602.19811"
      },
      "published_at": "2026-02-23T13:12:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8359507227921811,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.035950722792181
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19811",
      "summary": "Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema",
      "title": "Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)"
    },
    {
      "arxiv_id": "2602.20044",
      "authors": [
        "H. C. W. Price",
        "H. AlMuhanna",
        "P. M. Bassani",
        "M. Ho",
        "T. S. Evans"
      ],
      "categories": [
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:48.734281+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-agents",
          "tier": 1,
          "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook",
          "url": "https://arxiv.org/abs/2602.20044"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-agents",
        "tier": 1,
        "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook",
        "url": "https://arxiv.org/abs/2602.20044"
      },
      "published_at": "2026-02-23T16:57:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8491169967138702,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 8.79911699671387
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20044",
      "summary": "Within twelve days of launch, an AI-native social platform exhibits extreme attention concentration, hierarchical role separation, and one-way attention flow, consistent with the hypothesis that stratification in agent ecosystems can emerge rapidly rather than gradually. We analyse publicly observable traces from a 12-day window of Moltbook (28 January -- 8 February 2026), comprising 20,040 posts and 192,410 comments from 15,083 accounts across 759 submolts. We construct co-participation and dir",
      "title": "Let There Be Claws: An Early Social Network Analysis of AI Agents on Moltbook"
    },
    {
      "arxiv_id": "2602.19694",
      "authors": [
        "Bo Liu",
        "Tong Li",
        "Zhu Xiao",
        "Ruihui Li",
        "Geyong Min",
        "Zhuo Tang",
        "Kenli Li"
      ],
      "categories": [
        "cs.ET"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:47.844054+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-llm",
          "tier": 1,
          "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs",
          "url": "https://arxiv.org/abs/2602.19694"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-llm",
        "tier": 1,
        "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs",
        "url": "https://arxiv.org/abs/2602.19694"
      },
      "published_at": "2026-02-23T10:42:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.827307249334288,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 8.777307249334289
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19694",
      "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose",
      "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs"
    },
    {
      "arxiv_id": "2602.20157",
      "authors": [
        "Zhongxiao Cong",
        "Qitao Zhao",
        "Minsik Jeon",
        "Shubham Tulsiani"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.753878+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
          "url": "https://arxiv.org/abs/2602.20157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
        "url": "https://arxiv.org/abs/2602.20157"
      },
      "published_at": "2026-02-23T18:59:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563642614701934,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056364261470193
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20157",
      "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from ",
      "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning"
    },
    {
      "arxiv_id": "2602.20156",
      "authors": [
        "David Schmotz",
        "Luca Beurer-Kellner",
        "Sahar Abdelnabi",
        "Maksym Andriushchenko"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.761579+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
          "url": "https://arxiv.org/abs/2602.20156"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
        "url": "https://arxiv.org/abs/2602.20156"
      },
      "published_at": "2026-02-23T18:59:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563612879883367,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056361287988336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20156",
      "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark ev",
      "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks"
    },
    {
      "arxiv_id": "2602.20152",
      "authors": [
        "Zhenyao Ma",
        "Yue Liang",
        "Dongxu Li"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857733+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
          "url": "https://arxiv.org/abs/2602.20152"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
        "url": "https://arxiv.org/abs/2602.20152"
      },
      "published_at": "2026-02-23T18:59:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8563384916371045,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056338491637105
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20152",
      "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically in",
      "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data"
    },
    {
      "arxiv_id": "2602.20150",
      "authors": [
        "Wei-Cheng Huang",
        "Jiaheng Han",
        "Xiaohan Ye",
        "Zherong Pan",
        "Kris Hauser"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.754140+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
          "url": "https://arxiv.org/abs/2602.20150"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
        "url": "https://arxiv.org/abs/2602.20150"
      },
      "published_at": "2026-02-23T18:58:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8562988472542669,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056298847254267
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20150",
      "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical c",
      "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization"
    },
    {
      "arxiv_id": "2602.20144",
      "authors": [
        "Zehao Wang",
        "Mingzhe Han",
        "Wei Cheng",
        "Yue-Kai Huang",
        "Philip Ji",
        "Denton Wu",
        "Mahdi Safari",
        "Flemming Holtorf",
        "Kenaish AlQubaisi",
        "Norbert M. Linke",
        "Danyang Zhuo",
        "Yiran Chen",
        "Ting Wang",
        "Dirk Englund",
        "Tingjun Chen"
      ],
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.NI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.857998+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Agentic AI for Scalable and Robust Optical Systems Control",
          "url": "https://arxiv.org/abs/2602.20144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Agentic AI for Scalable and Robust Optical Systems Control",
        "url": "https://arxiv.org/abs/2602.20144"
      },
      "published_at": "2026-02-23T18:54:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8560689460241433,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056068946024144
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20144",
      "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordinati",
      "title": "Agentic AI for Scalable and Robust Optical Systems Control"
    },
    {
      "arxiv_id": "2602.20141",
      "authors": [
        "Clarisse Wibault",
        "Johannes Forkel",
        "Sebastian Towers",
        "Tiphaine Wibault",
        "Juan Duque",
        "George Whittle",
        "Andreas Schaab",
        "Yucheng Yang",
        "Chiyuan Wang",
        "Michael Osborne",
        "Benjamin Moll",
        "Jakob Foerster"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.858213+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
          "url": "https://arxiv.org/abs/2602.20141"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
        "url": "https://arxiv.org/abs/2602.20141"
      },
      "published_at": "2026-02-23T18:53:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8559867118693425,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055986711869343
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20141",
      "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected ",
      "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games"
    },
    {
      "arxiv_id": "2602.20137",
      "authors": [
        "Martin Sinnona",
        "Valentin Bonas",
        "Emmanuel Iarussi",
        "Viviana Siless"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.754344+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Do Large Language Models Understand Data Visualization Rules?",
          "url": "https://arxiv.org/abs/2602.20137"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Do Large Language Models Understand Data Visualization Rules?",
        "url": "https://arxiv.org/abs/2602.20137"
      },
      "published_at": "2026-02-23T18:47:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556717191755185,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05567171917552
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20137",
      "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivat",
      "title": "Do Large Language Models Understand Data Visualization Rules?"
    },
    {
      "arxiv_id": "2602.20135",
      "authors": [
        "Mohammad Amanlou",
        "Erfan Shafiee Moghaddam",
        "Yasaman Amou Jafari",
        "Mahdi Noori",
        "Farhan Farsi",
        "Behnam Bahrak"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.858466+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
          "url": "https://arxiv.org/abs/2602.20135"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
        "url": "https://arxiv.org/abs/2602.20135"
      },
      "published_at": "2026-02-23T18:46:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8555885329133298,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05558853291333
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20135",
      "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entiti",
      "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration"
    },
    {
      "arxiv_id": "2602.20133",
      "authors": [
        "Mert Cemri",
        "Shubham Agrawal",
        "Akshat Gupta",
        "Shu Liu",
        "Audrey Cheng",
        "Qiuyang Mang",
        "Ashwin Naren",
        "Lutfi Eren Erdogan",
        "Koushik Sen",
        "Matei Zaharia",
        "Alex Dimakis",
        "Ion Stoica"
      ],
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859012+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
          "url": "https://arxiv.org/abs/2602.20133"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
        "url": "https://arxiv.org/abs/2602.20133"
      },
      "published_at": "2026-02-23T18:45:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8555330798981192,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055533079898119
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20133",
      "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promis",
      "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization"
    },
    {
      "arxiv_id": "2602.20132",
      "authors": [
        "Wendi Li",
        "Sharon Li"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.762454+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LAD: Learning Advantage Distribution for Reasoning",
          "url": "https://arxiv.org/abs/2602.20132"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LAD: Learning Advantage Distribution for Reasoning",
        "url": "https://arxiv.org/abs/2602.20132"
      },
      "published_at": "2026-02-23T18:44:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8554528774314281,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055452877431428
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20132",
      "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishin",
      "title": "LAD: Learning Advantage Distribution for Reasoning"
    },
    {
      "arxiv_id": "2602.20130",
      "authors": [
        "Zaifu Zhan",
        "Min Zeng",
        "Shuang Zhou",
        "Yiran Song",
        "Xiaoyi Chen",
        "Yu Hou",
        "Yifan Wu",
        "Yang Ruan",
        "Rui Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859218+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
          "url": "https://arxiv.org/abs/2602.20130"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
        "url": "https://arxiv.org/abs/2602.20130"
      },
      "published_at": "2026-02-23T18:42:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8553736724986175,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055373672498618
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20130",
      "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and ",
      "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering"
    },
    {
      "arxiv_id": "2602.20122",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859433+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NanoKnow: How to Know What Your Language Model Knows",
          "url": "https://arxiv.org/abs/2602.20122"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NanoKnow: How to Know What Your Language Model Knows",
        "url": "https://arxiv.org/abs/2602.20122"
      },
      "published_at": "2026-02-23T18:37:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8550757296370228,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055075729637023
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20122",
      "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions q",
      "title": "NanoKnow: How to Know What Your Language Model Knows"
    },
    {
      "arxiv_id": "2602.20119",
      "authors": [
        "Jiahui Fu",
        "Junyu Nan",
        "Lingfeng Sun",
        "Hongyu Li",
        "Jianing Qian",
        "Jennifer L. Barry",
        "Kris Kitani",
        "George Konidaris"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859637+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
          "url": "https://arxiv.org/abs/2602.20119"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
        "url": "https://arxiv.org/abs/2602.20119"
      },
      "published_at": "2026-02-23T18:35:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8549263023765038,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054926302376504
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20119",
      "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high lev",
      "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning"
    },
    {
      "arxiv_id": "2602.20117",
      "authors": [
        "Andre He",
        "Nathaniel Weir",
        "Kaj Bostrom",
        "Allen Nie",
        "Darion Cassel",
        "Sam Bayless",
        "Huzefa Rangwala"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.859841+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
          "url": "https://arxiv.org/abs/2602.20117"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
        "url": "https://arxiv.org/abs/2602.20117"
      },
      "published_at": "2026-02-23T18:34:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8548778183476305,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05487781834763
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20117",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diver",
      "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models"
    },
    {
      "arxiv_id": "2602.20102",
      "authors": [
        "Thanh Q. Tran",
        "Arun Verma",
        "Kiwan Wong",
        "Bryan Kian Hsiang Low",
        "Daniela Rus",
        "Wei Xiao"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.860691+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
          "url": "https://arxiv.org/abs/2602.20102"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
        "url": "https://arxiv.org/abs/2602.20102"
      },
      "published_at": "2026-02-23T18:19:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8540045874257779,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054004587425778
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20102",
      "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints di",
      "title": "BarrierSteer: LLM Safety via Learning Barrier Steering"
    },
    {
      "arxiv_id": "2602.20097",
      "authors": [
        "Pu Jiao",
        "Sheng Di",
        "Jiannan Tian",
        "Mingze Xia",
        "Xuan Wu",
        "Yang Zhang",
        "Xin Liang",
        "Franck Cappello"
      ],
      "categories": [
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:54.700582+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation",
          "url": "https://arxiv.org/abs/2602.20097"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation",
        "url": "https://arxiv.org/abs/2602.20097"
      },
      "published_at": "2026-02-23T18:09:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8533771641830297,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05337716418303
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20097",
      "summary": "Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or lar",
      "title": "Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation"
    },
    {
      "arxiv_id": "2602.20094",
      "authors": [
        "Yuzhe Wang",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861133+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
          "url": "https://arxiv.org/abs/2602.20094"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
        "url": "https://arxiv.org/abs/2602.20094"
      },
      "published_at": "2026-02-23T18:06:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8532033457983548,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053203345798355
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20094",
      "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a ",
      "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching"
    },
    {
      "arxiv_id": "2602.20092",
      "authors": [
        "Leshem Choshen",
        "Ryan Cotterell",
        "Mustafa Omer Gul",
        "Jaap Jumelet",
        "Tal Linzen",
        "Aaron Mueller",
        "Suchir Salhan",
        "Raj Sanjay Shah",
        "Alex Warstadt",
        "Ethan Gotlieb Wilcox"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.829640+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
          "url": "https://arxiv.org/abs/2602.20092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
        "url": "https://arxiv.org/abs/2602.20092"
      },
      "published_at": "2026-02-23T18:02:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.85297427565611,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05297427565611
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20092",
      "summary": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.\n  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evalua",
      "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop"
    },
    {
      "arxiv_id": "2602.20091",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.829885+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "How Retrieved Context Shapes Internal Representations in RAG",
          "url": "https://arxiv.org/abs/2602.20091"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "How Retrieved Context Shapes Internal Representations in RAG",
        "url": "https://arxiv.org/abs/2602.20091"
      },
      "published_at": "2026-02-23T18:02:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8529555183262001,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0529555183262
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20091",
      "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediat",
      "title": "How Retrieved Context Shapes Internal Representations in RAG"
    },
    {
      "arxiv_id": "2602.20089",
      "authors": [
        "Zanxi Ruan",
        "Qiuyu Kong",
        "Songqun Gao",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861348+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
          "url": "https://arxiv.org/abs/2602.20089"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
        "url": "https://arxiv.org/abs/2602.20089"
      },
      "published_at": "2026-02-23T17:57:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8526919721014778,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052691972101478
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20089",
      "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them ",
      "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues"
    },
    {
      "arxiv_id": "2602.20084",
      "authors": [
        "Martin Sinnona",
        "Valentin Bonas",
        "Viviana Siless",
        "Emmanuel Iarussi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.755339+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Do Large Language Models Understand Data Visualization Principles?",
          "url": "https://arxiv.org/abs/2602.20084"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Do Large Language Models Understand Data Visualization Principles?",
        "url": "https://arxiv.org/abs/2602.20084"
      },
      "published_at": "2026-02-23T17:51:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8523061768093212,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05230617680932
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20084",
      "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into f",
      "title": "Do Large Language Models Understand Data Visualization Principles?"
    },
    {
      "arxiv_id": "2602.20083",
      "authors": [
        "Xinzhao Li",
        "Alptekin Vardar",
        "Franz M√ºller",
        "Navya Goli",
        "Umamaheswara Tida",
        "Kai Ni",
        "X. Sharon Hu",
        "Thomas K√§mpfe",
        "Ruiyang Qin"
      ],
      "categories": [
        "cs.ET",
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:55.702309+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-retrieval",
          "tier": 1,
          "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval",
          "url": "https://arxiv.org/abs/2602.20083"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-retrieval",
        "tier": 1,
        "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval",
        "url": "https://arxiv.org/abs/2602.20083"
      },
      "published_at": "2026-02-23T17:49:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8521917544946602,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052191754494661
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20083",
      "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimensio",
      "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval"
    },
    {
      "arxiv_id": "2602.20078",
      "authors": [
        "Shan Yang",
        "Yang Liu"
      ],
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861546+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
          "url": "https://arxiv.org/abs/2602.20078"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
        "url": "https://arxiv.org/abs/2602.20078"
      },
      "published_at": "2026-02-23T17:45:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8519530953216513,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051953095321652
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20078",
      "summary": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Œò(N)$, yielding sample complexity $\\mathcal{O}(N/Œµ)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analy",
      "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning"
    },
    {
      "arxiv_id": "2602.20070",
      "authors": [
        "Florentin Coeurdoux",
        "Etienne Lempereur",
        "Nathana√´l Cuvelle-Magar",
        "Thomas Eboli",
        "St√©phane Mallat",
        "Anastasia Borovykh",
        "Eric Vanden-Eijnden"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.763944+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
          "url": "https://arxiv.org/abs/2602.20070"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
        "url": "https://arxiv.org/abs/2602.20070"
      },
      "published_at": "2026-02-23T17:26:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508307165711506,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05083071657115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20070",
      "summary": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaœÜ(x)^\\topŒ∑_t$, where $Œ∑_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no diffi",
      "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants"
    },
    {
      "arxiv_id": "2602.20066",
      "authors": [
        "Kundan Thota",
        "Xuanhao Mu",
        "Thorsten Schlachter",
        "Veit Hagenmeyer"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.861987+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
          "url": "https://arxiv.org/abs/2602.20066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
        "url": "https://arxiv.org/abs/2602.20066"
      },
      "published_at": "2026-02-23T17:22:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8506387104734277,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050638710473429
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20066",
      "summary": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as a",
      "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images"
    },
    {
      "arxiv_id": "2602.20065",
      "authors": [
        "Natalia Moskvina",
        "Raquel Montero",
        "Masaya Yoshida",
        "Ferdy Hubers",
        "Paolo Morosi",
        "Walid Irhaymi",
        "Jin Yan",
        "Tamara Serrano",
        "Elena Pagliarini",
        "Fritz G√ºnther",
        "Evelina Leivada"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.862190+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
          "url": "https://arxiv.org/abs/2602.20065"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
        "url": "https://arxiv.org/abs/2602.20065"
      },
      "published_at": "2026-02-23T17:22:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8506308342255355,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050630834225537
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20065",
      "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked t",
      "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees"
    },
    {
      "arxiv_id": "2602.20064",
      "authors": [
        "Zac Garby",
        "Andrew D. Gordon",
        "David Sands"
      ],
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.862392+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
          "url": "https://arxiv.org/abs/2602.20064"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
        "url": "https://arxiv.org/abs/2602.20064"
      },
      "published_at": "2026-02-23T17:22:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8506200045037614,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050620004503761
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20064",
      "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or dist",
      "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow"
    },
    {
      "arxiv_id": "2602.20062",
      "authors": [
        "Nicolas Anguita",
        "Francesco Locatello",
        "Andrew M. Saxe",
        "Marco Mondelli",
        "Flavia Mancini",
        "Samuel Lippl",
        "Clementine Domine"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.764402+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
          "url": "https://arxiv.org/abs/2602.20062"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
        "url": "https://arxiv.org/abs/2602.20062"
      },
      "published_at": "2026-02-23T17:19:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8504408418458385,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05044084184584
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20062",
      "summary": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of",
      "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning"
    },
    {
      "arxiv_id": "2602.20061",
      "authors": [
        "Zoha Hayat Bhatti",
        "Bakhtawar Ahtisham",
        "Seemal Tausif",
        "Niklas George",
        "Nida ul Habib Bajwa",
        "Mobin Javed"
      ],
      "categories": [
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:46.724594+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cr",
          "tier": 1,
          "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios",
          "url": "https://arxiv.org/abs/2602.20061"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cr",
        "tier": 1,
        "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios",
        "url": "https://arxiv.org/abs/2602.20061"
      },
      "published_at": "2026-02-23T17:17:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8503424168888456,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050342416888846
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20061",
      "summary": "Large Language Models and commercial speech synthesis systems now enable highly realistic AI-generated voice scams (vishing), raising urgent concerns about deception at scale. Yet it remains unclear whether individuals can reliably distinguish AI-generated speech from human-recorded voices in realistic scam contexts and what perceptual strategies underlie their judgments. We conducted a controlled online study in which 22 participants evaluated 16 vishing-style audio clips (8 AI-generated, 8 hum",
      "title": "Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios"
    },
    {
      "arxiv_id": "2602.20060",
      "authors": [
        "Junli Wang",
        "Xueyi Liu",
        "Yinan Zheng",
        "Zebing Xing",
        "Pengfei Li",
        "Guang Li",
        "Kun Ma",
        "Guang Chen",
        "Hangjun Ye",
        "Zhongpu Xia",
        "Long Chen",
        "Qichao Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.756142+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
          "url": "https://arxiv.org/abs/2602.20060"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
        "url": "https://arxiv.org/abs/2602.20060"
      },
      "published_at": "2026-02-23T17:17:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8503158441035197,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050315844103519
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20060",
      "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propo",
      "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving"
    },
    {
      "arxiv_id": "2602.20059",
      "authors": [
        "Sarath Shekkizhar",
        "Adam Earle"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.862604+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
          "url": "https://arxiv.org/abs/2602.20059"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
        "url": "https://arxiv.org/abs/2602.20059"
      },
      "published_at": "2026-02-23T17:14:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8501416652967424,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050141665296742
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20059",
      "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal a",
      "title": "Interaction Theater: A case of LLM Agents Interacting at Scale"
    },
    {
      "arxiv_id": "2602.20057",
      "authors": [
        "Ge Yuan",
        "Qiyuan Qiao",
        "Jing Zhang",
        "Dong Xu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.862807+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
          "url": "https://arxiv.org/abs/2602.20057"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
        "url": "https://arxiv.org/abs/2602.20057"
      },
      "published_at": "2026-02-23T17:12:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8500196629794134,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050019662979414
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20057",
      "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world",
      "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation"
    },
    {
      "arxiv_id": "2602.20055",
      "authors": [
        "Apoorva Vashisth",
        "Manav Kulshrestha",
        "Pranav Bakshi",
        "Damon Conover",
        "Guillaume Sartoretti",
        "Aniket Bera"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.863021+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
          "url": "https://arxiv.org/abs/2602.20055"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
        "url": "https://arxiv.org/abs/2602.20055"
      },
      "published_at": "2026-02-23T17:10:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8498770211862274,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049877021186228
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20055",
      "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving plac",
      "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation"
    },
    {
      "arxiv_id": "2602.20053",
      "authors": [
        "Jiahui Chen",
        "Zehang Deng",
        "Zeyu Zhang",
        "Chaoyang Li",
        "Lianchen Jia",
        "Lifeng Sun"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.756541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Decoupling Defense Strategies for Robust Image Watermarking",
          "url": "https://arxiv.org/abs/2602.20053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Decoupling Defense Strategies for Robust Image Watermarking",
        "url": "https://arxiv.org/abs/2602.20053"
      },
      "published_at": "2026-02-23T17:02:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8494590710562272,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049459071056228
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20053",
      "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage ",
      "title": "Decoupling Defense Strategies for Robust Image Watermarking"
    },
    {
      "arxiv_id": "2602.20052",
      "authors": [
        "Marco Scharringhausen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.830328+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Entropy in Large Language Models",
          "url": "https://arxiv.org/abs/2602.20052"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Entropy in Large Language Models",
        "url": "https://arxiv.org/abs/2602.20052"
      },
      "published_at": "2026-02-23T17:02:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8494492394109121,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049449239410912
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20052",
      "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Ou",
      "title": "Entropy in Large Language Models"
    },
    {
      "arxiv_id": "2602.20049",
      "authors": [
        "Tobias G√ºrtler",
        "Benjamin Lucien Kaminski"
      ],
      "categories": [
        "cs.LO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:54.700858+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "noDice: Inference for Discrete Probabilistic Programs with Nondeterminism and Conditioning",
          "url": "https://arxiv.org/abs/2602.20049"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "noDice: Inference for Discrete Probabilistic Programs with Nondeterminism and Conditioning",
        "url": "https://arxiv.org/abs/2602.20049"
      },
      "published_at": "2026-02-23T16:59:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.849264425656414,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049264425656414
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20049",
      "summary": "Probabilistic programming languages (PPLs) are an expressive and intuitive means of representing complex probability distributions. In that realm, languages like Dice target an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis. Another important feature in the world of probabilistic modeling are nondeterministic ",
      "title": "noDice: Inference for Discrete Probabilistic Programs with Nondeterminism and Conditioning"
    },
    {
      "arxiv_id": "2602.20048",
      "authors": [
        "Tarakanath Paipuru"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.863466+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
          "url": "https://arxiv.org/abs/2602.20048"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
        "url": "https://arxiv.org/abs/2602.20048"
      },
      "published_at": "2026-02-23T16:58:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8492054510079424,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049205451007943
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20048",
      "summary": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI reposi",
      "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence"
    },
    {
      "arxiv_id": "2602.20046",
      "authors": [
        "Eleonora Grassucci",
        "Giordano Cicchetti",
        "Danilo Comminiello"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.764602+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Closing the gap in multimodal medical representation alignment",
          "url": "https://arxiv.org/abs/2602.20046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Closing the gap in multimodal medical representation alignment",
        "url": "https://arxiv.org/abs/2602.20046"
      },
      "published_at": "2026-02-23T16:57:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8491484460739183,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049148446073918
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20046",
      "summary": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remain",
      "title": "Closing the gap in multimodal medical representation alignment"
    },
    {
      "arxiv_id": "2602.20042",
      "authors": [
        "Han Bao",
        "Yue Huang",
        "Xiaoda Wang",
        "Zheyuan Zhang",
        "Yujun Zhou",
        "Carl Yang",
        "Xiangliang Zhang",
        "Yanfang Ye"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.830545+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
          "url": "https://arxiv.org/abs/2602.20042"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
        "url": "https://arxiv.org/abs/2602.20042"
      },
      "published_at": "2026-02-23T16:51:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8487986375361791,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04879863753618
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20042",
      "summary": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flatt",
      "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously"
    },
    {
      "arxiv_id": "2602.20041",
      "authors": [
        "Ghadah Alosaimi",
        "Maha Alsayyari",
        "Yixin Sun",
        "Stamos Katsigiannis",
        "Amir Atapour-Abarghouei",
        "Toby P. Breckon"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.757143+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
          "url": "https://arxiv.org/abs/2602.20041"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
        "url": "https://arxiv.org/abs/2602.20041"
      },
      "published_at": "2026-02-23T16:50:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8487180840806999,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0487180840807
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20041",
      "summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) s",
      "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover"
    },
    {
      "arxiv_id": "2602.20040",
      "authors": [
        "Fahmida Liza Piya",
        "Rahmatollah Beheshti"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.863665+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
          "url": "https://arxiv.org/abs/2602.20040"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
        "url": "https://arxiv.org/abs/2602.20040"
      },
      "published_at": "2026-02-23T16:49:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8486748634269533,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048674863426953
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20040",
      "summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context",
      "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization"
    },
    {
      "arxiv_id": "2602.20031",
      "authors": [
        "Theia Pearson-Vogel",
        "Martin Vanek",
        "Raymond Douglas",
        "Jan Kulveit"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.863879+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Latent Introspection: Models Can Detect Prior Concept Injections",
          "url": "https://arxiv.org/abs/2602.20031"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Latent Introspection: Models Can Detect Prior Concept Injections",
        "url": "https://arxiv.org/abs/2602.20031"
      },
      "published_at": "2026-02-23T16:39:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8480906183911995,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0480906183912
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20031",
      "summary": "We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen t",
      "title": "Latent Introspection: Models Can Detect Prior Concept Injections"
    },
    {
      "arxiv_id": "2602.20017",
      "authors": [
        "Gaurav Najpande",
        "Tampu Ravi Kumar",
        "Manan Roy Choudhury",
        "Neha Valeti",
        "Yanjie Fu",
        "Vivek Gupta"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.831223+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
          "url": "https://arxiv.org/abs/2602.20017"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
        "url": "https://arxiv.org/abs/2602.20017"
      },
      "published_at": "2026-02-23T16:23:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8471556823027934,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047155682302794
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20017",
      "summary": "Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation be",
      "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning"
    },
    {
      "arxiv_id": "2602.20003",
      "authors": [
        "Nuocheng Yang",
        "Sihua Wang",
        "Zhaohui Yang",
        "Mingzhe Chen",
        "Changchuan Yin",
        "Kaibin Huang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.864509+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Secure and Private Distributed Bayesian Federated Learning Design",
          "url": "https://arxiv.org/abs/2602.20003"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Secure and Private Distributed Bayesian Federated Learning Design",
        "url": "https://arxiv.org/abs/2602.20003"
      },
      "published_at": "2026-02-23T16:12:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8464627493373097,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04646274933731
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20003",
      "summary": "Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergen",
      "title": "A Secure and Private Distributed Bayesian Federated Learning Design"
    },
    {
      "arxiv_id": "2602.19991",
      "authors": [
        "Yaya Sy",
        "Dioula Doucour√©",
        "Christophe Cerisara",
        "Irina Illina"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:38.836410+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
          "url": "https://arxiv.org/abs/2602.19991"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
        "url": "https://arxiv.org/abs/2602.19991"
      },
      "published_at": "2026-02-23T15:57:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8455951780454765,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045595178045478
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19991",
      "summary": "Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation",
      "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text"
    },
    {
      "arxiv_id": "2602.19990",
      "authors": [
        "Monica Marconi Sciarroni",
        "Emanuele Storti"
      ],
      "categories": [
        "cs.DB",
        "cs.DC",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:44.745094+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT",
          "url": "https://arxiv.org/abs/2602.19990"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT",
        "url": "https://arxiv.org/abs/2602.19990"
      },
      "published_at": "2026-02-23T15:55:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8454933995663912,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045493399566391
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19990",
      "summary": "Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scena",
      "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT"
    },
    {
      "arxiv_id": "2602.19987",
      "authors": [
        "Ha-Anh Hoang Nguyen",
        "Tri-Duc Phan Le",
        "Duc-Hoang Pham",
        "Huy-Son Nguyen",
        "Cam-Van Thi Nguyen",
        "Duc-Trong Le",
        "Hoang-Quynh Le"
      ],
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.765526+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
          "url": "https://arxiv.org/abs/2602.19987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
        "url": "https://arxiv.org/abs/2602.19987"
      },
      "published_at": "2026-02-23T15:53:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8453691289988328,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045369128998832
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19987",
      "summary": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics ",
      "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction"
    },
    {
      "arxiv_id": "2602.19983",
      "authors": [
        "Zachary Ravichadran",
        "David Snyder",
        "Alexander Robey",
        "Hamed Hassani",
        "Vijay Kumar",
        "George J. Pappas"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.864709+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
          "url": "https://arxiv.org/abs/2602.19983"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
        "url": "https://arxiv.org/abs/2602.19983"
      },
      "published_at": "2026-02-23T15:51:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8452497681741149,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045249768174115
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19983",
      "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual r",
      "title": "Contextual Safety Reasoning and Grounding for Open-World Robots"
    },
    {
      "arxiv_id": "2602.19982",
      "authors": [
        "Alaa El Ichi",
        "Khalide Jbilou"
      ],
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.765906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Computationally Efficient Multidimensional Vision Transformer",
          "url": "https://arxiv.org/abs/2602.19982"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Computationally Efficient Multidimensional Vision Transformer",
        "url": "https://arxiv.org/abs/2602.19982"
      },
      "published_at": "2026-02-23T15:49:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8451548785615217,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045154878561522
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19982",
      "summary": "Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and ",
      "title": "A Computationally Efficient Multidimensional Vision Transformer"
    },
    {
      "arxiv_id": "2602.19980",
      "authors": [
        "Itamar Trainin",
        "Shauli Ravfogel",
        "Omri Abend",
        "Amir Feder"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.766138+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
          "url": "https://arxiv.org/abs/2602.19980"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
        "url": "https://arxiv.org/abs/2602.19980"
      },
      "published_at": "2026-02-23T15:47:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8450189212943496,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04501892129435
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19980",
      "summary": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct concl",
      "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks"
    },
    {
      "arxiv_id": "2602.19974",
      "authors": [
        "Tianyu Wang",
        "Zhiyuan Ma",
        "Qian Wang",
        "Xinyi Zhang",
        "Xinwei Long",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.757744+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
          "url": "https://arxiv.org/abs/2602.19974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
        "url": "https://arxiv.org/abs/2602.19974"
      },
      "published_at": "2026-02-23T15:39:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.844575011787423,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044575011787423
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19974",
      "summary": "Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture compri",
      "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection"
    },
    {
      "arxiv_id": "2602.19969",
      "authors": [
        "Yuxing Tian",
        "Fengran Mo",
        "Weixu Zhang",
        "Yiyan Qi",
        "Jian-Yun Nie"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.864906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
          "url": "https://arxiv.org/abs/2602.19969"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
        "url": "https://arxiv.org/abs/2602.19969"
      },
      "published_at": "2026-02-23T15:30:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8440463404207005,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044046340420701
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19969",
      "summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second,",
      "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting"
    },
    {
      "arxiv_id": "2602.19967",
      "authors": [
        "Yongsheng Chen",
        "Yong Chen",
        "Wei Guo",
        "Xinghui Zhong"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.766519+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
          "url": "https://arxiv.org/abs/2602.19967"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
        "url": "https://arxiv.org/abs/2602.19967"
      },
      "published_at": "2026-02-23T15:29:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8439857744536418,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043985774453642
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19967",
      "summary": "Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by ",
      "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Matthew Lee"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Customer Solutions",
        "Healthcare"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.297996+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials",
          "url": "https://aws.amazon.com/blogs/machine-learning/how-sonrai-uses-amazon-sagemaker-ai-to-accelerate-precision-medicine-trials"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials",
        "url": "https://aws.amazon.com/blogs/machine-learning/how-sonrai-uses-amazon-sagemaker-ai-to-accelerate-precision-medicine-trials"
      },
      "published_at": "2026-02-23T17:31:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8511616595289992,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 2.7,
        "total_score": 10.051161659528999
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:9b135fce3a12d59c",
      "summary": "In this post, we explore how Sonrai, a life sciences AI company, partnered with AWS to build a robust MLOps framework using Amazon SageMaker AI that addresses these challenges while maintaining the traceability and reproducibility required in regulated environments.",
      "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Laura Kulowski"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.297555+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Scaling data annotation using vision-language models to power physical AI systems",
          "url": "https://aws.amazon.com/blogs/machine-learning/scaling-data-annotation-using-vision-language-models-to-power-physical-ai-systems"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Scaling data annotation using vision-language models to power physical AI systems",
        "url": "https://aws.amazon.com/blogs/machine-learning/scaling-data-annotation-using-vision-language-models-to-power-physical-ai-systems"
      },
      "published_at": "2026-02-23T23:20:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8720344470079316,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 9.372034447007932
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:a33585692c1d4c8b",
      "summary": "In this post, we examine how Bedrock Robotics tackles this challenge. By joining the AWS Physical AI Fellowship, the startup partnered with the AWS Generative AI Innovation Center to apply vision-language models that analyze construction video footage, extract operational details, and generate labeled training datasets at scale, to improve data preparation for autonomous construction equipment.",
      "title": "Scaling data annotation using vision-language models to power physical AI systems"
    },
    {
      "arxiv_id": "2602.19966",
      "authors": [
        "Joydeep Chandra",
        "Satyam Kumar Navneet",
        "Yong Zhang"
      ],
      "categories": [
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:45.715071+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-hc",
          "tier": 1,
          "title": "GazeFlow: Personalized Ambient Soundscape Generation for Passive Strabismus Self-Monitoring",
          "url": "https://arxiv.org/abs/2602.19966"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-hc",
        "tier": 1,
        "title": "GazeFlow: Personalized Ambient Soundscape Generation for Passive Strabismus Self-Monitoring",
        "url": "https://arxiv.org/abs/2602.19966"
      },
      "published_at": "2026-02-23T15:28:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8439320502173423,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043932050217343
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19966",
      "summary": "Strabismus affects 2-4% of the population, yet individuals recovering from corrective surgery lack accessible tools for monitoring eye alignment. Dichoptic therapies require active engagement & clinical supervision, limiting their adoption for passive self-awareness. We present GazeFlow, a browser-based self-monitoring system that uses a personalized temporal autoencoder to detect eye drift patterns from webcam-based gaze tracking & provides ambient audio feedback. Unlike alert-based systems, Ga",
      "title": "GazeFlow: Personalized Ambient Soundscape Generation for Passive Strabismus Self-Monitoring"
    },
    {
      "arxiv_id": "2602.19964",
      "authors": [
        "Moritz A. Zanger",
        "Yijun Wu",
        "Pascal R. Van der Vaart",
        "Wendelin B√∂hmer",
        "Matthijs T. J. Spaan"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.PR",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.865137+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
          "url": "https://arxiv.org/abs/2602.19964"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
        "url": "https://arxiv.org/abs/2602.19964"
      },
      "published_at": "2026-02-23T15:28:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8439047010107276,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043904701010728
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19964",
      "summary": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. ",
      "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference"
    },
    {
      "arxiv_id": "2602.19948",
      "authors": [
        "Ian Steenstra",
        "Paola Pedrelli",
        "Weiyan Shi",
        "Stacy Marsella",
        "Timothy W. Bickmore"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.865337+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
          "url": "https://arxiv.org/abs/2602.19948"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
        "url": "https://arxiv.org/abs/2602.19948"
      },
      "published_at": "2026-02-23T15:17:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8432515138282424,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043251513828242
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19948",
      "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test cas",
      "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming"
    },
    {
      "arxiv_id": "2602.19946",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Brian Moser",
        "Stanislav Frolov",
        "Tobias Christian Nauen",
        "Federico Raue",
        "Andreas Dengel"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.865536+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
          "url": "https://arxiv.org/abs/2602.19946"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
        "url": "https://arxiv.org/abs/2602.19946"
      },
      "published_at": "2026-02-23T15:15:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8431685591372202,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04316855913722
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19946",
      "summary": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data",
      "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators"
    },
    {
      "arxiv_id": "2602.19945",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.865752+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models",
          "url": "https://arxiv.org/abs/2602.19945"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models",
        "url": "https://arxiv.org/abs/2602.19945"
      },
      "published_at": "2026-02-23T15:15:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.843162703820335,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043162703820336
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19945",
      "summary": "Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity t",
      "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models"
    },
    {
      "arxiv_id": "2602.19938",
      "authors": [
        "Zijie Liu",
        "Jie Peng",
        "Jinhao Duan",
        "Zirui Liu",
        "Kaixiong Zhou",
        "Mingfu Liang",
        "Luke Simon",
        "Xi Liu",
        "Zhaozhuo Xu",
        "Tianlong Chen"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.767349+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
          "url": "https://arxiv.org/abs/2602.19938"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
        "url": "https://arxiv.org/abs/2602.19938"
      },
      "published_at": "2026-02-23T15:11:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.842898281063878,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042898281063879
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19938",
      "summary": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deplo",
      "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs"
    },
    {
      "arxiv_id": "2602.19931",
      "authors": [
        "Pin-Han Huang",
        "Shang-Tse Chen",
        "Hsuan-Tien Lin"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.767551+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
          "url": "https://arxiv.org/abs/2602.19931"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
        "url": "https://arxiv.org/abs/2602.19931"
      },
      "published_at": "2026-02-23T15:06:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8426407681555841,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042640768155584
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19931",
      "summary": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations ",
      "title": "Expanding the Role of Diffusion Models for Robust Classifier Training"
    },
    {
      "arxiv_id": "2602.19926",
      "authors": [
        "Jin Liu",
        "Yinbin Miao",
        "Ning Xi",
        "Junkang Liu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.866171+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
          "url": "https://arxiv.org/abs/2602.19926"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
        "url": "https://arxiv.org/abs/2602.19926"
      },
      "published_at": "2026-02-23T15:05:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8425588487298279,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042558848729827
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19926",
      "summary": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs",
      "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models"
    },
    {
      "arxiv_id": "2602.19919",
      "authors": [
        "Xiang Li",
        "Zikai Wei",
        "Yiyan Qi",
        "Wanyun Zhou",
        "Xiang Liu",
        "Penglei Sun",
        "Yongqi Zhang",
        "Xiaowen Chu"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768144+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
          "url": "https://arxiv.org/abs/2602.19919"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
        "url": "https://arxiv.org/abs/2602.19919"
      },
      "published_at": "2026-02-23T14:58:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8421717896721439,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042171789672144
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19919",
      "summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and stati",
      "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling"
    },
    {
      "arxiv_id": "2602.19918",
      "authors": [
        "Jiaqi Xue",
        "Mengxin Zheng",
        "Qian Lou"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768380+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "RobPI: Robust Private Inference against Malicious Client",
          "url": "https://arxiv.org/abs/2602.19918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "RobPI: Robust Private Inference against Malicious Client",
        "url": "https://arxiv.org/abs/2602.19918"
      },
      "published_at": "2026-02-23T14:58:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8421298770728065,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042129877072806
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19918",
      "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredicta",
      "title": "RobPI: Robust Private Inference against Malicious Client"
    },
    {
      "arxiv_id": "2602.19917",
      "authors": [
        "Thanh Nguyen",
        "Tung Luu",
        "Tri Ton",
        "Sungwoong Kim",
        "Chang D. Yoo"
      ],
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.768601+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.19917"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.19917"
      },
      "published_at": "2026-02-23T14:57:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8421142822194801,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04211428221948
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19917",
      "summary": "Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations s",
      "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.19914",
      "authors": [
        "Thatchawin Leelawat",
        "Lewis D Griffin"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:36.866381+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
          "url": "https://arxiv.org/abs/2602.19914"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
        "url": "https://arxiv.org/abs/2602.19914"
      },
      "published_at": "2026-02-23T14:54:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.841925217600503,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041925217600504
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19914",
      "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replic",
      "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning"
    },
    {
      "arxiv_id": "2602.19910",
      "authors": [
        "Wei He",
        "Xianghan Meng",
        "Zhiyuan Huang",
        "Xianbiao Qi",
        "Rong Xiao",
        "Chun-Guang Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.758935+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
          "url": "https://arxiv.org/abs/2602.19910"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
        "url": "https://arxiv.org/abs/2602.19910"
      },
      "published_at": "2026-02-23T14:51:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8417215820800068,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041721582080006
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19910",
      "summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this",
      "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery"
    },
    {
      "arxiv_id": "2602.19907",
      "authors": [
        "Kiran Kokilepersaud",
        "Mohit Prabhushankar",
        "Ghassan AlRegib",
        "Stephanie Trejo Corona",
        "Charles Wykoff"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.769265+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
          "url": "https://arxiv.org/abs/2602.19907"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
        "url": "https://arxiv.org/abs/2602.19907"
      },
      "published_at": "2026-02-23T14:46:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8414283945002461,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041428394500246
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19907",
      "summary": "In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these sam",
      "title": "Gradient based Severity Labeling for Biomarker Classification in OCT"
    },
    {
      "arxiv_id": "2602.19898",
      "authors": [
        "Stefan Fabian",
        "Aljoscha Schmidt",
        "Jonas S√º√ü",
        " Dishant",
        "Aum Oza",
        "Oskar von Stryk"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:42.769069+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
          "url": "https://arxiv.org/abs/2602.19898"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
        "url": "https://arxiv.org/abs/2602.19898"
      },
      "published_at": "2026-02-23T14:38:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8409756642523083,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.040975664252308
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19898",
      "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manip",
      "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform"
    },
    {
      "arxiv_id": "2602.19893",
      "authors": [
        "Soumen Pachal",
        "Prashanth L. A.",
        "Shalabh Bhatnagar",
        "Avinash Achar"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:37.769886+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Generalized Random Direction Newton Algorithms for Stochastic Optimization",
          "url": "https://arxiv.org/abs/2602.19893"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Generalized Random Direction Newton Algorithms for Stochastic Optimization",
        "url": "https://arxiv.org/abs/2602.19893"
      },
      "published_at": "2026-02-23T14:33:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8406992778638471,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.040699277863848
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19893",
      "summary": "We present a family of generalized Hessian estimators of the objective using random direction stochastic approximation (RDSA) by utilizing only noisy function measurements. The form of each estimator and the order of the bias depend on the number of function measurements. In particular, we demonstrate that estimators with more function measurements exhibit lower-order estimation bias. We show the asymptotic unbiasedness of the estimators. We also perform asymptotic and non-asymptotic convergence",
      "title": "Generalized Random Direction Newton Algorithms for Stochastic Optimization"
    },
    {
      "arxiv_id": "2602.19891",
      "authors": [
        "Wen-Liang Lin",
        "Yun-Chien Cheng"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:39.759758+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
          "url": "https://arxiv.org/abs/2602.19891"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
        "url": "https://arxiv.org/abs/2602.19891"
      },
      "published_at": "2026-02-23T14:33:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8406846825169693,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04068468251697
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19891",
      "summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on ",
      "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images"
    },
    {
      "arxiv_id": "2602.19888",
      "authors": [
        "Naoki Wada",
        "Yuta Kimura",
        "Masaichiro Mizumaki",
        "Koji Amezawa",
        "Ichiro Akai",
        "Toru Aonishi"
      ],
      "categories": [
        "physics.chem-ph",
        "cond-mat.dis-nn",
        "cond-mat.mtrl-sci"
      ],
      "entities": [],
      "first_seen_at": "2026-02-24T09:02:54.703017+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-api-efficiency",
          "tier": 1,
          "title": "A Physics-Regularized Neural Network and Kirchhoff Markov Random Field Framework for Inferring Internal Electrochemical States from Operando Spectromicroscopy",
          "url": "https://arxiv.org/abs/2602.19888"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-api-efficiency",
        "tier": 1,
        "title": "A Physics-Regularized Neural Network and Kirchhoff Markov Random Field Framework for Inferring Internal Electrochemical States from Operando Spectromicroscopy",
        "url": "https://arxiv.org/abs/2602.19888"
      },
      "published_at": "2026-02-23T14:32:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.84063214136624,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04063214136624
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19888",
      "summary": "Quantitative understanding of coupled reaction and transport processes in lithium-ion battery (LIB) composite electrodes remains challenging because key internal states cannot be measured directly. In this study, we develop a physics-integrated, data-driven analysis pipeline to estimate internal electrochemical states from operando microscopic X-ray absorption fine structure ($Œº$-XAFS) hyperspectral data of LIB cathodes with LiPF$_6$ electrolyte. State-of-charge (SOC) maps are first constructed ",
      "title": "A Physics-Regularized Neural Network and Kirchhoff Markov Random Field Framework for Inferring Internal Electrochemical States from Operando Spectromicroscopy"
    }
  ],
  "run_date": "2026-02-23",
  "run_id": "0cf1f9be-77a8-43ed-9f15-f6121356847f",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-23T23:59:59+00:00",
    "items_total": 231,
    "run_id": "0cf1f9be-77a8-43ed-9f15-f6121356847f-2026-02-23",
    "started_at": "2026-02-22T23:59:59+00:00",
    "stories_total": 231,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": null,
      "authors": [
        "Sanhita Sarkar"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Bedrock",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Healthcare",
        "Partner solutions",
        "Technical How-to",
        "AI/ML",
        "Amazon OpenSearch Service",
        "Generative AI",
        "machine-learning"
      ],
      "entities": [
        "aws",
        "huggingface"
      ],
      "first_seen_at": "2026-02-24T09:02:36.298481+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS",
          "url": "https://aws.amazon.com/blogs/machine-learning/agentic-ai-with-multi-model-framework-using-hugging-face-smolagents-on-aws"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS",
        "url": "https://aws.amazon.com/blogs/machine-learning/agentic-ai-with-multi-model-framework-using-hugging-face-smolagents-on-aws"
      },
      "published_at": "2026-02-23T15:47:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8449983828896123,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.344998382889612
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:b23c2e47f2f1c478",
      "summary": "Hugging Face smolagents is an open source Python library designed to make it straightforward to build and run agents using a few lines of code. We will show you how to build an agentic AI solution by integrating Hugging Face smolagents with Amazon Web Services (AWS) managed services. You'll learn how to deploy a healthcare AI agent that demonstrates multi-model deployment options, vector-enhanced knowledge retrieval, and clinical decision support capabilities.",
      "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS"
    },
    {
      "arxiv_id": "2602.19672",
      "authors": [],
      "categories": [],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.870588+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
          "url": "https://arxiv.org/abs/2602.19672"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
        "url": "https://arxiv.org/abs/2602.19672"
      },
      "published_at": "2026-02-23T10:17:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8258721980910669,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.025872198091067
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.19672",
      "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
      "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Johannes Maunz, Tobias B√∂sch Borgards, Bartlomiej Gralewicz"
      ],
      "categories": [
        "Amazon SageMaker",
        "Amazon SageMaker HyperPod",
        "Artificial Intelligence",
        "Intermediate (200)"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-24T09:02:36.298239+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod",
          "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-ai-model-production-at-hexagon-with-amazon-sagemaker-hyperpod"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod",
        "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-ai-model-production-at-hexagon-with-amazon-sagemaker-hyperpod"
      },
      "published_at": "2026-02-23T17:29:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8510099613641129,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.45,
        "total_score": 10.801009961364112
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:ec9604dedcf28634",
      "summary": "In this blog post, we demonstrate how Hexagon collaborated with Amazon Web Services to scale their AI model production by pretraining state-of-the-art segmentation models, using the model training infrastructure of Amazon SageMaker HyperPod.",
      "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-24T09:02:59.820598+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 390092,
        "likes": 964,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-397b-a17b",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-397B-A17B",
        "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B"
      },
      "published_at": "2026-02-23T13:56:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8385137755347322,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.638513775534731
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-397b-a17b",
      "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5...",
      "title": "Qwen/Qwen3.5-397B-A17B"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-24T09:02:59.820823+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 68820,
        "likes": 83,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-397b-a17b-fp8",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-397B-A17B-FP8",
          "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-397B-A17B-FP8",
        "url": "https://huggingface.co/Qwen/Qwen3.5-397B-A17B-FP8"
      },
      "published_at": "2026-02-23T13:55:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.838466222282492,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.638466222282492
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-397b-a17b-fp8",
      "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Plus** is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a...",
      "title": "Qwen/Qwen3.5-397B-A17B-FP8"
    }
  ]
}