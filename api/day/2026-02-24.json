{
  "archive_dates": [
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-25T07:14:51.649087+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.989827+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 8,
          "likes": 2,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-76000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-76000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-76000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-76000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-76000"
        },
        "published_at": "2026-02-24T01:51:55+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8847715233187043,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.684771523318704
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-76000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-76000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990071+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 9,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-50000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-50000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-50000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-50000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-50000"
        },
        "published_at": "2026-02-24T01:51:34+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8847500187166321,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.684750018716631
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-50000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-50000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990268+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 6,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-26000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-26000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-26000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-26000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-26000"
        },
        "published_at": "2026-02-24T01:51:15+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8847305626222778,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.684730562622278
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-26000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-26000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990461+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 7,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-112000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-112000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-112000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-112000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-112000"
        },
        "published_at": "2026-02-24T01:50:59+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8847141788746719,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.684714178874671
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-112000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-112000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990642+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 9,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-2000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-2000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-2000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-2000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-2000"
        },
        "published_at": "2026-02-24T01:50:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.884685508046414,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.684685508046414
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-2000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-2000"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.776682+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 252,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-35b-a3b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-35B-A3B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B"
        },
        "published_at": "2026-02-24T16:31:54+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9405261000821498,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.74052610008215
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-35b-a3b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Flash** is the hosted version corresponding to Qwen3.5-35B-A3B with more production features, e.g., 1M context length by default and official built-in tools. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap...",
        "title": "Qwen/Qwen3.5-35B-A3B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.776941+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 179,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-122b-a10b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-122B-A10B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-122B-A10B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B"
        },
        "published_at": "2026-02-24T15:54:27+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9380832569998974,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.738083256999897
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-122b-a10b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and efficiency. Qwen3.5 features the following enhancement: - **Unified Vision-Language Foundation**: Early fusion training on multimodal tokens achieves cross-generational parity with...",
        "title": "Qwen/Qwen3.5-122B-A10B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.777152+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 41,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-35b-a3b-base",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-35B-A3B-Base",
            "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-Base"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B-Base",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-Base"
        },
        "published_at": "2026-02-24T15:20:29+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9358731173470355,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.735873117347035
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-35b-a3b-base",
        "summary": "> This repository contains model weights and configuration files for the pre-trained only model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > The intended use cases are fine-tuning, in-context learning experiments, and other research or development purposes, not direct interaction. > However, the control tokens, e.g., `` and `` were trained to allow efficient LoRA-style PEFT with the official chat template, mitigating the need to finetune embeddings, a significant optimization given Qwen3.5's larger vocabulary. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating...",
        "title": "Qwen/Qwen3.5-35B-A3B-Base"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.21201",
      "authors": [],
      "categories": [],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-25T06:31:29.261038+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Aletheia tackles FirstProof autonomously",
          "url": "https://arxiv.org/abs/2602.21201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Aletheia tackles FirstProof autonomously",
        "url": "https://arxiv.org/abs/2602.21201"
      },
      "published_at": "2026-02-24T18:56:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9499961368931572,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.149996136893158
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21201",
      "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
      "title": "Aletheia tackles FirstProof autonomously"
    },
    {
      "arxiv_id": "2602.20945",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:29.268544+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
          "url": "https://arxiv.org/abs/2602.20945"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
        "url": "https://arxiv.org/abs/2602.20945"
      },
      "published_at": "2026-02-24T14:28:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9324856395857951,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.132485639585795
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20945",
      "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization"
    },
    {
      "arxiv_id": "2602.21081",
      "authors": [
        "Huy Trinh",
        "Rebecca Ma",
        "Zeqi Yu",
        "Tahsin Reza"
      ],
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-25T06:31:30.398154+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
          "url": "https://arxiv.org/abs/2602.21081"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
        "url": "https://arxiv.org/abs/2602.21081"
      },
      "published_at": "2026-02-24T16:45:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9413951817234244,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.141395181723425
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21081",
      "summary": "Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of Vi",
      "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads"
    },
    {
      "arxiv_id": "2602.21035",
      "authors": [
        "Junhao Xiao",
        "Zhiyu Wu",
        "Hao Lin",
        "Yi Chen",
        "Yahui Liu",
        "Xiaoran Zhao",
        "Zixu Wang",
        "Zejiang He"
      ],
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.024704+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
          "url": "https://arxiv.org/abs/2602.21035"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
        "url": "https://arxiv.org/abs/2602.21035"
      },
      "published_at": "2026-02-24T15:55:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9381614338619714,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.138161433861972
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21035",
      "summary": "Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from ",
      "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning"
    },
    {
      "arxiv_id": "2602.20812",
      "authors": [
        "Jia-Rui Lin",
        "Yun-Hong Cai",
        "Xiang-Rui Ni",
        "Shaojie Zhou",
        "Peng Pan"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:29.270642+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
          "url": "https://arxiv.org/abs/2602.20812"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
        "url": "https://arxiv.org/abs/2602.20812"
      },
      "published_at": "2026-02-24T11:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9223795153799159,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.122379515379915
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20812",
      "summary": "As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with cor",
      "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset"
    },
    {
      "arxiv_id": "2602.20790",
      "authors": [
        "Sheng Zhong",
        "Zhongyang Ren",
        "Xiya Zhu",
        "Dehao Yuan",
        "Cornelia Fermuller",
        "Yi Zhou"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.031216+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Real-time Motion Segmentation with Event-based Normal Flow",
          "url": "https://arxiv.org/abs/2602.20790"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Real-time Motion Segmentation with Event-based Normal Flow",
        "url": "https://arxiv.org/abs/2602.20790"
      },
      "published_at": "2026-02-24T11:29:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9209564773441545,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.120956477344155
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20790",
      "summary": "Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fund",
      "title": "Real-time Motion Segmentation with Event-based Normal Flow"
    },
    {
      "arxiv_id": "2602.20597",
      "authors": [
        "Yuejiao Su",
        "Yi Wang",
        "Lei Yao",
        "Yawen Cui",
        "Lap-Pui Chau"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.036634+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing",
          "url": "https://arxiv.org/abs/2602.20597"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing",
        "url": "https://arxiv.org/abs/2602.20597"
      },
      "published_at": "2026-02-24T06:39:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9026064395945699,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.10260643959457
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20597",
      "summary": "A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability",
      "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing"
    },
    {
      "arxiv_id": "2602.21204",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.260707+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
          "url": "https://arxiv.org/abs/2602.21204"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
        "url": "https://arxiv.org/abs/2602.21204"
      },
      "published_at": "2026-02-24T18:59:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9502160688604336,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.150216068860434
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21204",
      "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
      "title": "Test-Time Training with KV Binding Is Secretly Linear Attention"
    },
    {
      "arxiv_id": "2602.21198",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261287+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
          "url": "https://arxiv.org/abs/2602.21198"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
        "url": "https://arxiv.org/abs/2602.21198"
      },
      "published_at": "2026-02-24T18:55:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9499389629202634,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.149938962920263
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21198",
      "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
      "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs"
    },
    {
      "arxiv_id": "2602.21185",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.395461+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
          "url": "https://arxiv.org/abs/2602.21185"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
        "url": "https://arxiv.org/abs/2602.21185"
      },
      "published_at": "2026-02-24T18:35:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9486249108193331,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.148624910819333
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21185",
      "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
      "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum"
    },
    {
      "arxiv_id": "2602.21015",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025117+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
          "url": "https://arxiv.org/abs/2602.21015"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
        "url": "https://arxiv.org/abs/2602.21015"
      },
      "published_at": "2026-02-24T15:33:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9366891122947608,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.13668911229476
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21015",
      "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
      "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning"
    },
    {
      "arxiv_id": "2602.20739",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.272403+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
          "url": "https://arxiv.org/abs/2602.20739"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
        "url": "https://arxiv.org/abs/2602.20739"
      },
      "published_at": "2026-02-24T10:08:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9158181986276047,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.115818198627604
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20739",
      "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
      "title": "PyVision-RL: Forging Open Agentic Vision Models via RL"
    },
    {
      "arxiv_id": "2602.21203",
      "authors": [
        "Abdulaziz Almuzairee",
        "Henrik I. Christensen"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.394018+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
          "url": "https://arxiv.org/abs/2602.21203"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
        "url": "https://arxiv.org/abs/2602.21203"
      },
      "published_at": "2026-02-24T18:58:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9501291896501803,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.15012918965018
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21203",
      "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challeng",
      "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics"
    },
    {
      "arxiv_id": "2602.21202",
      "authors": [
        "Hanxiang Qin",
        "Alexander Martin",
        "Rohan Jha",
        "Chunsheng Zuo",
        "Reno Kriz",
        "Benjamin Van Durme"
      ],
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.032436+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multi-Vector Index Compression in Any Modality",
          "url": "https://arxiv.org/abs/2602.21202"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multi-Vector Index Compression in Any Modality",
        "url": "https://arxiv.org/abs/2602.21202"
      },
      "published_at": "2026-02-24T18:57:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9500874024797583,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.150087402479759
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21202",
      "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce",
      "title": "Multi-Vector Index Compression in Any Modality"
    },
    {
      "arxiv_id": "2602.21196",
      "authors": [
        "Ravi Ghadia",
        "Maksim Abraham",
        "Sergei Vorobyov",
        "Max Ryabinin"
      ],
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.394739+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
          "url": "https://arxiv.org/abs/2602.21196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
        "url": "https://arxiv.org/abs/2602.21196"
      },
      "published_at": "2026-02-24T18:54:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9498960846987098,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14989608469871
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21196",
      "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend t",
      "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking"
    },
    {
      "arxiv_id": "2602.21189",
      "authors": [
        "Anas Barakat",
        "Souradip Chakraborty",
        "Khushbu Pahwa",
        "Amrit Singh Bedi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261535+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training",
          "url": "https://arxiv.org/abs/2602.21189"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training",
        "url": "https://arxiv.org/abs/2602.21189"
      },
      "published_at": "2026-02-24T18:43:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9491366914241984,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.149136691424198
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21189",
      "summary": "Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practi",
      "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training"
    },
    {
      "arxiv_id": "2602.21186",
      "authors": [
        "Haoyi Jiang",
        "Liu Liu",
        "Xinjie Wang",
        "Yonghao He",
        "Wei Sui",
        "Zhizhong Su",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.020866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
          "url": "https://arxiv.org/abs/2602.21186"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
        "url": "https://arxiv.org/abs/2602.21186"
      },
      "published_at": "2026-02-24T18:37:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.948769850696667,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.148769850696667
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21186",
      "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstruc",
      "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning"
    },
    {
      "arxiv_id": "2602.21179",
      "authors": [
        "Nicolás Gaggion",
        "Maria J. Ledesma-Carbayo",
        "Stergios Christodoulidis",
        "Maria Vakalopoulou",
        "Enzo Ferrante"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.021086+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision",
          "url": "https://arxiv.org/abs/2602.21179"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision",
        "url": "https://arxiv.org/abs/2602.21179"
      },
      "published_at": "2026-02-24T18:29:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9482198554326966,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.148219855432696
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21179",
      "summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise m",
      "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision"
    },
    {
      "arxiv_id": "2602.21178",
      "authors": [
        "Sepehr Salem Ghahfarokhi",
        "M. Moein Esfahani",
        "Raj Sunderraman",
        "Vince Calhoun",
        "Mohammed Alser"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261763+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence",
          "url": "https://arxiv.org/abs/2602.21178"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence",
        "url": "https://arxiv.org/abs/2602.21178"
      },
      "published_at": "2026-02-24T18:28:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9481485221314908,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14814852213149
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21178",
      "summary": "Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, men",
      "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence"
    },
    {
      "arxiv_id": "2602.21175",
      "authors": [
        "Jianglin Lu",
        "Simon Jenni",
        "Kushal Kafle",
        "Jing Shi",
        "Handong Zhao",
        "Yun Fu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.021511+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
          "url": "https://arxiv.org/abs/2602.21175"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
        "url": "https://arxiv.org/abs/2602.21175"
      },
      "published_at": "2026-02-24T18:20:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9476756632154374,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.147675663215438
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21175",
      "summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queri",
      "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models"
    },
    {
      "arxiv_id": "2602.21174",
      "authors": [
        "Victor Reijgwart",
        "Cesar Cadena",
        "Roland Siegwart",
        "Lionel Ott"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262024+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
          "url": "https://arxiv.org/abs/2602.21174"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
        "url": "https://arxiv.org/abs/2602.21174"
      },
      "published_at": "2026-02-24T18:18:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9475210204308303,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14752102043083
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21174",
      "summary": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the und",
      "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids"
    },
    {
      "arxiv_id": "2602.21172",
      "authors": [
        "Ishaan Rawal",
        "Shubh Gupta",
        "Yihan Hu",
        "Wei Zhan"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262250+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
          "url": "https://arxiv.org/abs/2602.21172"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
        "url": "https://arxiv.org/abs/2602.21172"
      },
      "published_at": "2026-02-24T18:17:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9474387739120302,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14743877391203
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21172",
      "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no r",
      "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning"
    },
    {
      "arxiv_id": "2602.21168",
      "authors": [
        "Jingya Cheng",
        "Alaleh Azhir",
        "Jiazi Tian",
        "Hossein Estiri"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.395688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma",
          "url": "https://arxiv.org/abs/2602.21168"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma",
        "url": "https://arxiv.org/abs/2602.21168"
      },
      "published_at": "2026-02-24T18:11:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9470462822219884,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14704628222199
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21168",
      "summary": "Counterfactual inference enables clinicians to ask \"what if\" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time",
      "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma"
    },
    {
      "arxiv_id": "2602.21165",
      "authors": [
        "Samah Fodeh",
        "Linhai Ma",
        "Yan Wang",
        "Srivani Talakokkul",
        "Ganesh Puthiaraju",
        "Afshan Khan",
        "Ashley Hagaman",
        "Sarah Lowe",
        "Aimee Roundtree"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262511+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data",
          "url": "https://arxiv.org/abs/2602.21165"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data",
        "url": "https://arxiv.org/abs/2602.21165"
      },
      "published_at": "2026-02-24T18:10:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9469553087660098,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14695530876601
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21165",
      "summary": "Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communicati",
      "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data"
    },
    {
      "arxiv_id": "2602.21161",
      "authors": [
        "Guangming Wang",
        "Qizhen Ying",
        "Yixiong Jing",
        "Olaf Wysocki",
        "Brian Sheil"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.629475+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
          "url": "https://arxiv.org/abs/2602.21161"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
        "url": "https://arxiv.org/abs/2602.21161"
      },
      "published_at": "2026-02-24T18:07:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9467646216903235,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146764621690323
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21161",
      "summary": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of l",
      "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking"
    },
    {
      "arxiv_id": "2602.21158",
      "authors": [
        "Dengjia Zhang",
        "Xiaoou Liu",
        "Lu Cheng",
        "Yaqing Wang",
        "Kenton Murray",
        "Hua Wei"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396145+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards",
          "url": "https://arxiv.org/abs/2602.21158"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards",
        "url": "https://arxiv.org/abs/2602.21158"
      },
      "published_at": "2026-02-24T18:04:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9466199881440203,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146619988144021
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21158",
      "summary": "Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolv",
      "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards"
    },
    {
      "arxiv_id": "2602.21157",
      "authors": [
        "Quanxin Shou",
        "Fangqi Zhu",
        "Shawn Chen",
        "Puxin Yan",
        "Zhengyang Yan",
        "Yikun Miao",
        "Xiaoyi Pang",
        "Zicong Hong",
        "Ruikai Shi",
        "Hao Huang",
        "Jie Zhang",
        "Song Guo"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.629687+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
          "url": "https://arxiv.org/abs/2602.21157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
        "url": "https://arxiv.org/abs/2602.21157"
      },
      "published_at": "2026-02-24T18:04:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9465947891047413,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146594789104741
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21157",
      "summary": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, an",
      "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning"
    },
    {
      "arxiv_id": "2602.21154",
      "authors": [
        "Ziwei Niu",
        "Hao Sun",
        "Shujun Bian",
        "Xihong Yang",
        "Lanfen Lin",
        "Yuxin Liu",
        "Yueming Jin"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262757+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning",
          "url": "https://arxiv.org/abs/2602.21154"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning",
        "url": "https://arxiv.org/abs/2602.21154"
      },
      "published_at": "2026-02-24T17:59:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9462552153228676,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146255215322867
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21154",
      "summary": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2",
      "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning"
    },
    {
      "arxiv_id": "2602.21144",
      "authors": [
        "Anurag Dutt",
        "Nimit Shah",
        "Hazem Masarani",
        "Anshul Gandhi"
      ],
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396382+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
          "url": "https://arxiv.org/abs/2602.21144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
        "url": "https://arxiv.org/abs/2602.21144"
      },
      "published_at": "2026-02-24T17:47:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9455031100560499,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14550311005605
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21144",
      "summary": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large pro",
      "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism"
    },
    {
      "arxiv_id": "2602.21143",
      "authors": [
        "Debjit Paul",
        "Daniel Murphy",
        "Milan Gritta",
        "Ronald Cardenas",
        "Victor Prokhorov",
        "Lena Sophia Bolliger",
        "Aysim Toker",
        "Roy Miles",
        "Andreea-Maria Oncescu",
        "Jasivan Alex Sivakumar",
        "Philipp Borchert",
        "Ismail Elezi",
        "Meiru Zhang",
        "Ka Yiu Lee",
        "Guchun Zhang",
        "Jun Wang",
        "Gerasimos Lampouras"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262979+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Benchmark for Deep Information Synthesis",
          "url": "https://arxiv.org/abs/2602.21143"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Benchmark for Deep Information Synthesis",
        "url": "https://arxiv.org/abs/2602.21143"
      },
      "published_at": "2026-02-24T17:43:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9452164384600341,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.145216438460034
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21143",
      "summary": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming probl",
      "title": "A Benchmark for Deep Information Synthesis"
    },
    {
      "arxiv_id": "2602.21142",
      "authors": [
        "Zhifan Jiang",
        "Dong Yang",
        "Vishwesh Nath",
        "Abhijeet Parida",
        "Nishad P. Kulkarni",
        "Ziyue Xu",
        "Daguang Xu",
        "Syed Muhammad Anwar",
        "Holger R. Roth",
        "Marius George Linguraru"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396809+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
          "url": "https://arxiv.org/abs/2602.21142"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
        "url": "https://arxiv.org/abs/2602.21142"
      },
      "published_at": "2026-02-24T17:42:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9451661157763105,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.145166115776311
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21142",
      "summary": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are es",
      "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis"
    },
    {
      "arxiv_id": "2602.21141",
      "authors": [
        "Jose Moises Araya-Martinez",
        "Thushar Tom",
        "Adrián Sanchis Reig",
        "Pablo Rey Valiente",
        "Jens Lambrecht",
        "Jörg Krüger"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.022348+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception",
          "url": "https://arxiv.org/abs/2602.21141"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception",
        "url": "https://arxiv.org/abs/2602.21141"
      },
      "published_at": "2026-02-24T17:42:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9451529885603084,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.145152988560309
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21141",
      "summary": "Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. ",
      "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception"
    },
    {
      "arxiv_id": "2602.21138",
      "authors": [
        "Kimon Fountoulakis",
        "David Martínez-Rubio"
      ],
      "categories": [
        "math.OC",
        "cs.DS",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.397046+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank",
          "url": "https://arxiv.org/abs/2602.21138"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank",
        "url": "https://arxiv.org/abs/2602.21138"
      },
      "published_at": "2026-02-24T17:35:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9447067716807657,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144706771680767
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21138",
      "summary": "We study the degree-weighted work required to compute $\\ell_1$-regularized PageRank using the standard one-gradient-per-iteration accelerated proximal-gradient method (FISTA). For non-accelerated local methods, the best known worst-case work scales as $\\widetilde{O} ((αρ)^{-1})$, where $α$ is the teleportation parameter and $ρ$ is the $\\ell_1$-regularization parameter. A natural question is whether FISTA can improve the dependence on $α$ from $1/α$ to $1/\\sqrtα$ while preserving the $1/ρ$ locali",
      "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank"
    },
    {
      "arxiv_id": "2602.21137",
      "authors": [
        "Joseph Raj Vishal",
        "Nagasiri Poluri",
        "Katha Naik",
        "Rutuja Patil",
        "Kashyap Hegde Kota",
        "Krishna Vinod",
        "Prithvi Jai Ramesh",
        "Mohammad Farhadi",
        "Yezhou Yang",
        "Bharatesh Chakravarthi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.022572+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
          "url": "https://arxiv.org/abs/2602.21137"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
        "url": "https://arxiv.org/abs/2602.21137"
      },
      "published_at": "2026-02-24T17:33:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9445384014516615,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144538401451662
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21137",
      "summary": "Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation with",
      "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics"
    },
    {
      "arxiv_id": "2602.21136",
      "authors": [
        "David Anugraha",
        "Vishakh Padmakumar",
        "Diyi Yang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263225+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery",
          "url": "https://arxiv.org/abs/2602.21136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery",
        "url": "https://arxiv.org/abs/2602.21136"
      },
      "published_at": "2026-02-24T17:33:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9445274693575018,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144527469357502
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21136",
      "summary": "Qualitative insights from user experiences are critical for informing product and policy decisions, but collecting such data at scale is constrained by the time and availability of experts to conduct semi-structured interviews. Recent work has explored using large language models (LLMs) to automate interviewing, yet existing systems lack a principled mechanism for balancing systematic coverage of predefined topics with adaptive exploration, or the ability to pursue follow-ups, deep dives, and em",
      "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery"
    },
    {
      "arxiv_id": "2602.21127",
      "authors": [
        "Xinfeng Li",
        "Shenyu Dai",
        "Kelong Zheng",
        "Yue Xiao",
        "Gelei Deng",
        "Wei Dong",
        "Xiaofeng Wang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CR",
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263503+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems",
          "url": "https://arxiv.org/abs/2602.21127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems",
        "url": "https://arxiv.org/abs/2602.21127"
      },
      "published_at": "2026-02-24T17:23:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9438816072505809,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14388160725058
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21127",
      "summary": "Large language model (LLM) agents are rapidly becoming trusted copilots in high-stakes domains like software development and healthcare. However, this deepening trust introduces a novel attack surface: Agent-Mediated Deception (AMD), where compromised agents are weaponized against their human users. While extensive research focuses on agent-centric threats, human susceptibility to deception by a compromised agent remains unexplored. We present the first large-scale empirical study with 303 parti",
      "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems"
    },
    {
      "arxiv_id": "2602.21119",
      "authors": [
        "Rui Zhao",
        "Xihui Li",
        "Yizheng Zhang",
        "Yuzhen Liu",
        "Zhong Zhang",
        "Yufeng Zhang",
        "Cheng Zhou",
        "Zhengyou Zhang",
        "Lei Han"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263727+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Cooperative-Competitive Team Play of Real-World Craft Robots",
          "url": "https://arxiv.org/abs/2602.21119"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Cooperative-Competitive Team Play of Real-World Craft Robots",
        "url": "https://arxiv.org/abs/2602.21119"
      },
      "published_at": "2026-02-24T17:15:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9433857627099286,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14338576270993
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21119",
      "summary": "Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement l",
      "title": "Cooperative-Competitive Team Play of Real-World Craft Robots"
    },
    {
      "arxiv_id": "2602.21103",
      "authors": [
        "Sanket Badhe",
        "Deep Shah"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.033853+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning",
          "url": "https://arxiv.org/abs/2602.21103"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning",
        "url": "https://arxiv.org/abs/2602.21103"
      },
      "published_at": "2026-02-24T17:03:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9425824799884657,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142582479988466
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21103",
      "summary": "Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressiv",
      "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning"
    },
    {
      "arxiv_id": "2602.21100",
      "authors": [
        "Noé Artru",
        "Rukhshanda Hussain",
        "Emeline Got",
        "Alexandre Messier",
        "David B. Lindell",
        "Abdallah Dib"
      ],
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.023208+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
          "url": "https://arxiv.org/abs/2602.21100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
        "url": "https://arxiv.org/abs/2602.21100"
      },
      "published_at": "2026-02-24T17:02:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9425061164458277,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142506116445828
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21100",
      "summary": "Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while",
      "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction"
    },
    {
      "arxiv_id": "2602.21099",
      "authors": [
        "Junjie Meng",
        "Ranxu zhang",
        "Wei Wu",
        "Rui Zhang",
        "Chuan Qin",
        "Qi Zhang",
        "Qi Liu",
        "Hui Xiong",
        "Chao Wang"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:37.769127+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering",
          "url": "https://arxiv.org/abs/2602.21099"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering",
        "url": "https://arxiv.org/abs/2602.21099"
      },
      "published_at": "2026-02-24T17:01:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9424799360839887,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142479936083989
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21099",
      "summary": "Large Language Models (LLMs) have shown great potential for enhancing recommender systems through their extensive world knowledge and reasoning capabilities. However, effectively translating these semantic signals into traditional collaborative embeddings remains an open challenge. Existing approaches typically fall into two extremes: direct inference methods are computationally prohibitive for large-scale retrieval, while embedding-based methods primarily focus on unilateral feature augmentatio",
      "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering"
    },
    {
      "arxiv_id": "2602.21098",
      "authors": [
        "Hao Lu",
        "Richard J. Radke"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.023463+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Optimizing Occupancy Sensor Placement in Smart Environments",
          "url": "https://arxiv.org/abs/2602.21098"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Optimizing Occupancy Sensor Placement in Smart Environments",
        "url": "https://arxiv.org/abs/2602.21098"
      },
      "published_at": "2026-02-24T17:01:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9424679369945191,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14246793699452
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21098",
      "summary": "Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on caref",
      "title": "Optimizing Occupancy Sensor Placement in Smart Environments"
    },
    {
      "arxiv_id": "2602.21092",
      "authors": [
        "Floriano Tori",
        "Lorenzo Bini",
        "Marco Sorbi",
        "Stéphane Marchand-Maillet",
        "Vincent Ginis"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264184+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology",
          "url": "https://arxiv.org/abs/2602.21092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology",
        "url": "https://arxiv.org/abs/2602.21092"
      },
      "published_at": "2026-02-24T16:52:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9418790785713234,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.141879078571323
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21092",
      "summary": "Curvature notions on graphs provide a theoretical description of graph topology, highlighting bottlenecks and denser connected regions. Artifacts of the message passing paradigm in Graph Neural Networks, such as oversmoothing and oversquashing, have been attributed to these regions. However, it remains unclear how the topology of a graph interacts with the learned preferences of GNNs. Through Massive Activations, which correspond to extreme edge activation values in Graph Transformers, we probe ",
      "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology"
    },
    {
      "arxiv_id": "2602.21082",
      "authors": [
        "Vishal Patil",
        "Shree Vaishnavi Bacha",
        "Revanth Yamani",
        "Yidan Sun",
        "Mayank Kejriwal"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034047+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
          "url": "https://arxiv.org/abs/2602.21082"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
        "url": "https://arxiv.org/abs/2602.21082"
      },
      "published_at": "2026-02-24T16:45:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9414006296279711,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.141400629627972
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21082",
      "summary": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing c",
      "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification"
    },
    {
      "arxiv_id": "2602.21066",
      "authors": [
        "Claire McNamara",
        "Lucy Hederman",
        "Declan O'Sullivan"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264664+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "The Initial Exploration Problem in Knowledge Graph Exploration",
          "url": "https://arxiv.org/abs/2602.21066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "The Initial Exploration Problem in Knowledge Graph Exploration",
        "url": "https://arxiv.org/abs/2602.21066"
      },
      "published_at": "2026-02-24T16:27:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9402496434644296,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14024964346443
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21066",
      "summary": "Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the",
      "title": "The Initial Exploration Problem in Knowledge Graph Exploration"
    },
    {
      "arxiv_id": "2602.21064",
      "authors": [
        "Mehdi Acheli",
        "Walid Gaaloul"
      ],
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264936+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Motivation is Something You Need",
          "url": "https://arxiv.org/abs/2602.21064"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Motivation is Something You Need",
        "url": "https://arxiv.org/abs/2602.21064"
      },
      "published_at": "2026-02-24T16:26:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9401974088241017,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.140197408824102
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21064",
      "summary": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broade",
      "title": "Motivation is Something You Need"
    },
    {
      "arxiv_id": "2602.21061",
      "authors": [
        "David Koplow",
        "Tomer Galanti",
        "Tomaso Poggio"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.265201+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Tool Building as a Path to \"Superintelligence\"",
          "url": "https://arxiv.org/abs/2602.21061"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Tool Building as a Path to \"Superintelligence\"",
        "url": "https://arxiv.org/abs/2602.21061"
      },
      "published_at": "2026-02-24T16:22:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9398905889105286,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.139890588910529
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21061",
      "summary": "The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the inform",
      "title": "Tool Building as a Path to \"Superintelligence\""
    },
    {
      "arxiv_id": "2602.21059",
      "authors": [
        "Anna Martin-Boyle",
        "William Humphreys",
        "Martha Brown",
        "Cara Leckey",
        "Harmanpreet Kaur"
      ],
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034258+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
          "url": "https://arxiv.org/abs/2602.21059"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
        "url": "https://arxiv.org/abs/2602.21059"
      },
      "published_at": "2026-02-24T16:16:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9395360211630661,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.139536021163066
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21059",
      "summary": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the ass",
      "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems"
    },
    {
      "arxiv_id": "2602.21053",
      "authors": [
        "Shimin Wen",
        "Zeyu Zhang",
        "Xingdou Bian",
        "Hongjie Zhu",
        "Lulu He",
        "Layi Shama",
        "Daji Ergu",
        "Ying Cai"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.024196+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
          "url": "https://arxiv.org/abs/2602.21053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
        "url": "https://arxiv.org/abs/2602.21053"
      },
      "published_at": "2026-02-24T16:10:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9391261510081639,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.139126151008163
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21053",
      "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel ",
      "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection"
    },
    {
      "arxiv_id": "2602.21046",
      "authors": [
        "Kunyu Zhang",
        "Yanwu Yang",
        "Jing Zhang",
        "Xiangjie Shi",
        "Shujian Yu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.399196+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
          "url": "https://arxiv.org/abs/2602.21046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
        "url": "https://arxiv.org/abs/2602.21046"
      },
      "published_at": "2026-02-24T16:04:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9387620928173072,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138762092817307
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21046",
      "summary": "Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating p",
      "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis"
    },
    {
      "arxiv_id": "2602.21045",
      "authors": [
        "Anna Martin-Boyle",
        "Cara A. C. Leckey",
        "Martha C. Brown",
        "Harmanpreet Kaur"
      ],
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034683+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
          "url": "https://arxiv.org/abs/2602.21045"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
        "url": "https://arxiv.org/abs/2602.21045"
      },
      "published_at": "2026-02-24T16:04:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9387599197594223,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138759919759423
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21045",
      "summary": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source ",
      "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A"
    },
    {
      "arxiv_id": "2602.21042",
      "authors": [
        "Bonan Liu",
        "Zeyu Zhang",
        "Bingbing Meng",
        "Han Wang",
        "Hanshuo Zhang",
        "Chengping Wang",
        "Daji Ergu",
        "Ying Cai"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.024449+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
          "url": "https://arxiv.org/abs/2602.21042"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
        "url": "https://arxiv.org/abs/2602.21042"
      },
      "published_at": "2026-02-24T16:02:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9386284591150515,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138628459115052
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21042",
      "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR intr",
      "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages"
    },
    {
      "arxiv_id": "2602.21031",
      "authors": [
        "Hayk Gevorgyan",
        "Konstantinos Kalogeropoulos",
        "Angelos Alexopoulos"
      ],
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.896122+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
          "url": "https://arxiv.org/abs/2602.21031"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
        "url": "https://arxiv.org/abs/2602.21031"
      },
      "published_at": "2026-02-24T15:52:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9379822881946112,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13798228819461
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21031",
      "summary": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulti",
      "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation"
    },
    {
      "arxiv_id": "2602.21028",
      "authors": [
        "Gayatri Indukumar",
        "Muhammad Awais",
        "Diana Cafiso",
        "Matteo Lo Preti",
        "Lucia Beccai"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.630667+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing",
          "url": "https://arxiv.org/abs/2602.21028"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing",
        "url": "https://arxiv.org/abs/2602.21028"
      },
      "published_at": "2026-02-24T15:51:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9378693897141326,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.137869389714133
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21028",
      "summary": "There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical complianc",
      "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing"
    },
    {
      "arxiv_id": "2602.21013",
      "authors": [
        "Sanjay Haresh",
        "Daniel Dijkman",
        "Apratim Bhattacharyya",
        "Roland Memisevic"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.630866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
          "url": "https://arxiv.org/abs/2602.21013"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
        "url": "https://arxiv.org/abs/2602.21013"
      },
      "published_at": "2026-02-24T15:30:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9365514377869877,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.136551437786988
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21013",
      "summary": "Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily \"stateless\" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratch",
      "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks"
    },
    {
      "arxiv_id": "2602.21010",
      "authors": [
        "Jiannan Huang",
        "Aditya Kane",
        "Fengzhe Zhou",
        "Yunchao Wei",
        "Humphrey Shi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025334+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
          "url": "https://arxiv.org/abs/2602.21010"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
        "url": "https://arxiv.org/abs/2602.21010"
      },
      "published_at": "2026-02-24T15:29:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9364864017509187,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.136486401750918
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21010",
      "summary": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it ",
      "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design"
    },
    {
      "arxiv_id": "2602.21009",
      "authors": [
        "Kun Yuan",
        "Junyu Bi",
        "Daixuan Cheng",
        "Changfa Wu",
        "Shuwen Xiao",
        "Binbin Cao",
        "Jian Wu",
        "Yuning Jiang"
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034896+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders",
          "url": "https://arxiv.org/abs/2602.21009"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders",
        "url": "https://arxiv.org/abs/2602.21009"
      },
      "published_at": "2026-02-24T15:28:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9364246216998203,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13642462169982
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21009",
      "summary": "Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we pro",
      "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders"
    },
    {
      "arxiv_id": "2602.20999",
      "authors": [
        "Bowen Zheng",
        "Yongli Xiang",
        "Ziming Hong",
        "Zerong Lin",
        "Chaojian Yu",
        "Tongliang Liu",
        "Xinge You"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
          "url": "https://arxiv.org/abs/2602.20999"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
        "url": "https://arxiv.org/abs/2602.20999"
      },
      "published_at": "2026-02-24T15:20:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9358427886170804,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135842788617081
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20999",
      "summary": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction In",
      "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models"
    },
    {
      "arxiv_id": "2602.20995",
      "authors": [
        "Junyu Bi",
        "Xinting Niu",
        "Daixuan Cheng",
        "Kun Yuan",
        "Tao Wang",
        "Binbin Cao",
        "Jian Wu",
        "Yuning Jiang"
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035082+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
          "url": "https://arxiv.org/abs/2602.20995"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
        "url": "https://arxiv.org/abs/2602.20995"
      },
      "published_at": "2026-02-24T15:14:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.935504906398041,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13550490639804
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20995",
      "summary": "Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debi",
      "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs"
    },
    {
      "arxiv_id": "2602.20989",
      "authors": [
        "Zheng Gu",
        "Min Lu",
        "Zhida Sun",
        "Dani Lischinski",
        "Daniel Cohen-O",
        "Hui Huang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025931+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
          "url": "https://arxiv.org/abs/2602.20989"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
        "url": "https://arxiv.org/abs/2602.20989"
      },
      "published_at": "2026-02-24T15:10:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9352255959431697,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13522559594317
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20989",
      "summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on wh",
      "title": "Cycle-Consistent Tuning for Layered Image Decomposition"
    },
    {
      "arxiv_id": "2602.20986",
      "authors": [
        "Thibault Formal",
        "Maxime Louis",
        "Hervé Déjean",
        "Stéphane Clinchant"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:37.769908+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval",
          "url": "https://arxiv.org/abs/2602.20986"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval",
        "url": "https://arxiv.org/abs/2602.20986"
      },
      "published_at": "2026-02-24T15:09:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9351281816840064,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135128181684006
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20986",
      "summary": "This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.\n  We evaluate five progressively enhanced runs, starting",
      "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval"
    },
    {
      "arxiv_id": "2602.20976",
      "authors": [
        "Xuan Luo",
        "Yubin Chen",
        "Zhiyu Hou",
        "Linpu Yu",
        "Geng Tu",
        "Jing Li",
        "Ruifeng Xu"
      ],
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035480+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Evaluating Proactive Risk Awareness of Large Language Models",
          "url": "https://arxiv.org/abs/2602.20976"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Evaluating Proactive Risk Awareness of Large Language Models",
        "url": "https://arxiv.org/abs/2602.20976"
      },
      "published_at": "2026-02-24T15:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9345428275272153,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.134542827527216
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20976",
      "summary": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological ",
      "title": "Evaluating Proactive Risk Awareness of Large Language Models"
    },
    {
      "arxiv_id": "2602.20974",
      "authors": [
        "Ahmed Mohamed Eisa Nasr",
        "Haris Moazam Sheikh"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.400764+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting",
          "url": "https://arxiv.org/abs/2602.20974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting",
        "url": "https://arxiv.org/abs/2602.20974"
      },
      "published_at": "2026-02-24T14:57:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9343719429595567,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.134371942959557
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20974",
      "summary": "In engineering design and scientific computing, computational cost and predictive accuracy are intrinsically coupled. High-fidelity simulations provide accurate predictions but at substantial computational costs, while lower-fidelity approximations offer efficiency at the expense of accuracy. Multi-fidelity surrogate modelling addresses this trade-off by combining abundant low-fidelity data with sparse high-fidelity observations. However, existing methods suffer from expensive training cost or r",
      "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting"
    },
    {
      "arxiv_id": "2602.20973",
      "authors": [
        "Yuliang Ji",
        "Fuchen Shen",
        "Jian Wu",
        "Qiujie Xie",
        "Yue Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035664+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
          "url": "https://arxiv.org/abs/2602.20973"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
        "url": "https://arxiv.org/abs/2602.20973"
      },
      "published_at": "2026-02-24T14:53:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9341254051164496,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13412540511645
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20973",
      "summary": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professio",
      "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving"
    },
    {
      "arxiv_id": "2602.20966",
      "authors": [
        "Paola Merlo",
        "Chunyang Jiang",
        "Giuseppe Samo",
        "Vivi Nastase"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035862+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
          "url": "https://arxiv.org/abs/2602.20966"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
        "url": "https://arxiv.org/abs/2602.20966"
      },
      "published_at": "2026-02-24T14:45:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9335784964664557,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.133578496466455
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20966",
      "summary": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core ques",
      "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models"
    },
    {
      "arxiv_id": "2602.20937",
      "authors": [
        "Akshita Gupta",
        "Marieme Ngom",
        "Sam Foreman",
        "Venkatram Vishwanath"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.401606+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers",
          "url": "https://arxiv.org/abs/2602.20937"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers",
        "url": "https://arxiv.org/abs/2602.20937"
      },
      "published_at": "2026-02-24T14:17:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.93181134238503,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13181134238503
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20937",
      "summary": "Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tune",
      "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers"
    },
    {
      "arxiv_id": "2602.20932",
      "authors": [
        "Anupam Sharma",
        "Harish Katti",
        "Prajwal Singh",
        "Shanmuganathan Raman",
        "Krishna Miyapuram"
      ],
      "categories": [
        "cs.LG",
        "cs.HC",
        "eess.SP"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.401832+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels",
          "url": "https://arxiv.org/abs/2602.20932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels",
        "url": "https://arxiv.org/abs/2602.20932"
      },
      "published_at": "2026-02-24T14:10:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9313477090658244,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.131347709065825
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20932",
      "summary": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations m",
      "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels"
    },
    {
      "arxiv_id": "2602.20923",
      "authors": [
        "Jiarong Wei",
        "Anna Rehr",
        "Christian Feist",
        "Abhinav Valada"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.631673+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models",
          "url": "https://arxiv.org/abs/2602.20923"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models",
        "url": "https://arxiv.org/abs/2602.20923"
      },
      "published_at": "2026-02-24T14:01:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.930757180451677,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130757180451678
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20923",
      "summary": "Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose Par",
      "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models"
    },
    {
      "arxiv_id": "2602.20911",
      "authors": [
        "Ruiqi Liu",
        "Boyu Diao",
        "Hangda Liu",
        "Zhulin An",
        "Fei Wang",
        "Yongjun Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402276+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
          "url": "https://arxiv.org/abs/2602.20911"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
        "url": "https://arxiv.org/abs/2602.20911"
      },
      "published_at": "2026-02-24T13:48:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9298957671115708,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12989576711157
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20911",
      "summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for bette",
      "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning"
    },
    {
      "arxiv_id": "2602.20904",
      "authors": [
        "Nathan Hu",
        "Jake Ward",
        "Thomas Icard",
        "Christopher Potts"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402520+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Transcoder Adapters for Reasoning-Model Diffing",
          "url": "https://arxiv.org/abs/2602.20904"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Transcoder Adapters for Reasoning-Model Diffing",
        "url": "https://arxiv.org/abs/2602.20904"
      },
      "published_at": "2026-02-24T13:40:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9293954370253124,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.129395437025313
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20904",
      "summary": "While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to ",
      "title": "Transcoder Adapters for Reasoning-Model Diffing"
    },
    {
      "arxiv_id": "2602.20901",
      "authors": [
        "Yuechen Xie",
        "Xiaoyan Zhang",
        "Yicheng Shan",
        "Hao Zhu",
        "Rui Tang",
        "Rong Wei",
        "Mingli Song",
        "Yuanyu Wan",
        "Jie Song"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402771+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.20901"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.20901"
      },
      "published_at": "2026-02-24T13:38:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9292760431977538,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.129276043197754
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20901",
      "summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects i",
      "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Kara Yang"
      ],
      "categories": [
        "Amazon Bedrock",
        "Amazon Rekognition",
        "Intermediate (200)",
        "Technical How-to"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626061+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-photo-search-using-amazon-rekognition-amazon-neptune-and-amazon-bedrock"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-photo-search-using-amazon-rekognition-amazon-neptune-and-amazon-bedrock"
      },
      "published_at": "2026-02-24T18:22:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9477732876115453,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 1.2000000000000002,
        "total_score": 8.647773287611546
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:a722197acaace989",
      "summary": "In this post, we show you how to build a comprehensive photo search system using the AWS Cloud Development Kit (AWS CDK) that integrates Amazon Rekognition for face and object detection, Amazon Neptune for relationship mapping, and Amazon Bedrock for AI-powered captioning.",
      "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock"
    },
    {
      "arxiv_id": "2602.20892",
      "authors": [
        "Seyed Himan Ghaderi",
        "Saeed Sarbazi Azad",
        "Mohammad Mehdi Jaziriyan",
        "Ahmad Akbari"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.036422+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter",
          "url": "https://arxiv.org/abs/2602.20892"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter",
        "url": "https://arxiv.org/abs/2602.20892"
      },
      "published_at": "2026-02-24T13:28:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9286158894948228,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.128615889494823
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20892",
      "summary": "Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products",
      "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter"
    },
    {
      "arxiv_id": "2602.20875",
      "authors": [
        "Louis Sharrock",
        "Nikolas Kantas",
        "Grigorios A. Pavliotis"
      ],
      "categories": [
        "math.ST",
        "math.OC",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.896305+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Efficient Online Learning in Interacting Particle Systems",
          "url": "https://arxiv.org/abs/2602.20875"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Efficient Online Learning in Interacting Particle Systems",
        "url": "https://arxiv.org/abs/2602.20875"
      },
      "published_at": "2026-02-24T13:19:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.928018500209492,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.128018500209492
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20875",
      "summary": "We introduce a new method for online parameter estimation in stochastic interacting particle systems, based on continuous observation of a small number of particles from the system. Our method recursively updates the model parameters using a stochastic approximation of the gradient of the asymptotic log likelihood, which is computed using the continuous stream of observations. Under suitable assumptions, we rigorously establish convergence of our method to the stationary points of the asymptotic",
      "title": "Efficient Online Learning in Interacting Particle Systems"
    },
    {
      "arxiv_id": "2602.20859",
      "authors": [
        "Zirui He",
        "Huopu Zhang",
        "Yanguang Liu",
        "Sirui Wu",
        "Mengnan Du"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.036611+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction",
          "url": "https://arxiv.org/abs/2602.20859"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction",
        "url": "https://arxiv.org/abs/2602.20859"
      },
      "published_at": "2026-02-24T13:02:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9269257149128844,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.126925714912884
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20859",
      "summary": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding spa",
      "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction"
    },
    {
      "arxiv_id": "2602.20850",
      "authors": [
        "Lei Ye",
        "Haibo Gao",
        "Huaiguang Yang",
        "Peng Xu",
        "Haoyu Wang",
        "Tie Liu",
        "Junqi Shan",
        "Zongquan Deng",
        "Liang Ding"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.632496+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion",
          "url": "https://arxiv.org/abs/2602.20850"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion",
        "url": "https://arxiv.org/abs/2602.20850"
      },
      "published_at": "2026-02-24T12:46:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9259231607868951,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.125923160786895
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20850",
      "summary": "Legged robots face significant challenges in navigating complex environments, as they require precise real-time decisions for foothold selection and contact planning. While existing research has explored methods to select footholds based on terrain geometry or kinematics, a critical gap remains: few existing methods efficiently validate the existence of a non-collision swing trajectory. This paper addresses this gap by introducing KCFRC, a novel approach for efficient foothold reachability analy",
      "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion"
    },
    {
      "arxiv_id": "2602.20846",
      "authors": [
        "Yuki Nakamura"
      ],
      "categories": [
        "cs.GT",
        "cs.MA",
        "cs.NE",
        "nlin.AO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:34.606600+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Body-Reservoir Governance in Repeated Games: Embodied Decision-Making, Dynamic Sentinel Adaptation, and Complexity-Regularized Optimization",
          "url": "https://arxiv.org/abs/2602.20846"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Body-Reservoir Governance in Repeated Games: Embodied Decision-Making, Dynamic Sentinel Adaptation, and Complexity-Regularized Optimization",
        "url": "https://arxiv.org/abs/2602.20846"
      },
      "published_at": "2026-02-24T12:36:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9252878783198252,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.125287878319826
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20846",
      "summary": "Standard game theory explains cooperation in repeated games through conditional strategies such as Tit-for-Tat (TfT), but these require continuous computation that imposes physical costs on embodied agents. We propose a three-layer Body-Reservoir Governance (BRG) architecture: (1) a body reservoir (echo state network) whose $d$-dimensional state performs implicit inference over interaction history, serving as both decision-maker and anomaly detector, (2) a cognitive filter providing costly strat",
      "title": "Body-Reservoir Governance in Repeated Games: Embodied Decision-Making, Dynamic Sentinel Adaptation, and Complexity-Regularized Optimization"
    },
    {
      "arxiv_id": "2602.20844",
      "authors": [
        "Subhrosekhar Ghosh",
        "Rathindra Nath Karmakar",
        "Samriddha Lahiry"
      ],
      "categories": [
        "math.ST",
        "cs.IT",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.896702+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Maximum entropy based testing in network models: ERGMs and constrained optimization",
          "url": "https://arxiv.org/abs/2602.20844"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Maximum entropy based testing in network models: ERGMs and constrained optimization",
        "url": "https://arxiv.org/abs/2602.20844"
      },
      "published_at": "2026-02-24T12:35:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9251882867207611,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12518828672076
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20844",
      "summary": "Stochastic network models play a central role across a wide range of scientific disciplines, and questions of statistical inference arise naturally in this context. In this paper we investigate goodness-of-fit and two-sample testing procedures for statistical networks based on the principle of maximum entropy (MaxEnt). Our approach formulates a constrained entropy-maximization problem on the space of networks, subject to prescribed structural constraints. The resulting test statistics are define",
      "title": "Maximum entropy based testing in network models: ERGMs and constrained optimization"
    },
    {
      "arxiv_id": "2602.20800",
      "authors": [
        "Dalia Nahhas",
        "Xiaohao Cai",
        "Imran Razzak",
        "Shoaib Jameel"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:37.770328+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking",
          "url": "https://arxiv.org/abs/2602.20800"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking",
        "url": "https://arxiv.org/abs/2602.20800"
      },
      "published_at": "2026-02-24T11:38:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9215631866328766,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.121563186632876
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20800",
      "summary": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separate",
      "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking"
    },
    {
      "arxiv_id": "2602.20799",
      "authors": [
        "Guangsheng Ou",
        "Qiming Zhang",
        "Sirong Chen",
        "Anji Li",
        "Dong Xu",
        "Tiancheng Luo",
        "Dekun Dai",
        "Cuiyun Gao",
        "Long Wang",
        "Jun Zhou",
        "Mingwei Liu",
        "Zibin Zheng"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:36.798264+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs",
          "url": "https://arxiv.org/abs/2602.20799"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs",
        "url": "https://arxiv.org/abs/2602.20799"
      },
      "published_at": "2026-02-24T11:36:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9214330676845819,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.121433067684581
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20799",
      "summary": "In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a co",
      "title": "Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs"
    },
    {
      "arxiv_id": "2602.20759",
      "authors": [
        "Yu Fu",
        "Seongho Son",
        "Ilija Bogunovic"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.036959+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Overton Pluralistic Reinforcement Learning for Large Language Models",
          "url": "https://arxiv.org/abs/2602.20759"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Overton Pluralistic Reinforcement Learning for Large Language Models",
        "url": "https://arxiv.org/abs/2602.20759"
      },
      "published_at": "2026-02-24T10:39:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9177855018430142,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.117785501843015
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20759",
      "summary": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow",
      "title": "Overton Pluralistic Reinforcement Learning for Large Language Models"
    },
    {
      "arxiv_id": "2602.20749",
      "authors": [
        "Azrin Sultana",
        "Firoz Ahmed"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.037342+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification",
          "url": "https://arxiv.org/abs/2602.20749"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification",
        "url": "https://arxiv.org/abs/2602.20749"
      },
      "published_at": "2026-02-24T10:25:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9168957664365955,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.116895766436595
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20749",
      "summary": "Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classificati",
      "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification"
    },
    {
      "arxiv_id": "2602.20727",
      "authors": [
        "Xindian Ma",
        "Rundong Kong",
        "Peng Zhang",
        "Ruoxiang Huang",
        "Yongyu Jiang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.037906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition",
          "url": "https://arxiv.org/abs/2602.20727"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition",
        "url": "https://arxiv.org/abs/2602.20727"
      },
      "published_at": "2026-02-24T09:45:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9143322608206703,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.114332260820671
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20727",
      "summary": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation",
      "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition"
    },
    {
      "arxiv_id": "2602.20717",
      "authors": [
        "Xiting Liu",
        "Yuetong Liu",
        "Yitong Zhang",
        "Jia Li",
        "Shi-Min Hu"
      ],
      "categories": [
        "cs.SE",
        "cs.CR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:36.798653+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring",
          "url": "https://arxiv.org/abs/2602.20717"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring",
        "url": "https://arxiv.org/abs/2602.20717"
      },
      "published_at": "2026-02-24T09:26:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9131277026048744,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.113127702604874
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20717",
      "summary": "As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminat",
      "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring"
    },
    {
      "arxiv_id": "2602.20715",
      "authors": [
        "Zhian Su",
        "Weijie Kong",
        "Haonan Dong",
        "Huixu Dong"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.633271+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
          "url": "https://arxiv.org/abs/2602.20715"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
        "url": "https://arxiv.org/abs/2602.20715"
      },
      "published_at": "2026-02-24T09:19:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9127251274214443,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.112725127421445
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20715",
      "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To ",
      "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation"
    },
    {
      "arxiv_id": "2602.20645",
      "authors": [
        "Keisuke Takeshita",
        "Takahiro Yamazaki",
        "Tomohiro Ono",
        "Takashi Yamamoto"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.633492+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments",
          "url": "https://arxiv.org/abs/2602.20645"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments",
        "url": "https://arxiv.org/abs/2602.20645"
      },
      "published_at": "2026-02-24T07:46:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9068294962375908,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.10682949623759
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20645",
      "summary": "The objective of this study is to enable fast and safe manipulation tasks in home environments. Specifically, we aim to develop a system that can recognize its surroundings and identify target objects while in motion, enabling it to plan and execute actions accordingly. We propose a periodic sampling-based whole-body trajectory planning method, called the \"Robot Local Planner (RLP).\" This method leverages unique features of home environments to enhance computational efficiency, motion optimality",
      "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments"
    },
    {
      "arxiv_id": "2602.20644",
      "authors": [
        "Fida Khandaker Safa",
        "Yupeng Jiang",
        "Xi Zheng"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:36.799072+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation",
          "url": "https://arxiv.org/abs/2602.20644"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation",
        "url": "https://arxiv.org/abs/2602.20644"
      },
      "published_at": "2026-02-24T07:44:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9066983093305881,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.106698309330588
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20644",
      "summary": "Real-world crash reports, which combine textual summaries and sketches, are valuable for scenario-based testing of autonomous driving systems (ADS). However, current methods cannot effectively translate this multimodal data into precise, executable simulation scenarios, hindering the scalability of ADS safety validation. In this work, we propose a scalable and verifiable pipeline that uses a large language model (GPT-4o mini) and a probabilistic intermediate representation (an Extended Scenic do",
      "title": "An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation"
    },
    {
      "arxiv_id": "2602.20598",
      "authors": [
        "Shoma Ansai",
        "Masaki Waga"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:36.799533+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "A Case Study on Runtime Verification of a Continuous Deployment Process",
          "url": "https://arxiv.org/abs/2602.20598"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "A Case Study on Runtime Verification of a Continuous Deployment Process",
        "url": "https://arxiv.org/abs/2602.20598"
      },
      "published_at": "2026-02-24T06:39:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9026377806400568,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.102637780640057
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20598",
      "summary": "We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it wa",
      "title": "A Case Study on Runtime Verification of a Continuous Deployment Process"
    },
    {
      "arxiv_id": "2602.20963",
      "authors": [
        " Ang",
        " Li",
        "Alexander Yin",
        "Alexander White",
        "Sahib Sandhu",
        "Matthew Francoeur",
        "Victor Jimenez-Santiago",
        "Van Remenar",
        "Codrin Tugui",
        "Mihai Duduta"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.631063+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators",
          "url": "https://arxiv.org/abs/2602.20963"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators",
        "url": "https://arxiv.org/abs/2602.20963"
      },
      "published_at": "2026-02-24T14:41:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9333408099819653,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.9000000000000004,
        "total_score": 8.033340809981965
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20963",
      "summary": "Short lifetime under high electrical fields hinders the widespread robotic application of linear dielectric elastomer actuators (DEAs). Systematic scanning is difficult due to time-consuming per-sample testing and the high-dimensional parameter space affecting performance. To address this, we propose an optimization pipeline enabled by a novel testing robot capable of scanning DEA lifetime. The robot integrates electro-mechanical property measurement, programmable voltage input, and multi-channe",
      "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators"
    },
    {
      "arxiv_id": "2602.21041",
      "authors": [
        "Fabian Frank",
        "Matija Novaković",
        "René Romen"
      ],
      "categories": [
        "cs.GT",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:34.606161+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Stability Under Valuation Updates in Coalition Formation",
          "url": "https://arxiv.org/abs/2602.21041"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Stability Under Valuation Updates in Coalition Formation",
        "url": "https://arxiv.org/abs/2602.21041"
      },
      "published_at": "2026-02-24T16:02:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9386143363393916,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 7.8886143363393915
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21041",
      "summary": "Coalition formation studies how to partition a set of agents into disjoint coalitions under consideration of their preferences. We study the classical objective of stability in a variant of additively separable hedonic games where agents can change their valuations. Our objective is to find a stable partition after each change. To minimize the reconfiguration cost, we search for nearby stable coalition structures. Our focus is on stability concepts based on single-agent deviations. We present a ",
      "title": "Stability Under Valuation Updates in Coalition Formation"
    },
    {
      "arxiv_id": "2602.20603",
      "authors": [
        "Yamin Vahmian",
        "Keith Paarporn"
      ],
      "categories": [
        "cs.GT",
        "cs.MA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:34.607208+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "The Tragedy of the Commons in Multi-Population Resource Games",
          "url": "https://arxiv.org/abs/2602.20603"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "The Tragedy of the Commons in Multi-Population Resource Games",
        "url": "https://arxiv.org/abs/2602.20603"
      },
      "published_at": "2026-02-24T06:53:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9034697606754725,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.75,
        "total_score": 7.853469760675472
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20603",
      "summary": "Self-optimizing behaviors can lead to outcomes where collective benefits are ultimately destroyed, a well-known phenomenon known as the ``tragedy of the commons\".\n  These scenarios are widely studied using game-theoretic approaches to analyze strategic agent decision-making.\n  In this paper, we examine this phenomenon in a bi-level decision-making hierarchy, where low-level agents belong to multiple distinct populations, and high-level agents make decisions that impact the choices of the local p",
      "title": "The Tragedy of the Commons in Multi-Population Resource Games"
    }
  ],
  "run_date": "2026-02-24",
  "run_id": "c69faafa-b71c-42aa-a997-f1ef500a209c",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-24T23:59:59+00:00",
    "items_total": 252,
    "run_id": "c69faafa-b71c-42aa-a997-f1ef500a209c-2026-02-24",
    "started_at": "2026-02-23T23:59:59+00:00",
    "stories_total": 252,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": "2602.21193",
      "authors": [],
      "categories": [],
      "entities": [
        "nvidia",
        "huggingface",
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:32.033083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
          "url": "https://arxiv.org/abs/2602.21193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
        "url": "https://arxiv.org/abs/2602.21193"
      },
      "published_at": "2026-02-24T18:51:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 6.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9496597395018952,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.149659739501896
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21193",
      "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
      "title": "On Data Engineering for Scaling LLM Terminal Capabilities"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Traci Lim"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence"
      ],
      "entities": [
        "anthropic",
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.627009+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan",
          "url": "https://aws.amazon.com/blogs/machine-learning/global-cross-region-inference-for-latest-anthropic-claude-opus-sonnet-and-haiku-models-on-amazon-bedrock-in-thailand-malaysia-singapore-indonesia-and-taiwan"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan",
        "url": "https://aws.amazon.com/blogs/machine-learning/global-cross-region-inference-for-latest-anthropic-claude-opus-sonnet-and-haiku-models-on-amazon-bedrock-in-thailand-malaysia-singapore-indonesia-and-taiwan"
      },
      "published_at": "2026-02-24T15:38:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9370360984409306,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.437036098440931
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:e6cfe5e86bdcce2c",
      "summary": "In this post, we are exciting to announce availability of Global CRIS for customers in Thailand, Malaysia, Singapore, Indonesia, and Taiwan and give a walkthrough of technical implementation steps, and cover quota management best practices to maximize the value of your AI Inference deployments. We also provide guidance on best practices for production deployments.",
      "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Hossam Basudan"
      ],
      "categories": [
        "Amazon Bedrock",
        "Announcements",
        "Artificial Intelligence"
      ],
      "entities": [
        "anthropic",
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.627235+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)",
          "url": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-global-cross-region-inference-for-anthropics-claude-models-in-the-middle-east-regions"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)",
        "url": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-global-cross-region-inference-for-anthropics-claude-models-in-the-middle-east-regions"
      },
      "published_at": "2026-02-24T15:33:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9367422362160874,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.436742236216087
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:fd7b0ba93aa214c1",
      "summary": "We’re excited to announce the availability of Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 through Amazon Bedrock global cross-Region inference for customers operating in the Middle East. In this post, we guide you through the capabilities of each Anthropic Claude model variant, the key advantages of global cross-Region inference including improved resilience, real-world use cases you can implement, and a code example to help you start building generative AI applications immediately.",
      "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Bruno Pistone"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Foundation models",
        "Generative AI",
        "Technical How-to",
        "AIML",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "AWS Deep Learning",
        "distributed training",
        "Hugging Face"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626522+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs",
          "url": "https://aws.amazon.com/blogs/machine-learning/train-codefu-7b-with-verl-and-ray-on-amazon-sagemaker-training-jobs"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs",
        "url": "https://aws.amazon.com/blogs/machine-learning/train-codefu-7b-with-verl-and-ray-on-amazon-sagemaker-training-jobs"
      },
      "published_at": "2026-02-24T15:46:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9375872029604199,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 11.43758720296042
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:bb52a5bca4fe6d16",
      "summary": "In this post, we demonstrate how to train CodeFu-7B, a specialized 7-billion parameter model for competitive programming, using Group Relative Policy Optimization (GRPO) with veRL, a flexible and efficient training library for large language models (LLMs) that enables straightforward extension of diverse RL algorithms and seamless integration with existing LLM infrastructure, within a distributed Ray cluster managed by SageMaker training jobs. We walk through the complete implementation, covering data preparation, distributed training setup, and comprehensive observability, showcasing how this unified approach delivers both computational scale and developer experience for sophisticated RL training workloads.",
      "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Clement Perrot"
      ],
      "categories": [
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Customer Solutions"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626780+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Generate structured output from LLMs with Dottxt Outlines in AWS",
          "url": "https://aws.amazon.com/blogs/machine-learning/generate-structured-output-from-llms-with-dottxt-outlines-in-aws"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Generate structured output from LLMs with Dottxt Outlines in AWS",
        "url": "https://aws.amazon.com/blogs/machine-learning/generate-structured-output-from-llms-with-dottxt-outlines-in-aws"
      },
      "published_at": "2026-02-24T15:42:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9373094404967546,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 11.187309440496755
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:c46101761e991db6",
      "summary": "This post explores the implementation of Dottxt’s Outlines framework as a practical approach to implementing structured outputs using AWS Marketplace in Amazon SageMaker.",
      "title": "Generate structured output from LLMs with Dottxt Outlines in AWS"
    }
  ]
}