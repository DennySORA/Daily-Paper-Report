{
  "archive_dates": [
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-25T08:12:21.187761+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.989827+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 8,
          "likes": 2,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-76000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-76000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-76000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-76000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-76000"
        },
        "published_at": "2026-02-24T01:51:55+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8812461302513477,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.681246130251347
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-76000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-76000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990071+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 9,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-50000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-50000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-50000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-50000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-50000"
        },
        "published_at": "2026-02-24T01:51:34+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8812247113348711,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.68122471133487
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-50000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-50000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990268+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 6,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-26000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-26000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-26000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-26000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-26000"
        },
        "published_at": "2026-02-24T01:51:15+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8812053327637852,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.681205332763785
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-26000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-26000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990461+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 7,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-112000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-112000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-112000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-112000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-112000"
        },
        "published_at": "2026-02-24T01:50:59+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8811890142976092,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.681189014297608
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-112000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-112000"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-24T09:02:58.990642+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 9,
          "likes": 0,
          "pipeline_tag": "text-generation"
        },
        "hf_model_id": "microsoft/dayhoff-170m-grs-2000",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-microsoft",
            "tier": 1,
            "title": "microsoft/Dayhoff-170M-GRS-2000",
            "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-2000"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-microsoft",
          "tier": 1,
          "title": "microsoft/Dayhoff-170M-GRS-2000",
          "url": "https://huggingface.co/microsoft/Dayhoff-170M-GRS-2000"
        },
        "published_at": "2026-02-24T01:50:31+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8811604577089449,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.681160457708945
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:microsoft/dayhoff-170m-grs-2000",
        "summary": "Dayhoff is an Atlas of both protein sequence data and generative language models — a centralized resource that brings together 3.34 billion protein sequences across 1.7 billion clusters of metagenomic and natural protein sequences (GigaRef), 46 million structure-derived synthetic sequences (BackboneRef), and 16 million multiple sequence alignments (OpenProteinSet). These models can natively predict zero-shot mutation effects on fitness, scaffold structural motifs by conditioning on evolutionary or structural context, and perform guided generation of novel proteins within specified families. Learning from metagenomic and structure-based synthetic data from the Dayhoff Atlas increased the cellular expression rates of generated proteins, highlighting the real-world value of expanding the...",
        "title": "microsoft/Dayhoff-170M-GRS-2000"
      }
    ],
    "qwen": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.776682+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 252,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-35b-a3b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-35B-A3B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B"
        },
        "published_at": "2026-02-24T16:31:54+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.936778551584589,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.736778551584589
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-35b-a3b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. > For users seeking managed, scalable inference without infrastructure maintenance, the official Qwen API service is provided by Alibaba Cloud Model Studio. > In particular, **Qwen3.5-Flash** is the hosted version corresponding to Qwen3.5-35B-A3B with more production features, e.g., 1M context length by default and official built-in tools. > For more information, please refer to the User Guide. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap...",
        "title": "Qwen/Qwen3.5-35B-A3B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.776941+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 179,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-122b-a10b",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-122B-A10B",
            "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-122B-A10B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B"
        },
        "published_at": "2026-02-24T15:54:27+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9343454420683928,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.734345442068392
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-122b-a10b",
        "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and efficiency. Qwen3.5 features the following enhancement: - **Unified Vision-Language Foundation**: Early fusion training on multimodal tokens achieves cross-generational parity with...",
        "title": "Qwen/Qwen3.5-122B-A10B"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "qwen"
        ],
        "first_seen_at": "2026-02-25T06:33:17.777152+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 41,
          "pipeline_tag": "image-text-to-text"
        },
        "hf_model_id": "qwen/qwen3.5-35b-a3b-base",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-qwen",
            "tier": 1,
            "title": "Qwen/Qwen3.5-35B-A3B-Base",
            "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-Base"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-35B-A3B-Base",
          "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B-Base"
        },
        "published_at": "2026-02-24T15:20:29+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.9321441087692671,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 10.732144108769267
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:qwen/qwen3.5-35b-a3b-base",
        "summary": "> This repository contains model weights and configuration files for the pre-trained only model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, etc. > The intended use cases are fine-tuning, in-context learning experiments, and other research or development purposes, not direct interaction. > However, the control tokens, e.g., `` and `` were trained to allow efficient LoRA-style PEFT with the official chat template, mitigating the need to finetune embeddings, a significant optimization given Qwen3.5's larger vocabulary. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating...",
        "title": "Qwen/Qwen3.5-35B-A3B-Base"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.21201",
      "authors": [],
      "categories": [],
      "entities": [
        "deepmind"
      ],
      "first_seen_at": "2026-02-25T06:31:29.261038+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Aletheia tackles FirstProof autonomously",
          "url": "https://arxiv.org/abs/2602.21201"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Aletheia tackles FirstProof autonomously",
        "url": "https://arxiv.org/abs/2602.21201"
      },
      "published_at": "2026-02-24T18:56:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9462108548098727,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.146210854809873
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21201",
      "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
      "title": "Aletheia tackles FirstProof autonomously"
    },
    {
      "arxiv_id": "2602.21196",
      "authors": [],
      "categories": [],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-25T06:31:30.394739+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
          "url": "https://arxiv.org/abs/2602.21196"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
        "url": "https://arxiv.org/abs/2602.21196"
      },
      "published_at": "2026-02-24T18:54:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9461112012757611,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.146111201275762
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21196",
      "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.",
      "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking"
    },
    {
      "arxiv_id": "2602.21042",
      "authors": [],
      "categories": [],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.024449+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
          "url": "https://arxiv.org/abs/2602.21042"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
        "url": "https://arxiv.org/abs/2602.21042"
      },
      "published_at": "2026-02-24T16:02:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9348884718128203,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.13488847181282
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21042",
      "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
      "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages"
    },
    {
      "arxiv_id": "2602.20945",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:29.268544+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
          "url": "https://arxiv.org/abs/2602.20945"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
        "url": "https://arxiv.org/abs/2602.20945"
      },
      "published_at": "2026-02-24T14:28:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9287701284933103,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.128770128493311
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20945",
      "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization"
    },
    {
      "arxiv_id": "2602.20903",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:33.028541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
          "url": "https://arxiv.org/abs/2602.20903"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
        "url": "https://arxiv.org/abs/2602.20903"
      },
      "published_at": "2026-02-24T13:40:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9256868819182695,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.12568688191827
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20903",
      "summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
      "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering"
    },
    {
      "arxiv_id": "2602.21081",
      "authors": [
        "Huy Trinh",
        "Rebecca Ma",
        "Zeqi Yu",
        "Tahsin Reza"
      ],
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-25T06:31:30.398154+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
          "url": "https://arxiv.org/abs/2602.21081"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
        "url": "https://arxiv.org/abs/2602.21081"
      },
      "published_at": "2026-02-24T16:45:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9376441703495025,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.137644170349503
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21081",
      "summary": "Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of Vi",
      "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads"
    },
    {
      "arxiv_id": "2602.21035",
      "authors": [
        "Junhao Xiao",
        "Zhiyu Wu",
        "Hao Lin",
        "Yi Chen",
        "Yahui Liu",
        "Xiaoran Zhao",
        "Zixu Wang",
        "Zejiang He"
      ],
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.024704+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
          "url": "https://arxiv.org/abs/2602.21035"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
        "url": "https://arxiv.org/abs/2602.21035"
      },
      "published_at": "2026-02-24T15:55:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9344233074329102,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.134423307432911
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21035",
      "summary": "Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from ",
      "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning"
    },
    {
      "arxiv_id": "2602.20812",
      "authors": [
        "Jia-Rui Lin",
        "Yun-Hong Cai",
        "Xiang-Rui Ni",
        "Shaojie Zhou",
        "Peng Pan"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:29.270642+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
          "url": "https://arxiv.org/abs/2602.20812"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset",
        "url": "https://arxiv.org/abs/2602.20812"
      },
      "published_at": "2026-02-24T11:51:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9187042723784289,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.118704272378428
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20812",
      "summary": "As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with cor",
      "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset"
    },
    {
      "arxiv_id": "2602.20790",
      "authors": [
        "Sheng Zhong",
        "Zhongyang Ren",
        "Xiya Zhu",
        "Dehao Yuan",
        "Cornelia Fermuller",
        "Yi Zhou"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.031216+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Real-time Motion Segmentation with Event-based Normal Flow",
          "url": "https://arxiv.org/abs/2602.20790"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Real-time Motion Segmentation with Event-based Normal Flow",
        "url": "https://arxiv.org/abs/2602.20790"
      },
      "published_at": "2026-02-24T11:29:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9172869044713885,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.117286904471388
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20790",
      "summary": "Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fund",
      "title": "Real-time Motion Segmentation with Event-based Normal Flow"
    },
    {
      "arxiv_id": "2602.20597",
      "authors": [
        "Yuejiao Su",
        "Yi Wang",
        "Lei Yao",
        "Yawen Cui",
        "Lap-Pui Chau"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-25T06:31:33.036634+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing",
          "url": "https://arxiv.org/abs/2602.20597"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing",
        "url": "https://arxiv.org/abs/2602.20597"
      },
      "published_at": "2026-02-24T06:39:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8990099828813581,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.099009982881359
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20597",
      "summary": "A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability",
      "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing"
    },
    {
      "arxiv_id": "2602.21204",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.260707+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
          "url": "https://arxiv.org/abs/2602.21204"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
        "url": "https://arxiv.org/abs/2602.21204"
      },
      "published_at": "2026-02-24T18:59:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9464299104530222,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.146429910453023
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21204",
      "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
      "title": "Test-Time Training with KV Binding Is Secretly Linear Attention"
    },
    {
      "arxiv_id": "2602.21198",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261287+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
          "url": "https://arxiv.org/abs/2602.21198"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
        "url": "https://arxiv.org/abs/2602.21198"
      },
      "published_at": "2026-02-24T18:55:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9461539086480265,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.146153908648026
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21198",
      "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
      "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs"
    },
    {
      "arxiv_id": "2602.21185",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.395461+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
          "url": "https://arxiv.org/abs/2602.21185"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
        "url": "https://arxiv.org/abs/2602.21185"
      },
      "published_at": "2026-02-24T18:35:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9448450924187813,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.144845092418782
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21185",
      "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
      "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum"
    },
    {
      "arxiv_id": "2602.21053",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.024196+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
          "url": "https://arxiv.org/abs/2602.21053"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
        "url": "https://arxiv.org/abs/2602.21053"
      },
      "published_at": "2026-02-24T16:10:27+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9353841806408095,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.13538418064081
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21053",
      "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
      "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection"
    },
    {
      "arxiv_id": "2602.21015",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025117+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
          "url": "https://arxiv.org/abs/2602.21015"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
        "url": "https://arxiv.org/abs/2602.21015"
      },
      "published_at": "2026-02-24T15:33:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9329568523658177,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.132956852365817
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21015",
      "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
      "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning"
    },
    {
      "arxiv_id": "2602.20739",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.272403+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
          "url": "https://arxiv.org/abs/2602.20739"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
        "url": "https://arxiv.org/abs/2602.20739"
      },
      "published_at": "2026-02-24T10:08:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9121690993479505,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.11216909934795
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20739",
      "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
      "title": "PyVision-RL: Forging Open Agentic Vision Models via RL"
    },
    {
      "arxiv_id": "2602.21203",
      "authors": [
        "Abdulaziz Almuzairee",
        "Henrik I. Christensen"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.394018+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
          "url": "https://arxiv.org/abs/2602.21203"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
        "url": "https://arxiv.org/abs/2602.21203"
      },
      "published_at": "2026-02-24T18:58:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9463433774150375,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146343377415038
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21203",
      "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challeng",
      "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics"
    },
    {
      "arxiv_id": "2602.21202",
      "authors": [
        "Hanxiang Qin",
        "Alexander Martin",
        "Rohan Jha",
        "Chunsheng Zuo",
        "Reno Kriz",
        "Benjamin Van Durme"
      ],
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.032436+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multi-Vector Index Compression in Any Modality",
          "url": "https://arxiv.org/abs/2602.21202"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multi-Vector Index Compression in Any Modality",
        "url": "https://arxiv.org/abs/2602.21202"
      },
      "published_at": "2026-02-24T18:57:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9463017567465847,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.146301756746585
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21202",
      "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce",
      "title": "Multi-Vector Index Compression in Any Modality"
    },
    {
      "arxiv_id": "2602.21189",
      "authors": [
        "Anas Barakat",
        "Souradip Chakraborty",
        "Khushbu Pahwa",
        "Amrit Singh Bedi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261535+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training",
          "url": "https://arxiv.org/abs/2602.21189"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training",
        "url": "https://arxiv.org/abs/2602.21189"
      },
      "published_at": "2026-02-24T18:43:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9453548338217184,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.145354833821719
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21189",
      "summary": "Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practi",
      "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training"
    },
    {
      "arxiv_id": "2602.21186",
      "authors": [
        "Haoyi Jiang",
        "Liu Liu",
        "Xinjie Wang",
        "Yonghao He",
        "Wei Sui",
        "Zhizhong Su",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.020866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
          "url": "https://arxiv.org/abs/2602.21186"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
        "url": "https://arxiv.org/abs/2602.21186"
      },
      "published_at": "2026-02-24T18:37:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9449894547797449,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144989454779745
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21186",
      "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstruc",
      "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning"
    },
    {
      "arxiv_id": "2602.21179",
      "authors": [
        "Nicolás Gaggion",
        "Maria J. Ledesma-Carbayo",
        "Stergios Christodoulidis",
        "Maria Vakalopoulou",
        "Enzo Ferrante"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.021086+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision",
          "url": "https://arxiv.org/abs/2602.21179"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision",
        "url": "https://arxiv.org/abs/2602.21179"
      },
      "published_at": "2026-02-24T18:29:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9444416509849161,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144441650984916
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21179",
      "summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise m",
      "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision"
    },
    {
      "arxiv_id": "2602.21178",
      "authors": [
        "Sepehr Salem Ghahfarokhi",
        "M. Moein Esfahani",
        "Raj Sunderraman",
        "Vince Calhoun",
        "Mohammed Alser"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.261763+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence",
          "url": "https://arxiv.org/abs/2602.21178"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence",
        "url": "https://arxiv.org/abs/2602.21178"
      },
      "published_at": "2026-02-24T18:28:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9443706019129366,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.144370601912938
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21178",
      "summary": "Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, men",
      "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence"
    },
    {
      "arxiv_id": "2602.21175",
      "authors": [
        "Jianglin Lu",
        "Simon Jenni",
        "Kushal Kafle",
        "Jing Shi",
        "Handong Zhao",
        "Yun Fu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.021511+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
          "url": "https://arxiv.org/abs/2602.21175"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
        "url": "https://arxiv.org/abs/2602.21175"
      },
      "published_at": "2026-02-24T18:20:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9438996271144215,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.143899627114422
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21175",
      "summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queri",
      "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models"
    },
    {
      "arxiv_id": "2602.21174",
      "authors": [
        "Victor Reijgwart",
        "Cesar Cadena",
        "Roland Siegwart",
        "Lionel Ott"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262024+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
          "url": "https://arxiv.org/abs/2602.21174"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
        "url": "https://arxiv.org/abs/2602.21174"
      },
      "published_at": "2026-02-24T18:18:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9437456005076483,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.143745600507648
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21174",
      "summary": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the und",
      "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids"
    },
    {
      "arxiv_id": "2602.21172",
      "authors": [
        "Ishaan Rawal",
        "Shubh Gupta",
        "Yihan Hu",
        "Wei Zhan"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262250+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
          "url": "https://arxiv.org/abs/2602.21172"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
        "url": "https://arxiv.org/abs/2602.21172"
      },
      "published_at": "2026-02-24T18:17:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9436636817020483,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.143663681702048
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21172",
      "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no r",
      "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning"
    },
    {
      "arxiv_id": "2602.21168",
      "authors": [
        "Jingya Cheng",
        "Alaleh Azhir",
        "Jiazi Tian",
        "Hossein Estiri"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.395688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma",
          "url": "https://arxiv.org/abs/2602.21168"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma",
        "url": "https://arxiv.org/abs/2602.21168"
      },
      "published_at": "2026-02-24T18:11:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9432727539044313,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.143272753904432
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21168",
      "summary": "Counterfactual inference enables clinicians to ask \"what if\" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time",
      "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma"
    },
    {
      "arxiv_id": "2602.21165",
      "authors": [
        "Samah Fodeh",
        "Linhai Ma",
        "Yan Wang",
        "Srivani Talakokkul",
        "Ganesh Puthiaraju",
        "Afshan Khan",
        "Ashley Hagaman",
        "Sarah Lowe",
        "Aimee Roundtree"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262511+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data",
          "url": "https://arxiv.org/abs/2602.21165"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data",
        "url": "https://arxiv.org/abs/2602.21165"
      },
      "published_at": "2026-02-24T18:10:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9431821429343403,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.143182142934341
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21165",
      "summary": "Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communicati",
      "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data"
    },
    {
      "arxiv_id": "2602.21161",
      "authors": [
        "Guangming Wang",
        "Qizhen Ying",
        "Yixiong Jing",
        "Olaf Wysocki",
        "Brian Sheil"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.629475+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
          "url": "https://arxiv.org/abs/2602.21161"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
        "url": "https://arxiv.org/abs/2602.21161"
      },
      "published_at": "2026-02-24T18:07:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9429922156558186,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142992215655818
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21161",
      "summary": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of l",
      "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking"
    },
    {
      "arxiv_id": "2602.21158",
      "authors": [
        "Dengjia Zhang",
        "Xiaoou Liu",
        "Lu Cheng",
        "Yaqing Wang",
        "Kenton Murray",
        "Hua Wei"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396145+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards",
          "url": "https://arxiv.org/abs/2602.21158"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards",
        "url": "https://arxiv.org/abs/2602.21158"
      },
      "published_at": "2026-02-24T18:04:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9428481584053025,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142848158405302
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21158",
      "summary": "Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolv",
      "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards"
    },
    {
      "arxiv_id": "2602.21157",
      "authors": [
        "Quanxin Shou",
        "Fangqi Zhu",
        "Shawn Chen",
        "Puxin Yan",
        "Zhengyang Yan",
        "Yikun Miao",
        "Xiaoyi Pang",
        "Zicong Hong",
        "Ruikai Shi",
        "Hao Huang",
        "Jie Zhang",
        "Song Guo"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.629687+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
          "url": "https://arxiv.org/abs/2602.21157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
        "url": "https://arxiv.org/abs/2602.21157"
      },
      "published_at": "2026-02-24T18:04:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9428230597721917,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142823059772192
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21157",
      "summary": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, an",
      "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning"
    },
    {
      "arxiv_id": "2602.21154",
      "authors": [
        "Ziwei Niu",
        "Hao Sun",
        "Shujun Bian",
        "Xihong Yang",
        "Lanfen Lin",
        "Yuxin Liu",
        "Yueming Jin"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262757+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning",
          "url": "https://arxiv.org/abs/2602.21154"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning",
        "url": "https://arxiv.org/abs/2602.21154"
      },
      "published_at": "2026-02-24T17:59:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9424848390300858,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.142484839030086
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21154",
      "summary": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2",
      "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning"
    },
    {
      "arxiv_id": "2602.21144",
      "authors": [
        "Anurag Dutt",
        "Nimit Shah",
        "Hazem Masarani",
        "Anshul Gandhi"
      ],
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396382+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
          "url": "https://arxiv.org/abs/2602.21144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism",
        "url": "https://arxiv.org/abs/2602.21144"
      },
      "published_at": "2026-02-24T17:47:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9417357305444977,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.141735730544498
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21144",
      "summary": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large pro",
      "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism"
    },
    {
      "arxiv_id": "2602.21143",
      "authors": [
        "Debjit Paul",
        "Daniel Murphy",
        "Milan Gritta",
        "Ronald Cardenas",
        "Victor Prokhorov",
        "Lena Sophia Bolliger",
        "Aysim Toker",
        "Roy Miles",
        "Andreea-Maria Oncescu",
        "Jasivan Alex Sivakumar",
        "Philipp Borchert",
        "Ismail Elezi",
        "Meiru Zhang",
        "Ka Yiu Lee",
        "Guchun Zhang",
        "Jun Wang",
        "Gerasimos Lampouras"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.262979+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Benchmark for Deep Information Synthesis",
          "url": "https://arxiv.org/abs/2602.21143"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Benchmark for Deep Information Synthesis",
        "url": "https://arxiv.org/abs/2602.21143"
      },
      "published_at": "2026-02-24T17:43:32+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9414502011982385,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14145020119824
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21143",
      "summary": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming probl",
      "title": "A Benchmark for Deep Information Synthesis"
    },
    {
      "arxiv_id": "2602.21142",
      "authors": [
        "Zhifan Jiang",
        "Dong Yang",
        "Vishwesh Nath",
        "Abhijeet Parida",
        "Nishad P. Kulkarni",
        "Ziyue Xu",
        "Daguang Xu",
        "Syed Muhammad Anwar",
        "Holger R. Roth",
        "Marius George Linguraru"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.396809+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
          "url": "https://arxiv.org/abs/2602.21142"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
        "url": "https://arxiv.org/abs/2602.21142"
      },
      "published_at": "2026-02-24T17:42:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9414000790264389,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14140007902644
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21142",
      "summary": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are es",
      "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis"
    },
    {
      "arxiv_id": "2602.21141",
      "authors": [
        "Jose Moises Araya-Martinez",
        "Thushar Tom",
        "Adrián Sanchis Reig",
        "Pablo Rey Valiente",
        "Jens Lambrecht",
        "Jörg Krüger"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.022348+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception",
          "url": "https://arxiv.org/abs/2602.21141"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception",
        "url": "https://arxiv.org/abs/2602.21141"
      },
      "published_at": "2026-02-24T17:42:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9413870041161395,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14138700411614
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21141",
      "summary": "Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. ",
      "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception"
    },
    {
      "arxiv_id": "2602.21138",
      "authors": [
        "Kimon Fountoulakis",
        "David Martínez-Rubio"
      ],
      "categories": [
        "math.OC",
        "cs.DS",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.397046+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank",
          "url": "https://arxiv.org/abs/2602.21138"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank",
        "url": "https://arxiv.org/abs/2602.21138"
      },
      "published_at": "2026-02-24T17:35:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9409425651983103,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14094256519831
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21138",
      "summary": "We study the degree-weighted work required to compute $\\ell_1$-regularized PageRank using the standard one-gradient-per-iteration accelerated proximal-gradient method (FISTA). For non-accelerated local methods, the best known worst-case work scales as $\\widetilde{O} ((αρ)^{-1})$, where $α$ is the teleportation parameter and $ρ$ is the $\\ell_1$-regularization parameter. A natural question is whether FISTA can improve the dependence on $α$ from $1/α$ to $1/\\sqrtα$ while preserving the $1/ρ$ locali",
      "title": "Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank"
    },
    {
      "arxiv_id": "2602.21137",
      "authors": [
        "Joseph Raj Vishal",
        "Nagasiri Poluri",
        "Katha Naik",
        "Rutuja Patil",
        "Kashyap Hegde Kota",
        "Krishna Vinod",
        "Prithvi Jai Ramesh",
        "Mohammad Farhadi",
        "Yezhou Yang",
        "Bharatesh Chakravarthi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.022572+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
          "url": "https://arxiv.org/abs/2602.21137"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
        "url": "https://arxiv.org/abs/2602.21137"
      },
      "published_at": "2026-02-24T17:33:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9407748658443673,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.140774865844367
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21137",
      "summary": "Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation with",
      "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics"
    },
    {
      "arxiv_id": "2602.21136",
      "authors": [
        "David Anugraha",
        "Vishakh Padmakumar",
        "Diyi Yang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263225+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery",
          "url": "https://arxiv.org/abs/2602.21136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery",
        "url": "https://arxiv.org/abs/2602.21136"
      },
      "published_at": "2026-02-24T17:33:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9407639773093955,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.140763977309396
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21136",
      "summary": "Qualitative insights from user experiences are critical for informing product and policy decisions, but collecting such data at scale is constrained by the time and availability of experts to conduct semi-structured interviews. Recent work has explored using large language models (LLMs) to automate interviewing, yet existing systems lack a principled mechanism for balancing systematic coverage of predefined topics with adaptive exploration, or the ability to pursue follow-ups, deep dives, and em",
      "title": "SparkMe: Adaptive Semi-Structured Interviewing for Qualitative Insight Discovery"
    },
    {
      "arxiv_id": "2602.21127",
      "authors": [
        "Xinfeng Li",
        "Shenyu Dai",
        "Kelong Zheng",
        "Yue Xiao",
        "Gelei Deng",
        "Wei Dong",
        "Xiaofeng Wang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CR",
        "cs.SI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263503+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems",
          "url": "https://arxiv.org/abs/2602.21127"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems",
        "url": "https://arxiv.org/abs/2602.21127"
      },
      "published_at": "2026-02-24T17:23:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9401206886553201,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.14012068865532
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21127",
      "summary": "Large language model (LLM) agents are rapidly becoming trusted copilots in high-stakes domains like software development and healthcare. However, this deepening trust introduces a novel attack surface: Agent-Mediated Deception (AMD), where compromised agents are weaponized against their human users. While extensive research focuses on agent-centric threats, human susceptibility to deception by a compromised agent remains unexplored. We present the first large-scale empirical study with 303 parti",
      "title": "\"Are You Sure?\": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems"
    },
    {
      "arxiv_id": "2602.21119",
      "authors": [
        "Rui Zhao",
        "Xihui Li",
        "Yizheng Zhang",
        "Yuzhen Liu",
        "Zhong Zhang",
        "Yufeng Zhang",
        "Cheng Zhou",
        "Zhengyou Zhang",
        "Lei Han"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.263727+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Cooperative-Competitive Team Play of Real-World Craft Robots",
          "url": "https://arxiv.org/abs/2602.21119"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Cooperative-Competitive Team Play of Real-World Craft Robots",
        "url": "https://arxiv.org/abs/2602.21119"
      },
      "published_at": "2026-02-24T17:15:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.939626819818971,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.139626819818972
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21119",
      "summary": "Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement l",
      "title": "Cooperative-Competitive Team Play of Real-World Craft Robots"
    },
    {
      "arxiv_id": "2602.21103",
      "authors": [
        "Sanket Badhe",
        "Deep Shah"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.033853+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning",
          "url": "https://arxiv.org/abs/2602.21103"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning",
        "url": "https://arxiv.org/abs/2602.21103"
      },
      "published_at": "2026-02-24T17:03:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9388267377965165,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138826737796517
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21103",
      "summary": "Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressiv",
      "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning"
    },
    {
      "arxiv_id": "2602.21100",
      "authors": [
        "Noé Artru",
        "Rukhshanda Hussain",
        "Emeline Got",
        "Alexandre Messier",
        "David B. Lindell",
        "Abdallah Dib"
      ],
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.023208+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
          "url": "https://arxiv.org/abs/2602.21100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
        "url": "https://arxiv.org/abs/2602.21100"
      },
      "published_at": "2026-02-24T17:02:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.938750678526221,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138750678526222
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21100",
      "summary": "Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while",
      "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction"
    },
    {
      "arxiv_id": "2602.21099",
      "authors": [
        "Junjie Meng",
        "Ranxu zhang",
        "Wei Wu",
        "Rui Zhang",
        "Chuan Qin",
        "Qi Zhang",
        "Qi Liu",
        "Hui Xiong",
        "Chao Wang"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:37.769127+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering",
          "url": "https://arxiv.org/abs/2602.21099"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering",
        "url": "https://arxiv.org/abs/2602.21099"
      },
      "published_at": "2026-02-24T17:01:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9387246024806531,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138724602480654
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21099",
      "summary": "Large Language Models (LLMs) have shown great potential for enhancing recommender systems through their extensive world knowledge and reasoning capabilities. However, effectively translating these semantic signals into traditional collaborative embeddings remains an open challenge. Existing approaches typically fall into two extremes: direct inference methods are computationally prohibitive for large-scale retrieval, while embedding-based methods primarily focus on unilateral feature augmentatio",
      "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering"
    },
    {
      "arxiv_id": "2602.21098",
      "authors": [
        "Hao Lu",
        "Richard J. Radke"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.023463+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Optimizing Occupancy Sensor Placement in Smart Environments",
          "url": "https://arxiv.org/abs/2602.21098"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Optimizing Occupancy Sensor Placement in Smart Environments",
        "url": "https://arxiv.org/abs/2602.21098"
      },
      "published_at": "2026-02-24T17:01:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9387126512018393,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138712651201839
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21098",
      "summary": "Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on caref",
      "title": "Optimizing Occupancy Sensor Placement in Smart Environments"
    },
    {
      "arxiv_id": "2602.21092",
      "authors": [
        "Floriano Tori",
        "Lorenzo Bini",
        "Marco Sorbi",
        "Stéphane Marchand-Maillet",
        "Vincent Ginis"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264184+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology",
          "url": "https://arxiv.org/abs/2602.21092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology",
        "url": "https://arxiv.org/abs/2602.21092"
      },
      "published_at": "2026-02-24T16:52:36+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9381261390989625,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.138126139098963
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21092",
      "summary": "Curvature notions on graphs provide a theoretical description of graph topology, highlighting bottlenecks and denser connected regions. Artifacts of the message passing paradigm in Graph Neural Networks, such as oversmoothing and oversquashing, have been attributed to these regions. However, it remains unclear how the topology of a graph interacts with the learned preferences of GNNs. Through Massive Activations, which correspond to extreme edge activation values in Graph Transformers, we probe ",
      "title": "Probing Graph Neural Network Activation Patterns Through Graph Topology"
    },
    {
      "arxiv_id": "2602.21082",
      "authors": [
        "Vishal Patil",
        "Shree Vaishnavi Bacha",
        "Revanth Yamani",
        "Yidan Sun",
        "Mayank Kejriwal"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034047+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
          "url": "https://arxiv.org/abs/2602.21082"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
        "url": "https://arxiv.org/abs/2602.21082"
      },
      "published_at": "2026-02-24T16:45:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9376495965467446,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.137649596546744
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21082",
      "summary": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing c",
      "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification"
    },
    {
      "arxiv_id": "2602.21066",
      "authors": [
        "Claire McNamara",
        "Lucy Hederman",
        "Declan O'Sullivan"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264664+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "The Initial Exploration Problem in Knowledge Graph Exploration",
          "url": "https://arxiv.org/abs/2602.21066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "The Initial Exploration Problem in Knowledge Graph Exploration",
        "url": "https://arxiv.org/abs/2602.21066"
      },
      "published_at": "2026-02-24T16:27:40+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9365031965148028,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.136503196514804
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21066",
      "summary": "Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the",
      "title": "The Initial Exploration Problem in Knowledge Graph Exploration"
    },
    {
      "arxiv_id": "2602.21064",
      "authors": [
        "Mehdi Acheli",
        "Walid Gaaloul"
      ],
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.264936+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Motivation is Something You Need",
          "url": "https://arxiv.org/abs/2602.21064"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Motivation is Something You Need",
        "url": "https://arxiv.org/abs/2602.21064"
      },
      "published_at": "2026-02-24T16:26:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9364511700046352,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.136451170004635
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21064",
      "summary": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broade",
      "title": "Motivation is Something You Need"
    },
    {
      "arxiv_id": "2602.21061",
      "authors": [
        "David Koplow",
        "Tomer Galanti",
        "Tomaso Poggio"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.265201+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Tool Building as a Path to \"Superintelligence\"",
          "url": "https://arxiv.org/abs/2602.21061"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Tool Building as a Path to \"Superintelligence\"",
        "url": "https://arxiv.org/abs/2602.21061"
      },
      "published_at": "2026-02-24T16:22:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9361455726222666,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.136145572622267
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21061",
      "summary": "The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the inform",
      "title": "Tool Building as a Path to \"Superintelligence\""
    },
    {
      "arxiv_id": "2602.21059",
      "authors": [
        "Anna Martin-Boyle",
        "William Humphreys",
        "Martha Brown",
        "Cara Leckey",
        "Harmanpreet Kaur"
      ],
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034258+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
          "url": "https://arxiv.org/abs/2602.21059"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
        "url": "https://arxiv.org/abs/2602.21059"
      },
      "published_at": "2026-02-24T16:16:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9357924176583826,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135792417658383
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21059",
      "summary": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the ass",
      "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems"
    },
    {
      "arxiv_id": "2602.21054",
      "authors": [
        "Seongheon Park",
        "Changdae Oh",
        "Hyeong Kyu Choi",
        "Xuefeng Du",
        "Sharon Li"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.265457+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation",
          "url": "https://arxiv.org/abs/2602.21054"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation",
        "url": "https://arxiv.org/abs/2602.21054"
      },
      "published_at": "2026-02-24T16:11:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9354350651920437,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135435065192043
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21054",
      "summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that expl",
      "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation"
    },
    {
      "arxiv_id": "2602.21052",
      "authors": [
        "Timur Nabiev",
        "Evgeny Frolov"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.265728+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations",
          "url": "https://arxiv.org/abs/2602.21052"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations",
        "url": "https://arxiv.org/abs/2602.21052"
      },
      "published_at": "2026-02-24T16:09:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9353408768200427,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135340876820043
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21052",
      "summary": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled wi",
      "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations"
    },
    {
      "arxiv_id": "2602.21046",
      "authors": [
        "Kunyu Zhang",
        "Yanwu Yang",
        "Jing Zhang",
        "Xiangjie Shi",
        "Shujian Yu"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.399196+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
          "url": "https://arxiv.org/abs/2602.21046"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
        "url": "https://arxiv.org/abs/2602.21046"
      },
      "published_at": "2026-02-24T16:04:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9350215730484276,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135021573048428
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21046",
      "summary": "Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating p",
      "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis"
    },
    {
      "arxiv_id": "2602.21045",
      "authors": [
        "Anna Martin-Boyle",
        "Cara A. C. Leckey",
        "Martha C. Brown",
        "Harmanpreet Kaur"
      ],
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034683+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
          "url": "https://arxiv.org/abs/2602.21045"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
        "url": "https://arxiv.org/abs/2602.21045"
      },
      "published_at": "2026-02-24T16:04:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9350194086491432,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.135019408649143
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21045",
      "summary": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source ",
      "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A"
    },
    {
      "arxiv_id": "2602.21044",
      "authors": [
        "Yanrui Wu",
        "Lingling Zhang",
        "Xinyu Zhang",
        "Jiayu Chang",
        "Pengyu Li",
        "Xu Jiang",
        "Jingtao Hu",
        "Jun Liu"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.265947+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification",
          "url": "https://arxiv.org/abs/2602.21044"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification",
        "url": "https://arxiv.org/abs/2602.21044"
      },
      "published_at": "2026-02-24T16:04:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9349934362485214,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.134993436248521
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21044",
      "summary": "Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework ",
      "title": "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification"
    },
    {
      "arxiv_id": "2602.21031",
      "authors": [
        "Hayk Gevorgyan",
        "Konstantinos Kalogeropoulos",
        "Angelos Alexopoulos"
      ],
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.896122+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-stat-ml",
          "tier": 1,
          "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
          "url": "https://arxiv.org/abs/2602.21031"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-stat-ml",
        "tier": 1,
        "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
        "url": "https://arxiv.org/abs/2602.21031"
      },
      "published_at": "2026-02-24T15:52:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9342448755757,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.1342448755757
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21031",
      "summary": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulti",
      "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation"
    },
    {
      "arxiv_id": "2602.21028",
      "authors": [
        "Gayatri Indukumar",
        "Muhammad Awais",
        "Diana Cafiso",
        "Matteo Lo Preti",
        "Lucia Beccai"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.630667+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing",
          "url": "https://arxiv.org/abs/2602.21028"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing",
        "url": "https://arxiv.org/abs/2602.21028"
      },
      "published_at": "2026-02-24T15:51:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9341324269418879,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.134132426941887
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21028",
      "summary": "There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical complianc",
      "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing"
    },
    {
      "arxiv_id": "2602.21013",
      "authors": [
        "Sanjay Haresh",
        "Daniel Dijkman",
        "Apratim Bhattacharyya",
        "Roland Memisevic"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.630866+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
          "url": "https://arxiv.org/abs/2602.21013"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
        "url": "https://arxiv.org/abs/2602.21013"
      },
      "published_at": "2026-02-24T15:30:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.932819726425378,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.132819726425378
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21013",
      "summary": "Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily \"stateless\" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratch",
      "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks"
    },
    {
      "arxiv_id": "2602.21010",
      "authors": [
        "Jiannan Huang",
        "Aditya Kane",
        "Fengzhe Zhou",
        "Yunchao Wei",
        "Humphrey Shi"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025334+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
          "url": "https://arxiv.org/abs/2602.21010"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
        "url": "https://arxiv.org/abs/2602.21010"
      },
      "published_at": "2026-02-24T15:29:55+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9327549495269335,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.132754949526934
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21010",
      "summary": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it ",
      "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design"
    },
    {
      "arxiv_id": "2602.21009",
      "authors": [
        "Kun Yuan",
        "Junyu Bi",
        "Daixuan Cheng",
        "Changfa Wu",
        "Shuwen Xiao",
        "Binbin Cao",
        "Jian Wu",
        "Yuning Jiang"
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.034896+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders",
          "url": "https://arxiv.org/abs/2602.21009"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders",
        "url": "https://arxiv.org/abs/2602.21009"
      },
      "published_at": "2026-02-24T15:28:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9326934156399104,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13269341563991
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21009",
      "summary": "Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we pro",
      "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders"
    },
    {
      "arxiv_id": "2602.20999",
      "authors": [
        "Bowen Zheng",
        "Yongli Xiang",
        "Ziming Hong",
        "Zerong Lin",
        "Chaojian Yu",
        "Tongliang Liu",
        "Xinge You"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025541+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
          "url": "https://arxiv.org/abs/2602.20999"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
        "url": "https://arxiv.org/abs/2602.20999"
      },
      "published_at": "2026-02-24T15:20:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.932113900884854,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.132113900884853
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20999",
      "summary": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction In",
      "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models"
    },
    {
      "arxiv_id": "2602.20995",
      "authors": [
        "Junyu Bi",
        "Xinting Niu",
        "Daixuan Cheng",
        "Kun Yuan",
        "Tao Wang",
        "Binbin Cao",
        "Jian Wu",
        "Yuning Jiang"
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035082+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
          "url": "https://arxiv.org/abs/2602.20995"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
        "url": "https://arxiv.org/abs/2602.20995"
      },
      "published_at": "2026-02-24T15:14:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9317773649655103,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13177736496551
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20995",
      "summary": "Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debi",
      "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs"
    },
    {
      "arxiv_id": "2602.20994",
      "authors": [
        "Yubin Ge",
        "Yongsong Huang",
        "Xiaofeng Liu"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.266424+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures",
          "url": "https://arxiv.org/abs/2602.20994"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures",
        "url": "https://arxiv.org/abs/2602.20994"
      },
      "published_at": "2026-02-24T15:14:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9317288361582003,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.131728836158201
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20994",
      "summary": "Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and pro",
      "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures"
    },
    {
      "arxiv_id": "2602.20989",
      "authors": [
        "Zheng Gu",
        "Min Lu",
        "Zhida Sun",
        "Dani Lischinski",
        "Daniel Cohen-O",
        "Hui Huang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.025931+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
          "url": "https://arxiv.org/abs/2602.20989"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
        "url": "https://arxiv.org/abs/2602.20989"
      },
      "published_at": "2026-02-24T15:10:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9314991674297546,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.131499167429755
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20989",
      "summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on wh",
      "title": "Cycle-Consistent Tuning for Layered Image Decomposition"
    },
    {
      "arxiv_id": "2602.20986",
      "authors": [
        "Thibault Formal",
        "Maxime Louis",
        "Hervé Déjean",
        "Stéphane Clinchant"
      ],
      "categories": [
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:37.769908+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ir",
          "tier": 1,
          "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval",
          "url": "https://arxiv.org/abs/2602.20986"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ir",
        "tier": 1,
        "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval",
        "url": "https://arxiv.org/abs/2602.20986"
      },
      "published_at": "2026-02-24T15:09:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9314021413200116,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.131402141320011
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20986",
      "summary": "This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.\n  We evaluate five progressively enhanced runs, starting",
      "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval"
    },
    {
      "arxiv_id": "2602.20985",
      "authors": [
        "Munish Monga",
        "Vishal Chudasama",
        "Pankaj Wasnik",
        "C. V. Jawahar"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.026127+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer",
          "url": "https://arxiv.org/abs/2602.20985"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer",
        "url": "https://arxiv.org/abs/2602.20985"
      },
      "published_at": "2026-02-24T15:06:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9312113527856924,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.131211352785693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20985",
      "summary": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters ",
      "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer"
    },
    {
      "arxiv_id": "2602.20981",
      "authors": [
        "Christian Simon",
        "MAsato Ishii",
        "Wei-Yao Wang",
        "Koichi Saito",
        "Akio Hayakawa",
        "Dongseok Shim",
        "Zhi Zhong",
        "Shuyang Cui",
        "Shusuke Takahashi",
        "Takashi Shibuya",
        "Yuki Mitsufuji"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.266648+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
          "url": "https://arxiv.org/abs/2602.20981"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
        "url": "https://arxiv.org/abs/2602.20981"
      },
      "published_at": "2026-02-24T15:01:39+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9309257819889061,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130925781988907
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20981",
      "summary": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-au",
      "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models"
    },
    {
      "arxiv_id": "2602.20980",
      "authors": [
        "Yang Zhang",
        "Danyang Li",
        "Yuxuan Li",
        "Xin Zhang",
        "Tianyu Xie",
        "Mingming Cheng",
        "Xiang Li"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.266863+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
          "url": "https://arxiv.org/abs/2602.20980"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
        "url": "https://arxiv.org/abs/2602.20980"
      },
      "published_at": "2026-02-24T15:01:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9309160848958494,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13091608489585
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20980",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent",
      "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs"
    },
    {
      "arxiv_id": "2602.20979",
      "authors": [
        "Mark Marron"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.267106+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Toward an Agentic Infused Software Ecosystem",
          "url": "https://arxiv.org/abs/2602.20979"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Toward an Agentic Infused Software Ecosystem",
        "url": "https://arxiv.org/abs/2602.20979"
      },
      "published_at": "2026-02-24T15:01:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9309150074473006,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130915007447301
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20979",
      "summary": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming langua",
      "title": "Toward an Agentic Infused Software Ecosystem"
    },
    {
      "arxiv_id": "2602.20976",
      "authors": [
        "Xuan Luo",
        "Yubin Chen",
        "Zhiyu Hou",
        "Linpu Yu",
        "Geng Tu",
        "Jing Li",
        "Ruifeng Xu"
      ],
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035480+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Evaluating Proactive Risk Awareness of Large Language Models",
          "url": "https://arxiv.org/abs/2602.20976"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Evaluating Proactive Risk Awareness of Large Language Models",
        "url": "https://arxiv.org/abs/2602.20976"
      },
      "published_at": "2026-02-24T15:00:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9308191195207071,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130819119520707
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20976",
      "summary": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological ",
      "title": "Evaluating Proactive Risk Awareness of Large Language Models"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [
        "Kara Yang"
      ],
      "categories": [
        "Amazon Bedrock",
        "Amazon Rekognition",
        "Intermediate (200)",
        "Technical How-to"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626061+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock",
          "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-photo-search-using-amazon-rekognition-amazon-neptune-and-amazon-bedrock"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock",
        "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-photo-search-using-amazon-rekognition-amazon-neptune-and-amazon-bedrock"
      },
      "published_at": "2026-02-24T18:22:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9439968625238134,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 1.2000000000000002,
        "total_score": 8.643996862523814
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:a722197acaace989",
      "summary": "In this post, we show you how to build a comprehensive photo search system using the AWS Cloud Development Kit (AWS CDK) that integrates Amazon Rekognition for face and object detection, Amazon Neptune for relationship mapping, and Amazon Bedrock for AI-powered captioning.",
      "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock"
    },
    {
      "arxiv_id": "2602.20974",
      "authors": [
        "Ahmed Mohamed Eisa Nasr",
        "Haris Moazam Sheikh"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.400764+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting",
          "url": "https://arxiv.org/abs/2602.20974"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting",
        "url": "https://arxiv.org/abs/2602.20974"
      },
      "published_at": "2026-02-24T14:57:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9306489158466513,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.13064891584665
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20974",
      "summary": "In engineering design and scientific computing, computational cost and predictive accuracy are intrinsically coupled. High-fidelity simulations provide accurate predictions but at substantial computational costs, while lower-fidelity approximations offer efficiency at the expense of accuracy. Multi-fidelity surrogate modelling addresses this trade-off by combining abundant low-fidelity data with sparse high-fidelity observations. However, existing methods suffer from expensive training cost or r",
      "title": "MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting"
    },
    {
      "arxiv_id": "2602.20973",
      "authors": [
        "Yuliang Ji",
        "Fuchen Shen",
        "Jian Wu",
        "Qiujie Xie",
        "Yue Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035664+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
          "url": "https://arxiv.org/abs/2602.20973"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
        "url": "https://arxiv.org/abs/2602.20973"
      },
      "published_at": "2026-02-24T14:53:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9304033603394128,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130403360339413
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20973",
      "summary": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professio",
      "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving"
    },
    {
      "arxiv_id": "2602.20972",
      "authors": [
        "Ming-Kun Xie",
        "Jia-Hao Xiao",
        "Zhiqiang Kou",
        "Zhongnian Li",
        "Gang Niu",
        "Masashi Sugiyama"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.026727+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?",
          "url": "https://arxiv.org/abs/2602.20972"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?",
        "url": "https://arxiv.org/abs/2602.20972"
      },
      "published_at": "2026-02-24T14:53:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9303839771379815,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130383977137981
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20972",
      "summary": "Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manua",
      "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?"
    },
    {
      "arxiv_id": "2602.20971",
      "authors": [
        "Himadri Mandal",
        "Vishnu Varadarajan",
        "Jaee Ponde",
        "Aritra Das",
        "Mihir More",
        "Debayan Gupta"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.267349+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization",
          "url": "https://arxiv.org/abs/2602.20971"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization",
        "url": "https://arxiv.org/abs/2602.20971"
      },
      "published_at": "2026-02-24T14:52:20+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9303236764270074,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.130323676427007
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20971",
      "summary": "Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifi",
      "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization"
    },
    {
      "arxiv_id": "2602.20967",
      "authors": [
        "Haoyang Li",
        "Changsong Liu",
        "Wei Rao",
        "Hao Shi",
        "Sakriani Sakti",
        "Eng Siong Chng"
      ],
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.267575+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR",
          "url": "https://arxiv.org/abs/2602.20967"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR",
        "url": "https://arxiv.org/abs/2602.20967"
      },
      "published_at": "2026-02-24T14:46:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.929972717736439,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12997271773644
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20967",
      "summary": "Automatic speech recognition (ASR) degrades severely in noisy environments. Although speech enhancement (SE) front-ends effectively suppress background noise, they often introduce artifacts that harm recognition. Observation addition (OA) addressed this issue by fusing noisy and SE enhanced speech, improving recognition without modifying the parameters of the SE or ASR models. This paper proposes an intelligibility-guided OA method, where fusion weights are derived from intelligibility estimates",
      "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR"
    },
    {
      "arxiv_id": "2602.20966",
      "authors": [
        "Paola Merlo",
        "Chunyang Jiang",
        "Giuseppe Samo",
        "Vivi Nastase"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:32.035862+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
          "url": "https://arxiv.org/abs/2602.20966"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
        "url": "https://arxiv.org/abs/2602.20966"
      },
      "published_at": "2026-02-24T14:45:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9298586308598742,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.129858630859875
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20966",
      "summary": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core ques",
      "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models"
    },
    {
      "arxiv_id": "2602.20951",
      "authors": [
        "Jaehyun Park",
        "Minyoung Ahn",
        "Minkyu Kim",
        "Jonghyun Lee",
        "Jae-Gil Lee",
        "Dongmin Park"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.268027+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
          "url": "https://arxiv.org/abs/2602.20951"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
        "url": "https://arxiv.org/abs/2602.20951"
      },
      "published_at": "2026-02-24T14:34:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9291539704457897,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12915397044579
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20951",
      "summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach",
      "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis"
    },
    {
      "arxiv_id": "2602.20943",
      "authors": [
        "Kaiyuan Tan",
        "Yingying Shen",
        "Mingfei Tu",
        "Haohui Zhu",
        "Bing Wang",
        "Guang Chen",
        "Hangjun Ye",
        "Haiyang Sun"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.027333+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling",
          "url": "https://arxiv.org/abs/2602.20943"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling",
        "url": "https://arxiv.org/abs/2602.20943"
      },
      "published_at": "2026-02-24T14:24:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9285487120122156,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.128548712012215
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20943",
      "summary": "Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconst",
      "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling"
    },
    {
      "arxiv_id": "2602.20937",
      "authors": [
        "Akshita Gupta",
        "Marieme Ngom",
        "Sam Foreman",
        "Venkatram Vishwanath"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.401606+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers",
          "url": "https://arxiv.org/abs/2602.20937"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers",
        "url": "https://arxiv.org/abs/2602.20937"
      },
      "published_at": "2026-02-24T14:17:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9280985180456948,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.128098518045695
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20937",
      "summary": "Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tune",
      "title": "Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers"
    },
    {
      "arxiv_id": "2602.20934",
      "authors": [
        "ChengYou Li",
        "XiaoDong Liu",
        "XiangBao Meng",
        "XinYu Zhao"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.268764+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence",
          "url": "https://arxiv.org/abs/2602.20934"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence",
        "url": "https://arxiv.org/abs/2602.20934"
      },
      "published_at": "2026-02-24T14:12:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9277441036604414,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.127744103660442
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20934",
      "summary": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured opera",
      "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence"
    },
    {
      "arxiv_id": "2602.20932",
      "authors": [
        "Anupam Sharma",
        "Harish Katti",
        "Prajwal Singh",
        "Shanmuganathan Raman",
        "Krishna Miyapuram"
      ],
      "categories": [
        "cs.LG",
        "cs.HC",
        "eess.SP"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.401832+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels",
          "url": "https://arxiv.org/abs/2602.20932"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels",
        "url": "https://arxiv.org/abs/2602.20932"
      },
      "published_at": "2026-02-24T14:10:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9276367320844185,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.127636732084419
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20932",
      "summary": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations m",
      "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels"
    },
    {
      "arxiv_id": "2602.20930",
      "authors": [
        "Cristian Valero-Abundio",
        "Emilio Sansano-Sansano",
        "Raúl Montoliu",
        "Marina Martínez García"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.027741+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Computing a Characteristic Orientation for Rotation-Independent Image Analysis",
          "url": "https://arxiv.org/abs/2602.20930"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Computing a Characteristic Orientation for Rotation-Independent Image Analysis",
        "url": "https://arxiv.org/abs/2602.20930"
      },
      "published_at": "2026-02-24T14:08:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9274767714875741,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.127476771487574
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20930",
      "summary": "Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a prepr",
      "title": "Computing a Characteristic Orientation for Rotation-Independent Image Analysis"
    },
    {
      "arxiv_id": "2602.20926",
      "authors": [
        "Yuqi Huang",
        "Ning Liao",
        "Kai Yang",
        "Anning Hu",
        "Shengchao Hu",
        "Xiaoxing Wang",
        "Junchi Yan"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.268981+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG",
          "url": "https://arxiv.org/abs/2602.20926"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG",
        "url": "https://arxiv.org/abs/2602.20926"
      },
      "published_at": "2026-02-24T14:05:29+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.927301812628596,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.127301812628597
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20926",
      "summary": "Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic nois",
      "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG"
    },
    {
      "arxiv_id": "2602.20923",
      "authors": [
        "Jiarong Wei",
        "Anna Rehr",
        "Christian Feist",
        "Abhinav Valada"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:35.631673+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models",
          "url": "https://arxiv.org/abs/2602.20923"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models",
        "url": "https://arxiv.org/abs/2602.20923"
      },
      "published_at": "2026-02-24T14:01:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9270485564455054,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.127048556445505
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20923",
      "summary": "Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose Par",
      "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models"
    },
    {
      "arxiv_id": "2602.20918",
      "authors": [
        "Hyewon Jang",
        "Nikolai Ilinykh",
        "Sharid Loáiciga",
        "Jey Han Lau",
        "Shalom Lappin"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:29.269469+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts",
          "url": "https://arxiv.org/abs/2602.20918"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts",
        "url": "https://arxiv.org/abs/2602.20918"
      },
      "published_at": "2026-02-24T13:54:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.926603379629139,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12660337962914
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20918",
      "summary": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However,",
      "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts"
    },
    {
      "arxiv_id": "2602.20913",
      "authors": [
        "Jihao Qiu",
        "Lingxi Xie",
        "Xinyue Huo",
        "Qi Tian",
        "Qixiang Ye"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:33.028144+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
          "url": "https://arxiv.org/abs/2602.20913"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
        "url": "https://arxiv.org/abs/2602.20913"
      },
      "published_at": "2026-02-24T13:49:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9262913470131924,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.126291347013193
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20913",
      "summary": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the age",
      "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding"
    },
    {
      "arxiv_id": "2602.20911",
      "authors": [
        "Ruiqi Liu",
        "Boyu Diao",
        "Hangda Liu",
        "Zhulin An",
        "Fei Wang",
        "Yongjun Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402276+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
          "url": "https://arxiv.org/abs/2602.20911"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
        "url": "https://arxiv.org/abs/2602.20911"
      },
      "published_at": "2026-02-24T13:48:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.926190575427233,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.126190575427234
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20911",
      "summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for bette",
      "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning"
    },
    {
      "arxiv_id": "2602.20904",
      "authors": [
        "Nathan Hu",
        "Jake Ward",
        "Thomas Icard",
        "Christopher Potts"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402520+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Transcoder Adapters for Reasoning-Model Diffing",
          "url": "https://arxiv.org/abs/2602.20904"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Transcoder Adapters for Reasoning-Model Diffing",
        "url": "https://arxiv.org/abs/2602.20904"
      },
      "published_at": "2026-02-24T13:40:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9256922389180404,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.12569223891804
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20904",
      "summary": "While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to ",
      "title": "Transcoder Adapters for Reasoning-Model Diffing"
    },
    {
      "arxiv_id": "2602.20901",
      "authors": [
        "Yuechen Xie",
        "Xiaoyan Zhang",
        "Yicheng Shan",
        "Hao Zhu",
        "Rui Tang",
        "Rong Wei",
        "Mingli Song",
        "Yuanyu Wan",
        "Jie Song"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-25T06:31:30.402771+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
          "url": "https://arxiv.org/abs/2602.20901"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
        "url": "https://arxiv.org/abs/2602.20901"
      },
      "published_at": "2026-02-24T13:38:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9255733208180126,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.125573320818013
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.20901",
      "summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects i",
      "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models"
    }
  ],
  "run_date": "2026-02-24",
  "run_id": "0cf1f9be-77a8-43ed-9f15-f6121356847f",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-24T23:59:59+00:00",
    "items_total": 252,
    "run_id": "0cf1f9be-77a8-43ed-9f15-f6121356847f-2026-02-24",
    "started_at": "2026-02-23T23:59:59+00:00",
    "stories_total": 252,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": "2602.21193",
      "authors": [],
      "categories": [],
      "entities": [
        "nvidia",
        "huggingface",
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:31:32.033083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
          "url": "https://arxiv.org/abs/2602.21193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
        "url": "https://arxiv.org/abs/2602.21193"
      },
      "published_at": "2026-02-24T18:51:04+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 6.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9458757978019748,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 15.145875797801976
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21193",
      "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
      "title": "On Data Engineering for Scaling LLM Terminal Capabilities"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Traci Lim"
      ],
      "categories": [
        "Amazon Bedrock",
        "Artificial Intelligence"
      ],
      "entities": [
        "anthropic",
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.627009+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan",
          "url": "https://aws.amazon.com/blogs/machine-learning/global-cross-region-inference-for-latest-anthropic-claude-opus-sonnet-and-haiku-models-on-amazon-bedrock-in-thailand-malaysia-singapore-indonesia-and-taiwan"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan",
        "url": "https://aws.amazon.com/blogs/machine-learning/global-cross-region-inference-for-latest-anthropic-claude-opus-sonnet-and-haiku-models-on-amazon-bedrock-in-thailand-malaysia-singapore-indonesia-and-taiwan"
      },
      "published_at": "2026-02-24T15:38:22+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9333024559374787,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.43330245593748
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:e6cfe5e86bdcce2c",
      "summary": "In this post, we are exciting to announce availability of Global CRIS for customers in Thailand, Malaysia, Singapore, Indonesia, and Taiwan and give a walkthrough of technical implementation steps, and cover quota management best practices to maximize the value of your AI Inference deployments. We also provide guidance on best practices for production deployments.",
      "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Hossam Basudan"
      ],
      "categories": [
        "Amazon Bedrock",
        "Announcements",
        "Artificial Intelligence"
      ],
      "entities": [
        "anthropic",
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.627235+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)",
          "url": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-global-cross-region-inference-for-anthropics-claude-models-in-the-middle-east-regions"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)",
        "url": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-global-cross-region-inference-for-anthropics-claude-models-in-the-middle-east-regions"
      },
      "published_at": "2026-02-24T15:33:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.933009764613623,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.433009764613622
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:fd7b0ba93aa214c1",
      "summary": "We’re excited to announce the availability of Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 through Amazon Bedrock global cross-Region inference for customers operating in the Middle East. In this post, we guide you through the capabilities of each Anthropic Claude model variant, the key advantages of global cross-Region inference including improved resilience, real-world use cases you can implement, and a code example to help you start building generative AI applications immediately.",
      "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Bruno Pistone"
      ],
      "categories": [
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Foundation models",
        "Generative AI",
        "Technical How-to",
        "AIML",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "AWS Deep Learning",
        "distributed training",
        "Hugging Face"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626522+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs",
          "url": "https://aws.amazon.com/blogs/machine-learning/train-codefu-7b-with-verl-and-ray-on-amazon-sagemaker-training-jobs"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs",
        "url": "https://aws.amazon.com/blogs/machine-learning/train-codefu-7b-with-verl-and-ray-on-amazon-sagemaker-training-jobs"
      },
      "published_at": "2026-02-24T15:46:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9338513645679716,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 11.433851364567971
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:bb52a5bca4fe6d16",
      "summary": "In this post, we demonstrate how to train CodeFu-7B, a specialized 7-billion parameter model for competitive programming, using Group Relative Policy Optimization (GRPO) with veRL, a flexible and efficient training library for large language models (LLMs) that enables straightforward extension of diverse RL algorithms and seamless integration with existing LLM infrastructure, within a distributed Ray cluster managed by SageMaker training jobs. We walk through the complete implementation, covering data preparation, distributed training setup, and comprehensive observability, showcasing how this unified approach delivers both computational scale and developer experience for sophisticated RL training workloads.",
      "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Clement Perrot"
      ],
      "categories": [
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Artificial Intelligence",
        "Customer Solutions"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-25T06:31:27.626780+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Generate structured output from LLMs with Dottxt Outlines in AWS",
          "url": "https://aws.amazon.com/blogs/machine-learning/generate-structured-output-from-llms-with-dottxt-outlines-in-aws"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Generate structured output from LLMs with Dottxt Outlines in AWS",
        "url": "https://aws.amazon.com/blogs/machine-learning/generate-structured-output-from-llms-with-dottxt-outlines-in-aws"
      },
      "published_at": "2026-02-24T15:42:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.9335747088554144,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 11.183574708855414
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:c46101761e991db6",
      "summary": "This post explores the implementation of Dottxt’s Outlines framework as a practical approach to implementing structured outputs using AWS Marketplace in Amazon SageMaker.",
      "title": "Generate structured output from LLMs with Dottxt Outlines in AWS"
    }
  ]
}