{
  "archive_dates": [
    "2026-02-27",
    "2026-02-26",
    "2026-02-25",
    "2026-02-24",
    "2026-02-23"
  ],
  "entity_catalog": {
    "01-ai": {
      "name": "01.AI",
      "type": "organization"
    },
    "andrej-karpathy": {
      "name": "Andrej Karpathy",
      "type": "researcher"
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "organization"
    },
    "aws": {
      "name": "AWS",
      "type": "organization"
    },
    "cohere": {
      "name": "Cohere",
      "type": "organization"
    },
    "deepmind": {
      "name": "DeepMind",
      "type": "organization"
    },
    "deepseek": {
      "name": "DeepSeek",
      "type": "organization"
    },
    "geoffrey-hinton": {
      "name": "Geoffrey Hinton",
      "type": "researcher"
    },
    "google-research": {
      "name": "Google Research",
      "type": "institution"
    },
    "huggingface": {
      "name": "Hugging Face",
      "type": "organization"
    },
    "ilya-sutskever": {
      "name": "Ilya Sutskever",
      "type": "researcher"
    },
    "langchain": {
      "name": "LangChain",
      "type": "organization"
    },
    "llama-cpp": {
      "name": "llama.cpp",
      "type": "organization"
    },
    "meta-ai": {
      "name": "Meta AI",
      "type": "institution"
    },
    "microsoft-research": {
      "name": "Microsoft Research",
      "type": "institution"
    },
    "mistral-ai": {
      "name": "Mistral AI",
      "type": "organization"
    },
    "nvidia": {
      "name": "NVIDIA",
      "type": "organization"
    },
    "ollama": {
      "name": "Ollama",
      "type": "organization"
    },
    "openai": {
      "name": "OpenAI",
      "type": "organization"
    },
    "qwen": {
      "name": "Qwen",
      "type": "organization"
    },
    "stability-ai": {
      "name": "Stability AI",
      "type": "organization"
    },
    "vllm": {
      "name": "vLLM",
      "type": "organization"
    },
    "yann-lecun": {
      "name": "Yann LeCun",
      "type": "researcher"
    },
    "yoshua-bengio": {
      "name": "Yoshua Bengio",
      "type": "researcher"
    }
  },
  "generated_at": "2026-02-27T07:11:06.032487+00:00",
  "model_releases_by_entity": {
    "other": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-25T09:22:32.580536+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 416361,
          "likes": 537
        },
        "hf_model_id": "mistralai/devstral-small-2-24b-instruct-2512",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Devstral-Small-2-24B-Instruct-2512",
            "url": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Devstral-Small-2-24B-Instruct-2512",
          "url": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"
        },
        "published_at": "2026-02-25T08:50:48+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8244195909475535,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.624419590947554
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/devstral-small-2-24b-instruct-2512",
        "summary": "Devstral is an agentic LLM for software engineering tasks. **Devstral Small 2** excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. This model is an Instruct model in **FP8**, fine-tuned to follow instructions, making it ideal for chat, agentic and instruction based tasks for SWE use cases. For enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we invite companies to reach out to us. The Devstral Small 2 Instruct model offers the following capabilities: - **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. - **Lightweight**: with its compact size of...",
        "title": "mistralai/Devstral-Small-2-24B-Instruct-2512"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [],
        "first_seen_at": "2026-02-25T09:22:32.580850+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 14797,
          "likes": 290
        },
        "hf_model_id": "mistralai/devstral-2-123b-instruct-2512",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-mistralai",
            "tier": 1,
            "title": "mistralai/Devstral-2-123B-Instruct-2512",
            "url": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-mistralai",
          "tier": 1,
          "title": "mistralai/Devstral-2-123B-Instruct-2512",
          "url": "https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"
        },
        "published_at": "2026-02-25T08:50:20+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 0.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8243928740789095,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 4.0,
          "total_score": 8.624392874078909
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:mistralai/devstral-2-123b-instruct-2512",
        "summary": "Devstral is an agentic LLM for software engineering tasks. **Devstral 2** excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. This model is an Instruct model in **FP8**, fine-tuned to follow instructions, making it ideal for chat, agentic and instruction based tasks for SWE use cases. For enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we invite companies to reach out to us. The Devstral 2 Instruct model offers the following capabilities: - **Agentic Coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. - **Improved Performance**: Devstral 2 is a step-up...",
        "title": "mistralai/Devstral-2-123B-Instruct-2512"
      }
    ],
    "stability-ai": [
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.445951+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 0,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sdxl-turbo-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sdxl-turbo-amdnpu",
            "url": "https://huggingface.co/stabilityai/sdxl-turbo-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sdxl-turbo-amdnpu",
          "url": "https://huggingface.co/stabilityai/sdxl-turbo-amdnpu"
        },
        "published_at": "2026-02-25T22:40:19+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8733050627742823,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 2.7,
          "total_score": 9.373305062774282
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sdxl-turbo-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: other license_name: stabilityai-ai-community license_link: LICENSE \"SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation\" (stabilityai/sdxl-turbo). This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/sdxl-turbo. | Model Details | Description |",
        "title": "stabilityai/sdxl-turbo-amdnpu"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.446196+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 1,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sd-turbo-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sd-turbo-amdnpu",
            "url": "https://huggingface.co/stabilityai/sd-turbo-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sd-turbo-amdnpu",
          "url": "https://huggingface.co/stabilityai/sd-turbo-amdnpu"
        },
        "published_at": "2026-02-25T22:22:37+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.8722322847478516,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 2.7,
          "total_score": 9.372232284747852
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sd-turbo-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: other license_name: stabilityai-ai-community license_link: LICENSE \"SD-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\" (stabilityai/sd-turbo) This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/sd-turbo. | Model Details | Description |",
        "title": "stabilityai/sd-turbo-amdnpu"
      },
      {
        "arxiv_id": null,
        "authors": [],
        "categories": [],
        "entities": [
          "stability-ai"
        ],
        "first_seen_at": "2026-02-26T06:30:34.445697+00:00",
        "github_release_url": null,
        "hf_metadata": {
          "downloads": 0,
          "likes": 0,
          "pipeline_tag": "text-to-image"
        },
        "hf_model_id": "stabilityai/sdxl-base-amdnpu",
        "item_count": 1,
        "links": [
          {
            "link_type": "huggingface",
            "source_id": "hf-stabilityai",
            "tier": 1,
            "title": "stabilityai/sdxl-base-amdnpu",
            "url": "https://huggingface.co/stabilityai/sdxl-base-amdnpu"
          }
        ],
        "primary_link": {
          "link_type": "huggingface",
          "source_id": "hf-stabilityai",
          "tier": 1,
          "title": "stabilityai/sdxl-base-amdnpu",
          "url": "https://huggingface.co/stabilityai/sdxl-base-amdnpu"
        },
        "published_at": "2026-02-25T22:52:04+00:00",
        "scores": {
          "citation_score": 0.0,
          "cross_source_score": 0.0,
          "entity_score": 2.0,
          "kind_score": 1.8,
          "llm_raw_score": 0.0,
          "llm_relevance_score": 0.0,
          "recency_score": 0.874017946254037,
          "semantic_score": 0.0,
          "tier_score": 2.0,
          "topic_score": 1.2000000000000002,
          "total_score": 7.874017946254037
        },
        "section": null,
        "source_name": null,
        "story_id": "hf:stabilityai/sdxl-base-amdnpu",
        "summary": "language: en - stable-diffusion pipeline_tag: text-to-image license: openrail++ license_link: LICENSE \"SDXL is a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder.\" (stabilityai/stable-diffusion-xl-base-1.0) This version of the model has been optimized to run on AMD NPUs. For more information, refer to the original model card under stabilityai/stable-diffusion-xl-base-1.0. | Model Details | Description | | ----------- | ----------- |",
        "title": "stabilityai/sdxl-base-amdnpu"
      }
    ]
  },
  "papers": [
    {
      "arxiv_id": "2602.22098",
      "authors": [
        "Mariano Barone",
        "Francesco Di Serio",
        "Giuseppe Riccio",
        "Antonio Romano",
        "Marco Postiglione",
        "Antonino Ferraro",
        "Vincenzo Moscato"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-26T06:28:45.371989+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D",
          "url": "https://arxiv.org/abs/2602.22098"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D",
        "url": "https://arxiv.org/abs/2602.22098"
      },
      "published_at": "2026-02-25T16:46:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8521236930715411,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.052123693071541
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22098",
      "summary": "Current medical vision-language models (VLMs) process volumetric brain MRI using 2D slice-based approximations, fragmenting the spatial context required for accurate neuroradiological interpretation. We developed \\textbf{Brain3D}, a staged vision-language framework for automated radiology report generation from 3D brain tumor MRI. Our approach inflates a pretrained 2D medical encoder into a native 3D architecture and progressively aligns it with a causal language model through three stages: cont",
      "title": "Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D"
    },
    {
      "arxiv_id": "2602.22092",
      "authors": [
        "Hexin Dong",
        "Yi Lin",
        "Pengyu Zhou",
        "Fengnian Zhao",
        "Alan Clint Legasto",
        "Mingquan Lin",
        "Hao Chen",
        "Yuzhe Yang",
        "George Shih",
        "Yifan Peng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.372482+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Overview of the CXR-LT 2026 Challenge: Multi-Center Long-Tailed and Zero Shot Chest X-ray Classification",
          "url": "https://arxiv.org/abs/2602.22092"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Overview of the CXR-LT 2026 Challenge: Multi-Center Long-Tailed and Zero Shot Chest X-ray Classification",
        "url": "https://arxiv.org/abs/2602.22092"
      },
      "published_at": "2026-02-25T16:39:21+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8516859086696169,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.051685908669617
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22092",
      "summary": "Chest X-ray (CXR) interpretation is hindered by the long-tailed distribution of pathologies and the open-world nature of clinical environments. Existing benchmarks often rely on closed-set classes from single institutions, failing to capture the prevalence of rare diseases or the appearance of novel findings. To address this, we present the CXR-LT 2026 challenge. This third iteration of the benchmark introduces a multi-center dataset comprising over 145,000 images from PadChest and NIH Chest X-r",
      "title": "Overview of the CXR-LT 2026 Challenge: Multi-Center Long-Tailed and Zero Shot Chest X-ray Classification"
    },
    {
      "arxiv_id": "2602.21693",
      "authors": [
        "Jiafeng Lin",
        "Yuxuan Wang",
        "Huakun Luo",
        "Zhongyi Pei",
        "Jianmin Wang"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [
        "huggingface"
      ],
      "first_seen_at": "2026-02-26T06:28:42.951969+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
          "url": "https://arxiv.org/abs/2602.21693"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
        "url": "https://arxiv.org/abs/2602.21693"
      },
      "published_at": "2026-02-25T08:51:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8244339039119182,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.024433903911918
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21693",
      "summary": "Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and",
      "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts"
    },
    {
      "arxiv_id": "2602.21638",
      "authors": [
        "Anqi Li",
        "Ruihan Wang",
        "Zhaoming Chen",
        "Yuqian Chen",
        "Yu Lu",
        "Yi Zhu",
        "Yuan Xie",
        "Zhenzhong Lan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:43.854284+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
          "url": "https://arxiv.org/abs/2602.21638"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
        "url": "https://arxiv.org/abs/2602.21638"
      },
      "published_at": "2026-02-25T07:05:05+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8183893304228339,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.018389330422835
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21638",
      "summary": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human co",
      "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs"
    },
    {
      "arxiv_id": "2602.21637",
      "authors": [
        "Di Zhang",
        "Zhangpeng Gong",
        "Xiaobo Pang",
        "Jiashuai Liu",
        "Junbo Lu",
        "Hao Cui",
        "Jiusong Ge",
        "Zhi Zeng",
        "Kai Yi",
        "Yinghua Li",
        "Si Liu",
        "Tingsong Yu",
        "Haoran Wang",
        "Mireia Crispin-Ortuzar",
        "eimiao Yu",
        "Chen Li",
        "Zeyu Gao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.386588+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis",
          "url": "https://arxiv.org/abs/2602.21637"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis",
        "url": "https://arxiv.org/abs/2602.21637"
      },
      "published_at": "2026-02-25T07:01:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8182084333327851,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.018208433332786
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21637",
      "summary": "Foundation models have recently achieved impressive success in computational pathology, demonstrating strong generalization across diverse histopathology tasks. However, existing models overlook the heterogeneous and non-uniform organization of pathological regions of interest (ROIs) because they rely on natural image backbones not tailored for tissue morphology. Consequently, they often fail to capture the coherent tissue architecture beyond isolated patches, limiting interpretability and clini",
      "title": "CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis"
    },
    {
      "arxiv_id": "2602.21667",
      "authors": [
        "Sheng Xu",
        "Enshu Wang",
        "Hongfei Xue",
        "Jian Teng",
        "Bingyi Liu",
        "Yi Zhu",
        "Pu Wang",
        "Libing Wu",
        "Chunming Qiao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [
        "01-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.385478+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
          "url": "https://arxiv.org/abs/2602.21667"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
        "url": "https://arxiv.org/abs/2602.21667"
      },
      "published_at": "2026-02-25T08:00:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8215619869125632,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 3.45,
        "total_score": 9.471561986912564
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21667",
      "summary": "Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoi",
      "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception"
    },
    {
      "arxiv_id": "2602.22208",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.369723+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
          "url": "https://arxiv.org/abs/2602.22208"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
        "url": "https://arxiv.org/abs/2602.22208"
      },
      "published_at": "2026-02-25T18:59:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.859986663129532,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.059986663129532
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22208",
      "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.",
      "title": "Solaris: Building a Multiplayer Video World Model in Minecraft"
    },
    {
      "arxiv_id": "2602.22190",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.856821+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
          "url": "https://arxiv.org/abs/2602.22190"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
        "url": "https://arxiv.org/abs/2602.22190"
      },
      "published_at": "2026-02-25T18:34:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8585505710065806,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.058550571006581
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22190",
      "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.",
      "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL"
    },
    {
      "arxiv_id": "2602.22010",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374497+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "World Guidance: World Modeling in Condition Space for Action Generation",
          "url": "https://arxiv.org/abs/2602.22010"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "World Guidance: World Modeling in Condition Space for Action Generation",
        "url": "https://arxiv.org/abs/2602.22010"
      },
      "published_at": "2026-02-25T15:27:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8474263375868485,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.047426337586849
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22010",
      "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
      "title": "World Guidance: World Modeling in Condition Space for Action Generation"
    },
    {
      "arxiv_id": "2602.21818",
      "authors": [],
      "categories": [],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.380577+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
          "url": "https://arxiv.org/abs/2602.21818"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
        "url": "https://arxiv.org/abs/2602.21818"
      },
      "published_at": "2026-02-25T11:47:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8345692502081486,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 9.034569250208149
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21818",
      "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
      "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model"
    },
    {
      "arxiv_id": "2602.22212",
      "authors": [
        "Julian Kaltheuner",
        "Hannah Dr√∂ge",
        "Markus Plack",
        "Patrick Stotko",
        "Reinhard Klein"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.369169+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences",
          "url": "https://arxiv.org/abs/2602.22212"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences",
        "url": "https://arxiv.org/abs/2602.22212"
      },
      "published_at": "2026-02-25T18:59:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8600384231436866,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.060038423143688
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22212",
      "summary": "Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on ",
      "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences"
    },
    {
      "arxiv_id": "2602.22209",
      "authors": [
        "Yufei Ye",
        "Jiaman Li",
        "Ryan Rong",
        "C. Karen Liu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.369494+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos",
          "url": "https://arxiv.org/abs/2602.22209"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos",
        "url": "https://arxiv.org/abs/2602.22209"
      },
      "published_at": "2026-02-25T18:59:10+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.859995621370597,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059995621370597
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22209",
      "summary": "Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand an",
      "title": "WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos"
    },
    {
      "arxiv_id": "2602.22207",
      "authors": [
        "Hanna Yukhymenko",
        "Anton Alexandrov",
        "Martin Vechev"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.856202+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
          "url": "https://arxiv.org/abs/2602.22207"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
        "url": "https://arxiv.org/abs/2602.22207"
      },
      "published_at": "2026-02-25T18:58:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8599508310984074,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059950831098408
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22207",
      "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies,",
      "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets"
    },
    {
      "arxiv_id": "2602.22197",
      "authors": [
        "Xavier Pleimling",
        "Sifat Muhammad Abdullah",
        "Gunjan Balde",
        "Peng Gao",
        "Mainack Mondal",
        "Murtuza Jadliwala",
        "Bimal Viswanath"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.856479+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes",
          "url": "https://arxiv.org/abs/2602.22197"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes",
        "url": "https://arxiv.org/abs/2602.22197"
      },
      "published_at": "2026-02-25T18:46:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.859239476353224,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059239476353223
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22197",
      "summary": "Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as g",
      "title": "Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes"
    },
    {
      "arxiv_id": "2602.22193",
      "authors": [
        "Melody Ma",
        "John Hewitt"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.841548+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
          "url": "https://arxiv.org/abs/2602.22193"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
        "url": "https://arxiv.org/abs/2602.22193"
      },
      "published_at": "2026-02-25T18:43:01+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8590316530521276,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.059031653052127
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22193",
      "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowle",
      "title": "Improving Parametric Knowledge Access in Reasoning Language Models"
    },
    {
      "arxiv_id": "2602.22182",
      "authors": [
        "Sourav Saha",
        "Dwaipayan Roy",
        "Mandar Mitra"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.842011+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "LiCQA : A Lightweight Complex Question Answering System",
          "url": "https://arxiv.org/abs/2602.22182"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "LiCQA : A Lightweight Complex Question Answering System",
        "url": "https://arxiv.org/abs/2602.22182"
      },
      "published_at": "2026-02-25T18:28:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8581740440281864,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.058174044028187
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22182",
      "summary": "Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of trainin",
      "title": "LiCQA : A Lightweight Complex Question Answering System"
    },
    {
      "arxiv_id": "2602.22176",
      "authors": [
        "Eric Zimmermann",
        "Julian Viret",
        "Michal Zelechowski",
        "James Brian Hall",
        "Neil Tenenholtz",
        "Adam Casson",
        "George Shaikovski",
        "Eugene Vorontsov",
        "Siqi Liu",
        "Kristen A Severson"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.370175+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology",
          "url": "https://arxiv.org/abs/2602.22176"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology",
        "url": "https://arxiv.org/abs/2602.22176"
      },
      "published_at": "2026-02-25T18:23:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8578800903136049,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057880090313605
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22176",
      "summary": "In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context ",
      "title": "Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology"
    },
    {
      "arxiv_id": "2602.22175",
      "authors": [
        "Xi Ye",
        "Wuwei Zhang",
        "Fangcong Yin",
        "Howard Yen",
        "Danqi Chen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.842276+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
          "url": "https://arxiv.org/abs/2602.22175"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
        "url": "https://arxiv.org/abs/2602.22175"
      },
      "published_at": "2026-02-25T18:21:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8577539991510066,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.057753999151007
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22175",
      "summary": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads speciali",
      "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs"
    },
    {
      "arxiv_id": "2602.22159",
      "authors": [
        "Wenhao Guo",
        "Zhaoran Zhao",
        "Peng Lu",
        "Sheng Li",
        "Qian Qiao",
        "RuiDe Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.370419+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness",
          "url": "https://arxiv.org/abs/2602.22159"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness",
        "url": "https://arxiv.org/abs/2602.22159"
      },
      "published_at": "2026-02-25T18:05:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8568173352744535,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056817335274454
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22159",
      "summary": "Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiri",
      "title": "CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness"
    },
    {
      "arxiv_id": "2602.22157",
      "authors": [
        "Leon Pielage",
        "Ole H√§tscher",
        "Mitja Back",
        "Bernhard Marschall",
        "Benjamin Risse"
      ],
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.939591+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
          "url": "https://arxiv.org/abs/2602.22157"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
        "url": "https://arxiv.org/abs/2602.22157"
      },
      "published_at": "2026-02-25T18:05:11+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8567776687234421,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056777668723441
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22157",
      "summary": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring th",
      "title": "Dynamic Personality Adaptation in Large Language Models via State Machines"
    },
    {
      "arxiv_id": "2602.22154",
      "authors": [
        "Hossein B. Jond",
        "Veli Bakƒ±rcƒ±oƒülu",
        "Logan E. Beaver",
        "Nejat T√ºkenmez",
        "Adel Akbarimajd",
        "Martin Saska"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.278695+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing",
          "url": "https://arxiv.org/abs/2602.22154"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing",
        "url": "https://arxiv.org/abs/2602.22154"
      },
      "published_at": "2026-02-25T18:01:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.856528803028196,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056528803028197
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22154",
      "summary": "Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent c",
      "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing"
    },
    {
      "arxiv_id": "2602.22146",
      "authors": [
        "Yining Li",
        "Peizhong Ju",
        "Ness Shroff"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.857659+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
          "url": "https://arxiv.org/abs/2602.22146"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
        "url": "https://arxiv.org/abs/2602.22146"
      },
      "published_at": "2026-02-25T17:54:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8561640628863393,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.056164062886339
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22146",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parame",
      "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual"
    },
    {
      "arxiv_id": "2602.22145",
      "authors": [
        "Satyam Kumar Navneet",
        "Joydeep Chandra",
        "Yong Zhang"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.857888+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
          "url": "https://arxiv.org/abs/2602.22145"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
        "url": "https://arxiv.org/abs/2602.22145"
      },
      "published_at": "2026-02-25T17:54:42+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8561541536374012,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0561541536374
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22145",
      "summary": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using tw",
      "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models"
    },
    {
      "arxiv_id": "2602.22143",
      "authors": [
        "Yuetan Chu",
        "Xinhua Ma",
        "Xinran Jin",
        "Gongning Luo",
        "Xin Gao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371050+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining",
          "url": "https://arxiv.org/abs/2602.22143"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining",
        "url": "https://arxiv.org/abs/2602.22143"
      },
      "published_at": "2026-02-25T17:49:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8558182979350646,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055818297935065
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22143",
      "summary": "Medical vision-language pretraining increasingly relies on medical reports as large-scale supervisory signals; however, raw reports often exhibit substantial stylistic heterogeneity, variable length, and a considerable amount of image-irrelevant content. Although text normalization is frequently adopted as a preprocessing step in prior work, its design principles and empirical impact on vision-language pretraining remain insufficiently and systematically examined. In this study, we present MedTr",
      "title": "MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining"
    },
    {
      "arxiv_id": "2602.22142",
      "authors": [
        "Yulin Zhang",
        "Cheng Shi",
        "Sibei Yang"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371331+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
          "url": "https://arxiv.org/abs/2602.22142"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs",
        "url": "https://arxiv.org/abs/2602.22142"
      },
      "published_at": "2026-02-25T17:45:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8556221953793972,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055622195379398
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22142",
      "summary": "Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in stream",
      "title": "WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs"
    },
    {
      "arxiv_id": "2602.22140",
      "authors": [
        "Dhruv Verma",
        "Andrew Qiu",
        "Roberto Rangel",
        "Ayandev Barman",
        "Hao Yang",
        "Chenjia Hu",
        "Fengqi Zhang",
        "Roman Genov",
        "David B. Lindell",
        "Kiriakos N. Kutulakos",
        "Alex Mariakakis"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371537+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels",
          "url": "https://arxiv.org/abs/2602.22140"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels",
        "url": "https://arxiv.org/abs/2602.22140"
      },
      "published_at": "2026-02-25T17:42:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8554429692256583,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.055442969225659
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22140",
      "summary": "We present Lumosaic, a compact active hyperspectral video system designed for real-time capture of dynamic scenes. Our approach combines a narrowband LED array with a coded-exposure-pixel (CEP) camera capable of high-speed, per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength within each video frame. Unlike passive snapshot systems that divide light across multiple spectral channels simultaneously and assume no motion during a frame's exposu",
      "title": "Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels"
    },
    {
      "arxiv_id": "2602.22136",
      "authors": [
        "Qunyou Liu",
        "Pengbo Yu",
        "Marina Zapater",
        "David Atienza"
      ],
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.940060+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
          "url": "https://arxiv.org/abs/2602.22136"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference",
        "url": "https://arxiv.org/abs/2602.22136"
      },
      "published_at": "2026-02-25T17:34:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8549381692515289,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.054938169251528
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22136",
      "summary": "Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bit",
      "title": "SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference"
    },
    {
      "arxiv_id": "2602.22125",
      "authors": [
        "Thanmay Jayakumar",
        "Mohammed Safi Ur Rahman Khan",
        "Raj Dabre",
        "Ratish Puduppully",
        "Anoop Kunchukuttan"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.843129+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
          "url": "https://arxiv.org/abs/2602.22125"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages",
        "url": "https://arxiv.org/abs/2602.22125"
      },
      "published_at": "2026-02-25T17:12:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8536557352943264,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053655735294326
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22125",
      "summary": "Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) ca",
      "title": "IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages"
    },
    {
      "arxiv_id": "2602.22124",
      "authors": [
        "Patrick Tser Jern Kon",
        "Archana Pradeep",
        "Ang Chen",
        "Alexander P. Ellis",
        "Warren Hunt",
        "Zijian Wang",
        "John Yang",
        "Samuel Thompson"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.858412+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "SWE-Prot√©g√©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
          "url": "https://arxiv.org/abs/2602.22124"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "SWE-Prot√©g√©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
        "url": "https://arxiv.org/abs/2602.22124"
      },
      "published_at": "2026-02-25T17:11:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8536083112930444,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053608311293045
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22124",
      "summary": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot√©g√©, a post-training framework that reframes software repair as an expert-prot√©g√© collaboration problem. In SWE-Prot√©g√©, an SLM remains the sole decision-maker while learning to selectively seek guidance fr",
      "title": "SWE-Prot√©g√©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents"
    },
    {
      "arxiv_id": "2602.22122",
      "authors": [
        "Elio Moreau",
        "Florentin Coeurdoux",
        "Gr√©goire Ferre",
        "Eric Vanden-Eijnden"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.940800+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Probing the Geometry of Diffusion Models with the String Method",
          "url": "https://arxiv.org/abs/2602.22122"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Probing the Geometry of Diffusion Models with the String Method",
        "url": "https://arxiv.org/abs/2602.22122"
      },
      "published_at": "2026-02-25T17:10:59+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8535589140932496,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05355891409325
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22122",
      "summary": "Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. Operating on pretrained models w",
      "title": "Probing the Geometry of Diffusion Models with the String Method"
    },
    {
      "arxiv_id": "2602.22120",
      "authors": [
        "Abhipsa Basu",
        "Mohana Singh",
        "Shashank Agnihotri",
        "Margret Keuper",
        "R. Venkatesh Babu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.371779+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
          "url": "https://arxiv.org/abs/2602.22120"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models",
        "url": "https://arxiv.org/abs/2602.22120"
      },
      "published_at": "2026-02-25T17:08:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8534245681713117,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053424568171312
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22120",
      "summary": "Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical ",
      "title": "GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models"
    },
    {
      "arxiv_id": "2602.22118",
      "authors": [
        "Benjamin Bokser",
        "Daniel Gonzalez",
        "Surya Singh",
        "Aaron Preston",
        "Alex Bahner",
        "Annika Wollschl√§ger",
        "Arianna Ilvonen",
        "Asa Eckert-Erdheim",
        "Ashwin Khadke",
        "Bilal Hammoud",
        "Dean Molinaro",
        "Fabian Jenelten",
        "Henry Mayne",
        "Howie Choset",
        "Igor Bogoslavskyi",
        "Itic Tinman",
        "James Tigue",
        "Jan Preisig",
        "Kaiyu Zheng",
        "Kenny Sharma",
        "Kim Ang",
        "Laura Lee",
        "Liana Margolese",
        "Nicole Lin",
        "Oscar Frias",
        "Paul Drews",
        "Ravi Boggavarapu",
        "Rick Burnham",
        "Samuel Zapolsky",
        "Sangbae Kim",
        "Scott Biddlestone",
        "Sean Mayorga",
        "Shamel Fahmi",
        "Tyler McCollum",
        "Velin Dimitrov",
        "William Moyne",
        "Yu-Ming Chen",
        "Farbod Farshidian",
        "Marco Hutter",
        "David Perry",
        "Al Rizzi",
        "Gabe Nelson"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.278958+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot",
          "url": "https://arxiv.org/abs/2602.22118"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot",
        "url": "https://arxiv.org/abs/2602.22118"
      },
      "published_at": "2026-02-25T17:05:09+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8532132137248657,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.053213213724867
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22118",
      "summary": "Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to s",
      "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot"
    },
    {
      "arxiv_id": "2602.22107",
      "authors": [
        "Andrea Apicella",
        "Francesco Isgr√≤",
        "Andrea Pollastro",
        "Roberto Prevete"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.858632+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
          "url": "https://arxiv.org/abs/2602.22107"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
        "url": "https://arxiv.org/abs/2602.22107"
      },
      "published_at": "2026-02-25T16:56:14+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8526850565805069,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052685056580508
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22107",
      "summary": "Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selectio",
      "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection"
    },
    {
      "arxiv_id": "2602.22100",
      "authors": [
        "Andreas Kernbach",
        "Daniel Bargmann",
        "Werner Kraus",
        "Marco F. Huber"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.279219+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study",
          "url": "https://arxiv.org/abs/2602.22100"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study",
        "url": "https://arxiv.org/abs/2602.22100"
      },
      "published_at": "2026-02-25T16:47:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.852146377221781,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.052146377221781
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22100",
      "summary": "Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This wor",
      "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study"
    },
    {
      "arxiv_id": "2602.22094",
      "authors": [
        "Nguyen Cong Nhat Le",
        "John G. Rogers",
        "Claire N. Bonial",
        "Neil T. Dantam"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859097+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
          "url": "https://arxiv.org/abs/2602.22094"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
        "url": "https://arxiv.org/abs/2602.22094"
      },
      "published_at": "2026-02-25T16:39:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8517144958291805,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05171449582918
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22094",
      "summary": "Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and",
      "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning"
    },
    {
      "arxiv_id": "2602.22091",
      "authors": [
        "Matthew Strong",
        "Wei-Jer Chang",
        "Quentin Herau",
        "Jiezhi Yang",
        "Yihan Hu",
        "Chensheng Peng",
        "Wei Zhan"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.372685+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
          "url": "https://arxiv.org/abs/2602.22091"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
        "url": "https://arxiv.org/abs/2602.22091"
      },
      "published_at": "2026-02-25T16:38:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8516583081846241,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051658308184624
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22091",
      "summary": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework",
      "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos"
    },
    {
      "arxiv_id": "2602.22090",
      "authors": [
        "Bo-Wei Chen",
        "Chung-Chi Chen",
        "An-Zi Yen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.843597+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
          "url": "https://arxiv.org/abs/2602.22090"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference",
        "url": "https://arxiv.org/abs/2602.22090"
      },
      "published_at": "2026-02-25T16:38:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8516090238289661,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.051609023828966
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22090",
      "summary": "Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, en",
      "title": "Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference"
    },
    {
      "arxiv_id": "2602.22088",
      "authors": [
        "Hongjie Fang",
        "Shirun Tang",
        "Mingyu Mei",
        "Haoxiang Qin",
        "Zihao He",
        "Jingjing Chen",
        "Ying Feng",
        "Chenxi Wang",
        "Wanxi Liu",
        "Zaixing He",
        "Cewu Lu",
        "Shiquan Wang"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.279438+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation",
          "url": "https://arxiv.org/abs/2602.22088"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation",
        "url": "https://arxiv.org/abs/2602.22088"
      },
      "published_at": "2026-02-25T16:35:24+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8514523185322813,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05145231853228
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22088",
      "summary": "Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itse",
      "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation"
    },
    {
      "arxiv_id": "2602.22076",
      "authors": [
        "Eduardo Miranda"
      ],
      "categories": [
        "cs.SE"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.230938+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Visual Milestone Planning in a Hybrid Development Context",
          "url": "https://arxiv.org/abs/2602.22076"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Visual Milestone Planning in a Hybrid Development Context",
        "url": "https://arxiv.org/abs/2602.22076"
      },
      "published_at": "2026-02-25T16:25:18+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508553287221279,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050855328722129
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22076",
      "summary": "This paper explains the Visual Milestone Planning (VMP) method using an agile vocabulary to facilitate its adoption by agile practitioners as a front end for a hybrid development process. VMP is a visual and collaborative planning approach which promotes a shared understanding of the work approach and commitment through the direct manipulation by team members of the reified planning constructs involved in the development of the plan. Once the product backlog has been established and relevant mil",
      "title": "Visual Milestone Planning in a Hybrid Development Context"
    },
    {
      "arxiv_id": "2602.22073",
      "authors": [
        "Artur Xarles",
        "Sergio Escalera",
        "Thomas B. Moeslund",
        "Albert Clap√©s"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.372887+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting",
          "url": "https://arxiv.org/abs/2602.22073"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting",
        "url": "https://arxiv.org/abs/2602.22073"
      },
      "published_at": "2026-02-25T16:24:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508257856472285,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050825785647229
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22073",
      "summary": "Precise Event Spotting aims to localize fast-paced actions or events in videos with high temporal precision, a key task for applications in sports analytics, robotics, and autonomous systems. Existing methods typically process all frames uniformly, overlooking the inherent spatio-temporal redundancy in video data. This leads to redundant computation on non-informative regions while limiting overall efficiency. To remain tractable, they often spatially downsample inputs, losing fine-grained detai",
      "title": "AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting"
    },
    {
      "arxiv_id": "2602.22072",
      "authors": [
        "Christian Nickel",
        "Laura Schrewe",
        "Florian Mai",
        "Lucie Flek"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859357+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
          "url": "https://arxiv.org/abs/2602.22072"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
        "url": "https://arxiv.org/abs/2602.22072"
      },
      "published_at": "2026-02-25T16:24:35+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8508129839666704,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.05081298396667
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22072",
      "summary": "Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks",
      "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models"
    },
    {
      "arxiv_id": "2602.22070",
      "authors": [
        "Jessica Y. Bo",
        "Lillio Mok",
        "Ashton Anderson"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859598+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
          "url": "https://arxiv.org/abs/2602.22070"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
        "url": "https://arxiv.org/abs/2602.22070"
      },
      "published_at": "2026-02-25T16:18:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8504516621404357,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050451662140436
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22070",
      "summary": "Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdiff",
      "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts"
    },
    {
      "arxiv_id": "2602.22067",
      "authors": [
        "Giuseppe Canonaco",
        "Alberto Pozanco",
        "Daniel Borrajo"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.859845+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Semantic Partial Grounding via LLMs",
          "url": "https://arxiv.org/abs/2602.22067"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Semantic Partial Grounding via LLMs",
        "url": "https://arxiv.org/abs/2602.22067"
      },
      "published_at": "2026-02-25T16:13:26+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.850154449732706,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050154449732705
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22067",
      "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL desc",
      "title": "Semantic Partial Grounding via LLMs"
    },
    {
      "arxiv_id": "2602.22066",
      "authors": [
        "Jinpeng Li",
        "Zhongyi Pei",
        "Huaze Xue",
        "Bojian Zheng",
        "Chen Wang",
        "Jianmin Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.860068+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models",
          "url": "https://arxiv.org/abs/2602.22066"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models",
        "url": "https://arxiv.org/abs/2602.22066"
      },
      "published_at": "2026-02-25T16:13:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8501406741935079,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.050140674193509
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22066",
      "summary": "Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, ",
      "title": "DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models"
    },
    {
      "arxiv_id": "2602.22061",
      "authors": [
        "Quoc Hoan Tran",
        "Koki Chinzei",
        "Yasuhiro Endo",
        "Hirotaka Oshima"
      ],
      "categories": [
        "quant-ph",
        "cs.LG",
        "nlin.CD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.942402+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Learning Quantum Data Distribution via Chaotic Quantum Diffusion Model",
          "url": "https://arxiv.org/abs/2602.22061"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Learning Quantum Data Distribution via Chaotic Quantum Diffusion Model",
        "url": "https://arxiv.org/abs/2602.22061"
      },
      "published_at": "2026-02-25T16:09:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8499419376853856,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049941937685386
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22061",
      "summary": "Generative models for quantum data pose significant challenges but hold immense potential in fields such as chemoinformatics and quantum physics. Quantum denoising diffusion probabilistic models (QuDDPMs) enable efficient learning of quantum data distributions by progressively scrambling and denoising quantum states; however, existing implementations typically rely on circuit-based random unitary dynamics that can be costly to realize and sensitive to control imperfections, particularly on analo",
      "title": "Learning Quantum Data Distribution via Chaotic Quantum Diffusion Model"
    },
    {
      "arxiv_id": "2602.22059",
      "authors": [
        "Dengdi Sun",
        "Xiaoya Zhou",
        "Xiao Wang",
        "Hao Si",
        "Wanli Lyu",
        "Jin Tang",
        "Bin Luo"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.860327+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
          "url": "https://arxiv.org/abs/2602.22059"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
        "url": "https://arxiv.org/abs/2602.22059"
      },
      "published_at": "2026-02-25T16:08:46+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8498789813550797,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04987898135508
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22059",
      "summary": "Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on",
      "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training"
    },
    {
      "arxiv_id": "2602.22056",
      "authors": [
        "Edgar Welte",
        "Yitian Shi",
        "Rosa Wolf",
        "Maximillian Gilles",
        "Rania Rayyes"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.942640+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
          "url": "https://arxiv.org/abs/2602.22056"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
        "url": "https://arxiv.org/abs/2602.22056"
      },
      "published_at": "2026-02-25T16:06:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8497639013684044,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049763901368404
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22056",
      "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorre",
      "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation"
    },
    {
      "arxiv_id": "2602.22055",
      "authors": [
        "Hamza Haruna Mohammed",
        "Dusica Marijan",
        "Arnbj√∏rn Maressa"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.860631+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach",
          "url": "https://arxiv.org/abs/2602.22055"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach",
        "url": "https://arxiv.org/abs/2602.22055"
      },
      "published_at": "2026-02-25T16:06:28+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8497432476356919,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049743247635693
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22055",
      "summary": "Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univar",
      "title": "Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach"
    },
    {
      "arxiv_id": "2602.22049",
      "authors": [
        "Mohamed Amine Kerkouri",
        "Marouane Tliba",
        "Aladine Chetouani",
        "Alessandro Bruno"
      ],
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.373582+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation",
          "url": "https://arxiv.org/abs/2602.22049"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation",
        "url": "https://arxiv.org/abs/2602.22049"
      },
      "published_at": "2026-02-25T15:57:48+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8492319819570342,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.049231981957034
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22049",
      "summary": "Understanding human visual attention is key to preserving cultural heritage We introduce SPGen a novel deep learning model to predict scanpaths the sequence of eye movementswhen viewers observe paintings.\n  Our architecture uses a Fully Convolutional Neural Network FCNN with differentiable fixation selection and learnable Gaussian priors to simulate natural viewing biases To address the domain gap between photographs and artworks we employ unsupervised domain adaptation via a gradient reversal l",
      "title": "SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation"
    },
    {
      "arxiv_id": "2602.22041",
      "authors": [
        "Vassil Guenov",
        "Ashwin George",
        "Arkady Zgonnikov",
        "David A. Abbink",
        "Luciano Cavalcante Siebert"
      ],
      "categories": [
        "cs.MA",
        "cs.CY"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:47.271531+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ma",
          "tier": 1,
          "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions",
          "url": "https://arxiv.org/abs/2602.22041"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ma",
        "tier": 1,
        "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions",
        "url": "https://arxiv.org/abs/2602.22041"
      },
      "published_at": "2026-02-25T15:48:52+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8487053069815232,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048705306981523
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22041",
      "summary": "Heralding the advent of autonomous vehicles and mobile robots that interact with humans, responsibility in spatial interaction is burgeoning as a research topic. Even though metrics of responsibility tailored to spatial interactions have been proposed, they are mostly focused on the responsibility of individual agents. Metrics of causal responsibility focusing on individuals fail in cases of causal overdeterminism -- when many actors simultaneously cause an outcome. To fill the gaps in causal re",
      "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions"
    },
    {
      "arxiv_id": "2602.22039",
      "authors": [
        "Cheng-Yeh Yang",
        "Chien-Chun Wang",
        "Li-Wei Chen",
        "Hung-Shin Lee",
        "Hsin-Min Wang",
        "Berlin Chen"
      ],
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.860861+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
          "url": "https://arxiv.org/abs/2602.22039"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
        "url": "https://arxiv.org/abs/2602.22039"
      },
      "published_at": "2026-02-25T15:47:34+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8486286912108236,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048628691210824
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22039",
      "summary": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recogn",
      "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition"
    },
    {
      "arxiv_id": "2602.22033",
      "authors": [
        "Yanqiu Yu",
        "Zhifan Jin",
        "Sijia Chen",
        "Tongfei Chu",
        "En Yu",
        "Liman Liu",
        "Wenbing Tao"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.373803+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking",
          "url": "https://arxiv.org/abs/2602.22033"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking",
        "url": "https://arxiv.org/abs/2602.22033"
      },
      "published_at": "2026-02-25T15:41:31+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.848272224183493,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.048272224183494
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22033",
      "summary": "Referring Multi-Object Tracking has attracted increasing attention due to its human-friendly interactive characteristics, yet it exhibits limitations in low-visibility conditions, such as nighttime, smoke, and other challenging scenarios. To overcome this limitation, we propose a new RGB-Thermal RMOT task, named RT-RMOT, which aims to fuse RGB appearance features with the illumination robustness of the thermal modality to enable all-day referring multi-object tracking. To promote research on RT-",
      "title": "RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking"
    },
    {
      "arxiv_id": "2602.22026",
      "authors": [
        "Xiaoyu Xian",
        "Shiao Wang",
        "Xiao Wang",
        "Daxin Tian",
        "Yan Tian"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861085+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models",
          "url": "https://arxiv.org/abs/2602.22026"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models",
        "url": "https://arxiv.org/abs/2602.22026"
      },
      "published_at": "2026-02-25T15:34:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8478442688737244,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047844268873725
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22026",
      "summary": "Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption. Specifical",
      "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models"
    },
    {
      "arxiv_id": "2602.22020",
      "authors": [
        "Andr√©s Rodriguez",
        "Juan Cruz Gardey",
        "Alejandra Garrido"
      ],
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:49.231190+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-se",
          "tier": 1,
          "title": "Detecting UX smells in Visual Studio Code using LLMs",
          "url": "https://arxiv.org/abs/2602.22020"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-se",
        "tier": 1,
        "title": "Detecting UX smells in Visual Studio Code using LLMs",
        "url": "https://arxiv.org/abs/2602.22020"
      },
      "published_at": "2026-02-25T15:32:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8477284832345823,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047728483234582
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22020",
      "summary": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concent",
      "title": "Detecting UX smells in Visual Studio Code using LLMs"
    },
    {
      "arxiv_id": "2602.22014",
      "authors": [
        "Louis Est√®ve",
        "Christophe Servan",
        "Thomas Lavergne",
        "Agata Savary"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844505+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
          "url": "https://arxiv.org/abs/2602.22014"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
        "url": "https://arxiv.org/abs/2602.22014"
      },
      "published_at": "2026-02-25T15:29:30+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8475646441423423,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047564644142343
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22014",
      "summary": "Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven samplin",
      "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT"
    },
    {
      "arxiv_id": "2602.22013",
      "authors": [
        "I-Hsiang Chen",
        "Yu-Wei Liu",
        "Tse-Yu Wu",
        "Yu-Chien Chiang",
        "Jen-Chien Yang",
        "Wei-Ting Chen"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374248+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
          "url": "https://arxiv.org/abs/2602.22013"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
        "url": "https://arxiv.org/abs/2602.22013"
      },
      "published_at": "2026-02-25T15:27:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8474734181356065,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.047473418135606
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22013",
      "summary": "Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address",
      "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations"
    },
    {
      "arxiv_id": "2602.22001",
      "authors": [
        "Freek Stulp",
        "Samuel Bustamante",
        "Jo√£o Silv√©rio",
        "Alin Albu-Sch√§ffer",
        "Jeannette Bohg",
        "Shuran Song"
      ],
      "categories": [
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.280283+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
          "url": "https://arxiv.org/abs/2602.22001"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
        "url": "https://arxiv.org/abs/2602.22001"
      },
      "published_at": "2026-02-25T15:19:44+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8469899861705269,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046989986170527
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22001",
      "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond",
      "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?"
    },
    {
      "arxiv_id": "2602.21997",
      "authors": [
        "WeiZhe Xu",
        "Mengyu Liu",
        "Fanxin Kong"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861362+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
          "url": "https://arxiv.org/abs/2602.21997"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
        "url": "https://arxiv.org/abs/2602.21997"
      },
      "published_at": "2026-02-25T15:16:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8468125681952844,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046812568195284
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21997",
      "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LL",
      "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code"
    },
    {
      "arxiv_id": "2602.21995",
      "authors": [
        "Ana Rodrigues",
        "Rui Rego"
      ],
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.943935+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Outpatient Appointment Scheduling Optimization with a Genetic Algorithm Approach",
          "url": "https://arxiv.org/abs/2602.21995"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Outpatient Appointment Scheduling Optimization with a Genetic Algorithm Approach",
        "url": "https://arxiv.org/abs/2602.21995"
      },
      "published_at": "2026-02-25T15:15:57+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8467674844670413,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046767484467042
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21995",
      "summary": "The optimization of complex medical appointment scheduling remains a significant operational challenge in multi-center healthcare environments, where clinical safety protocols and patient logistics must be reconciled. This study proposes and evaluates a Genetic Algorithm (GA) framework designed to automate the scheduling of multiple medical acts while adhering to rigorous inter-procedural incompatibility rules. Using a synthetic dataset encompassing 50 medical acts across four healthcare facilit",
      "title": "Outpatient Appointment Scheduling Optimization with a Genetic Algorithm Approach"
    },
    {
      "arxiv_id": "2602.21992",
      "authors": [
        "Zekai Lin",
        "Xu Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.374735+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
          "url": "https://arxiv.org/abs/2602.21992"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
        "url": "https://arxiv.org/abs/2602.21992"
      },
      "published_at": "2026-02-25T15:12:17+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8465518998243727,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046551899824372
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21992",
      "summary": "360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) ground",
      "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning"
    },
    {
      "arxiv_id": "2602.21987",
      "authors": [
        "Jitindra Fartiyal",
        "Pedro Freire",
        "Sergei K. Turitsyn",
        "Sergei G. Solovski"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861588+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images",
          "url": "https://arxiv.org/abs/2602.21987"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images",
        "url": "https://arxiv.org/abs/2602.21987"
      },
      "published_at": "2026-02-25T15:08:43+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8463422474252643,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.046342247425265
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21987",
      "summary": "Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, li",
      "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images"
    },
    {
      "arxiv_id": "2602.21978",
      "authors": [
        "Miyu Oba",
        "Saku Sugawara"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844747+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
          "url": "https://arxiv.org/abs/2602.21978"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models",
        "url": "https://arxiv.org/abs/2602.21978"
      },
      "published_at": "2026-02-25T14:57:23+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8456764067855236,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045676406785525
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21978",
      "summary": "Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or c",
      "title": "CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models"
    },
    {
      "arxiv_id": "2602.21977",
      "authors": [
        "Liangwei Lyu",
        "Jiaqi Xu",
        "Jianwei Ding",
        "Qiyao Deng"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.375190+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters",
          "url": "https://arxiv.org/abs/2602.21977"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters",
        "url": "https://arxiv.org/abs/2602.21977"
      },
      "published_at": "2026-02-25T14:56:51+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.845645086017142,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045645086017142
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21977",
      "summary": "Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoR",
      "title": "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters"
    },
    {
      "arxiv_id": "2602.21967",
      "authors": [
        "Xiangqi Meng",
        "Pengxu Hou",
        "Zhenjun Zhao",
        "Javier Civera",
        "Daniel Cremers",
        "Hesheng Wang",
        "Haoang Li"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.375426+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments",
          "url": "https://arxiv.org/abs/2602.21967"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments",
        "url": "https://arxiv.org/abs/2602.21967"
      },
      "published_at": "2026-02-25T14:48:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.845173457245894,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045173457245895
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21967",
      "summary": "In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches st",
      "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments"
    },
    {
      "arxiv_id": "2602.21965",
      "authors": [
        "Joseph Margaryan",
        "Thomas Hamelryck"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944174+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Compact Circulant Layers with Spectral Priors",
          "url": "https://arxiv.org/abs/2602.21965"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Compact Circulant Layers with Spectral Priors",
        "url": "https://arxiv.org/abs/2602.21965"
      },
      "published_at": "2026-02-25T14:48:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8451499805314819,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.045149980531482
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21965",
      "summary": "Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simp",
      "title": "Compact Circulant Layers with Spectral Priors"
    },
    {
      "arxiv_id": "2602.21961",
      "authors": [
        "Bendeg√∫z Sulyok",
        "Gergely Palla",
        "Filippo Radicchi",
        "Santo Fortunato"
      ],
      "categories": [
        "cs.LG",
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944417+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
          "url": "https://arxiv.org/abs/2602.21961"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
        "url": "https://arxiv.org/abs/2602.21961"
      },
      "published_at": "2026-02-25T14:44:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8449054701959877,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044905470195989
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21961",
      "summary": "We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of t",
      "title": "Robustness in sparse artificial neural networks trained with adaptive topology"
    },
    {
      "arxiv_id": "2602.21959",
      "authors": [
        "Dusica Marijan",
        "Hamza Haruna Mohammed",
        "Bakht Zaman"
      ],
      "categories": [
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944647+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions",
          "url": "https://arxiv.org/abs/2602.21959"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions",
        "url": "https://arxiv.org/abs/2602.21959"
      },
      "published_at": "2026-02-25T14:41:07+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8447216450244552,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044721645024456
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21959",
      "summary": "To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating",
      "title": "Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions"
    },
    {
      "arxiv_id": "2602.21957",
      "authors": [
        "Yuchun Tu",
        "Zhiwei Li",
        "Bingli Sun",
        "Yixuan Li",
        "Xiao Song"
      ],
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.944877+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation",
          "url": "https://arxiv.org/abs/2602.21957"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation",
        "url": "https://arxiv.org/abs/2602.21957"
      },
      "published_at": "2026-02-25T14:39:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8446434336782749,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044643433678274
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21957",
      "summary": "Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective t",
      "title": "Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation"
    },
    {
      "arxiv_id": "2602.21956",
      "authors": [
        "Junxin Lu",
        "Tengfei Song",
        "Zhanglin Wu",
        "Pengfei Li",
        "Xiaowei Liang",
        "Hui Yang",
        "Kun Chen",
        "Ning Xie",
        "Yunfei Lu",
        "Jing Zhao",
        "Shiliang Sun",
        "Daimeng Wei"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.375872+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation",
          "url": "https://arxiv.org/abs/2602.21956"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation",
        "url": "https://arxiv.org/abs/2602.21956"
      },
      "published_at": "2026-02-25T14:38:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8445847799208818,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044584779920882
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21956",
      "summary": "Text Image Machine Translation (TIMT) aims to translate text embedded in images in the source-language into target-language, requiring synergistic integration of visual perception and linguistic understanding. Existing TIMT methods, whether cascaded pipelines or end-to-end multimodal large language models (MLLMs),struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, resulting in text omission, semantic drift, and contextual inconsis",
      "title": "Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation"
    },
    {
      "arxiv_id": "2602.21952",
      "authors": [
        "Lingjun Zhang",
        "Yujian Yuan",
        "Changjie Wu",
        "Xinyuan Chang",
        "Xin Cai",
        "Shuang Zeng",
        "Linzhe Shi",
        "Sijin Wang",
        "Hang Zhang",
        "Mu Xu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.376079+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
          "url": "https://arxiv.org/abs/2602.21952"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
        "url": "https://arxiv.org/abs/2602.21952"
      },
      "published_at": "2026-02-25T14:34:50+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8443531373954801,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04435313739548
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21952",
      "summary": "Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evoluti",
      "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving"
    }
  ],
  "radar": [
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [
        "Deep",
        "Learning,",
        "Machine",
        "AI,",
        "LLM"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:40.615813+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "sebastian-raschka-blog",
          "tier": 0,
          "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026",
          "url": "https://sebastianraschka.com/blog/2026/a-dream-of-spring-for-open-weight.html"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "sebastian-raschka-blog",
        "tier": 0,
        "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026",
        "url": "https://sebastianraschka.com/blog/2026/a-dream-of-spring-for-open-weight.html"
      },
      "published_at": "2026-02-25T08:15:00+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8223725378973076,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 3.75,
        "total_score": 9.072372537897307
      },
      "section": null,
      "source_name": "Sebastian Raschka Blog",
      "story_id": "fallback:9956cdb45be84036",
      "summary": "A Round Up And Comparison of 10 Open-Weight LLM Releases in Spring 2026",
      "title": "A Dream of Spring for Open-Weight LLMs: 10 Architectures from Jan-Feb 2026"
    },
    {
      "arxiv_id": "2602.21951",
      "authors": [
        "Bo Xue",
        "Yuan Jin",
        "Luoyi Fu",
        "Jiaxin Ding",
        "Xinbing Wang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.844970+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
          "url": "https://arxiv.org/abs/2602.21951"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
        "url": "https://arxiv.org/abs/2602.21951"
      },
      "published_at": "2026-02-25T14:34:02+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8443062301908371,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044306230190838
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21951",
      "summary": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We ",
      "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning"
    },
    {
      "arxiv_id": "2602.21950",
      "authors": [
        "Boqi Chen",
        "Xudong Liu",
        "Jiachuan Peng",
        "Marianne Frey-Marti",
        "Bang Zheng",
        "Kyle Lam",
        "Lin Li",
        "Jianing Qiu"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845205+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models",
          "url": "https://arxiv.org/abs/2602.21950"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2602.21950"
      },
      "published_at": "2026-02-25T14:33:33+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8442778916841647,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044277891684164
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21950",
      "summary": "Multimodal large language models (MLLMs) have shown great potential in medical applications, yet existing benchmarks inadequately capture real-world clinical complexity. We introduce MEDSYN, a multilingual, multimodal benchmark of highly complex clinical cases with up to 7 distinct visual clinical evidence (CE) types per case. Mirroring clinical workflow, we evaluate 18 MLLMs on differential diagnosis (DDx) generation and final diagnosis (FDx) selection. While top models often match or even outp",
      "title": "MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models"
    },
    {
      "arxiv_id": "2602.21947",
      "authors": [
        "Sohan Venkatesh",
        "Ashish Mahendran Kurapath",
        "Tejas Melkote"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845442+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Large Language Models are Algorithmically Blind",
          "url": "https://arxiv.org/abs/2602.21947"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Large Language Models are Algorithmically Blind",
        "url": "https://arxiv.org/abs/2602.21947"
      },
      "published_at": "2026-02-25T14:32:15+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.844201675592638,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.044201675592639
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21947",
      "summary": "Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider ",
      "title": "Large Language Models are Algorithmically Blind"
    },
    {
      "arxiv_id": "2602.21939",
      "authors": [
        "Maxim Chupilkin"
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.861820+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
          "url": "https://arxiv.org/abs/2602.21939"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
        "url": "https://arxiv.org/abs/2602.21939"
      },
      "published_at": "2026-02-25T14:24:47+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8437640548575114,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043764054857512
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21939",
      "summary": "How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper proposes that a list experiment, a simple method widely used in the social sciences, can be applied to study the hidden beliefs of LLMs. List experiments were originally developed to circumvent social des",
      "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments"
    },
    {
      "arxiv_id": "2602.21935",
      "authors": [
        "Mahmut S. Gokmen",
        "Moneera N. Haque",
        "Steve W. Leung",
        "Caroline N. Leach",
        "Seth Parker",
        "Stephen B. Hobbs",
        "Vincent L. Sorrell",
        "W. Brent Seales",
        "V. K. Cody Bumgardner"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862083+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography",
          "url": "https://arxiv.org/abs/2602.21935"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography",
        "url": "https://arxiv.org/abs/2602.21935"
      },
      "published_at": "2026-02-25T14:17:54+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8433608242083293,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04336082420833
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21935",
      "summary": "Coronary artery calcium (CAC) scoring is a key predictor of cardiovascular risk, but it relies on ECG-gated CT scans, restricting its use to specialized cardiac imaging settings. We introduce an automated framework for CAC detection and lesion-specific Agatston scoring that operates across both gated and non-gated CT scans. At its core is CARD-ViT, a self-supervised Vision Transformer trained exclusively on gated CT data using DINO. Without any non-gated training data, our framework achieves 0.7",
      "title": "A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography"
    },
    {
      "arxiv_id": "2602.21933",
      "authors": [
        "Bitan Majumder",
        "Anirban Sen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845688+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
          "url": "https://arxiv.org/abs/2602.21933"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text",
        "url": "https://arxiv.org/abs/2602.21933"
      },
      "published_at": "2026-02-25T14:12:16+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8430309628560386,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.043030962856038
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21933",
      "summary": "Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest ove",
      "title": "Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text"
    },
    {
      "arxiv_id": "2602.21929",
      "authors": [
        "JiaKui Hu",
        "Jialun Liu",
        "Liying Yang",
        "Xinliang Zhang",
        "Kaiwen Li",
        "Shuang Zeng",
        "Yuanwei Li",
        "Haibin Huang",
        "Chi Zhang",
        "Yanye Lu"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.377195+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
          "url": "https://arxiv.org/abs/2602.21929"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
        "url": "https://arxiv.org/abs/2602.21929"
      },
      "published_at": "2026-02-25T14:09:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8428426679431964,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042842667943196
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21929",
      "summary": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an auto",
      "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context"
    },
    {
      "arxiv_id": "2602.21926",
      "authors": [
        "Somyajit Chakraborty",
        "Angshuman Jana",
        "Avijit Gayen"
      ],
      "categories": [
        "cs.SI",
        "cs.DL",
        "cs.LG",
        "physics.soc-ph"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.945580+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Bridging Through Absence: How Comeback Researchers Bridge Knowledge Gaps Through Structural Re-emergence",
          "url": "https://arxiv.org/abs/2602.21926"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Bridging Through Absence: How Comeback Researchers Bridge Knowledge Gaps Through Structural Re-emergence",
        "url": "https://arxiv.org/abs/2602.21926"
      },
      "published_at": "2026-02-25T14:04:03+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8425500650411042,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.042550065041105
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21926",
      "summary": "Understanding the role of researchers who return to academia after prolonged inactivity, termed \"comeback researchers\", is crucial for developing inclusive models of scientific careers. This study investigates the structural and semantic behaviors of comeback researchers, focusing on their role in cross-disciplinary knowledge transfer and network reintegration. Using the AMiner citation dataset, we analyze 113,637 early-career researchers and identify 1,425 comeback cases based on a three-year-o",
      "title": "Bridging Through Absence: How Comeback Researchers Bridge Knowledge Gaps Through Structural Re-emergence"
    },
    {
      "arxiv_id": "2602.21919",
      "authors": [
        "Cuong Anh Pham",
        "Praneeth Vepakomma",
        "Samuel Horv√°th"
      ],
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.945847+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "Learning in the Null Space: Small Singular Values for Continual Learning",
          "url": "https://arxiv.org/abs/2602.21919"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "Learning in the Null Space: Small Singular Values for Continual Learning",
        "url": "https://arxiv.org/abs/2602.21919"
      },
      "published_at": "2026-02-25T13:55:06+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8420265594749906,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04202655947499
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21919",
      "summary": "Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on",
      "title": "Learning in the Null Space: Small Singular Values for Continual Learning"
    },
    {
      "arxiv_id": "2602.21910",
      "authors": [
        "Alexander Heinlein",
        "Johannes Taraz"
      ],
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.946095+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions",
          "url": "https://arxiv.org/abs/2602.21910"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions",
        "url": "https://arxiv.org/abs/2602.21910"
      },
      "published_at": "2026-02-25T13:38:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8410350337205583,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.041035033720558
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21910",
      "summary": "Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for fur",
      "title": "The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions"
    },
    {
      "arxiv_id": "2602.21904",
      "authors": [
        "Mariia Baidachna",
        "James Carty",
        "Aidan Ferguson",
        "Joseph Agrane",
        "Varad Kulkarni",
        "Aubrey Agub",
        "Michael Baxendale",
        "Aaron David",
        "Rachel Horton",
        "Elliott Atkinson"
      ],
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.378313+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing",
          "url": "https://arxiv.org/abs/2602.21904"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing",
        "url": "https://arxiv.org/abs/2602.21904"
      },
      "published_at": "2026-02-25T13:34:56+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8408481578111572,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.040848157811157
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21904",
      "summary": "Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estim",
      "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing"
    },
    {
      "arxiv_id": "2602.21899",
      "authors": [
        "Arnau Romero",
        "Carmen Delgado",
        "Jana Baguer",
        "Ra√∫l Su√°rez",
        "Xavier Costa-P√©rez"
      ],
      "categories": [
        "cs.RO",
        "cs.NI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:48.281336+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ro",
          "tier": 1,
          "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios",
          "url": "https://arxiv.org/abs/2602.21899"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ro",
        "tier": 1,
        "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios",
        "url": "https://arxiv.org/abs/2602.21899"
      },
      "published_at": "2026-02-25T13:29:38+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8405387359210119,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.040538735921013
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21899",
      "summary": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilitie",
      "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios"
    },
    {
      "arxiv_id": "2602.21893",
      "authors": [
        "Yinheng Lin",
        "Yiming Huang",
        "Beilei Cui",
        "Long Bai",
        "Huxin Gao",
        "Hongliang Ren",
        "Jiewen Lai"
      ],
      "categories": [
        "cs.CV"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:45.378522+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cv",
          "tier": 1,
          "title": "EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion",
          "url": "https://arxiv.org/abs/2602.21893"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cv",
        "tier": 1,
        "title": "EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion",
        "url": "https://arxiv.org/abs/2602.21893"
      },
      "published_at": "2026-02-25T13:21:49+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8400825950732793,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.04008259507328
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21893",
      "summary": "Accurate depth estimation plays a critical role in the navigation of endoscopic surgical robots, forming the foundation for 3D reconstruction and safe instrument guidance. Fine-tuning pretrained models heavily relies on endoscopic surgical datasets with precise depth annotations. While existing self-supervised depth estimation techniques eliminate the need for accurate depth annotations, their performance degrades in environments with weak textures and variable lighting, leading to sparse recons",
      "title": "EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion"
    },
    {
      "arxiv_id": "2602.21889",
      "authors": [
        "Otto Nyberg",
        "Fausto Carcassi",
        "Giovanni Cin√†"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862343+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
          "url": "https://arxiv.org/abs/2602.21889"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
        "url": "https://arxiv.org/abs/2602.21889"
      },
      "published_at": "2026-02-25T13:11:12+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8394634568865684,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.039463456886569
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21889",
      "summary": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how",
      "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support"
    },
    {
      "arxiv_id": "2602.21887",
      "authors": [
        "Changjiang Gao",
        "Zixian Huang",
        "Kaichen Yang",
        "Jiajun Chen",
        "Jixing Li",
        "Shujian Huang"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.845911+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
          "url": "https://arxiv.org/abs/2602.21887"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
        "url": "https://arxiv.org/abs/2602.21887"
      },
      "published_at": "2026-02-25T13:10:58+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8394498545796472,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.039449854579647
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21887",
      "summary": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language sel",
      "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection"
    },
    {
      "arxiv_id": "2602.21873",
      "authors": [
        "Shiwei Lu",
        "Yuhang He",
        "Jiashuo Li",
        "Qiang Wang",
        "Yihong Gong"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:42.946526+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-lg",
          "tier": 1,
          "title": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task",
          "url": "https://arxiv.org/abs/2602.21873"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-lg",
        "tier": 1,
        "title": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task",
        "url": "https://arxiv.org/abs/2602.21873"
      },
      "published_at": "2026-02-25T12:57:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8386797407626986,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.0386797407627
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21873",
      "summary": "Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integr",
      "title": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task"
    },
    {
      "arxiv_id": "2602.21864",
      "authors": [
        "Yanbin Wei",
        "Jiangyue Yan",
        "Chun Kang",
        "Yang Chen",
        "Hua Liu",
        "James Kwok",
        "Yu Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862565+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
          "url": "https://arxiv.org/abs/2602.21864"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
        "url": "https://arxiv.org/abs/2602.21864"
      },
      "published_at": "2026-02-25T12:45:45+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8379811321060986,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.037981132106099
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21864",
      "summary": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, re",
      "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs"
    },
    {
      "arxiv_id": "2602.21862",
      "authors": [
        "Chia Cheng Chang",
        "An-Zi Yen",
        "Hen-Hsen Huang",
        "Hsin-Hsi Chen"
      ],
      "categories": [
        "cs.CL"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:43.851484+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-cl",
          "tier": 1,
          "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
          "url": "https://arxiv.org/abs/2602.21862"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-cl",
        "tier": 1,
        "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
        "url": "https://arxiv.org/abs/2602.21862"
      },
      "published_at": "2026-02-25T12:43:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8378453591267819,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.037845359126782
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21862",
      "summary": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large la",
      "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access"
    },
    {
      "arxiv_id": "2602.21858",
      "authors": [
        "Dezhi Kong",
        "Zhengzhao Feng",
        "Qiliang Liang",
        "Hao Wang",
        "Haofei Sun",
        "Changpeng Yang",
        "Yang Li",
        "Peng Zhou",
        "Shuai Nie",
        "Hongzhen Wang",
        "Linfeng Zhou",
        "Hao Jia",
        "Jiaming Xu",
        "Runyu Shi",
        "Ying Huang"
      ],
      "categories": [
        "cs.AI"
      ],
      "entities": [],
      "first_seen_at": "2026-02-26T06:28:41.862801+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "arxiv-cs-ai",
          "tier": 1,
          "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
          "url": "https://arxiv.org/abs/2602.21858"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "arxiv-cs-ai",
        "tier": 1,
        "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
        "url": "https://arxiv.org/abs/2602.21858"
      },
      "published_at": "2026-02-25T12:32:37+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 0.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8372172106925441,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 8.037217210692544
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21858",
      "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complex",
      "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices"
    }
  ],
  "run_date": "2026-02-25",
  "run_id": "12e2480f-d9a3-4496-996d-47b6310dc61e",
  "run_info": {
    "error_summary": null,
    "finished_at": "2026-02-25T23:59:59+00:00",
    "items_total": 227,
    "run_id": "12e2480f-d9a3-4496-996d-47b6310dc61e-2026-02-25",
    "started_at": "2026-02-24T23:59:59+00:00",
    "stories_total": 227,
    "success": true
  },
  "sources_status": [],
  "top5": [
    {
      "arxiv_id": null,
      "authors": [
        "Danielle Robinson"
      ],
      "categories": [
        "Advanced (300)",
        "Amazon Bedrock",
        "Amazon Bedrock AgentCore",
        "Amazon Machine Learning",
        "Amazon SageMaker",
        "Amazon SageMaker AI",
        "Amazon SageMaker Lakehouse",
        "Announcements",
        "Artificial Intelligence"
      ],
      "entities": [
        "aws",
        "vllm"
      ],
      "first_seen_at": "2026-02-26T06:28:40.333502+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
          "url": "https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
        "url": "https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock"
      },
      "published_at": "2026-02-25T20:56:13+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 4.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8670145598671718,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 13.367014559867172
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:92a11eed3231b3dd",
      "summary": "In this post, we explain how we implemented multi-LoRA inference for Mixture of Experts (MoE) models in vLLM, describe the kernel-level optimizations we performed, and show you how you can benefit from this work. We use GPT-OSS 20B as our primary example throughout this post.",
      "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock"
    },
    {
      "arxiv_id": null,
      "authors": [
        "Dani Mitchell"
      ],
      "categories": [
        "Amazon Bedrock",
        "Amazon Bedrock AgentCore",
        "Amazon Bedrock Knowledge Bases",
        "Amazon Machine Learning",
        "Artificial Intelligence"
      ],
      "entities": [
        "aws"
      ],
      "first_seen_at": "2026-02-26T06:28:40.333906+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "blog",
          "source_id": "aws-ml-blog",
          "tier": 0,
          "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
          "url": "https://aws.amazon.com/blogs/machine-learning/building-intelligent-event-agents-using-amazon-bedrock-agentcore-and-amazon-bedrock-knowledge-bases"
        }
      ],
      "primary_link": {
        "link_type": "blog",
        "source_id": "aws-ml-blog",
        "tier": 0,
        "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
        "url": "https://aws.amazon.com/blogs/machine-learning/building-intelligent-event-agents-using-amazon-bedrock-agentcore-and-amazon-bedrock-knowledge-bases"
      },
      "published_at": "2026-02-25T19:51:08+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.5,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8631047771644701,
        "semantic_score": 0.0,
        "tier_score": 3.0,
        "topic_score": 4.0,
        "total_score": 11.36310477716447
      },
      "section": null,
      "source_name": "AWS Machine Learning Blog",
      "story_id": "fallback:6b7719d61556b08b",
      "summary": "This post demonstrates how to quickly deploy a production-ready event assistant using the components of Amazon Bedrock AgentCore. We'll build an intelligent companion that remembers attendee preferences and builds personalized experiences over time, while Amazon Bedrock AgentCore handles the heavy lifting of production deployment:&nbsp;Amazon Bedrock AgentCore Memory for maintaining both conversation context and long-term preferences without custom storage solutions,&nbsp;Amazon Bedrock AgentCore Identity&nbsp;for secure multi-IDP authentication, and&nbsp;Amazon Bedrock AgentCore Runtime&nbsp;for serverless scaling and session isolation. We will also use Amazon Bedrock Knowledge Bases for managed RAG and event data retrieval.",
      "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases"
    },
    {
      "arxiv_id": "2602.22144",
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-26T06:28:41.858128+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
          "url": "https://arxiv.org/abs/2602.22144"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
        "url": "https://arxiv.org/abs/2602.22144"
      },
      "published_at": "2026-02-25T17:50:41+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8559153754233801,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.05591537542338
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.22144",
      "summary": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.",
      "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors"
    },
    {
      "arxiv_id": "2602.21835",
      "authors": [],
      "categories": [],
      "entities": [
        "meta-ai"
      ],
      "first_seen_at": "2026-02-26T06:28:45.379748+00:00",
      "github_release_url": null,
      "hf_metadata": null,
      "hf_model_id": null,
      "item_count": 1,
      "links": [
        {
          "link_type": "arxiv",
          "source_id": "hf-daily-papers",
          "tier": 1,
          "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
          "url": "https://arxiv.org/abs/2602.21835"
        }
      ],
      "primary_link": {
        "link_type": "arxiv",
        "source_id": "hf-daily-papers",
        "tier": 1,
        "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
        "url": "https://arxiv.org/abs/2602.21835"
      },
      "published_at": "2026-02-25T12:08:53+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 1.0,
        "entity_score": 2.0,
        "kind_score": 1.2,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8358384891778947,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 11.035838489177895
      },
      "section": null,
      "source_name": null,
      "story_id": "arxiv:2602.21835",
      "summary": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.",
      "title": "UniVBench: Towards Unified Evaluation for Video Foundation Models"
    },
    {
      "arxiv_id": null,
      "authors": [],
      "categories": [],
      "entities": [
        "qwen"
      ],
      "first_seen_at": "2026-02-25T06:33:17.776387+00:00",
      "github_release_url": null,
      "hf_metadata": {
        "downloads": 0,
        "likes": 184,
        "pipeline_tag": "image-text-to-text"
      },
      "hf_model_id": "qwen/qwen3.5-27b",
      "item_count": 1,
      "links": [
        {
          "link_type": "huggingface",
          "source_id": "hf-qwen",
          "tier": 1,
          "title": "Qwen/Qwen3.5-27B",
          "url": "https://huggingface.co/Qwen/Qwen3.5-27B"
        }
      ],
      "primary_link": {
        "link_type": "huggingface",
        "source_id": "hf-qwen",
        "tier": 1,
        "title": "Qwen/Qwen3.5-27B",
        "url": "https://huggingface.co/Qwen/Qwen3.5-27B"
      },
      "published_at": "2026-02-25T02:43:25+00:00",
      "scores": {
        "citation_score": 0.0,
        "cross_source_score": 0.0,
        "entity_score": 2.0,
        "kind_score": 1.8,
        "llm_raw_score": 0.0,
        "llm_relevance_score": 0.0,
        "recency_score": 0.8036524351622288,
        "semantic_score": 0.0,
        "tier_score": 2.0,
        "topic_score": 4.0,
        "total_score": 10.603652435162228
      },
      "section": null,
      "source_name": null,
      "story_id": "hf:qwen/qwen3.5-27b",
      "summary": "> This repository contains model weights and configuration files for the post-trained model in the Hugging Face Transformers format. > These artifacts are compatible with Hugging Face Transformers, vLLM, SGLang, KTransformers, etc. Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and efficiency. Qwen3.5 features the following enhancement: - **Unified Vision-Language Foundation**: Early fusion training on multimodal tokens achieves cross-generational parity with...",
      "title": "Qwen/Qwen3.5-27B"
    }
  ]
}